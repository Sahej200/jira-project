{"instruction": "Summarize this bug in 1 sentence.", "input": "h3. Description\r\n\r\nCurrently, Kafka fetch requests only support limiting the total size of messages fetched ({{{}fetch.max.bytes{}}}) and the size per partition ({{{}max.partition.fetch.bytes{}}}). However, there is no way to limit the *number of messages* fetched per request—neither globally nor on a per-partition basis.\r\n\r\nWhile Kafka was originally designed as a high-throughput distributed messaging platform and has traditionally focused more on throughput than individual message control, its role has since evolved. Kafka is now not only a leading message queue but also a core component in modern {*}data pipelines and stream processing frameworks{*}.\r\n\r\nIn these newer use cases, especially for downstream services and streaming applications, *rate-limiting by message count* is a common requirement.\r\n\r\nCurrently, the workaround is for clients to {*}fetch a batch of messages, manually truncate them based on count, and then adjust offsets manually{*}, which is inefficient, error-prone, and significantly reduces throughput. In practice, this forces developers to use external tools such as Redis to implement additional buffering or rate control mechanisms—adding complexity and overhead.\r\n\r\nAdding *native support* for a message count limit in fetch requests would offer the following benefits:\r\nh3. Benefits\r\n # {*}Make Kafka a more mature and production-ready stream processing platform{*}, by supporting more granular rate-limiting use cases.\r\n\r\n # *Improve overall system throughput* for consumers that need to limit by message count, by eliminating inefficient post-processing workarounds.\r\n\r\n----\r\nh3. Potential Challenges\r\n # Due to compression and batching, Kafka consumers do not always have direct access to message counts in a fetch response. This means any solution would need to {*}estimate or calculate message counts indirectly{*}—possibly based on batch metadata.\r\n\r\n # Implementation must ensure that {*}Kafka’s high-performance characteristics are preserved{*}. Any s", "output": "Add a Kafka client parameter to limit number of messages fetched"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Relex Py4J requirement to 0.10.9.7+\nDescription: JVM are compatible with  0.10.9.7+ and above versions have some correctness fixes", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Protect `ExecutorPodsAllocator.numOutstandingPods` as `protected val`\nDescription: \nQ: Issue resolved by pull request 52614\n[https://github.com/apache/spark/pull/52614]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [ReadAheadV2] Refactor ReadBufferManager to isolate new code with the current working code\nDescription: Read Buffer Manager used today was introduced way back and has been stable for quite a while.\r\nRead Buffer Manager to be introduced as part of https://issues.apache.org/jira/browse/HADOOP-19596 will introduce many changes incrementally over time. While the development goes on and we are able to fully stabilise the optimized version we need the current flow to be functional and undisturbed. \r\n\r\nThis work item is to isolate that from new code by refactoring ReadBufferManager class to have 2 different implementations with same public interfaces: ReadBufferManagerV1 and ReadBufferManagerV2.\r\n\r\nThis will also introduce new configs that can be used to toggle between new and old code.\nQ: anujmodi2021 commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3072020510\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 818, Failures: 0, Errors: 0, Skipped: 165\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 821, Failures: 0, Errors: 0, Skipped: 117\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 660, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 818, Failures: 0, Errors: 0, Skipped: 176\r\n   [WARNING] Tests run: 133, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 664, Failures: 0, Errors: 0, Skipped: 134\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 657, Failures: 0, Errors: 0, Skipped: 225\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 661, Failures: 0, Errors: 0, Skipped: 147\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 659, Failures: 0, Errors: 0, Skipped: 165\r\n   [WARNING] Tests run: 133, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 692, Failures: 0, Errors: 0, Skipped: 172\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 657, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24", "output": "bhattmanish98 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2212899426\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -773,9 +788,14 @@ byte[] getBuffer() {\n     return buffer;\n   }\n \n+  /**\n+   * Checks if any version of read ahead is enabled.\n+   * If both are disabled, then skip read ahead logic.\n+   * @return true if read ahead is enabled, false otherwise.\n+   */\n   @VisibleForTesting\n   public boolean isReadAheadEnabled() {\n-    return readAheadEnabled;\n+    return (readAheadEnabled || readAheadV2Enabled) && readBufferManager != null;\n\nReview Comment:\n   The method name suggests we're only checking readAhead, but we're also checking readBufferManager here. Should we either move this check outside the method or update the method name to reflect its functionality?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -259,10 +259,15 @@ public final class ConfigurationKeys {\n   public static final String AZURE_KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT = \"fs.azure.shellkeyprovider.script\";\n \n   /**\n-   * Enable or disable readahead buffer in AbfsInputStream.\n+   * Enable or disable readahead V1 in AbfsInputStream.\n    * Value: {@value}.\n    */\n   public static final String FS_AZURE_ENABLE_READAHEAD = \"fs.azure.enable.readahead\";\n+  /**\n+   * Enable or disable readahead V2 in AbfsInputStream. This will work independent of V1.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_ENABLE_READAHEAD_V2 = \"fs.azure.enable.readahead.v2\";\n\nReview Comment:\n   If both V1 and V2 are enabled, does V2 take precedence over V1? If so, would it be inaccurate to refer to them as independent?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: TestGridmixSubmission failures\nDescription: This is on trunk, with Java 8:\r\n\r\n{noformat}\r\n[ERROR] Failures: \r\n[ERROR]   TestGridmixSubmission.testReplaySubmit:165->CommonJobTest.doSubmission:369 expected:  but was: \r\n[ERROR]   TestGridmixSubmission.testStressSubmit:175->CommonJobTest.doSubmission:369 expected:  but was: \r\n{noformat}\r\n\r\nThese are probably just bad assumptions on the length of the output.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "from: https://github.com/apache/kafka/pull/3705#discussion_r140830112", "output": "Move Gauge#value to MetricValueProvider"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update commons-lang3 to 3.17.0\nDescription: The commons-text version used by Hadoop is incompatible with commons-lang3 3.12.\r\n\r\nThis is actually with an older Hadoop, but with the same {_}commons-lang3{_}, _commons-configuration2_ and _commons-text_ versions as trunk:\r\n{noformat}\r\njava.lang.NoSuchMethodError: 'org.apache.commons.lang3.Range org.apache.commons.lang3.Range.of(java.lang.Comparable, java.lang.Comparable)'\r\n    at org.apache.commons.text.translate.NumericEntityEscaper.(NumericEntityEscaper.java:97)\r\n    at org.apache.commons.text.translate.NumericEntityEscaper.between(NumericEntityEscaper.java:59)\r\n    at org.apache.commons.text.StringEscapeUtils.(StringEscapeUtils.java:271)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.unescapePropertyName(PropertiesConfiguration.java:690)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.initPropertyName(PropertiesConfiguration.java:583)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$Properties\nQ: stoty opened a new pull request, #7591:\nURL: https://github.com/apache/hadoop/pull/7591\n\n   ### Description of PR\r\n   \r\n   Update commons-lang3 to 3.17.0\r\n   The commons-text version used by Hadoop is incompatible with commons-langs3 3.12.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   CI\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7591:\nURL: https://github.com/apache/hadoop/pull/7591#issuecomment-2788690570\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  4s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  1s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  76m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 32s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 17s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 117m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7591/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7591 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 03fdc7c4e0a5 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f57909957f7d4aaf34a779eb6c7bae373523a36f |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7591/1/testReport/ |\r\n   | Max. process+thread count | 544 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7591/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0\nDescription: *  kafka server version is 2.5.1\r\n *  kafka-client version bigger than 3.1.1 \r\n\r\n \r\n{code:java}\r\nimport org.apache.kafka.clients.producer.KafkaProducer;\r\nimport org.apache.kafka.clients.producer.Producer;\r\nimport org.apache.kafka.clients.producer.ProducerRecord;\r\n\r\nimport java.util.Properties;\r\n\r\n\r\npublic class KafkaSendTest {\r\n\r\n  public static void main(String[] args) {\r\n    Properties props = new Properties();\r\n    props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\");\r\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"transaction.timeout.ms\", \"300000\");\r\n    props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully\r\n    props.put(\"compression.type\", \"lz4\");\r\n    props.put(\"security.protocol\", \"SASL_PLAINTEXT\");\r\n    props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\");\r\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"xxx\\\" password=\\\"xxxx\\\";\");\r\n\r\n    Producer producer = new KafkaProducer<>(props);\r\n\r\n    try {\r\n      String topic = \"topic1\";\r\n      byte[] value = new byte[]{1,2}; // example\r\n      ProducerRecord record = new ProducerRecord<>(topic, null, value);\r\n      producer.send(record, (metadata, exception) -> {\r\n        if (exception == null) {\r\n          System.out.printf(\"Sent record(key=%s value=%s) meta(partition=%d, offset=%d)\\n\",\r\n     ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M artifact, with unknown content inside\r\n{code}\r\n 92K    share/hadoop/common/lib/hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n912K    share/hadoop/common/lib/hadoop-aws-3.5.0-20250916.124028-686.jar\r\n808K    share/hadoop/common/lib/hadoop-azure-3.5.0-20250916.124028-685.jar\r\n 36K    share/hadoop/common/lib/hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n 72K    share/hadoop/common/lib/hadoop-cos-3.5.0-20250916.124028-683.jar\r\n136K    share/hadoop/common/lib/hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n140K    share/hadoop/common/lib/hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n3.8M    share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar\r\n{code}\r\n\r\nOne thing it includes is yet-another mozilla/public-suffix-list.txt. These are a recurrent PITA and I don't want\r\nto find them surfacing again.\r\n{code}\r\n\r\n15. Required Resources\r\n======================\r\n\r\nresource: mozilla/public-suffix-list.txt\r\n       jar:file:/Users/stevel/Projects/Releases/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar!/mozilla/public-suffix-list.txt\r\n{code}\r\n\r\nPlan\r\n* Move the shade stage behind a profile; off for ASF releases. Exclude mozilla/public-suffix-list.txt  \r\n* Explicitly declare and manage httpclient5 dependency\r\n* hadoop-cloud-storage pom to include hadoop-tos but not dependencies in build, unless asked.\r\n* LICENSE-binary to declare optional redist of ve-tos-java-sdk-hadoop and its license.", "output": "volcano tos: disable shading when -DskipShade is set on a build"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Create python3 symlink needed for mvnsite\nDescription: The mvnsite compilation step needs python3. Although we're installing python3 in the build environment, the python3 executable is missing.\r\nThus, we need to create a symbolic link python3 pointing to python.exe needed for mvnsite.\r\n\r\nFailure -\r\n{code}\r\n[INFO] -------------------------------------\r\n[INFO] Building Apache Hadoop Common 3.5.0-SNAPSHOT                    [11/115]\r\n[INFO]   from hadoop-common-project\\hadoop-common\\pom.xml\r\n[INFO] --------------------------------[ jar ]---------------------------------\r\n[INFO] \r\n[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ hadoop-common ---\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\target\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\src\\site\\markdown (includes = [UnixShellAPI.md], excludes = [])\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\src\\site\\resources (includes = [configuration.xsl, core-default.xml], excludes = [])\r\n[INFO] \r\n[INFO] --- exec-maven-plugin:1.3.1:exec (shelldocs) @ hadoop-common ---\r\n/usr/bin/env: 'python3': No such file or directory\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Reactor Summary for Apache Hadoop Main 3.5.0-SNAPSHOT:\r\n[INFO] \r\n[INFO] Apache Hadoop Main ................................. SUCCESS [21:20 min]\r\n[INFO] Apache Hadoop Build Tools .......................... SUCCESS [  0.017 s]\r\n[INFO] Apache Hadoop Project POM .......................... SUCCESS [  2.107 s]\r\n[INFO] Apache Had", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Acked record on new topic not immediately visible to consumer\nDescription: h2. Steps to reproduce\r\n * program uses a single broker (we see the issue with an in-JVM embedded kafka server)\r\n * create a new topic with 1 partition\r\n * produce a record with {{acks=all}}\r\n * await acknowledgement from the broker\r\n * start a consumer (configured to read from beginning of topic)\r\n * spuriously, _the consumer never sees the record_\r\n\r\nThe problem seems to occur more on a very busy server (e.g. a build server running many tests in parallel). We have not seen it on a modern laptop.\r\n\r\nA delay of 10ms after receiving the acknowledgement, and before starting the consumer, makes the record always visible.\r\n\r\nWe observe this problem in ~1 in 6 runs of [zio-kafka|https://github.com/zio/zio-kafka]'s test suite, when run from a GitHub action. Since we run the test-suite for 3 different JVM versions, more than half of the zio-kafka builds fails due to this issue.\r\nh2. Expected behavior\r\n\r\nA record, produced with {{{}acks=all{}}}, of which producing was acknowledged, should be immediately visible for all consumers.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager\nDescription: We have observed this across multiple workload runs that when we start reading data from input stream. The first read which came to input stream has to be read synchronously even if we trigger prefetch request for that particular offset. Most of the times we end up doing extra work of checking if the prefetch is trigerred, removing prefetch from the pending queue and go ahead to do a direct remote read in workload thread itself.\r\n\r\nTo avoid all this overhead, we will always bypass read ahead for the very first read of each input stream and trigger read aheads for second read onwards.\nQ: are you using the openFile seek policy as suggested? parquet will tell you when its a parquet file and its read policy is common: 8 byte footer, reall footer, rowgroups.", "output": "Thanks for the feedback steve. We will definitely incorporate that.\r\nI will hold onto this PR and will make this change with the Read Policy suggested by user taken into consideration.\r\n\r\nWill work diligently on all the read policies and have reads happening in way optimal for each one of them."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SparkConnectClient defines every error class\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use Java-friendly `KubernetesClientUtils` APIs\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Migrate from Java to Jakarta - Upgrade to Jersey 3.x, Jetty 12.x\nDescription: This Jira is to track the migration of Javax to Jakarta in Hadoop and identify all the transitive dependencies which need to be migrated as well. Currently, I see the following major upgrades required:\r\n # Jersey: 2.46 to 3.x\r\n # Jetty: 9.x to 12.x\nQ: Jersey 3.x uses the Jakarta namespace, which introduces some compatibility considerations with Jetty versions:\r\n * Jetty 9.x is EOL and must be upgraded.\r\n\r\n * Jetty 12 EE8 supports the legacy {{javax}} namespace.\r\n\r\n * Jetty 12 EE10 supports the newer {{jakarta}} namespace.\r\n\r\nFrom my initial work on the patch, here's how Jersey versions map to Jetty:\r\n * Jersey 3.0.x requires Jetty 11, which has also reached EOL and is no longer community-supported.\r\n\r\n * Jersey 3.1.4+ requires Jetty 12 EE10, aligning with the Jakarta namespace and offering long-term support.\r\n\r\nReference: [Jersey Migration Guide|https://eclipse-ee4j.github.io/jersey.github.io/documentation/latest31x/migration.html#mig-3.0.0]\r\nRelated prior work: [Hadoop PR #7130|https://github.com/apache/hadoop/pull/7130]\r\n\r\nGiven that both Jetty 9 and 11 are EOL, and that Jersey 3.1.4+ requires EE10, the cleaner long-term approach would be to migrate directly to Jersey 3.1.4+ with Jetty 12 EE10 (requires JDK 17).\r\n\r\nWould like to confirm if this is the right direction before proceeding further. Kindly provide your inputs on the same [~stevel@apache.org], [~fanningpj]", "output": "[~palsai] I already created HADOOP-19395 earlier this year,\r\nbut as I've mentioned, it needs to wait for Java 17 as the baseline."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h3. {color:#172b4d}I have already set [auto.leader.rebalance.enable|https://kafka.apache.org/documentation/#brokerconfigs_auto.leader.rebalance.enable] to true, but the leader of the __consumer_offsets topic is not balanced at all, remaining on only one node or two nodes.{color}\r\nh3. {color:#172b4d}Only this internal topic is unbalanced, resulting in an uneven number of connections and uneven CPU pressure.{color}\r\nh3. {color:#172b4d}Except for this topic{color}{color:#172b4d}__consumer_offsets{color}{color:#172b4d}, other topics are normal, the leader is balanced to three nodes.{color}\r\n\r\n \r\n\r\n*3.5.0 version,The leader is only on nodes 1 and 0 and will not be balanced to node 2. :*\r\nTopic: __consumer_offsets TopicId: 480x8Ow1Qwyb-YvuWWqwVQ PartitionCount: 50 ReplicationFactor: 3 Configs: compression.type=producer,deletion.filterDeadGroup.enable=true,min.insync.replicas=1,cleanup.policy=compact,segment.bytes=104857600,max.message.bytes=15728658,index.interval.bytes=4096,unclean.leader.election.enable=false,retention.bytes=1073741824\r\nTopic: __consumer_offsetsPartition: 0Leader: 0Replicas: 0,1,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 1Leader: 1Replicas: 1,0,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 2Leader: 0Replicas: 0,1,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 3Leader: 1Replicas: 1,0,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 4Leader: 1Replicas: 1,0,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 5Leader: 0Replicas: 0,1,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 6Leader: 0Replicas: 0,1,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 7Leader: 1Replicas: 1,0,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 8Leader: 1Replicas: 1,0,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 9Leader: 0Replicas: 0,1,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 10Leader: 1Replicas: 1,0,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 11Leader: 0Replicas: 0,1,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 12Leader: 0Replicas: 0,1,2Isr: 0,2,1\r", "output": "leader of the __consumer_offsets topic is not balanced"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Maven site task fails with Java 17\nDescription: If I try to build site with Java 17, I get an IllegalArgumentException from analyze-report\r\n{code}\r\n$ mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site\r\n...\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report: IllegalArgumentException -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:347)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.bui", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In Spark Connect environment, QueryExecution#observedMetrics can be called by two threads concurrently.\r\n\r\n \r\n\r\nThread1(ObservationManager)\r\n{code:java}\r\nprivate def tryComplete(qe: QueryExecution): Unit = {\r\n  val allMetrics = qe.observedMetrics\r\n  qe.logical.foreach {\r\n    case c: CollectMetrics =>\r\n      allMetrics.get(c.name).foreach { metrics =>\r\n        val observation = observations.remove((c.name, c.dataframeId))\r\n        if (observation != null) {\r\n          observation.setMetricsAndNotify(metrics)\r\n        }\r\n      }\r\n    case _ =>\r\n  }\r\n}\r\n{code}\r\nThread2(SparkConnectPlanExecution)\r\n{code:java}\r\nprivate def createObservedMetricsResponse(\r\n    sessionId: String,\r\n    observationAndPlanIds: Map[String, Long],\r\n    dataframe: DataFrame): Option[ExecutePlanResponse] = {\r\n  val observedMetrics = dataframe.queryExecution.observedMetrics.collect {\r\n    case (name, row) if !executeHolder.observations.contains(name) =>\r\n      val values = SparkConnectPlanExecution.toObservedMetricsValues(row)\r\n      name -> values\r\n  } {code}\r\n\r\nThis can cause race condition issues. We can see CI failure caused by this issue.\r\nhttps://github.com/apache/spark/actions/runs/18422173471/job/52497913985\r\n\r\n{code}\r\n======================================================================\r\nERROR [0.181s]: test_observe_with_map_type (pyspark.sql.tests.connect.test_parity_observation.DataFrameObservationParityTests.test_observe_with_map_type)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/testing/utils.py\", line 228, in wrapper\r\n    lastValue = condition(*args, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_observation.py\", line 226, in test_observe_with_map_type\r\n    assertDataFrameEqual(df, [Row(id=id) for id in range(10)])\r\n  File \"/__w/spark/spark/python/pyspark/testing/utils.py\", line 1098, in assertDataFrameEqual\r\n    actual", "output": "Fix race condition issue related to ObservedMetrics"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Intermittent test failures when using chained emit strategy on window close\nDescription: Hi,\r\n\r\nI have a test case that contains a topology with 2 time windows and chained emitStrategy calls. The standalone reproduction use case is attached to this issue\r\nThe problem is that about 25% of the time the test fails. About 75% of the time the test succeeds. I followed the conclusions of \r\n!https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype|width=16,height=16! KAFKA-19810\r\nto make sure that I am sending events at the correct times (at least I think my calculations are correct). I really need to get to the bottom of why the test fails intermittently, as the test somewhat reflects a real life topology of a project I am working on, and we cannot have an intermittently failing test in our build pipeline. \r\nGreg\nQ: Most of the time I run the test, I see this:\r\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.880 s – in com.k8sflowprocessor.ChainedEmitStrategyTopologyTest3\r\n[INFO] \r\n[INFO] Results:\r\n[INFO] \r\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0\r\n[INFO] \r\n\r\nHowever, like I mentioned, every once in a while I see this:\r\n\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.058 s  but was:\r\n    at org.junit.Assert.fail(Assert.java:89)\r\n    at org.junit.Assert.failNotEquals(Assert.java:835)\r\n    at org.junit.Assert.assertEquals(Assert.java:647)\r\n    at org.junit.Assert.assertEquals(Assert.java:633)\r\n    at com.k8sflowprocessor.ChainedEmitStrategyTopologyTest3.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopologyTest3.java:116)\r\n\r\nWhat that means, most of the time a record passes from input to output topic in the time expected, but every once in a while it does not.", "output": "the goal, of course, is to have unit test that is reliable, i.e. doesn't fail intermittently. I need to understand whether the issue is in my test code (attached) or elsewhere in kafka test framework."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove unused commons-collections 3.x\nDescription: \nQ: Issue resolved by pull request 52743\n[https://github.com/apache/spark/pull/52743]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Revise execute methods of SparkConnectStatement\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce Geography and Geometry physical types\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix S3A failing to initialize S3 buckets having namespace with dot followed by number\nDescription: S3A fails to initialize when S3 bucket namespace is having dot followed by a number. \r\n\r\n{*}Specific Problem{*}: URI parsing fails when S3 bucket names contain a dot followed by a number (like {{{}bucket-v1.1-us-east-1{}}}). Java's\r\nURI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null.\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\nhadoop dfs -ls s3a://bucket-v1.1-us-east-1/\r\n\r\nWARNING: Use of this script to execute dfs is deprecated.\r\nWARNING: Attempting to execute replacement \"hdfs dfs\" instead.\r\n\r\n2025-09-08 06:13:06,670 WARN fs.FileSystem: Failed to initialize filesystem s3://bucket-v1.1-us-east-1/: java.lang.IllegalArgumentException: bucket is null/empty\r\n-ls: bucket is null/empty{code}\r\n \r\n\r\n{*}Please Note{*}: Although there has been discussion on not allowing S3 buckets with such a namespace ([https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/]) , Amazon S3 still allows you to create a bucket with \nQ: shameersss1 opened a new pull request, #7942:\nURL: https://github.com/apache/hadoop/pull/7942\n\n   ### Description of PR\r\n   \r\n   S3A fails to initialize when S3 bucket namespace is having dot followed by a number. \r\n   \r\n   Specific Problem: URI parsing fails when S3 bucket names contain a dot followed by a number (like bucket-v1.1-us-east-1). Java's\r\n   URI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Tested in us-east-1 with bucket having namespace with dot followed by a number.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "shameersss1 commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3267057573\n\n   Test `ITestBucketTool,ITestS3ACommitterMRJob` are failing even without the change."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: spark-network-common no longer shades all of Guava\nDescription: Shading of Guava in `spark-network-common` has changed from 3.x to 4.x such that conflicting classes prevent downstream code from using Guava. \r\n\r\nIn spark jars prior to 4.x, `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class` was not a class included in the jar. Post 4.x, Guava is still shaded to `sparkproject` but `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class`, used in Guava's `InternetDomainName` and elsewhere, is now present in the jar. \r\n{code:java}\r\n$ jar tf spark-network-common_2.13-4.0.0.jar | rg PublicSuffixPatterns \r\ncom/google/thirdparty/publicsuffix/PublicSuffixPatterns.class {code}\r\nIf a library depends on spark but also depends on Guava, calls to `InternetDomainName` that call `PublicSuffixPatterns` will fail with `Exception in thread \"main\" java.lang.NoSuchFieldError: EXACT`. \r\n\r\nInspecting the code locations in such a library via `classOf[InternetDomainName].getProtectionDomain.getCodeSource.getLocation` and `classOf[PublicSuffixPatterns].getProtectionDomain.getCodeSource.getLocation` reveals that `InternetDomainName` is sourced from Guava, `target/bg-jobs/sbt_2062a9c3/target/5fcb43b5/1685140132000/guava-32.0.0-jre.jar`, while `PublicSuffixPatterns` is sourced instead from spark jar, `target/bg-jobs/sbt_2062a9c3/target/db746978/1747651686000/spark-network-common_2.13-4.0.0.jar`.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix file permission errors as per the platform\nDescription: The file permission denial error message in Linux systems end with \"(Permission denied)\" particularly.\r\nHowever, an error message in the same scenario on Windows ends with \"(Access is denied)\" error.\r\n\r\nThis results in a bug in *org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker* and also leads to a unit test failure *org.apache.hadoop.fs.TestFsShellCopy#testPutSrcFileNoPerm*.\r\n\r\nThus, we need to make the appropriate check in accordance with the platform.\nQ: GauthamBanasandra commented on PR #7767:\nURL: https://github.com/apache/hadoop/pull/7767#issuecomment-3082098761\n\n   Yes @slfan1989 . It's ready. But since this PR changes a source file, I'm trying to get the Yetus to run locally on this one, so that we run all the tests.\n   \n   The Jenkins build agents for Windows are failing due to low disk space. Hence I'm working on getting Yetus to run locally.\n   \n   I should be able to merge this by next week.", "output": "github-actions[bot] commented on PR #7767:\nURL: https://github.com/apache/hadoop/pull/7767#issuecomment-3447890882\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove `SPARK_JENKINS` and related logics from `dev/run-tests.py`\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The test for it fails in CI on Centos 7, likely because it has different default charsets than the other CI environments.\r\n{noformat}\r\n[ERROR] org.apache.hadoop.tools.TestHadoopArchiveLogs.testGenerateScript  Time elapsed: 0.036 s  <<< ERROR!\r\njava.lang.IllegalStateException: Mismatched Charset(UTF-8) and CharsetEncoder(US-ASCII)\r\n\tat org.apache.commons.io.output.FileWriterWithEncoding$Builder.get(FileWriterWithEncoding.java:113)\r\n\tat org.apache.hadoop.tools.HadoopArchiveLogs.generateScript(HadoopArchiveLogs.java:512)\r\n\tat org.apache.hadoop.tools.TestHadoopArchiveLogs._testGenerateScript(TestHadoopArchiveLogs.java:280)\r\n\tat org.apache.hadoop.tools.TestHadoopArchiveLogs.testGenerateScript(TestHadoopArchiveLogs.java:248)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.lang.Thread.run(Thread.java:750){noformat}", "output": "Set charsetEncoder in HadoopArchiveLogs"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support TIME in the make_timestamp function in Python\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "running 3.4.2 rc1 on older JDK 8 runtimes can fail\r\n{code}\r\n\r\n\r\n[INFO] -------------------------------------------------------\r\n[INFO] Running org.apache.parquet.hadoop.TestParquetReader\r\nException in thread \"Thread-8\" java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;\r\n        at org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler.completed(RawLocalFileSystem.java:428)\r\n        at org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler.completed(RawLocalFileSystem.java:362)\r\n        at sun.nio.ch.Invoker.invokeUnchecked(Invoker.java:126)\r\n        at sun.nio.ch.SimpleAsynchronousFileChannelImpl$2.run(SimpleAsynchronousFileChannelImpl.java:335)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:750)\r\n{code}\r\n\r\n\r\nthis can be fixed trivially, casting to java.io.Buffer first: ((Byffer)buff).flip())", "output": "JDK8: RawLocalFileSystem calls ByteBuffer.flip"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Set Curator Connection Timeout\nDescription: Curator 5.2.0 has a default \"connection timeout\" of 15s:\r\n\r\n[https://github.com/apache/curator/blob/apache-curator-5.2.0/curator-framework/src/main/java/org/apache/curator/framework/CuratorFrameworkFactory.java#L63]\r\n\r\nAnd it will throw a warning if the Zookeeper session timeout is less than the Curator connection timeout:\r\n\r\n[https://github.com/apache/curator/blob/apache-curator-5.2.0/curator-client/src/main/java/org/apache/curator/CuratorZookeeperClient.java#L117-L120]\r\n\r\nThe Hadoop default for ZK timeout is set to 10s:\r\n\r\n[https://github.com/apache/hadoop/blob/0dd9bf8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeys.java#L414-L416]\r\n\r\nWhich means without setting the \"connection timeout\" a default installation will keep warning about the connection timeout for Curator being lower than the requested session timeout. This sets both timeout values to override the Curator default.\r\n\r\nAnother option is to change the default Hadoop ZK timeout from 10s to 15s.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In [https://kafka.apache.org/protocol#protocol_types,] we didn't specify the encoding of a struct. In particular, how a nullable struct is represented. We should document that if a struct is nullable, the first byte indicates whether is null and the rest of the bytes are the serialization of each field.", "output": "document the encoding of nullable struct"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove `ignore.symbol.file` Javac option\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Spark Connect sessions leak pyspark UDF daemon processes and threads\nDescription: Each Spark Connect session that uses Python UDFs seems to leak one PySpark `daemon` process. Over time, these can accumulate in the 100s until the corresponding node or container goes OOM (although it can take a while as each process doesn't use a lot of memory and some of those pages are shared).\r\n{code:java}\r\nspark        263  0.0  0.0 121424 59504 ?        S    05:00   0:01  \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark       1515  0.0  0.0 121324 60148 ?        S    05:04   0:01  \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark       1525  0.0  0.0 121324 60400 ?        S    05:04   0:01  \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark       1568  0.0  0.0 121324 60280 ?        S    05:04   0:01  \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon{code}\r\nIn addition there are also threads leaking - e.g., here is a thread dump histogram from a sample executor with 200+ leaked daemon processes:\r\n{code:java}\r\n# These threads seem to be leaking\r\n226 threads   Idle Worker Monitor for /opt/spark/.venv/bin/python3\r\n226 threads   process reaper\r\n226 threads   stderr reader for /opt/spark/.venv/bin/python3\r\n226 threads   stdout reader for /opt/spark/.venv/bin/python3\r\n250 threads   Worker Monitor for /opt/spark/.venv/bin/python3\r\n\r\n# These threads seem fine, Spark is configured with 24 cores/executor\r\n21 threads    stdout writer for /opt/spark/.venv/bin/python3\r\n21 threads    Writer Monitor for /opt/spark/.venv/bin/python3{code}\r\nWith Spark Connect, each ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Fix issue about 'Apache Project Website Checks' shows at https://whimsy.apache.org/site/project/hadoop", "output": "Comply with ASF website policy "}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Bump Avro 1.12.1\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is on trunk, with Java 8:\r\n\r\n{noformat}\r\n[ERROR] Failures: \r\n[ERROR]   TestGridmixSubmission.testReplaySubmit:165->CommonJobTest.doSubmission:369 expected:  but was: \r\n[ERROR]   TestGridmixSubmission.testStressSubmit:175->CommonJobTest.doSubmission:369 expected:  but was: \r\n{noformat}\r\n\r\nThese are probably just bad assumptions on the length of the output.", "output": "TestGridmixSubmission failures"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce Geography and Geometry physical types\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52629.", "output": "Issue resolved by pull request 52629\n[https://github.com/apache/spark/pull/52629]"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [Gradle 8 --> 9 upgrade] GitHub Actions build issues: JUnit tests parsing fails (for a new/flaky steps)\nDescription: (!) {*}Note{*}: this blocks Gradle upgrade from 8 to 9 (KAFKA-19174) - _Gradle upgrade related stuff are done, but can't be merged due to CI build issues._\r\n\r\nSee here for more details: [https://github.com/apache/kafka/pull/19513#issuecomment-3236158518]\r\n\r\nInterestingly enough: only parsing fails (not tests) and on top of that it happens for flaky/new matrix {*}_only_{*}.\r\n_______________________________________________________________________________________________________________________________\r\n\r\n -{*}(i) Update{*}: after so many testing on Gradle upgrade PR ([https://github.com/apache/kafka/pull/19513]) I can confirm that something strange is going on between GitHub Actions and Develocity instances.-\r\n\r\n-Let me describe just a tip of the issues here; will use 'com.gradle.develocity' plugin version as an example (and please keep in mind that no classes/test are added or changed):-\r\n * Gradle 9/PR branch (with plugin version 3.19, i.e. trunk version): *parsing step fails for JUnit\nQ: It turns out that scan publishing issues are gone now (after Develocity instance [https://develocity.apache.org|https://develocity.apache.org/] was upgraded from 2025.2.1 to 2025.2.4), see comment here: https://github.com/apache/kafka/pull/19513#issuecomment-3274834504\r\n\r\nAs for issues with JUnit parsing (https://github.com/apache/kafka/pull/19513#issuecomment-3282145505) commit is pushed (solution is similar/related to KAFKA-16801).", "output": "Separate related issue is created here: KAFKA-19707"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Support for S3express Access Points\nDescription: * Currently, the endpoint for using S3 accesspoint is resolved for S3 and S3 on outposts: https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/ArnResource.java#L29-L30. However, for s3express, the endpoint is \"s3express-..amazonaws.com\". (https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-az-networking.html). This ticket adds support for s3express endpoint.\r\n\r\n* Additionally, for access objects using s3express in the `S3AFileSystem`, we need to parse the access point ARN for an S3express access point into the access point name, since ARNs aren't supported on S3express other than in IAM policies. (https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-directory-buckets-restrictions-limitations-naming-rules.html). This ticket also addresses this change.\r\n\r\n* This ticket also updates the SDK version used by the changes to `2.31.12`, which is the version (at minimum) needed to use s3express access points\nQ: Bucket names will be s3express patterns, yes, but access points with the s3express pattern will only be recognized by SDK version 2.31.12 or above. Buckets should work with the current SDK version but access points will not.\r\nhttps://github.com/aws/aws-sdk-java-v2/blob/master/CHANGELOG.md#23112-2025-03-31", "output": "[~stevel@apache.org] couple of the S3 Express AWS teams need an upgraded SDK. Now that the third party issue is fixed, we should be good to move to a 2.30.x release. I had advised [~revathivijay] to just cut the PR with the version bump, the SDK upgrade will still need to happen separately but don't think it's possible for someone completely new to the project to get that right. I can do it once 3.4.2 release wraps up."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Support for S3express Access Points\nDescription: * Currently, the endpoint for using S3 accesspoint is resolved for S3 and S3 on outposts: https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/ArnResource.java#L29-L30. However, for s3express, the endpoint is \"s3express-..amazonaws.com\". (https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-az-networking.html). This ticket adds support for s3express endpoint.\r\n\r\n* Additionally, for access objects using s3express in the `S3AFileSystem`, we need to parse the access point ARN for an S3express access point into the access point name, since ARNs aren't supported on S3express other than in IAM policies. (https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-directory-buckets-restrictions-limitations-naming-rules.html). This ticket also addresses this change.\r\n\r\n* This ticket also updates the SDK version used by the changes to `2.31.12`, which is the version (at minimum) needed to use s3express access points\nQ: * SDK update is a separate process, you will need to do all of it, including that third party testing. \r\nhttps://github.com/steveloughran/engineering-proposals/blob/trunk/qualifying-an-SDK-upgrade.md\r\n\r\nNew code must go into org.apache.hadoop.fs.s3a.impl.S3ExpressStorage\r\n\r\nWe have some logic which recognises when a store is s3express and therefore that list calls will return prefixes of all incomplete uploads, and exports this information through filesystem capability probes. This will need to be the same.\r\n\r\nPresumably bucket names will be s3express patterns, right? in which case that recognition should still work.", "output": "Bucket names will be s3express patterns, yes, but access points with the s3express pattern will only be recognized by SDK version 2.31.12 or above. Buckets should work with the current SDK version but access points will not.\r\nhttps://github.com/aws/aws-sdk-java-v2/blob/master/CHANGELOG.md#23112-2025-03-31"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: REBOOTSTRAP_REQUIRED leave connections to group coordinator untouched\nDescription: KIP-1102 has introduced a new error code to instruct clients to rebootstrap. This error has been introduced to help proxies to instruct the clients to reboostrap when brokers have changed.\r\n\r\nHowever, the current implementation in group consumers leave the connections to the group coordinator intact.\r\n\r\nAs a result, when doing a cluster failover:\r\n - the clients receive REBOOTSTRAP_REQUIRED\r\n - the broker connections are reboostrapped, connections to the primary cluster are closed and connections to the secondary clusters are opened\r\n\r\n - the consumers are still connected to the group coordinator of the primary cluster\r\n\r\n==> the consumers are:\r\n - fetching data from the secondary cluster\r\n - sending hearbeats to the primary cluster\r\n\r\nThe consumers are in an inconsistent state", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-streaming.\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "While working on QfK implementation in librdkafka, I found that {{memberId}} field doesn't accept {{/}} and {{{}+{}}}.\r\n\r\nI get following error on the {*}broker side{*}.\r\n{code:java}\r\n[2025-10-23 15:10:55,319] ERROR [KafkaApi-1] Unexpected error handling request RequestHeader(apiKey=SHARE_FETCH, apiVersion=1, clientId=rdkafka, correlationId=4, headerVersion=2) -- ShareFetchRequestData(groupId='share-group-1', memberId='/FvjN95PRhGaAuA8aQMuTw', shareSessionEpoch=0, maxWaitMs=500, minBytes=1, maxBytes=52428800, maxRecords=500, batchSize=500, topics=[FetchTopic(topicId=sgd0qCnHRL-t80afMzN9nA, partitions=[FetchPartition(partitionIndex=0, partitionMaxBytes=0, acknowledgementBatches=[])])], forgottenTopicsData=[]) with context RequestContext(header=RequestHeader(apiKey=SHARE_FETCH, apiVersion=1, clientId=rdkafka, correlationId=4, headerVersion=2), connectionId='127.0.0.1:9092-127.0.0.1:41442-0-4', clientAddress=/127.0.0.1, principal=User:ANONYMOUS, listenerName=ListenerName(PLAINTEXT), securityProtocol=PLAINTEXT, clientInformation=ClientInformation(softwareName=librdkafka, softwareVersion=2.12.0-RC1-13-g82dbc3-dirty-devel-O0), fromPrivilegedListener=true, principalSerde=Optional[org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder@1648925]) (kafka.server.KafkaApis) java.lang.IllegalArgumentException: Illegal base64 character 2f at java.base/java.util.Base64$Decoder.decode0(Base64.java:848) ~[?:?] at java.base/java.util.Base64$Decoder.decode(Base64.java:566) ~[?:?] at java.base/java.util.Base64$Decoder.decode(Base64.java:589) ~[?:?] at org.apache.kafka.common.Uuid.fromString(Uuid.java:136) ~[kafka-clients-4.1.0.jar:?] at kafka.server.KafkaApis.handleShareFetchRequest(KafkaApis.scala:3157) [kafka_2.13-4.1.0.jar:?] at kafka.server.KafkaApis.handle(KafkaApis.scala:236) [kafka_2.13-4.1.0.jar:?] at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:158) [kafka_2.13-4.1.0.jar:?] at java.base/java.lang.Thread.run(Thread.java:840) [?:?] {code}\r\n*", "output": "Fix ShareFetch RPC doesn't allow '/' and '+' in memberId field"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade commons-beanutils to 1.11.0 due to CVE-2025-48734.\nDescription: Upgrade commons-beanutils to 1.11.0 due to CVE-2025-48734.\nQ: ? we are still at commons-beanutils:commons-beanutils:1.9.4 in branch 3.4", "output": "[~stevel@apache.org] Confirmed, I’ve submitted a PR#7743 to fix it. Could you please review it when you have time?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Due to the extensive JUnit4 dependencies in the Hadoop modules, we will attempt to remove JUnit4 dependencies on a module-by-module basis.", "output": "[JDK17] Remove JUnit4 Dependency"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Backport HADOOP-15984 (Update jersey from 1.19 to 2.x) on branch-3.4.0 \nDescription: In Hadoop 3.5.0 we uses GlassFish Jersey (modern Jersey).\r\n\r\nIn Hadoop 3.4.0 we use Sun Jersey (legacy Jersey).\r\n\r\nJetty submodule is not published under _com.sun.jersey.jersey-test-framework_ so upgrading jersey will be a better idea to remove grizzly-http-* dependencies (which is already done in 3.5.0 through  YARN-11793).\nQ: I agree that backporting this to 3.4 is problematic. I think this is much too large a change for a patch release.\r\nI think the priority should be on pushing for a 3.5.0 release.", "output": "Thank you [~slfan1989] and [~fanningpj] for the insights. That makes sense, I agree it's best to keep branch-3.4 unchanged and focus on 3.5.0."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK17] Disallow JUnit4 Imports After JUnit5 Migration\nDescription: As our project has fully migrated to JUnit5, we should now enforce a rule that prevents the import and usage of JUnit4 classes (such as org.junit.Test, org.junit.Assert, etc.) to ensure consistency, avoid regressions, and allow safe removal of legacy dependencies.\r\n\r\nThis task involves identifying and eliminating any remaining JUnit4 imports, and introducing static code analysis or linting rules to ban future usage.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have done work to detect snapshot lag. Now we need to trigger snapshot generation for the next microbatch, when snapshot lag is detected.", "output": "Trigger snapshot generation for next batch when lag is detected"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Insert Overwrite Jobs With MagicCommitter Fails On S3 Express Storage\nDescription: Query engines which uses Magic Committer to overwrite a directory would ideally upload the MPUs (not complete) and then delete the contents of the directory before committing the MPU.\r\n\r\n \r\n\r\nFor S3 express storage, The directory purge operation is enabled by default. Refer [here|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L688] for code pointers.\r\n\r\n \r\n\r\nDue to this, the pending MPU uploads are purged and query fails with \r\n\r\n{{NoSuchUpload: The specified multipart upload does not exist. The upload ID might be invalid, or the multipart upload might have been aborted or completed. }}\nQ: [~stevel@apache.org]  - I understand this was done because to delete a directory in S3 express directory, All the pending upload needs to be purged. But this causes problem in insert overwrite type of job with MagicCommitter.\r\n\r\n \r\n\r\nI think we  should make  directory purge operation  \"false\" by default for all types of buckets - Any thoughts on this ?", "output": "people trying to do INSERT OVERWRITE? hmm. \r\n\r\nok, set to default, mark as incompatible, and update the docs with \r\n* a section on this. \r\n* what the error means\r\n\r\npeople should always have auto cleanup enabled anyway, shouldn't they? always always always. in fact, aws cost analyzer should warn if you don't (does it do this?). and given that, shouldn't really matter much. we have CLI tools in terms of \"hadoop s3guard uploads\" to enum/purge it too. \r\n\r\nFWIW one of our qe buckets didn't do the cleanup, I used it for a scale test of MPU cleanup. We could actually have an ILoadTest for this now that createFile() lets you create a zero byte MPU"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/pull/19513#issuecomment-3390047209", "output": "Create a CI to verify all gradle tasks"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Wrap IllegalArgumentException with proper error code for invalid datetime patterns\nDescription: When `TO_TIMESTAMP` is called with an invalid datetime pattern (e.g., `'yyyyMMddHHMIss'` with invalid letter `'I'`), the `IllegalArgumentException` from Java's `DateTimeFormatterBuilder` escapes during constant folding optimization, resulting in a raw exception without proper error codes or `SQLSTATE`.\nQ: Issue resolved by pull request 52762\n[https://github.com/apache/spark/pull/52762]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][Optimizations] Reduce Network Calls In Create and Mkdir Flow\nDescription: Implementing Create and Mkdir file system APIs for FNS(HNS Disabled) accounts on Blob Endpoint involves a lot of checks and marker file creations to handle implicit explicit cases of paths involved in these APIs.\r\n\r\nThis Jira proposes a few optimizations to reduce the network calls wherever possible and in case where create/mkdir is bound to fail, it should fail faster before doing any post checks,\nQ: anmolanmol1234 opened a new pull request, #7353:\nURL: https://github.com/apache/hadoop/pull/7353\n\n   Implementing Create and Mkdir file system APIs for FNS(HNS Disabled) accounts on Blob Endpoint involves a lot of checks and marker file creations to handle implicit explicit cases of paths involved in these APIs.\r\n   \r\n   This PR proposes a few optimizations to reduce the network calls wherever possible and in case where create/mkdir is bound to fail, it should fail faster before doing any post checks,", "output": "anmolanmol1234 closed pull request #7340: HADOOP-19448: [ABFS][FNSOverBlob] Optimizing the current create and mkdir flow\nURL: https://github.com/apache/hadoop/pull/7340"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip tests in Hadoop common that depend on SecurityManager if the JVM does not support it\nDescription: TestExternalCall fails when SecurityManager cannot be set, we need to skip it on thos JVMs.\r\n\r\nTestGridmixSubmission has already been rewritten to use ExitUtil, we just need to remove the leftover SecurityManager calls.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Just the new APIs, with no modifications to Streams behaviour. This should keep review nice and simple, as it will just be lots of call-site method renames.", "output": "Add new StateStore APIs"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Publish multi-arch hadoop-runner image to GitHub\nDescription: Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image to GitHub Container Registry.  Build for both amd64 and arm64.\nQ: slfan1989 commented on PR #8021:\nURL: https://github.com/apache/hadoop/pull/8021#issuecomment-3383664657\n\n   @adoroszlai Many thanks for your contribution. Could we consider upgrading the image to ubuntu 24.04?", "output": "smengcl merged PR #8021:\nURL: https://github.com/apache/hadoop/pull/8021"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The goal of this ticket is to start a KIP with the following proposed changes:\r\n\r\nCurrently, the broker overrides settings upon a client initial request. I.e. there's some logic to consolidate/merge settings from the broker and from the client. During that step, it should be doable to throw an exception if there's an invalid configuration. This ticket is specifically for throwing an exception if the client overrides `replication.factor` with a value that is less than `min.insync.replicas` specified by the broker. This check should happen where the “merging” logic happens, on the broker side.", "output": "Throw an exception if client tries to override topic replication factor with a value that it < min.insync.replicas"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Following Cobination of test Suites will run as a part of CI after blob endpoint support has been added.\r\n\r\nHNS-OAuth-DFS\r\nHNS-SharedKey-DFS\r\nNonHNS-SharedKey-DFS\r\nAppendBlob-HNS-OAuth-DFS\r\nNonHNS-SharedKey-Blob\r\nNonHNS-OAuth-DFS\r\nNonHNS-OAuth-Blob\r\nAppendBlob-NonHNS-OAuth-Blob\r\nHNS-Oauth-DFS-IngressBlob\r\nNonHNS-Oauth-DFS-IngressBlob", "output": "ABFS: [FnsOverBlob][Tests] Update Test Scripts to Run Tests with Blob Endpoint"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When i try to run the *org.apache.hadoop.yarn.webapp.TestWebApp* tests over JDK17 i am facing the following errors:\r\n\r\n{noformat}\r\nCaused by: A MultiException has 2 exceptions.  They are:\r\n1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver\r\n{noformat}\r\n\r\nRepro steps:\r\n- run: ./start-build-env.sh ubuntu_24\r\n- in docker run: mvn clean install -Pjdk17+ -T 32 -DskipTests \r\n- in hadoop-yarn-common modul run: mvn test -Dtest=org.apache.hadoop.yarn.webapp.TestWebApp\r\n\r\nI found a similar error here:\r\nhttps://issues.apache.org/jira/browse/HDDS-5068\r\n\r\nBased on my understanding the problem is the JDK11+ environments does not have the JAXB runtime, so we have to explicit provide them. Maybe this is just a temporal solution, till the whole Jakarta upgrade can be done.\r\n\r\n{panel:title=Full error log}\r\n{code}\r\n[ERROR] testRobotsText  Time elapsed: 0.064 s  (ApplicationHandler.java:261)\r\n\tat org.glassfish.jersey.servlet.WebComponent.(WebComponent.java:314)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:154)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:360)\r\n\tat javax.servlet.GenericServlet.init(GenericServlet.java:244)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:632)\r\n\t... 104 more\r\nCaused by: javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n - with linked exception:\r\n[java.lang.ClassNotFoundException: com.sun.xml.bind.v2.ContextFactory]\r\n\tat javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:232)\r\n\tat javax.xml.bind.ContextFinder.find(ContextFinder.java:375)\r\n\tat javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:691)\r\n\tat javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:632)\r\n\tat org.glassfish.jersey.jettison.JettisonJaxbC", "output": "[JDK 17] Implementation of JAXB-API has not been found on module path or classpath"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Block unsupported aggregates in both signatures of `visitAggregateFunction`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Reduce number of events generated in AsyncKafkaConsumer.updateFetchPositions()\nDescription: We create—and wait on—{{{}CheckAndUpdatePositionsEvents{}}} in {{updateFetchPositions()}} even though, in a stable system, 99.9%+ of the time there are no updates to perform.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed.\r\n\r\n* filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. \r\n* Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. \r\n\r\nThis adds a lot more stuff into the distribution, so I'm doing the following design\r\n* all hadoop-* modules in common/lib\r\n* minimal dependencies for hadoop-azure and hadoop-gcs (once we get those right!)\r\n* hadoop-aws: everything except bundle.jar\r\n* other connectors: only included with explicit profiles.\r\n\r\nASF releases will support azure out the box, the others once you add the dependencies. And anyone can build their own release with everything\r\n\r\nOne concern here, we make hadoop-cloud-storage artifact incomplete at pulling in things when depended on. We may need a separate module for the distro setup.\r\n\r\nNoticed during this that the hadoop-tos component is shaded and includes stuff (httpclient5) that we need under control. Filed HADOOP-19708 and incorporating here.", "output": "hadoop binary distribution to move cloud connectors to hadoop common/lib"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix ShareFetch RPC doesn't allow '/' and '+' in memberId field\nDescription: While working on QfK implementation in librdkafka, I found that {{memberId}} field doesn't accept {{/}} and {{{}+{}}}.\r\n\r\nI get following error on the {*}broker side{*}.\r\n{code:java}\r\n[2025-10-23 15:10:55,319] ERROR [KafkaApi-1] Unexpected error handling request RequestHeader(apiKey=SHARE_FETCH, apiVersion=1, clientId=rdkafka, correlationId=4, headerVersion=2) -- ShareFetchRequestData(groupId='share-group-1', memberId='/FvjN95PRhGaAuA8aQMuTw', shareSessionEpoch=0, maxWaitMs=500, minBytes=1, maxBytes=52428800, maxRecords=500, batchSize=500, topics=[FetchTopic(topicId=sgd0qCnHRL-t80afMzN9nA, partitions=[FetchPartition(partitionIndex=0, partitionMaxBytes=0, acknowledgementBatches=[])])], forgottenTopicsData=[]) with context RequestContext(header=RequestHeader(apiKey=SHARE_FETCH, apiVersion=1, clientId=rdkafka, correlationId=4, headerVersion=2), connectionId='127.0.0.1:9092-127.0.0.1:41442-0-4', clientAddress=/127.0.0.1, principal=User:ANONYMOUS, listenerName=ListenerName(PLAINTEXT), securityProtocol=PLAINTEXT, clientInformation=ClientInformation(softwareName=librdkafka, softwareVersion=2.12.0-RC1-13-g82dbc3-dirty-devel-O0), fromPrivilegedListener=true, principalSerde=Optional[org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder@1648925]) (kafka.server.KafkaApis) java.lang.IllegalArgumentException: Illegal base64 character 2f at java.base/java.util.Base64$Decoder.decode0(Base64.java:848) ~[?:?] at java.base/java.util.Base64$Decoder.decode(Base64.java:566) ~", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, different KRaft replica states handle polling during graceful shutdown differently. This Jira aims to unify their logic.", "output": "Unify kraft shutdown logic in poll methods"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: SDK reads content twice during PUT to S3 Express store.\nDescription: The commit on trunk {{60c7d4fea010}} and on branch 3.4 {{f3ec55b}} fixes the logging, but does not\r\naddress the underlying issue.\r\n\r\nh2. problem\r\n\r\nDuring PUT calls, even of 0 byte objects, our UploadContentProver is reporting a recreation of \r\nof the input stream of an UploadContentProvider, as seen by our logging at info of this happening\r\n\r\n{code}\r\n bin/hadoop fs -touchz $v3/4\r\n2025-03-26 13:38:53,377 [main] INFO  impl.UploadContentProviders (UploadContentProviders.java:newStream(289)) - Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-stevel/s3a/s3ablock-0001-659277820991634509.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-03-26T13:38:53.355, streamCreationCount=2, currentStream=null}\r\n{code}\r\n\r\nThis code was added in HADOOP-19221, S3A: Unable to recover from failure of multipart block upload attempt \"Status Code: 400; Error Code: RequestTimeout\"; it logs at INFO because it is considered both rare and serious enough that we should log it, based on our hypothesis that it was triggered by a transient failure of the S3 service front and and the inability of the SDK to recover from it\r\n\r\nIt turns out that uploading even a zero byte file to S3 triggers the dual creation of the stream, apparently from a dual signing.\r\n\r\nThis *does not* happen on multipart uploads.", "output": "Reopened"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Function version() should return full spark version instead of short version\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Ticket for KIP-1222. The KIP proposes adding a new AcknowledgementType to be used in ShareFetch and ShareAcknowledge RPCs which will allow renewal of acquisition lock timeout with causing a state transition of the record in the share partition.", "output": "Acquisition lock timeout renewal in share consumer explicit mode"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "https://www.cve.org/CVERecord?id=CVE-2025-58057\r\n\r\nfixed in 4.1.125 but no harm upgrading to latest", "output": "Bump netty to 4.1.127 due to CVE-2025-58057"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Move Gauge#value to MetricValueProvider\nDescription: from: https://github.com/apache/kafka/pull/3705#discussion_r140830112\nQ: the main benefit is to remove the type check from `KafkaMetric`\r\n\r\n{code:java}\r\n// before\r\n    public Object metricValue() {\r\n        long now = time.milliseconds();\r\n        synchronized (this.lock) {\r\n            if (isMeasurable())\r\n                return ((Measurable) metricValueProvider).measure(config, now);\r\n            else if (this.metricValueProvider instanceof Gauge)\r\n                return ((Gauge) metricValueProvider).value(config, now);\r\n            else\r\n                throw new IllegalStateException(\"Not a valid metric: \" + this.metricValueProvider.getClass());\r\n        }\r\n    }\r\n{code}\r\n\r\n\r\n{code:java}\r\n// after\r\n    public Object metricValue() {\r\n        long now = time.milliseconds();\r\n        synchronized (this.lock) {\r\n           return metricValueProvider.value(config, now);\r\n        }\r\n    }\r\n{code}\r\n\r\nAlso, the following code could be removed\r\n\r\n{code:java}\r\n        if (!(valueProvider instanceof Measurable) && !(valueProvider instanceof Gauge))\r\n            throw new IllegalArgumentException(\"Unsupported metric value provider of class \" + valueProvider.getClass());\r\n{code}", "output": "we may need to keep the method in `Guave` to avoid possible compatibility issue (see KAFKA-6174)"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see discussion https://github.com/apache/kafka/pull/20295#issuecomment-3146551515", "output": "Upgrade spotbug to 4.9.4"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix `FileStreamSinkSuite` flakiness by using `walkFileTree` instead of `walk`\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: `ci-complete` needs to work with active branches after the JDK is updated\nDescription: The JDK version used by `ci-complete` is inconsistent with other active branches, which is causing the build report task to fail\nQ: Another idea to be considered (on): https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/github-code-transformation-workflow-advanced.html\r\nFYI [~mingyen066]", "output": "Do we know if it's possible to upload a build scan using Gradle version 9 that was produced using Gradle 8? I expect it might not work since the build scan data format may change. \r\n\r\n \r\n\r\nOne thing we could try is to include a file in the build scan archive that indicates what version of Gradle and JDK were used. We can load these properties before running setup-gradle so we can select the correct version. There's nothing built in to GHA that helps here (as far as i know), maybe we can use JSON and \"jq\"? \r\n\r\n \r\n\r\nI won't be able to work on this unfortunately, but I'm happy to help with reviews."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use batch.num_columns instead of len(batch.columns).\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "While testing KAFKA-18884, I noticed there are no official tools available to measure the performance of aborted transactions. `ProducerPerformance` should serve this purpose by allowing us to configure a ratio of transactions to abort", "output": "Enable ProducerPerformance to abort transaction randomly"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add `spark.kubernetes.driver.pod.excludedFeatureSteps` usage example\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use `Utils. getRootCause` instead of `Throwables.getRootCause`\nDescription: \nQ: Issue resolved by pull request 52658\n[https://github.com/apache/spark/pull/52658]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We've observed that the `StreamThread` blocks waiting for a `Future` from the `StateUpdater` in the `StreamsPartitionAssigner#onAssignment()` method when we are moving a task out of the `StateUpdater` and onto the `StreamThread`.\r\n\r\n \r\n\r\nThis can cause problems because, during restoration or with warmup replicas, the `StateUpdater#runOnce()` method can take a long time (upwards of 20 seconds) when RocksDB stalls writes to allow compaction to keep up. In EOS this blockage may cause the transaction to time out, which is a big mess. This is because the `StreamThread` may have an open transaction before the `StreamsPartitionAssignor#onAssignment()` method is called.\r\n\r\n \r\n\r\nSome screenshots from the JFR below (credit to [~eduwerc]).", "output": "StreamThread blocks on StateUpdater during onAssignment()"}
{"instruction": "Answer the question based on the bug.", "input": "Title: High number of Threads Launched when Calling fs.getFileStatus() via proxyUser after Kerberos authentication.\nDescription: We have observed an issue where very large number of threads are being launched when performing concurrent {{fs.getFileStatus(path) operations}} as proxyUser.\r\n\r\nAlthough this issue was observed in our hive services, we were able to isolate and replicate this issue without hive by writing a sample standalone program which first logs in via a principal and keytab and then creates a proxy user and fires concurrent {{fs.getFileStatus(path)}} for a few mins. Eventually when the concurrency increases it tries to create more threads than max available threads(ulimit range) and the process eventually slows down.\r\n{code:java}\r\nUserGroupInformation proxyUserUGI = UserGroupInformation.createProxyUser(\r\n\"hive\", UserGroupInformation.getLoginUser());{code}\r\nIn this particular case, when launching 30 concurrent threads calling , the max number of threads launched by the PID are 6066.\r\n \r\n{code:java}\r\nEvery 1.0s: ps -eo nlwp,pid,args --sort -nlwp | head                                                \nQ: [~stevel@apache.org] , can i get some help regarding this?", "output": "don't go near HDFS' I'll only break things. Afraid you'll have to do more debugging yourself"}
{"instruction": "Answer the question based on the bug.", "input": "Title: volcano tos: disable shading when -DskipShade is set on a build\nDescription: hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M artifact, with unknown content inside\r\n{code}\r\n 92K    share/hadoop/common/lib/hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n912K    share/hadoop/common/lib/hadoop-aws-3.5.0-20250916.124028-686.jar\r\n808K    share/hadoop/common/lib/hadoop-azure-3.5.0-20250916.124028-685.jar\r\n 36K    share/hadoop/common/lib/hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n 72K    share/hadoop/common/lib/hadoop-cos-3.5.0-20250916.124028-683.jar\r\n136K    share/hadoop/common/lib/hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n140K    share/hadoop/common/lib/hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n3.8M    share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar\r\n{code}\r\n\r\nOne thing it includes is yet-another mozilla/public-suffix-list.txt. These are a recurrent PITA and I don't want\r\nto find them surfacing again.\r\n{code}\r\n\r\n15. Required Resources\r\n======================\r\n\r\nresource: mozilla/public-suffix-list.txt\r\n       jar:file:/Users\nQ: doing this inside HADOOP-19696, as that's where I need the leaner artifacts", "output": "(note the shading is troubled anyway\r\n{code}\r\n[INFO] Dependency-reduced POM written at: /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/dependency-reduced-pom.xml\r\n[WARNING] httpclient5-5.3.jar, httpcore5-5.2.4.jar, httpcore5-h2-5.2.4.jar define 3 overlapping resources: \r\n[WARNING]   - META-INF/DEPENDENCIES\r\n[WARNING]   - META-INF/LICENSE\r\n[WARNING]   - META-INF/NOTICE\r\n[WARNING] hadoop-tos-3.5.0-SNAPSHOT.jar, httpclient5-5.3.jar, httpcore5-5.2.4.jar, httpcore5-h2-5.2.4.jar, nimbus-jose-jwt-10.4.jar, ve-tos-java-sdk-hadoop-2.8.9.jar define 1 overlapping resource: \r\n[WARNING]   - META-INF/MANIFEST.MF\r\n[WARNING] maven-shade-plugin has detected that some files are\r\n[WARNING] present in two or more JARs. When this happens, only one\r\n[WARNING] single version of the file is copied to the uber jar.\r\n[WARNING] Usually this is not harmful and you can skip these warnings,\r\n[WARNING] otherwise try to manually exclude artifacts based on\r\n[WARNING] mvn dependency:tree -Ddetail=true and the above output.\r\n[WARNING] See https://maven.apache.org/plugins/maven-shade-plugin/\r\n[INFO] Replacing original artifact with shaded artifact.\r\n[INFO] Replacing /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/target/hadoop-tos-3.5.0-SNAPSHOT.jar with /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/target/hadoop-tos-3.5.0-SNAPSHOT-shaded.jar\r\n[INFO] \r\n[INFO] --- cyclonedx:2.9.1:makeBom (default) @ hadoop-tos ---\r\n{code}"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Offline migration essentially preserves offsets and nothing else. So effectively write tombstones for classic group type when a streams heartbeat is sent to with the group ID of an empty classic group, and write tombstones for the streams group type when a classic consumer attempts to join with a group ID of an empty streams group.\r\n\r\nAC:\r\n * Implementation on the broker-side", "output": "Implement offline migration"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, when adding a controller via CLI, we have to run:\r\n{code:java}\r\n$ bin/kafka-metadata-quorum.sh --command-config config/controller.properties --bootstrap-server localhost:9092 add-controller\r\n\r\nor\r\n\r\n$ bin/kafka-metadata-quorum.sh --command-config config/controller.properties --bootstrap-controller localhost:9093 add-controller{code}\r\nThe controller.properties file is expected to be the controller property file that to be added. But if we want to pass configs to the admin client, what can we do?\r\n{code:java}\r\nbin/kafka-metadata-quorum.sh --help\r\nusage: kafka-metadata-quorum [-h] [--command-config COMMAND_CONFIG] (--bootstrap-server BOOTSTRAP_SERVER | --bootstrap-controller BOOTSTRAP_CONTROLLER)\r\n                             {describe,add-controller,remove-controller} ...This tool describes kraft metadata quorum status.\r\n\r\n...\r\n\r\n --command-config COMMAND_CONFIG\r\n   Property file containing configs to be passed to Admin Client.  For add-controller,  the file is used to specify the controller properties as well.{code}\r\nAs this help output said, the \"--command-config\" can pass configs to admin client, but when add controller, it is also used as controller property file.\r\n\r\nFor example, we want to set the \"client-id\" to the admin client, when doing the add-controller, we have to add one more line in the controller.properties file:\r\n{code:java}\r\nclient.id=test-admin-client{code}\r\nThis is not ideal to ask users to mix the client config into the controller config.\r\n\r\n \r\n\r\nMaybe we can consider to add one more \"--add-controller-command-config\" for add controller use, or ask users to explicitly pass the controller id, dir UUID, ... like remove-controller did, but the controller advertised listener value might be a little complicated for users.", "output": "Separate the controller config and admin config when add controller"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve testing of OffsetsRequestManager to better match OffsetFetcher\nDescription: OffsetsRequestManager lacks the same level of testing compared to the code it eventually intends to replace, OffsetFetcher. The addition of new unit tests could potentially surface issues with its implementation and assist with the resolution of KAFKA-18216 and KAFKA-18217", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Expose Rack ID in MemberDescription\nDescription: Currently, the {{{}AdminClient{}}}’s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does *not* include rack information, even though the underlying {{ConsumerGroupDescribeResponse}} protocol already supports a {{rackId}} field.\r\nThis causes users to be unable to retrieve member rack information through the Admin API.\r\n\r\nRack information is crucial for:\r\n * Monitoring and visualization tools\r\n\r\n * Operational analysis of rack distribution\r\n\r\n * Diagnosing rack-aware assignment issues\r\n\r\nIn addition, StreamsGroupMemberDescription already includes the rackId, so adding it here would also make the behavior more consistent.\r\n\r\nThe PR:  [https://github.com/apache/kafka/pull/20691]\r\n\r\nThe KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1227%3A+Expose+Rack+ID+in+MemberDescription+and+ShareMemberDescription", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add `SparkOperatorConfManager.getAll` method\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Support new SQL syntax: `Show Table As JSON`", "output": "SHOW TABLES AS JSON"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [RISC-V]  Add rv bulk CRC32 (non-CRC32C) optimized path\nDescription: \nQ: PeterPtroc opened a new pull request, #8031:\nURL: https://github.com/apache/hadoop/pull/8031\n\n   \r\n   \r\n   ### Description of PR\r\n   - Introduces a riscv64 native implementation path for CRC32 (CRC32C not optimized).\r\n   - Adds runtime CPU feature detection on linux-riscv64 to enable hardware-accelerated CRC32 when available; falls back to the existing implementation if native is unavailable or disabled.\r\n   \r\n   Below are the performance changes observed using the built-in CRC32 benchmark. Although performance is poor when bpc  64. To keep the codebase simple and maintainable, I did not add bpc-size-specific handling.\r\n   \r\n   | bpc | #T | Native (origin) | Native (new) | Δ (MB/s) | Δ% |\r\n   |---:|---:|---:|---:|---:|---:|\r\n   | 32 | 1 | 661.5 | 463.5 | -198.0 | -29.9% |\r\n   | 32 | 2 | 642.6 | 491.4 | -151.2 | -23.5% |\r\n   | 32 | 4 | 663.7 | 480.5 | -183.2 | -27.6% |\r\n   | 32 | 8 | 653.0 | 472.0 | -181.0 | -27.7% |\r\n   | 32 | 16 | 656.1 | 473.4 | -182.7 | -27.8% |\r\n   | 64 | 1 | 793.9 | 318.0 | -475.9 | -59.9% |\r\n   | 64 | 2 | 771.3 | 322.1 | -449.2 | -58.2% |\r\n   | 64 | 4 | 787.3 | 315.0 | -472.3 | -60.0% |\r\n   | 64 | 8 | 778.0 | 309.3 | -468.7 | -60.2% |\r\n   | 64 | 16 | 773.5 | 308.1 | -465.4 | -60.2% |\r\n   | 128 | 1 | 878.8 | 2398.8 | +1520.0 | +173.0% |\r\n   | 128 | 2 | 846.8 | 1723.9 | +877.1 | +103.6% |\r\n   | 128 | 4 | 861.2 | 1690.0 | +828.8 | +96.2% |\r\n   | 128 | 8 | 857.8 | 1373.3 | +515.5 | +60.1% |\r\n   | 128 | 16 | 853.8 | 1361.3 | +507.5 | +59.4% |\r\n   | 256 | 1 | 783.9 | 2752.5 | +1968.6 | +251.1% |\r\n   | 256 | 2 | 810.0 | 2053.3 | +1243.3 | +153.5% |\r\n   | 256 | 4 | 835.2 | 1966.5 | +1131.3 | +135.5% |\r\n   | 256 | 8 | 812.4 | 1756.3 | +943.9 | +116.2% |\r\n   | 256 | 16 | 811.8 | 1524.7 | +712.9 | +87.8% |\r\n   | 512 | 1 | 923.6 | 3328.9 | +2405.3 | +260.4% |\r\n   | 512 | 2 | 886.5 | 3295.1 | +2408.6 | +271.7% |\r\n   | 512 | 4 | 910.5 | 2359.9 | +1449.4 | +159.2% |\r\n   | 512 | 8 | 888.1 | 1637.4 | +749.3 | +84.4% |\r\n   | 512 | 16 | 897.0 | 1840.1 | +943.1 | +105.1% |\r\n   | 1024 | 1 | 950.4 | 3045.0 | +2094.6 | +220.4% |\r\n   | 1024 | 2 | 918.0 | 2202.9 | +1284.9 | +140.0% |\r\n   | 1024 | 4 | 937.6 | 2040.4 | +1102.8 | +117.6% |\r\n   | 1024 | 8 | 916.5 | 1961.5 | +1045.0 | +114.0% |\r\n   | 1024 | 16 | 927.4 | 2003.9 | +1076.5 | +116.1% |\r\n   | 2048 | 1 | 962.3 | 3189.1 | +2226.8 | +231.4% |\r\n   | 2048 | 2 | 970.1 | 3192.3 | +2222.2 | +229.1% |\r\n   | 2048 | 4 | 943.4 | 2411.2 | +1467.8 | +155.6% |\r\n   | 2048 | 8 | 937.6 | 1837.7 | +900.1 | +96.0% |\r\n   | 2048 | 16 | 933.1 | 1864.0 | +930.9 | +99.8% |\r\n   | 4096 | 1 | 969.9 | 3654.5 | +2684.6 | +276.8% |\r\n   | 4096 | 2 | 972.0 | 2798.0 | +1826.0 | +187.9% |\r\n   | 4096 | 4 | 960.1 | 2307.0 | +1346.9 | +140.3% |\r\n   | 4096 | 8 | 948.2 | 2753.1 | +1804.9 | +190.4% |\r\n   | 4096 | 16 | 938.7 | 2170.5 | +1231.8 | +131.2% |\r\n   | 8192 | 1 | 973.6 | 4008.1 | +3034.5 | +311.7% |\r\n   | 8192 | 2 | 922.5 | 3018.2 | +2095.7 | +227.2% |\r\n   | 8192 | 4 | 955.6 | 2968.7 | +2013.1 | +210.7% |\r\n   | 8192 | 8 | 943.4 | 2077.9 | +1134.5 | +120.3% |\r\n   | 8192 | 16 | 944.9 | 2191.7 | +1246.8 | +132.0% |\r\n   | 16384 | 1 | 974.4 | 4090.3 | +3115.9 | +319.8% |\r\n   | 16384 | 2 | 978.3 | 2999.6 | +2021.3 | +206.6% |\r\n   | 16384 | 4 | 956.6 | 3248.9 | +2292.3 | +239.6% |\r\n   | 16384 | 8 | 950.8 | 3228.0 | +2277.2 | +239.5% |\r\n   | 16384 | 16 | 941.2 | 2832.1 | +1890.9 | +200.9% |\r\n   | 32768 | 1 | 972.2 | 4205.7 | +3233.5 | +332.6% |\r\n   | 32768 | 2 | 938.6 | 4115.2 | +3176.6 | +338.4% |\r\n   | 32768 | 4 | 957.4 | 2508.9 | +1551.5 | +162.1% |\r\n   | 32768 | 8 | 952.8 | 2319.8 | +1367.0 | +143.5% |\r\n   | 32768 | 16 | 944.5 | 1657.7 | +713.2 | +75.5% |\r\n   | 65536 | 1 | 976.3 | 4226.6 | +3250.3 | +332.9% |\r\n   | 65536 | 2 | 940.0 | 3075.8 | +2135.8 | +227.2% |\r\n   | 65536 | 4 | 958.5 | 1345.2 | +386.7 | +40.3% |\r\n   | 65536 | 8 | 950.2 | 1954.7 | +1004.5 | +105.7% |\r\n   | 65536 | 16 | 945.8 | 2414.0 | +1468.2 | +155.2% |\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Built hadoop-common with native profile on riscv64; verified it's function by TestNativeCrc32.\r\n   Ran Hadoop’s CRC32 benchmark on riscv64 (OpenEuler/EulixOS) with JDK 17.\r\n   Here is the commands and results:\r\n   \r\n   Command： \r\n   \r\n   ```\r\n   mvn -Pnative \\\r\n     -Dtest=org.apache.hadoop.util.TestNativeCrc32 \\\r\n     -Djava.library.path=\"$HADOOP_COMMON_LIB_NATIVE_DIR\" \\\r\n     test\r\n   ```\r\n   \r\n   Results\r\n   \r\n   ```\r\n   [INFO] -------------------------------------------------------\r\n   [INFO]  T E S T S\r\n   [INFO] -------------------------------------------------------\r\n   [INFO] Running org.apache.hadoop.util.TestNativeCrc32\r\n   [INFO] Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.017 s", "output": "hadoop-yetus commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3398778305\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  14m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  | 101m 41s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 25s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 206m 55s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8031 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux cf2b3cead534 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0e62b049f710f680823489b1a77892ed49252fc4 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_462-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/testReport/ |\r\n   | Max. process+thread count | 1376 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/console |\r\n   | versions | git=2.43.7 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: upgrade gson due to security fixes\nDescription: not sure why https://github.com/google/gson/pull/2588 didn't attract a CVE\r\n\r\nlinked to https://issues.apache.org/jira/browse/HADOOP-19632 - nimbus-jose-jwt uses gson", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/actions/runs/16651849478/job/47166854957?pr=20269", "output": "Fix flaky RemoteLogManagerTest#testCopyQuota"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add RealTimeScanExec and ability to execute long running batches\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix file permission errors as per the platform\nDescription: The file permission denial error message in Linux systems end with \"(Permission denied)\" particularly.\r\nHowever, an error message in the same scenario on Windows ends with \"(Access is denied)\" error.\r\n\r\nThis results in a bug in *org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker* and also leads to a unit test failure *org.apache.hadoop.fs.TestFsShellCopy#testPutSrcFileNoPerm*.\r\n\r\nThus, we need to make the appropriate check in accordance with the platform.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix Flaky Test:`OracleJoinPushdownIntegrationSuite`\nDescription: Recently, `OracleJoinPushdownIntegrationSuite` frequently fails.\r\n\r\nhttps://github.com/beliefer/spark/actions/runs/18904457292/job/53959322233#logs\r\n{code:java}\r\n[info] org.apache.spark.sql.jdbc.v2.join.OracleJoinPushdownIntegrationSuite *** ABORTED *** (10 minutes, 15 seconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 599 times over 10.012648813883333 minutes. Last failure message: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info]   https://docs.oracle.com/error-help/db/ora-12541/. (DockerJDBCIntegrationSuite.scala:214)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.sca", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side\nDescription: Support create and rename idempotency on FNS Blob from client side\nQ: hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3234744350\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 147m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux bf9d5b01e663 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3ef89e1b92e564468a24cb8d200567b30f283b10 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3234745585\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 22s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 59s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5ba8302b1e20 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3ef89e1b92e564468a24cb8d200567b30f283b10 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update the documents of arrow-batching related configures\nDescription: \nQ: Issue resolved by pull request 52753\n[https://github.com/apache/spark/pull/52753]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "1. The logic to skip vector IO contract tests doesn't work  if the analytics stream is set on a per-bucket basis for the test bucket\r\n2. tests with SSE-C are failing. Test bucket is normally set up to use SSE-KMS, FWIW\r\n\r\n{code}\r\n  \r\n    fs.s3a.bucket.stevel-london.encryption.algorithm\r\n    SSE-KMS\r\n  \r\n\r\n\r\n{code}\r\n\r\nthis only happens when the analytics stream is set for the test bucket fs.s3a.bucket.stevel-london.input.stream.type=analytics; set it globally all is good\r\n\r\n+ various other tests. FInal pr message\r\n{code}\r\nCode changes related to HADOOP-19485.\r\n\r\nAwsSdkWorkarounds no longer needs to cut back on transfer manager logging\r\n(HADOOP-19272) :\r\n- Remove log downgrade and change assertion to expect nothing to be logged.\r\n- remove false positives from log.\r\n\r\nITestS3AEndpointRegion failure:\r\n- Change in state of AwsExecutionAttribute.ENDPOINT_OVERRIDDEN\r\n  attribute requires test tuning to match.\r\n\r\nSome tests against third-party stores fail\r\n- Includes fix for the assumeStoreAwsHosted() logic.\r\n- Documents how to turn off multipart uploads with third-party stores.\r\n{code}", "output": "S3A: Test failures during AWS SDK upgrade"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `netty-tcnative` to 2.0.74.Final\nDescription: \nQ: Issue resolved by pull request 52547\n[https://github.com/apache/spark/pull/52547]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade Netty to 4.2.7.Final\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: error stack traces printed on analytics stream factory close\nDescription: When you close an s3a filesystem there is a lot of ERROR level stack traces about a CancellationException -despite that being exactly what is wanted.\r\n\r\nCore of it comes from netty.\r\n{code}\r\n        Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 1 failure: Unable to execute HTTP request: The connection was closed during the request. The request will usually succeed on a retry, but if it does not: consider disabling any proxies you have configured, enabling debug logging, or performing a TCP dump to identify the root cause. If this is a streaming operation, validate that data is being read or written in a timely manner. Channel Information: ChannelDiagnostics(channel=[id: 0x801baead, L:0.0.0.0/0.0.0.0:59534 ! R:bucket.vpce-0117f6033eaf7aee5-2hxd9fg4.s3.us-west-2.vpce.amazonaws.com/10.80.134.179:443], channelAge=PT0.676S, requestCount=1, responseCount=0, lastIdleDuration=PT0.006284125S)\r\nCaused by: java.lang.IllegalStateException: executor not accep\nQ: Full log. me trying to do minimal bandwidth test and calling control-c mid download.\r\n\r\nI don't mind the noise as much as the way the cancellation exception was escalated to become the failure cause.\r\n{code}\r\n2025-05-13 17:09:16,705 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(241)) - Exiting with status -1: java.util.concurrent.CancellationException\r\n{code}\r\n\r\nProposed: service stop code in {{AnalyticsStreamFactory}} to catch and swallow those. \r\n\r\n{code}\r\n> bin/hadoop jar $CLOUDSTORE bandwidth -csv out.csv 1M s3a://stevel-usw2/bw\r\n\r\nBandwidth test against s3a://stevel-usw2/bw with data size 1m\r\n=============================================================\r\n\r\nBlock size 1 MB\r\nSaving statistics as CSV data to out.csv\r\nUsing filesystem s3a://stevel-usw2\r\nUpload size in Megabytes 1 MB\r\nWriting data as 1 blocks each of size 1,048,576 bytes\r\nStarting: Opening s3a://stevel-usw2/bw for upload\r\nDuration of Opening s3a://stevel-usw2/bw for upload: 0:00:00.018\r\nWrite block 0 in 0.015 seconds\r\n\r\nStarting: upload stream close()\r\nDuration of upload stream close(): 0:00:03.793\r\n\r\nProgress callbacks 4; in close 4\r\n\r\nDownload s3a://stevel-usw2/bw\r\n=============================\r\n\r\nStarting: open s3a://stevel-usw2/bw\r\n^C2025-05-13 17:09:16,700 [shutdown-hook-0] INFO  s3a.S3AFileSystem (S3AFileSystem.java:processDeleteOnExit(4331)) - Ignoring failure to deleteOnExit for path s3a://stevel-usw2/bw\r\n2025-05-13 17:09:16,702 [s3a-transfer-stevel-usw2-unbounded-pool2-t1] ERROR s3.telemetry (LogHelper.java:logAtLevel(38)) - [2025-05-13T17:09:15.996Z] [failure] [6lu5cof10v8ch] metadata.store.head.join(thread_id=30, uri=s3://stevel-usw2/bw): 705,819,458 ns [java.util.concurrent.CancellationException: 'null']\r\njava.util.concurrent.CancellationException\r\n        at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2276)\r\n        at software.amazon.s3.analyticsaccelerator.io.physical.data.MetadataStore.safeCancel(MetadataStore.java:147)\r\n        at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608)\r\n        at java.util.Collections$SynchronizedCollection.forEach(Collections.java:2064)\r\n        at software.amazon.s3.analyticsaccelerator.io.physical.data.MetadataStore.close(MetadataStore.java:157)\r\n        at software.amazon.s3.analyticsaccelerator.S3SeekableInputStreamFactory.close(S3SeekableInputStreamFactory.java:162)\r\n        at org.apache.hadoop.util.functional.LazyAutoCloseableReference.close(LazyAutoCloseableReference.java:82)\r\n        at org.apache.hadoop.fs.s3a.impl.streams.AnalyticsStreamFactory.serviceStop(AnalyticsStreamFactory.java:101)\r\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\r\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:53)\r\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:102)\r\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:160)\r\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:134)\r\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\r\n        at org.apache.hadoop.service.AbstractService.close(AbstractService.java:248)\r\n        at org.apache.hadoop.fs.s3a.S3AUtils.closeAutocloseables(S3AUtils.java:1567)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$stopAllServices$24(S3AFileSystem.java:4375)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.stopAllServices(S3AFileSystem.java:4374)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.close(S3AFileSystem.java:4354)\r\n        at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:3820)\r\n        at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:3837)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:750)\r\n2025-05-13 17:09:16,702 [shutdown-hook-0] ERROR s3.telemetry (LogHelper.java:logAtLevel(38)) - [2025-05-13T17:09:15.995Z] [failure] [272un7pgiak2a] metadata.store.head.async(thread_id=30, uri=s3://stevel-usw2/bw): 706,204,250 ns [java.util.concurrent.CancellationException: 'null']\r\njava.util.concurrent.CancellationException\r\n        at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2276)\r\n        at software.amazon.s3.analyticsaccelerator.io.physical.data.MetadataStore.safeCancel(MetadataStore.java:147)\r\n        at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608)\r\n        at java.util.Collections$SynchronizedCollection.forEach(Collections.java:2064)\r\n        at software.amazon.s3.analyticsaccelerator.io.physical.data.MetadataStore.close(MetadataStore.java:157)\r\n        at software.amazon.s3.analyticsaccelerator.S3SeekableInputStreamFactory.close(S3SeekableInputStreamFactory.java:162)\r\n        at org.apache.hadoop.util.functional.LazyAutoCloseableReference.close(LazyAutoCloseableReference.java:82)\r\n        at org.apache.hadoop.fs.s3a.impl.streams.AnalyticsStreamFactory.serviceStop(AnalyticsStreamFactory.java:101)\r\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\r\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:53)\r\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:102)\r\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:160)\r\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:134)\r\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\r\n        at org.apache.hadoop.service.AbstractService.close(AbstractService.java:248)\r\n        at org.apache.hadoop.fs.s3a.S3AUtils.closeAutocloseables(S3AUtils.java:1567)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$stopAllServices$24(S3AFileSystem.java:4375)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.stopAllServices(S3AFileSystem.java:4374)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.close(S3AFileSystem.java:4354)\r\n        at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:3820)\r\n        at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:3837)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:750)\r\njava.util.concurrent.CancellationException\r\n        at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2276)\r\n        at software.amazon.s3.analyticsaccelerator.io.physical.data.MetadataStore.safeCancel(MetadataStore.java:147)\r\n        at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608)\r\n        at java.util.Collections$SynchronizedCollection.forEach(Collections.java:2064)\r\n        at software.amazon.s3.analyticsaccelerator.io.physical.data.MetadataStore.close(MetadataStore.java:157)\r\n        at software.amazon.s3.analyticsaccelerator.S3SeekableInputStreamFactory.close(S3SeekableInputStreamFactory.java:162)\r\n        at org.apache.hadoop.util.functional.LazyAutoCloseableReference.close(LazyAutoCloseableReference.java:82)\r\n        at org.apache.hadoop.fs.s3a.impl.streams.AnalyticsStreamFactory.serviceStop(AnalyticsStreamFactory.java:101)\r\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\r\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:53)\r\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:102)\r\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:160)\r\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:134)\r\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\r\n        at org.apache.hadoop.service.AbstractService.close(AbstractService.java:248)\r\n        at org.apache.hadoop.fs.s3a.S3AUtils.closeAutocloseables(S3AUtils.java:1567)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$stopAllServices$24(S3AFileSystem.java:4375)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.stopAllServices(S3AFileSystem.java:4374)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.close(S3AFileSystem.java:4354)\r\n        at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:3820)\r\n        at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:3837)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:750)\r\n2025-05-13 17:09:16,705 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(241)) - Exiting with status -1: java.util.concurrent.CancellationException\r\n2025-05-13 17:09:17,429 [sdk-async-response-2-1] ERROR s3.telemetry (LogHelper.java:logAtLevel(38)) - [2025-05-13T17:09:15.993Z] [failure] [-38acq15b21262] s3.client.head(thread_id=30, uri=s3://stevel-usw2/bw): 1,435,559,459 ns [java.util.concurrent.CompletionException: 'software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: executor not accepting a task']\r\njava.util.concurrent.CompletionException: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: executor not accepting a task\r\n        at software.amazon.awssdk.utils.CompletableFutureUtils.errorAsCompletionException(CompletableFutureUtils.java:64)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncExecutionFailureExceptionReportingStage.lambda$execute$0(AsyncExecutionFailureExceptionReportingStage.java:51)\r\n        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)\r\n        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)\r\n        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\r\n        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)\r\n        at software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:78)\r\n        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\r\n        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\r\n        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\r\n        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage2$RetryingExecutor.maybeAttemptExecute(AsyncRetryableStage2.java:135)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage2$RetryingExecutor.maybeRetryExecute(AsyncRetryableStage2.java:152)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage2$RetryingExecutor.lambda$attemptExecute$1(AsyncRetryableStage2.java:113)\r\n        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\r\n        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\r\n        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\r\n        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)\r\n        at software.amazon.awssdk.utils.CompletableFutureUtils.lambda$forwardExceptionTo$0(CompletableFutureUtils.java:78)\r\n        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\r\n        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\r\n        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\r\n        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.lambda$execute$0(MakeAsyncHttpRequestStage.java:108)\r\n        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)\r\n        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)\r\n        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)\r\n        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.completeResponseFuture(MakeAsyncHttpRequestStage.java:255)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.MakeAsyncHttpRequestStage.lambda$executeHttpRequest$3(MakeAsyncHttpRequestStage.java:167)\r\n        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)\r\n        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)\r\n        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:750)\r\nCaused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: executor not accepting a task\r\n        at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)\r\n        at software.amazon.awssdk.core.exception.SdkClientException.create(SdkClientException.java:47)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper2.setLastException(RetryableStageHelper2.java:226)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper2.setLastException(RetryableStageHelper2.java:221)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.AsyncRetryableStage2$RetryingExecutor.maybeRetryExecute(AsyncRetryableStage2.java:151)\r\n        ... 23 more\r\n        Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 1 failure: Unable to execute HTTP request: The connection was closed during the request. The request will usually succeed on a retry, but if it does not: consider disabling any proxies you have configured, enabling debug logging, or performing a TCP dump to identify the root cause. If this is a streaming operation, validate that data is being read or written in a timely manner. Channel Information: ChannelDiagnostics(channel=[id: 0x801baead, L:0.0.0.0/0.0.0.0:59534 ! R:bucket.vpce-0117f6033eaf7aee5-2hxd9fg4.s3.us-west-2.vpce.amazonaws.com/10.80.134.179:443], channelAge=PT0.676S, requestCount=1, responseCount=0, lastIdleDuration=PT0.006284125S)\r\nCaused by: java.lang.IllegalStateException: executor not accepting a task\r\n        at software.amazon.awssdk.thirdparty.io.netty.resolver.AddressResolverGroup.getResolver(AddressResolverGroup.java:61)\r\n        at software.amazon.awssdk.thirdparty.io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:208)\r\n        at software.amazon.awssdk.thirdparty.io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:47)\r\n        at software.amazon.awssdk.thirdparty.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:189)\r\n        at software.amazon.awssdk.thirdparty.io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:175)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105)\r\n        at software.amazon.awssdk.thirdparty.io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\r\n        at software.amazon.awssdk.thirdparty.io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:988)\r\n        at software.amazon.awssdk.thirdparty.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:515)\r\n        at software.amazon.awssdk.thirdparty.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:428)\r\n        at software.amazon.awssdk.thirdparty.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:485)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\r\n        at software.amazon.awssdk.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\r\n        at software.amazon.awssdk.thirdparty.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n        ... 1 more\r\n{code}", "output": "steveloughran opened a new pull request, #7701:\nURL: https://github.com/apache/hadoop/pull/7701\n\n   \r\n   HADOOP-19567.\r\n   \r\n   catch all exception raised in stream close; log at debug\r\n   \r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Surfaced during a cloudstore bandwidth test; manual testing can check.\r\n   \r\n   Hard to write an ITest as it probably depends on the stream state at the time...will have to explore.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Decouple ConsumerConfig and ShareConsumerConfig\nDescription: ShareConsumerConfig and ConsumerConfig are inherent at the moment.\r\nThe drawback is the config logic is mixed, for example: \r\nShareAcknowledgementMode.\r\nWe can decouple to prevent the logic is complicated in the future.\nQ: Feel free to close if this is not necessary.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Producer performance test failing in parsing aggregate statistics\nDescription: When running benchmark test, some of the producer perf tests are failing in statistics parsing. Logs attached\r\n\r\ncmd: `TC_PATHS=\"tests/kafkatest/benchmarks/core/benchmark_test.py\" bash tests/docker/run_tests.sh`\r\n\r\nJDK: `azul/zulu-openjdk:17`(used azul's jdk because with openjdk tests were failing to run)\nQ: [~aheev] Just to check, this should be \"used azul\" not \"used azure\". Just checking so that someone else has a chance to reproduce the issue and fix it.", "output": "Hello [~schofielaj], I think the main reason is that this is a performance test, so it didn’t finish running. In your test result, the last line is `printWindow()` instead of `printTotal()`, which caused the parse error.\r\n\r\nI tested on both the trunk and the [https://github.com/apache/kafka/pull/20385] branch, and all the tests passed. I think this is not a bug."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][BugFix] IsNonEmptyDirectory Check should loop on listing using updated continuation token\nDescription: As part of recent change, loop over listing was added when checking if a directory is empty or not.\r\nContinuation token was not getting updated in subsequent listblob calls leading to infinite loop of list calls.\r\n\r\nCaused by: https://issues.apache.org/jira/browse/HADOOP-19572\nQ: hadoop-yetus commented on PR #7716:\nURL: https://github.com/apache/hadoop/pull/7716#issuecomment-2916874387\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 47s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  5s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 42s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 136m 36s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7716/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7716 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 9f9fe8efdf57 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 10c93ee3528d558bba275188bb6f454a041b79eb |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7716/4/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7716/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "anujmodi2021 commented on PR #7716:\nURL: https://github.com/apache/hadoop/pull/7716#issuecomment-2917205819\n\n   ------------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 813, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 816, Failures: 0, Errors: 0, Skipped: 116\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 655, Failures: 0, Errors: 0, Skipped: 222\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 813, Failures: 0, Errors: 0, Skipped: 175\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 659, Failures: 0, Errors: 0, Skipped: 133\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 652, Failures: 0, Errors: 0, Skipped: 224\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 656, Failures: 0, Errors: 0, Skipped: 146\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 654, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 687, Failures: 0, Errors: 0, Skipped: 171\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 652, Failures: 0, Errors: 0, Skipped: 222\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   Time taken: 375 mins 17 secs."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Managed Identity Token Provider is not implemented\nDescription: Managed Identity Token Provider is not implemented in the hadoop-azure jar. Now, if one wants to use either User Assigned Managed Identity or System Assigned Managed Identity in Azure, this will throw an error because it's not implemented yet.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK17] Do not use Long(long) and similar constructors\nDescription: 'Long(long)' is deprecated since version 9 and marked for removal.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix testRenameFileWithFullQualifiedPath on Windows\nDescription: The Apache Commons Net FTP library is used for FTPFileSystem. In *TestFTPFileSystem#testRenameFileWithFullQualifiedPath()*, the FS operations (such as touch and rename) are made using absolute paths. However, the library expects relative paths.\r\nThis caused *FTPFileSystem#getFileStatus()* to throw FileNotFoundException since the library was trying to look for the absolute path under which the FileSystem was mounted.\r\n\r\nThis worked fine on Linux as it just appended the absolute path under the FileSystem's mount path -\r\n\r\n !image-2025-04-27-23-05-05-212.png! \r\n\r\nHowever, this fails on Windows since suffixing the absolute path under the FileSystem's mount path doesn't yield a valid path due to the drive letter in the absolute path.\r\n\r\nConsider the following illustration -\r\n+On Linux+\r\n{text}\r\npath1 => /mnt/d/a/b\r\npath2 => /mnt/d/x/y\r\npath1 + path2 yields a valid path => /mnt/d/a/b/mnt/d/x/y\r\n{text}\r\n\r\n+On Windows+\r\n{text}\r\npath1 => C:\\a\\b\r\npath2 => C:\\x\\y\r\npath1 + path2 doesn't yield a va\nQ: GauthamBanasandra opened a new pull request, #7654:\nURL: https://github.com/apache/hadoop/pull/7654\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   The Apache Commons Net FTP library is used for `FTPFileSystem`. In `TestFTPFileSystem#testRenameFileWithFullQualifiedPath()`, the FS operations (such as touch and rename) are made using absolute paths. However, the library expects relative paths.\r\n   This caused `FTPFileSystem#getFileStatus()` to throw `FileNotFoundException` since the library was trying to look for the absolute path under which the FileSystem was mounted.\r\n   \r\n   This worked fine on Linux as it just appended the absolute path under the FileSystem's mount path -\r\n   ![image](https://github.com/user-attachments/assets/7ebf1d96-76ca-4388-8549-ee5a4942394f)\r\n   \r\n   However, this fails on Windows since suffixing the absolute path under the FileSystem's mount path doesn't yield a valid path due to the drive letter in the absolute path.\r\n   \r\n   Consider the following illustration -\r\n   ## On Linux\r\n   ```\r\n   path1 => /mnt/d/a/b\r\n   path2 => /mnt/d/x/y\r\n   path1 + path2 yields a valid path => /mnt/d/a/b/mnt/d/x/y\r\n   ```\r\n   \r\n   ## On Windows\r\n   \r\n   ```\r\n   path1 => C:\\a\\b\r\n   path2 => C:\\x\\y\r\n   path1 + path2 doesn't yield a valid path => C:\\a\\b\\C:\\x\\y\r\n   ```\r\n   \r\n   So, to fix this, we need to treat the FS operations as purely relative.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - Ran the unit tests locally.\r\n   - Jenkins CI validation.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7654:\nURL: https://github.com/apache/hadoop/pull/7654#issuecomment-2833646931\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 45s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 36s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 16s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 45s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 36s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 56s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 207m 22s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7654/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7654 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c1bf5de9a12a 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 12929572d9070158a679f0a295fccd3b6a3d1850 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7654/1/testReport/ |\r\n   | Max. process+thread count | 1269 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7654/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump commons-lang3 to 3.18.0 due to CVE-2025-48924\nDescription: https://www.cve.org/CVERecord?id=CVE-2025-48924\r\n\r\nWill update commons-text to 1.14.0 which was released with commons-lang3 3.18.0. Due to https://issues.apache.org/jira/browse/HADOOP-19532 - seems best to upgrade them together.\nQ: hadoop-yetus commented on PR #7970:\nURL: https://github.com/apache/hadoop/pull/7970#issuecomment-3305413646\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 19s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  24m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  14m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 58s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  31m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 23s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  22m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 11s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  11m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  6s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  32m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 963m 33s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1144m 26s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7970 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux e242a07f343d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 82dae1282632826d8977c56821441f5b32ee1ec8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/testReport/ |\r\n   | Max. process+thread count | 4334 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 merged PR #7970:\nURL: https://github.com/apache/hadoop/pull/7970"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Weird behavior on Kafka Connect 4.1 class loading\nDescription: I have the [DebeziumOpenLineageEmitter|https://github.com/debezium/debezium/blob/main/debezium-openlineage/debezium-openlineage-api/src/main/java/io/debezium/openlineage/DebeziumOpenLineageEmitter.java] class in the *debezium-openlineage-api* that internally has a static map to maintain the registered emitter, the key of this map is \"connectoLogicalName-taskid\"\r\nThen there is the [OpenLineage SMT|https://github.com/debezium/debezium/blob/main/debezium-core/src/main/java/io/debezium/transforms/openlineage/OpenLineage.java], which is part of the *debezium-core.* In this SMT, I simply pass the same context to instantiate the same emitter via the connector.\r\n\r\nNow I'm running the following image\r\n{code:java}\r\nFROM quay.io/debezium/connect:3.3.0.Final\r\nENV MAVEN_REPO=\"https://repo1.maven.org/maven2\"\r\nENV GROUP_ID=\"io/debezium\"\r\nENV DEBEZIUM_VERSION=\"3.3.0.Final\"\r\nENV ARTIFACT_ID=\"debezium-openlineage-core\"\r\nENV CLASSIFIER=\"-libs\"\r\nCOPY log4j.properties /kafka/config/log4j.properties\r\n\r\nAdd \nQ: [~gharris] [~snehashisp] WDYT?", "output": "I have not had a chance to inspect the code or reproduce this for myself, but my first impressions:\r\n # I don't think communication via static fields is good practice, and should be avoided if possible. I personally have been on the receiving end of bugs caused by static fields within a single plugin; I also think I personally would not like to debug static fields shared across plugins.\r\n # In deployments which make use of the plugin.version configs, changing the pinned version will necessarily change which instance of a static variable is used. So even if the isolation is bug-free, it is trivial for a user or automated tooling to foot-gun a plugin which uses static fields by changing the plugin.version\r\n # The concern about the backwards-compatibility when plugin.version is unset is valid. We made some attempts to use the connector's class loader or delegating class loader like in the old implementation, so perhaps the version being explicitly injected is not just a cosmetic problem.\r\n\r\nIMHO we should investigate the backwards-compatibility problem and try and make a minimal fix to preserve the old behavior when plugin.version is unset. And the affected plugins should eliminate their usage of static variables in order to be well behaved when the plugin.version is set."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The Apache Commons Net FTP library is used for FTPFileSystem. In *TestFTPFileSystem#testRenameFileWithFullQualifiedPath()*, the FS operations (such as touch and rename) are made using absolute paths. However, the library expects relative paths.\r\nThis caused *FTPFileSystem#getFileStatus()* to throw FileNotFoundException since the library was trying to look for the absolute path under which the FileSystem was mounted.\r\n\r\nThis worked fine on Linux as it just appended the absolute path under the FileSystem's mount path -\r\n\r\n !image-2025-04-27-23-05-05-212.png! \r\n\r\nHowever, this fails on Windows since suffixing the absolute path under the FileSystem's mount path doesn't yield a valid path due to the drive letter in the absolute path.\r\n\r\nConsider the following illustration -\r\n+On Linux+\r\n{text}\r\npath1 => /mnt/d/a/b\r\npath2 => /mnt/d/x/y\r\npath1 + path2 yields a valid path => /mnt/d/a/b/mnt/d/x/y\r\n{text}\r\n\r\n+On Windows+\r\n{text}\r\npath1 => C:\\a\\b\r\npath2 => C:\\x\\y\r\npath1 + path2 doesn't yield a valid path => C:\\a\\b\\C:\\x\\y\r\n{text}\r\n\r\nSo, to fix this, we need to treat the FS operations as purely relative.", "output": "Fix testRenameFileWithFullQualifiedPath on Windows"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add a config entry to make IPC.Client checkAsyncCall off by default\nDescription: Add a config entry to make IPC.Client checkAsyncCall off by default\nQ: hfutatzhanghb commented on PR #7521:\nURL: https://github.com/apache/hadoop/pull/7521#issuecomment-2735176237\n\n   Hi, @KeeProMise , Please help review this PR when you have free time, Thanks ahead.  And i will close HDFS-17758 https://github.com/apache/hadoop/pull/7501 .", "output": "hadoop-yetus commented on PR #7521:\nURL: https://github.com/apache/hadoop/pull/7521#issuecomment-2735299687\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 18s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 47s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 54s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   7m 54s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 38s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7521/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 1 new + 122 unchanged - 1 fixed = 123 total (was 123)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  13m 10s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 120m 28s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7521/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7521 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux 57d7e3c0c399 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3108d17431305b6f84cba6612a395e82e1c552f5 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7521/1/testReport/ |\r\n   | Max. process+thread count | 1262 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7521/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade semanticdb-shared to 4.13.10\nDescription: Ammonite was upgraded to 3.0.3 so semanticdb-shared should be upgraded correspondingly.\r\n\r\nhttps://mvnrepository.com/artifact/com.lihaoyi/ammonite-interp-3.3.6_3.3.6/3.0.3\nQ: Issue resolved in https://github.com/apache/spark/pull/52652", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Failures in the StateUpdater thread may lead to inability to shut down a stream thread\nDescription: If during rebalance a failure occurs in the StateUpdater thread, and this failure leads to the thread shutdown, the Stream thread may get into an infinite wait state during it's shutdown.\r\n\r\nSee the attached test that reproduces the issue.", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Enable substitution for SQLConf settings\nDescription: Values set for custom catalogs are not being substituted:\r\n\r\n{code:java}\r\nspark.sql.catalog.mssql=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog\r\nspark.sql.catalog.mssql.user=${env:MSSQL__USER}\r\n{code}\r\n\r\n\r\nFails with:\r\n{code:java}\r\nspark.sql(\"show tables in mssql\").show()\r\n\r\ncom.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '${env:MSSQL__USER}'\r\n{code}", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Resolve build error caused by missing Checker Framework (NonNull not recognized)\nDescription: In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed.\r\n{code:java}\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[216,50] package org.checkerframework.checker.nullness.qual does not exist\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[2509,30] cannot find symbol\r\n[ERROR]   symbol:   class NonNull\r\n[ERROR]   location: class org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.AsyncThreadFactory\r\n {code}\r\nI checked the usage in the related modules, and we should use *org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.\nQ: slfan1989 opened a new pull request, #8015:\nURL: https://github.com/apache/hadoop/pull/8015\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19717. Resolve build error caused by missing Checker Framework  (NonNull not recognized).\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "pan3793 commented on code in PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#discussion_r2409341008\n\n\n##########\nhadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java:\n##########\n@@ -19,7 +19,7 @@\n package org.apache.hadoop.fs.tosfs.util;\n \n import org.apache.hadoop.util.Preconditions;\n-import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n\nReview Comment:\n   I suspect this makes the `Nullable` useless, I don't think the static analyzer tools can recognize such a relocated annotation."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade libopenssl to 3.5.2-1 needed for rsync\nDescription: The currently used libopenssl-3.1.4-1 is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n\r\nThe Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows.\r\n\r\n{code}\r\n00:25:33 SUCCESS: Specified value was saved.\r\n00:25:46 Removing intermediate container 5ce7355571a1\r\n00:25:46 ---> a13a4bc69545\r\n00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n00:25:46 ---> Running in d2dafad446f9\r\n00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n00:25:54 \u001b[0m\u001b[91mAt line:1 char:1\r\n00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ...\r\n00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.Ht\nQ: GauthamBanasandra opened a new pull request, #7875:\nURL: https://github.com/apache/hadoop/pull/7875\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   The currently used `libopenssl-3.1.4-1` is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n   \r\n   The Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows.\r\n   \r\n   ```\r\n   00:25:33 SUCCESS: Specified value was saved.\r\n   00:25:46 Removing intermediate container 5ce7355571a1\r\n   00:25:46 ---> a13a4bc69545\r\n   00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n   00:25:46 ---> Running in d2dafad446f9\r\n   00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n   00:25:54 \u001b[0m\u001b[91mAt line:1 char:1\r\n   00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ...\r\n   00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n   00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:Htt\r\n   ```\r\n   \r\n   Thus, we need to upgrade to the latest available version to address this.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Jenkins CI - In progress.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7875:\nURL: https://github.com/apache/hadoop/pull/7875#issuecomment-3192521324\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7875/1/console in case of problems."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Exclude `.idea` directory from `Spotless` check\nDescription: \nQ: Issue resolved by pull request 377\n[https://github.com/apache/spark-kubernetes-operator/pull/377]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix the \"6.6 Java Version\" for branch 3.9\nDescription: see https://lists.apache.org/thread/xv04txcy4wwtpkn1zfqxqf95098ls61h", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support logging in Pandas/Arrow UDFs\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade libopenssl to 3.1.2 on Windows\nDescription: The current libopenssl 3.1.1 isn't available for download from the repo.msys2.org website. Hence, we're upgrading to 3.1.2.\nQ: GauthamBanasandra opened a new pull request, #7680:\nURL: https://github.com/apache/hadoop/pull/7680\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   * The current libopenssl 3.1.1 isn't available for download from the repo.msys2.org website.\r\n   * This PR upgrades the same to 3.1.2.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Jenkins CI validation.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7680:\nURL: https://github.com/apache/hadoop/pull/7680#issuecomment-2869042256\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7680/1/console in case of problems."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consumer NoOffsetForPartitionException for partitions being revoked\nDescription: Currently, the consumer attempts to update positions for all partitions it owns that don't have a position, even when the partition may be already being revoked. This could lead to NoOffsetForPartitionException for such partitions (if there is no reset strategy defined). There are 2 main issues around this:\r\n * is probably unneeded work to fetch offsets and update positions for partitions being revoked. At that point we're actually not allowing fetching from there anymore\r\n * throwing NoOffsetForPartitionException may lead applications to take action to set positions themselves, which will most probably fail (the partition will most probably not owned by the consumer anymore since it was already being revoked when the NoOffsetForPartitionException was generated). We've seen this on Kafka Streams, that handled NoOffsetForPartitionException by calling seek to set positions.   \r\n\r\nThis task is to review if there are no other implications I may be missing? Then fix to ensure we don't updat\nQ: Hi [~lianetm]  can i pick this up ?", "output": "This is what i understand , can can try reproducing it through test also\r\n\r\n.  1. Partitions marked for revocation (pendingRevocation = true) are excluded from fetching but NOT from position updates\r\n  2. shouldInitialize() only checks if state is INITIALIZING, ignoring pendingRevocation\r\n  3. hasValidPosition() only checks fetch state, ignoring pendingRevocation\r\n  4. This causes updateFetchPositions() to attempt fetching offsets for partitions being revoked"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is an umbrella JIRA for supporting Java 9 Modularity.", "output": "Support Java Modularity"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hadoop uses a pretty old Junit 5 version, it's a good idea to update to the latest.", "output": "Update Junit 5 version to 5.12.1"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In core-default.xml, the property hadoop.security.crypto.buffer.size is currently documented as “The buffer size used by CryptoInputStream and CryptoOutputStream.” It does not specify the legal value constraints.\r\n{code:java}\r\n  \r\nhadoop.security.crypto.buffer.size  \r\n8192  \r\nThe buffer size used by CryptoInputStream and CryptoOutputStream.  \r\n {code}\r\nThe runtime enforces two hidden constraints that are not documented:\r\n1. Minimum value is 512 bytes. Values below 512 cause IllegalArgumentException at stream construction time.\r\n2. Block-size flooring: The effective buffer size is floored to a multiple of the cipher algorithm’s block size (e.g., 16 bytes for AES/CTR/NoPadding).\r\n\r\nAs a result, users may be surprised that:\r\n1. Setting a value like 4100 results in an actual capacity of 4096.\r\n2. Setting values <512 fails fast with IllegalArgumentException.\r\n\r\n*Expected*\r\ncore-default.xml (and user-facing docs) should explicitly document:\r\n1. Minimum legal value: 512 bytes.\r\n\r\n2. The effective value is floored to the nearest multiple of the cipher algorithm block size (e.g., 16 for AES).", "output": "Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: hadoop-thirdparty build to update maven plugin dependencies\nDescription: github action builds of PRs for hadoopHthirdparty fail because of throttling NVE throttling of requests; needs an update to a later version with either retries or use of a github source cve list.\r\n\r\ndependency checker 11+ \r\n\r\n{code}\r\nMandatory Upgrade Notice\r\nUpgrading to 10.0.2 or later is mandatory\r\n\r\nOlder versions of dependency-check are causing numerous, duplicative requests that end in processing failures are causing unnecassary load on the NVD API. Dependency-check 10.0.2 uses an updated User-Agent header that will allow the NVD to block calls from the older client.\r\n{code}\r\n\r\n----\r\n\r\nThe upgraded dependency checker now *requires* java11+, and *prefers* the provision of an API key for the national vulnerabilities database. It also skips the sonatype check as that no longer supports anonymous checks at all\nQ: 10.x cannot parse the current DB files, even if it manages to download them, [~stevel@apache.org].", "output": "latest pr does it, just needs java11 for that action. And I've turned off the sonatype checking"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update script `free_disk_space_container`\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip `test_collect_time` test if pandas or pyarrow are unavailable\nDescription: \nQ: Issue resolved by pull request 52555\n[https://github.com/apache/spark/pull/52555]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We introduced a regression bug in AK 4.1.0 release, when range scans are use. We are leaking some metrics/sensors.\r\n\r\nThanks to [~eduwerc] and [~hermankj] for reporting this issue.\r\n\r\nAnybody who is using iterator, ie, any type of all/rang/prefix scan etc is affected. Note that certain DSL operators like session-windows, sliding-windows, stream-stream joins, and FK-join all use range scans internally, so the number of affected users is expected to be quite high.", "output": "Memory leak using range scans"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add bswap support for RISC-V\nDescription: *Problem*\r\n\r\nWhile building Hadoop native code on the riscv64 architecture, the build fails due to missing support for bswap operations. The standard implementation relies on compiler intrinsics or platform-specific assembly which are not defined for RISC-V targets.\r\n\r\nThis results in compilation errors such as:\r\n\r\n* Error: unrecognized opcode `bswap a4'\r\n* Error: unrecognized opcode `bswap a3'\r\n\r\n*Resolution*\r\n\r\nAdd bswap support for RISC-V.\nQ: [~stevel@apache.org] ,\r\n\r\nHi, I’d like to help with this issue — may I go ahead and open a PR?", "output": "leiwen2025 opened a new pull request, #7809:\nURL: https://github.com/apache/hadoop/pull/7809\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   While building Hadoop native code on the riscv64 architecture, the build fails due to missing support for bswap operations. The standard implementation relies on compiler intrinsics or platform-specific assembly which are not defined for RISC-V targets.\r\n   \r\n   This results in compilation errors such as:\r\n   ```\r\n   Error: unrecognized opcode `bswap a4'\r\n   Error: unrecognized opcode `bswap a3'\r\n   ```\r\n   Add bswap support for RISC-V can solve the problem.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   mvn package -Pdist,native -DskipTests -Dtar\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Backport KAFKA-19716 fix to 4.0 and 4.1\nDescription: KAFKA-19716\nQ: Does the bug occur in 3.9 as well? If so, it would be goo to backport it to 3.9 since we will have a patch release for that branch", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Seem like on Commit c54bf19 (moving to aws sdk v2) the support of CSE-KMS was dropped or the way to configure it was changed without updating documentation. \r\n\r\n[https://github.com/apache/hadoop/commit/81d90fd65b3c0c7ac66ebae78e22dcb240a0547b#diff-a8dabc9bdb3ac3b04f92eadd1e3d9a7076d8983ca4fb7d1d146a1ac725caa309]\r\n\r\nin file: DefaultS3ClientFactory.java\r\n\r\nfor example code that was removed:\r\n\r\n!image-2025-07-21-18-13-16-765.png!\r\n\r\n \r\nFYI: [~stevel@apache.org]", "output": "S3A: Support of CSE-KMS in version 3.4.1 seems to be broken"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Set `io.netty.noUnsafe` to `true` to avoid JEP-498 warnings\nDescription: *BEFORE*\r\n{code}\r\nStarting Operator...\r\nWARNING: A terminally deprecated method in sun.misc.Unsafe has been called\r\nWARNING: sun.misc.Unsafe::allocateMemory has been called by io.netty.util.internal.PlatformDependent0$2 (file:/opt/spark-operator/operator/spark-kubernetes-operator.jar)\r\nWARNING: Please consider reporting this to the maintainers of class io.netty.util.internal.PlatformDependent0$2\r\nWARNING: sun.misc.Unsafe::allocateMemory will be removed in a future release\r\n25/10/09 03:35:46 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:35:46 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.\r\n{code}\r\n\r\n*AFTER*\r\n{code}\r\nStarting Operator...\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Configuring operator w\nQ: Issue resolved by pull request 378\n[https://github.com/apache/spark-kubernetes-operator/pull/378]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-datajoin.\nDescription: \nQ: hadoop-yetus commented on PR #7618:\nURL: https://github.com/apache/hadoop/pull/7618#issuecomment-2808864846\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 27s |  |  hadoop-datajoin in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  68m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7618/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7618 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 29ec6f5791bb 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / dd8d3f30c5c1e0beeaf093acdf28bc1ec7426c16 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7618/1/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-datajoin U: hadoop-tools/hadoop-datajoin |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7618/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on PR #7618:\nURL: https://github.com/apache/hadoop/pull/7618#issuecomment-2816566946\n\n   @cnauroth Could you please review this PR? Thank you very much!"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: NPE when close() called on uninitialized filesystem\nDescription: code\r\n{code}\r\n  public void testABFSConstructor() throws Throwable {\r\n    new AzureBlobFileSystem().close();\r\n  }\r\n{code}\r\n\r\nstack\r\n{code}\r\n[ERROR] org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor -- Time elapsed: 0.003 s <<< ERROR!\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getClient()\" because \"this.abfsStore\" is null\r\n        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.close(AzureBlobFileSystem.java:800)\r\n        at org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor(TestRuntimeValid.java:49)\r\n{code}\nQ: Thanks for reporting this.\r\nWill work on the patch on priority.", "output": "hadoop-yetus commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3197386039\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 11s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 141m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7880 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 160d07de834c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fd85bee514d271d663f801bf3498b3145ec08fec |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/testReport/ |\r\n   | Max. process+thread count | 530 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Separate the controller config and admin config when add controller\nDescription: Currently, when adding a controller via CLI, we have to run:\r\n{code:java}\r\n$ bin/kafka-metadata-quorum.sh --command-config config/controller.properties --bootstrap-server localhost:9092 add-controller\r\n\r\nor\r\n\r\n$ bin/kafka-metadata-quorum.sh --command-config config/controller.properties --bootstrap-controller localhost:9093 add-controller{code}\r\nThe controller.properties file is expected to be the controller property file that to be added. But if we want to pass configs to the admin client, what can we do?\r\n{code:java}\r\nbin/kafka-metadata-quorum.sh --help\r\nusage: kafka-metadata-quorum [-h] [--command-config COMMAND_CONFIG] (--bootstrap-server BOOTSTRAP_SERVER | --bootstrap-controller BOOTSTRAP_CONTROLLER)\r\n                             {describe,add-controller,remove-controller} ...This tool describes kraft metadata quorum status.\r\n\r\n...\r\n\r\n --command-config COMMAND_CONFIG\r\n   Property file containing configs to be passed to Admin Client.  For add-controller,  the file is used to specify\nQ: Improve the doc first: https://github.com/apache/kafka/pull/20261", "output": "Hi [~showuon], if you are not working on this, may i take it. Thanks."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Wrong generic type for UnregisterBrokerOptions\nDescription: This UnregisterBrokerOptions extends from the UpdateFeaturesOptions, which is obviously wrong.\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/UnregisterBrokerOptions.java#L23]\r\n\r\n \r\n\r\nAnd that's why in KafkaAdminClientTest.java, we set timeout via `options.timeoutMs = 10;`. We should set it via timeoutMs(int) method gracefully like other tests.\nQ: [~showuon], can I take over this?", "output": "Thank you, [~brandboat] !"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add support to track `spark_conf` at the dataset level\nDescription: We need to add {{sql_confs}} to {{DefineOutput}} in SDP to support following use case\r\n\r\nSQL file #1\r\n{code:java}\r\nSET some_conf = some_value;\r\nCREATE STREAMING TABLE st; {code}\r\nSQL file #2\r\nCREATE FLOW INSERT INTO st FROM ....\r\n \r\nSince we don't store {{sql_confs}} at the Dataset level, the conf would be ignored. We should store it at dataset level and will merge the {{sql_conf}} stored at the dataset level to its corresponding flow when we construct the DataflowGraph.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Introduce row checksum creation and verification for state store, both {*}HDFS and RocksDB state store{*}. This will help detect corruption at the row level and help prevent corruption.", "output": "State Store Row Checksum implementation"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add a Kafka client parameter to limit number of messages fetched\nDescription: h3. Description\r\n\r\nCurrently, Kafka fetch requests only support limiting the total size of messages fetched ({{{}fetch.max.bytes{}}}) and the size per partition ({{{}max.partition.fetch.bytes{}}}). However, there is no way to limit the *number of messages* fetched per request—neither globally nor on a per-partition basis.\r\n\r\nWhile Kafka was originally designed as a high-throughput distributed messaging platform and has traditionally focused more on throughput than individual message control, its role has since evolved. Kafka is now not only a leading message queue but also a core component in modern {*}data pipelines and stream processing frameworks{*}.\r\n\r\nIn these newer use cases, especially for downstream services and streaming applications, *rate-limiting by message count* is a common requirement.\r\n\r\nCurrently, the workaround is for clients to {*}fetch a batch of messages, manually truncate them based on count, and then adjust offsets manually{*}, which is inefficient, error-prone, \nQ: To Federico Valeri: Thanks for the feedback. Yes, I will start drafting a KIP for this proposal.", "output": "[~corkitse] Thanks for the proposal. I will review the KIP once drafted. However, just highlighting so you can reference, ShareGroups/QueuesForKafka already have that in request spec - [MaxRecords|https://github.com/apache/kafka/blob/abbb6b3c13f0b87b8e905f853b7b94282f8d355c/clients/src/main/resources/common/message/ShareFetchRequest.json#L39]."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "During broker restarts, the topic-based RemoteLogMetadataManager constructs the state by reading the internal {{__remote_log_metadata}} topic. When the partition is not ready to perform remote storage operations, then [ReplicaNotAvailableException|https://sourcegraph.com/github.com/apache/kafka/-/blob/storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemotePartitionMetadataStore.java?L127] thrown back to the consumer. The clients retries the request immediately. \r\n\r\nThis result in lot of FetchConsumer requests on the broker and utilizes the request handler threads. Using CountdownLatch the frequency of ReplicaNotAvailableException thrown back to the clients can be reduced. This will improve the request handler thread usage on the broker.\r\n\r\nReproducer: \r\n# Standalone one node cluster with LocalTieredStorage setup. \r\n# Create a topic with remote storage enabled. RF = 1 and partitionCount = 2\r\n# Produce few message and ensure that the segments are uploaded to remote storage. \r\n# Use console-consumer to read the produced messages from the beginning of the topic.\r\n# Update [RemoteLogMetadataPartitionStore|https://sourcegraph.com/github.com/apache/kafka/-/blob/storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemotePartitionMetadataStore.java?L166] to micmic that the partition is not ready.\r\n# Replace the kafka-storage module jar and restart the broker. \r\n# Start the console-consumer to read from the beginning of the topic.  \r\n\r\n~9K FetchConsumer requests per second are received on the broker for one consumer [1107088 / (60 * 2) = ~9225 requests / sec per partition]:\r\n{code:java}\r\n% sh kafka-topics.sh --bootstrap-server localhost:9092  --topic apple --replication-factor 1 --partitions 2 --create  --config segment.bytes=1048576 --config local.retention.ms=60000 --config remote.storage.enable=true\r\n% sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic apple --from-beginning --property print.key=false --prop", "output": "Reduce the frequency of ReplicaNotAvailableException thrown to clients when RLMM is not ready"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: JVM GC Metrics supports fine-grained metrics\nDescription: See [https://bugs.openjdk.org/browse/JDK-8307059] , The Generational ZGC separate the Cycles and Pauses as Minor and Major,like this:\r\n * Old ZGC: \"ZGC Cycles\", \"ZGC Pauses\"\r\n * Generational ZGC: \"ZGC Minor Cycles\", \"ZGC Minor Pauses\", \"ZGC Major Cycles\", \"ZGC Major Pauses\"\r\n\r\nlet us separate it same and give 2 new metric about these, may be better...\r\n\r\n-----------------------------------------------------------------------------------------\r\n\r\nImpl to support get minor/major GC time/count from Hadoop Jmx", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add golden files for Analyzer edge cases\nDescription: New golden files for edge-cases discovered during the Analyzer support and development.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix deadlock in Observation\nDescription: Observation class has been evolved a few times during Spark 3.5 to Spark 4.0.0. Previously it uses locking mechanism (synchronized) between get and onFinish methods to coordinate metrics update and retrieval.\r\n\r\nBut it has a potential deadlocking bug. If get is called before ObservationListener is triggered to call onFinish, get will forever be waiting for metrics because it locks the observation object by synchronized so later onFinish call is locked out from updating the metrics.\r\n\r\nThis locking mechanism was replaced by a promise by SPARK-49423 which is a large refactoring on the observation feature. But in the PR, I don’t see the deadlock bug was mentioned, and there is no bug fix PR proposed to earlier versions. So I think that the bug was not known and the fix is unintentional in Spark 4.0.0. The bug is still in Spark 3.5 branch.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Enhance performance of ABFS driver for write-heavy workloads\nDescription: The goal of this work item is to enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes.\nQ: anmolanmol1234 opened a new pull request, #7669:\nURL: https://github.com/apache/hadoop/pull/7669\n\n   Enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes. \r\n   \r\n   ![{05B55BCA-EF1F-496D-B1ED-17DCD394DDA1}](https://github.com/user-attachments/assets/5ebd5ad7-51db-4028-812f-ce9da9266984)\r\n   \r\n   \r\n   The proposed design advocates for a centralized `WriteThreadPoolSizeManager` class to handle the collective thread allocation required for all write operations across the system, replacing the current CachedThreadPool in AzureBlobFileSystemStore. This centralized approach ensures that the initial thread pool size is set at `4 * number of available processors` and dynamically adjusts the pool size based on the system's current CPU utilization. This adaptive scaling and descaling mechanism optimizes resource usage and responsiveness. Moreover, this shared thread pool is accessible and utilized by all output streams, streamlining resource management and promoting efficient concurrency across write operations.", "output": "anmolanmol1234 commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-2848521492\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 798, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 801, Failures: 0, Errors: 0, Skipped: 117\r\n   [ERROR] Tests run: 146, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 640, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 798, Failures: 0, Errors: 0, Skipped: 171\r\n   [WARNING] Tests run: 132, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 643, Failures: 0, Errors: 0, Skipped: 144\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 637, Failures: 0, Errors: 0, Skipped: 217\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [ERROR] Tests run: 640, Failures: 0, Errors: 0, Skipped: 146\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [ERROR] Tests run: 638, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 132, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [ERROR] Tests run: 672, Failures: 0, Errors: 0, Skipped: 167\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 637, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: setting --no-initial-controllers flag should not validate kraft version against metadata version\nDescription: Just because a controller node sets `--no-initial-controllers` flag does not mean it is necessarily running kraft.version=1. The more precise meaning is that the controller node does not know what kraft version the cluster should be in.\r\n\r\nIt is a valid configuration to run a static quorum defined by `controller.quorum.voters` but have all the controllers format with `--no-initial-controllers`.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution:\r\n{code:java}\r\njava.lang.IllegalStateException: Failed to set environment variable\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:120)\tat org.apache.hadoop.fs.tosfs.object.ObjectStorageTestBase.setUp(ObjectStorageTestBase.java:59)\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.util.Map java.util.Collections$UnmodifiableMap.m accessible: module java.base does not \"opens java.util\" to unnamed module @69eee410\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)\tat java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)\tat java.base/java.lang.reflect.Field.setAccessible(Field.java:172)\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:116)\t... 4 more\r\n {code}\r\nThis error occurs due to the module system restrictions in JDK 17, where reflection cannot access private fields in the java.util.Collections$UnmodifiableMap class.\r\n\r\n \r\n\r\nTo resolve this issue, JDK 17 compile options have been added to ensure the maven-surefire-plugin works correctly in a JDK 17 environment. This PR adds the necessary compile options for maven-surefire-plugin to support JDK 17, fixing the error and ensuring that unit tests can run smoothly.", "output": "Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Factor out streaming tests from `spark-sql` and `spark-connect`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add built-in TTL (Time-to-Live) support for Kafka Streams State Stores\nDescription: In business cases Kafka Streams users frequently need *per-key Time-To-Live (TTL)* behavior for state stores such as keyValueStore or kTables , typically to model cache-like or deduplication scenarios.\r\n\r\nToday, achieving this requires *manual handling* using:\r\n * Custom timestamp tracking per key,\r\n\r\n * Punctuators to periodically scan and remove expired entries, and\r\n\r\n * Manual emission of tombstones to maintain changelog consistency.\r\n\r\nThese workarounds are:\r\n * {*}Inconsistent across applications{*}, and\r\n\r\n * {*}Operationally costly{*}, as each developer must reimplement the same logic.\r\n\r\n*Proposal* is to introduce a *built-in TTL mechanism* for Kafka Streams state stores, allowing automatic expiration of records after a configured duration.\r\n\r\nIntroduction to new Api's like : \r\n\r\nStoreBuilder withTTL(Duration ttl);\r\nMaterialized withTtl(Duration ttl);\r\n\r\nWhen configured:\r\n * Each record’s timestamp (from event-time or processing-time) is tracked.\r\n\r\n * Expired keys are automatically evicted by a background task (via ProcessorContext.Schedule()).\r\n\r\n * Corresponding tombstones are flushed to changelog.\r\n\r\nThis feature can provide a *TTL abstraction* that simplifies common use cases as:\r\n * Maintaining cache-like state (e.g., last-seen values with limited lifespan)\r\n\r\n * Automatically purging inactive or stale keys without manual cleanup.\r\n\r\nPoints of Risk and Benifits i considered it can bring : \r\n * Consistency as automatic changelog tombstones will preserve correctn", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: CLONE - Broker Startup: Handle Checkpoint Creation Failure via logDirFailureChannel\nDescription: In our production environment, we encountered a scenario where a broker failed to start due to checkpoint creation failure on a single disk (caused by disk corruption or filesystem errors). According to Kafka's design, such disk-level failures should be isolated via {{{}logDirFailureChannel{}}}, allowing other healthy disks to continue serving traffic. However, upon reviewing the {{CheckpointFileWithFailureHandler}} implementation, we observed that while methods like {{{}write{}}}, {{{}read{}}}, and {{writeIfDirExists}} handle {{IOException}} by routing the affected {{log.dir}} to {{{}logDirFailureChannel{}}}, the checkpoint initialization process lacks this fault-tolerant behavior. Should checkpoint creation adopt the same failure-handling logic? If this is not an intentional design, I will submit a PR to fix this issue.", "output": "Closed"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Debian 10 to 11 in build env Dockerfile\nDescription: Debian 10 EOL, and the apt repo is unavailable\r\n{code:bash}\r\ndocker run --rm -it debian:10 bash\r\nroot@bc2a4c509cb3:/# apt update\r\nIgn:1 http://deb.debian.org/debian buster InRelease\r\nIgn:2 http://deb.debian.org/debian-security buster/updates InRelease\r\nIgn:3 http://deb.debian.org/debian buster-updates InRelease\r\nErr:4 http://deb.debian.org/debian buster Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:5 http://deb.debian.org/debian-security buster/updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:6 http://deb.debian.org/debian buster-updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nReading package lists... Done\r\nE: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian-security buster/updates \nQ: pan3793 commented on code in PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#discussion_r2299802155\n\n\n##########\nstart-build-env.sh:\n##########\n@@ -93,7 +93,7 @@ RUN userdel -r \\$(getent passwd ${USER_ID} | cut -d: -f1) 2>/dev/null || :\n RUN groupadd --non-unique -g ${GROUP_ID} ${USER_NAME}\n RUN useradd -g ${GROUP_ID} -u ${USER_ID} -k /root -m ${USER_NAME} -d \"${DOCKER_HOME_DIR}\"\n RUN echo \"${USER_NAME} ALL=NOPASSWD: ALL\" > \"/etc/sudoers.d/hadoop-build-${USER_ID}\"\n-ENV HOME \"${DOCKER_HOME_DIR}\"\n+ENV HOME=\"${DOCKER_HOME_DIR}\"\n\nReview Comment:\n   update because\r\n   ```\r\n    1 warning found (use docker --debug to expand):\r\n    - LegacyKeyValueFormat: \"ENV key=value\" should be used instead of legacy \"ENV key value\" format (line 7)\r\n   ```", "output": "slfan1989 commented on PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#issuecomment-3223070232\n\n   LGTM."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "After HADOOP-19278 , The S3N folder marker *_$folder$* is not skipped during listing of S3 directories leading to S3A filesystem not able to read data written by legacy Hadoop S3N filesystem and AWS EMR's EMRFS (S3 filesystem) leading to compatibility issues and possible migration risks to S3A filesystem.", "output": "S3A: Restore Compatibility with EMRFS FileSystem"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Optimizing the null check logic of Lists#addAll method\nDescription: Optimizing the null check logic of Lists#addAll method\nQ: hadoop-yetus commented on PR #7361:\nURL: https://github.com/apache/hadoop/pull/7361#issuecomment-2639686269\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  19m 30s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  17m 25s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 15s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 30s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  18m 47s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  18m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 15s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  javac  |  17m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 58s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   5m 21s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  2s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 219m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7361/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7361 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5b41c598efe9 5.15.0-125-generic #135-Ubuntu SMP Fri Sep 27 13:53:58 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 19fa2b5bc78eff24363161d437d7ffc1ae44f9ec |\r\n   | Default Java | Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7361/1/testReport/ |\r\n   | Max. process+thread count | 586 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7361/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hfutatzhanghb commented on PR #7361:\nURL: https://github.com/apache/hadoop/pull/7361#issuecomment-2639789627\n\n   @Hexiaoqiao @haiyang1987   Sir, could you please review this simple modification when you have free time? Thanks a lot."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade to wildfly version with support for openssl 3\nDescription: Wildfly 2.1.4\r\n* doesn't work with openssl 3 (that symbol change...why did they do that?)\r\n\r\n\r\nwe need a version with \r\nhttps://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d\r\n\r\n2.2.5.Final does the openssl 3 support.\nQ: steveloughran commented on PR #8019:\nURL: https://github.com/apache/hadoop/pull/8019#issuecomment-3381330281\n\n   the test which is parameterized on ssl (and storediag when a store is forced to OpenSSL)\r\n   ```\r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractSeek.testReadFullyZeroBytebufferPastEOF", "output": "steveloughran commented on PR #8019:\nURL: https://github.com/apache/hadoop/pull/8019#issuecomment-3381381104\n\n   s3a tests all good, s3 london `-Dparallel-tests -DtestsThreadCount=8`"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `protobuf-java` to 4.33.0\nDescription: \nQ: Issue resolved by pull request 52660\n[https://github.com/apache/spark/pull/52660]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add a Kafka client parameter to limit number of messages fetched\nDescription: h3. Description\r\n\r\nCurrently, Kafka fetch requests only support limiting the total size of messages fetched ({{{}fetch.max.bytes{}}}) and the size per partition ({{{}max.partition.fetch.bytes{}}}). However, there is no way to limit the *number of messages* fetched per request—neither globally nor on a per-partition basis.\r\n\r\nWhile Kafka was originally designed as a high-throughput distributed messaging platform and has traditionally focused more on throughput than individual message control, its role has since evolved. Kafka is now not only a leading message queue but also a core component in modern {*}data pipelines and stream processing frameworks{*}.\r\n\r\nIn these newer use cases, especially for downstream services and streaming applications, *rate-limiting by message count* is a common requirement.\r\n\r\nCurrently, the workaround is for clients to {*}fetch a batch of messages, manually truncate them based on count, and then adjust offsets manually{*}, which is inefficient, error-prone, and significantly reduces throughput. In practice, this forces developers to use external tools such as Redis to implement additional buffering or rate control mechanisms—adding complexity and overhead.\r\n\r\nAdding *native support* for a message count limit in fetch requests would offer the following benefits:\r\nh3. Benefits\r\n # {*}Make Kafka a more mature and production-ready stream processing platform{*}, by supporting more granular rate-limiting use cases.\r\n\r\n # *Improve overall system throughpu", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add PipelineAnalysisContext message to support pipeline analysis during Spark Connect query execution\nDescription: When a user invokes an API like `spark.sql` inside a query function that triggers analysis, we want to include information that allows us to do SDP-specific special handling. Adding a PipelineAnalysisContext will allow us to include information along with related RPCs so that the server knows to do this special handling.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade\nDescription: After https://issues.apache.org/jira/browse/HADOOP-19425\r\n\r\nmost of the integration tests are getting skipped. All tests need to be fixed with this PR\nQ: anujmodi2021 opened a new pull request, #7868:\nURL: https://github.com/apache/hadoop/pull/7868\n\n   ### Description of PR\r\n   After https://issues.apache.org/jira/browse/HADOOP-19425\r\n   most of the integration tests are getting skipped. All tests need to be fixed with this PR \r\n   \r\n   ### How was this patch tested?\r\n   Test Suite ran.", "output": "Copilot commented on code in PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#discussion_r2272827582\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAppend.java:\n##########\n@@ -608,12 +608,12 @@ public void testCreateExplicitDirectoryOverDfsAppendOverBlob()\n    **/\n   @Test\n   public void testRecreateAppendAndFlush() throws IOException {\n+      assumeThat(isAppendBlobEnabled()).as(\"Not valid for APPEND BLOB\").isFalse();\n+      assumeThat(getIngressServiceType()).isEqualTo(AbfsServiceType.BLOB);\n\nReview Comment:\n   This assumption is placed after the assertThrows() call begins, but should be before it. The assumption should be checked before setting up the exception assertion to ensure the test conditions are met first.\n   ```suggestion\n         assumeThat(getIngressServiceType()).isEqualTo(AbfsServiceType.BLOB);\n   ```"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update streams documentation with KIP-1147 changes\nDescription: docs/streams/quickstart.html and docsk/streams/developer-guide/datatypes.html both need to be updated to change the flags for kafka-console-consumer.sh to rename --property to --formatter-property.\nQ: Hi [~schofielaj], I've assigned this ticket to myself. If someone else is already handling it, feel free to reassign.", "output": "Hi [~schofielaj] , should I update the docs from `--property` to `--reader-property` for consoleProducer as mentioned in https://github.com/apache/kafka/pull/20554#issuecomment-3308459767 , or would you like to do it in another ticket?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Share consumer changes to support renew ack.\nDescription: \nQ: [~sushmahajn]  can i pick this ?", "output": "[~goyarpit] I'm afraid the work on this issue is already in progress so I'm going to take it. If we get some subtasks, you can certainly help."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hadoop trunk today mostly supports JDK17, but doesn't work at all on JDK23. (and conversely on JDK24 to be released in less than two weeks)\r\n\r\nWhile there are many smaller issues, the major breaking change is the SecurityManager removal (JEP411/486), and its many consequences.\r\n\r\nThe obvious change is that Subjec.doAs() and Subject.current() no longer work by default, and the replacement APIs must be used.\r\n\r\nThe more insidius change is that when SecurityManager is disabled then JDK22+ does not propapage the Subject to new Threads, which is something that Hadoop absolutely relies on.\r\n\r\n\r\nNote that Hadoop is always built with with JDK 17  (if the JDK is 17 or newer), unless the target version is specifically overriden.\r\nThis is not a problem, JDK17 class files running on a JDK 24 JVM is the expected use case for binary distributions.\r\n\r\nWe may want to run some tests where Hadoop is also compiled for the lastest JVM later. (taget Java 24 + JVM 24)", "output": "Fully Support Java 23, 24 and 25"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A Analytics-Accelerator: Add support SSE-C\nDescription: Pass down SSE-C key to AAL so it can be attached while making the GET request.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Analytics accelerator for S3 to be enabled by default\nDescription: Make \"analytics\" the default input stream in S3A. \r\n\r\nGoals\r\n* Parquet performance through applications running queries over the data (spark etc)\r\n* Performance for other formats good as/better than today. Examples: avro manifests in iceberg, ORC in hive/spark\r\n* Performance for other uses as good as today (whole-file/sequential reads of parquet data in distcp etc)\r\n* better resilience to bad uses (incomplete reads not retaining http streams, buffer allocations on long-retained data)\r\n* efficient on applications like Impala, which caches parquet footers itself, and uses unbuffer() to discard all stream-side resources. Maybe just throw alway all state on unbuffer() and stop trying to be sophisticated, or support some new openFile flag which can be used to disable footer parsing\nQ: ahmarsuhail opened a new pull request, #7776:\nURL: https://github.com/apache/hadoop/pull/7776\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Draft PR to make turn AAL default ON. \r\n   \r\n   Prerequisite PR's: \r\n   \r\n   * readVectored(): https://github.com/apache/hadoop/pull/7720\r\n   * auditing: https://github.com/apache/hadoop/pull/7723\r\n   * SSE-C: https://github.com/apache/hadoop/pull/7738\r\n   * IoStats: https://github.com/apache/hadoop/pull/7763 [WiP] \r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7776:\nURL: https://github.com/apache/hadoop/pull/7776#issuecomment-3027248675\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  2s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 11s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 13s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 13s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-aws in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-aws in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 12s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) |  hadoop-tools/hadoop-aws: The patch generated 18 new + 7 unchanged - 0 fixed = 25 total (was 7)  |\r\n   | -1 :x: |  mvnsite  |   0m 15s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 12s | [/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 9 new + 0 unchanged - 0 fixed = 9 total (was 0)  |\r\n   | -1 :x: |  spotbugs  |   0m 12s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 15s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  75m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7776 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2e0d17a78cf4 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 198da21540a93c15c514e3fc013ebb843caf56c9 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/testReport/ |\r\n   | Max. process+thread count | 557 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In the consumer, we invoke the consumer rebalance {{onPartitionRevoked}} or {{onPartitionLost}} callbacks, when the consumer closes. The point is that the application may want to commit, or wipe the state if we are closing unsuccessfully.\r\n\r\nIn the {{{}StreamsRebalanceListener{}}}, we did not implement this behavior, which means when closing the consumer we may lose some progress, and in the worst case also miss that we have to wipe our local state state since we got fenced.\r\n\r\nWe should implement something like the {{ConsumerRebalanceListenerInvoker}} and invoke it in {{{}Consumer.close{}}}.", "output": "StreamsRebalanceListener is not triggered on Consumer.close()"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Support WebIdentityTokenFileCredentialsProvider\nDescription: The current default s3 credential provider chain is set in the order of \r\n{code:java}\r\norg.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,software.amazon.awssdk.auth.credentials.EnvironmentVariableCredentialsProvider,org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider{code}\r\nRefer [code ref |https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml#L1450]for more details.\r\n\r\n \r\n\r\nThis works perfectly fine when used in AWS EC2, EMR Serverless, but not with AWS EKS pods.\r\n\r\n \r\n\r\nFor EKS pods, It is recommended to use\r\n{code:java}\r\nsoftware.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider , software.amazon.awssdk.auth.credentials.ContainerCredentialsProvider (PodIdentity is enabled){code}\r\nWebIdentityTokenFileCredentialsProvider is an AWS credentials provider that enables applications to obtain temporary AWS credentials by assuming an IAM rol\nQ: shameersss1 opened a new pull request, #7802:\nURL: https://github.com/apache/hadoop/pull/7802\n\n   ### Description of PR\r\n   \r\n   Add WebIdentityTokenFileCredentialsProvider to default S3 credential provider chain to support Amazon EKS / K8s deployment\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   * Tested in us-east-1 by running Unit tests and Integration tests\r\n   * Tested in Amazon EC2 , Amazon EKS / K8s , Amazon Container / EMR Serverless\r\n   \r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "steveloughran commented on code in PR #7802:\nURL: https://github.com/apache/hadoop/pull/7802#discussion_r2205569618\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSCredentialProviderList.java:\n##########\n@@ -197,7 +197,17 @@ public AwsCredentials resolveCredentials() {\n       } catch (SdkException e) {\n         lastException = e;\n         LOG.debug(\"No credentials provided by {}: {}\",\n-            provider, e.toString(), e);\n+            provider, e);\n\nReview Comment:\n   seems good\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/CredentialProviderListFactory.java:\n##########\n@@ -184,6 +198,8 @@ private static Map initCredentialProvidersMap() {\n         EC2_IAM_CREDENTIALS_V2);\n     v1v2CredentialProviderMap.put(ENVIRONMENT_CREDENTIALS_V1,\n         ENVIRONMENT_CREDENTIALS_V2);\n+    v1v2CredentialProviderMap.put(WEB_IDENTITY_CREDENTIALS_V1,\n\nReview Comment:\n   no need to care about v1 to v2 migration; this wasn't something in use. if it was, we'd have had complaints about"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The current libopenssl 3.1.1 isn't available for download from the repo.msys2.org website. Hence, we're upgrading to 3.1.2.", "output": "Upgrade libopenssl to 3.1.2 on Windows"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add Support for Propagating Access Token via RPC Header in HDFS\nDescription: *Description:*\r\nTo support modern authentication models (e.g., bearer tokens, OAuth2), we propose adding support in HDFS to propagate an access token via the RPC request header. This enables downstream services (e.g., NameNode, Router) to validate access tokens in a secure and standardized way.\r\n\r\nThe token will be passed in a dedicated field in the {{{}RpcRequestHeaderProto{}}}, mimicking the behavior of an HTTP {{Authorization: Bearer }} header. The caller context or UGI may extract this token and use it for authorization decisions or auditing.\r\n\r\n*Benefits:*\r\n * Enables secure, token-based authentication in multi-tenant environments\r\n\r\n * Lays the foundation for fine-grained, per-request authorization\r\n\r\n*Scope:*\r\n * Add optional {{authorization_token}} field to RPC header\r\n\r\n * Ensure token is thread-local or caller-context scoped\r\n\r\n * Wire it through relevant client and server code paths\r\n\r\n * Provide configuration to enable/disable this feature\r\n\r\n*Notes:*\r\nThis feature is inten\nQ: mccormickt12 opened a new pull request, #7844:\nURL: https://github.com/apache/hadoop/pull/7844\n\n   Add a new auth header to the rpc header proto for access token support. This should support different access tokens within the same connection.\r\n   \r\n   Contributed-by: Tom McCormick \r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7844:\nURL: https://github.com/apache/hadoop/pull/7844#issuecomment-3137884160\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  10m  7s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  buf  |   0m  0s |  |  buf was not available.  |\r\n   | +0 :ok: |  buf  |   0m  0s |  |  buf was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ branch-3.3 Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  17m 47s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 41s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  compile  |  19m 58s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  checkstyle  |   2m 58s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 10s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 34s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  spotbugs  |   5m 55s |  |  branch-3.3 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   0m 31s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   1m  2s | [/patch-mvninstall-hadoop-hdfs-project_hadoop-hdfs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-mvninstall-hadoop-hdfs-project_hadoop-hdfs.txt) |  hadoop-hdfs in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 55s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  cc  |   0m 55s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javac  |   0m 55s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   2m 31s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 14 new + 171 unchanged - 0 fixed = 185 total (was 171)  |\r\n   | -1 :x: |  mvnsite  |   0m 36s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  mvnsite  |   1m  5s | [/patch-mvnsite-hadoop-hdfs-project_hadoop-hdfs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-mvnsite-hadoop-hdfs-project_hadoop-hdfs.txt) |  hadoop-hdfs in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   1m 44s |  |  the patch passed  |\r\n   | -1 :x: |  spotbugs  |   0m 33s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-spotbugs-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   1m  2s | [/patch-spotbugs-hadoop-hdfs-project_hadoop-hdfs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-spotbugs-hadoop-hdfs-project_hadoop-hdfs.txt) |  hadoop-hdfs in the patch failed.  |\r\n   | -1 :x: |  shadedclient  |  13m  2s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 33s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  unit  |   1m  5s | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt) |  hadoop-hdfs in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 31s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 158m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7844 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat |\r\n   | uname | Linux 529b4c69fed7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.3 / 86e1950d32a832764d99afb7d16f90635b45973b |\r\n   | Default Java | Private Build-1.8.0_362-8u372-ga~us1-0ubuntu1~18.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/testReport/ |\r\n   | Max. process+thread count | 557 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7844/1/console |\r\n   | versions | git=2.17.1 maven=3.6.0 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade to netty 4.1.118 due to CVE-2025-24970\nDescription: https://github.com/advisories/GHSA-4g8c-wm8x-jfhw\nQ: hadoop-yetus commented on PR #7413:\nURL: https://github.com/apache/hadoop/pull/7413#issuecomment-2672872624\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  17m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  3s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  33m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 29s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 41s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  50m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 42s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  30m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 39s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  18m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 21s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 44s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  51m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  | 394m 40s |  |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 686m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7413/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7413 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux 43c8b5ca518e 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4b03a24469f9bcc0e16dd73ccaeae07fe75c12ba |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7413/1/testReport/ |\r\n   | Max. process+thread count | 3416 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7413/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 merged PR #7413:\nURL: https://github.com/apache/hadoop/pull/7413"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to upgrade the project’s Java compatibility. This pipeline will serve as the foundation for testing and validating future JDK 17 support and ensuring smooth integration with existing CI/CD processes.", "output": "[JDK17] Set Up CI Support JDK17 & JDK21"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix time unit mismatch in method updateDeferredMetrics\nDescription: Fix time unit mismatch in method updateDeferredMetrics.\r\n\r\nThe time unit of passed param is nanos. But rpcmetrics's is mills.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Release hadoop-thirdparty 1.5.0\nDescription: Release hadoop-thirdparty 1.5.0", "output": "In Progress"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use empty schema when altering a view which is not Hive compatible\nDescription: Spark attempts to save views in a Hive-compatible format and only sets the schema to empty if the save operation fails.\r\n\r\nHowever, due to certain Hive compatibility issues, the save operation may succeed while subsequent read operations fail. This issue arises after the change introduced in SPARK-46934, which removed the {{verifyColumnDataType}} check during the {{ALTER TABLE}} operation.\nQ: Issue resolved by pull request 52730\n[https://github.com/apache/spark/pull/52730]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see：https://github.com/apache/kafka/pull/20543#discussion_r2371009016", "output": "Remove addMetric(MetricName metricName, Measurable measurable) method."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Controller keeps switching and occasionally goes offline.\nDescription: Inter-cluster communication is normal without packet loss, and the cluster is properly configured.\r\nThe Kafka server continuously prints the following logs:\r\n{code:java}\r\n[2025-08-25 19:08:55,581] INFO [RaftManager id=1] Become candidate due to fetch timeout (org.apache.kafka.raft.KafkaRaftClient)\r\n[2025-08-25 19:08:55,686] INFO [RaftManager id=1] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)\r\n[2025-08-25 19:08:55,686] INFO [RaftManager id=1] Cancelled in-flight FETCH request with correlation id 128927 due to node 2 being disconnected (elapsed time since creation: 5147ms, elapsed time since send: 5146ms, throttle time: 0ms, request timeout: 5000ms) (org.apache.kafka.clients.NetworkClient)\r\n[2025-08-25 19:09:33,274] INFO [NodeToControllerChannelManager id=1 name=heartbeat] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)\r\n[2025-08-25 19:09:33,274] INFO [NodeToControllerChannelManager id=1 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 871 due to node 3 being disconnected (elapsed time since creation: 4004ms, elapsed time since send: 4004ms, throttle time: 0ms, request timeout: 4000ms) (org.apache.kafka.clients.NetworkClient)\r\n[2025-08-25 19:09:33,807] INFO [RaftManager id=1] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)\r\n[2025-08-25 19:09:33,807] INFO [RaftManager id=1] Cancelled in-flight FETCH request with corre", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Problem Statement:* Currently, Kafka administrators have no way to validate server configuration files without actually starting the Kafka broker. This leads to:\r\n * Wasted time during deployments when configuration errors are discovered only at startup\r\n * Potential service disruptions in production environments\r\n * Difficulty in CI/CD pipelines to validate Kafka configurations before deployment\r\n * No quick way to test configuration changes without full broker startup overhead\r\n * *Critical cluster stability issues during rolling restarts* - misconfigured brokers can cause:\r\n ** Partition leadership imbalances\r\n ** Replication factor violations\r\n ** Network connectivity issues between brokers\r\n ** Data consistency problems\r\n ** Cascading failures across the cluster when multiple brokers restart with incompatible configurations\r\n\r\n\r\n*Proposed Solution:* Add a {*}--check-config{*}{{{}{}}} command-line option to the Kafka server startup script that would:\r\n\r\n \r\n * Parse and validate the server configuration file\r\n * Check for common configuration errors and inconsistencies\r\n * Validate property values and ranges\r\n * *Detect configuration incompatibilities that could affect cluster operations*\r\n * Support property overrides for testing different configurations\r\n * Exit with appropriate status codes (0 for valid config, non-zero for errors)\r\n * Provide clear error messages for invalid configurations\r\n\r\n*Usage Example:*\r\n{code:java}\r\n# Validate default server.properties\r\nkafka-server-start.sh --check-config config/server.properties\r\n\r\n# Validate with property overrides\r\nkafka-server-start.sh --check-config config/server.properties --override broker.id=1,log.dirs=/tmp/kafka-logs {code}\r\n \r\n*Expected Benefits:*\r\n * Faster feedback loop for configuration changes\r\n * Reduced deployment failures due to configuration issues\r\n * Better integration with automated deployment pipelines\r\n * Improved operational efficiency for Kafka administrators\r\n * *Prevention of cluster-wide i", "output": "Add configuration validation option to Kafka server startup"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/blob/656242775c321c263a3a01411b560098351e8ec4/core/src/main/scala/kafka/tools/TestRaftServer.scala#L118\r\n\r\nwe always hard code the `recordsPerSec` and `recordSize`", "output": "the `record-size` and `throughput`arguments don't work in TestRaftServer"}
{"instruction": "Answer the question based on the bug.", "input": "Title: hadoop-client-api exclude webapps/static front-end resources\nDescription: \nQ: hadoop-yetus commented on PR #7804:\nURL: https://github.com/apache/hadoop/pull/7804#issuecomment-3076022688\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  72m 32s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 115m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   8m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 51s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 19s |  |  hadoop-client-api in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 190m 18s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7804 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 8a0fed5ae6d7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0f2cc88fd55ceb31d30a1157bebd34a35e39289f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-client-modules/hadoop-client-api U: hadoop-client-modules/hadoop-client-api |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "pan3793 commented on PR #7804:\nURL: https://github.com/apache/hadoop/pull/7804#issuecomment-3078313651\n\n   should they go `hadoop-client-minicluster` instead of removing?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Kafka consumer throughput in best-case drops by ~10 times after upgrading to kafka v3.9.0 from v3.5.1. Note that this is in ZK mode and KRAFT migration is not done yet.", "output": "Consumer throughput drops by 10 times with Kafka v3.9.0 in ZK mode"}
{"instruction": "Answer the question based on the bug.", "input": "Title: SBT assembly should correct handle META-INF\nDescription: \nQ: Issue resolved in https://github.com/apache/spark/pull/52636", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Set Curator Connection Timeout\nDescription: Curator 5.2.0 has a default \"connection timeout\" of 15s:\r\n\r\n[https://github.com/apache/curator/blob/apache-curator-5.2.0/curator-framework/src/main/java/org/apache/curator/framework/CuratorFrameworkFactory.java#L63]\r\n\r\nAnd it will throw a warning if the Zookeeper session timeout is less than the Curator connection timeout:\r\n\r\n[https://github.com/apache/curator/blob/apache-curator-5.2.0/curator-client/src/main/java/org/apache/curator/CuratorZookeeperClient.java#L117-L120]\r\n\r\nThe Hadoop default for ZK timeout is set to 10s:\r\n\r\n[https://github.com/apache/hadoop/blob/0dd9bf8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeys.java#L414-L416]\r\n\r\nWhich means without setting the \"connection timeout\" a default installation will keep warning about the connection timeout for Curator being lower than the requested session timeout. This sets both timeout values to override the Curator default.\r\n\r\nAnother option is to change the default Hadoop ZK timeout f\nQ: hadoop-yetus commented on PR #7426:\nURL: https://github.com/apache/hadoop/pull/7426#issuecomment-2675863099\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 59s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  37m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 34s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 15s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 47s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 32s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 28s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  14m 46s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  5s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 201m 51s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7426/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7426 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 1457f150b128 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8785f857c5086f15d305f04ada532436dbfa67b6 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7426/1/testReport/ |\r\n   | Max. process+thread count | 3151 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7426/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on PR #7426:\nURL: https://github.com/apache/hadoop/pull/7426#issuecomment-2675879198\n\n   @xyu Thanks for the contribution! LGTM."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support getSQLKeywords for SparkConnectDatabaseMetaData\nDescription: \nQ: Issue resolved by pull request 52757\n[https://github.com/apache/spark/pull/52757]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: use blockchain datasets for parquet data\nDescription: * we need to worry about noaa data going away if the US govt decides to do that\r\n* there's a big dataset of blockchain data *in parquet format* https://registry.opendata.aws/aws-public-blockchain/\r\n\r\nWe should look at this and see if it is good to play with as\r\n* large\r\n* complex", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use java.time.Clock instead of org.apache.hadoop.util.Clock\nDescription: Hadoop's {{Clock}} interface was recently moved from {{org.apache.hadoop.yarn.util}} (in hadoop-yarn) to {{org.apache.hadoop.util}} (in hadoop-common) as part of YARN-11765.\r\n\r\nI propose to seize the opportunity of this being targeted done for 3.5.0 to modernize it usage:\r\n# Deprecate {{org.apache.hadoop.util.Clock}}\r\n# Replace all of its usages with {{java.time.Clock}}\r\n# Replace existing usages of its simple implementations, e.g. {{SystemClock}}/{{UTCClock}} with standard {{java.time.Clock}} subclasses, e.g. {{Clock.systemUTC()}}\r\n# Re-implement other implementations, e.g. {{MonotonicClock}}/{{ControllerClock}}, as {{java.time.Clock}} subclasses.\r\n\r\nThe standard {{java.time.Clock}} has a richer API supports modern {{java.time}} classes such as {{Instant}} and {{ZoneId}}, and migration would be straightforward:\r\nJust changing {{org.apache.hadoop.util.Clock.getTime()}} to {{java.time.Clock.millis()}}\nQ: hadoop-yetus commented on PR #7569:\nURL: https://github.com/apache/hadoop/pull/7569#issuecomment-2769358580\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 21s |  |  https://github.com/apache/hadoop/pull/7569 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7569 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7569/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "YanivKunda opened a new pull request, #7570:\nURL: https://github.com/apache/hadoop/pull/7570\n\n   ### Description of PR\r\n   \r\n   Closes HADOOP-19525\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran all existing tests.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [V] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce the framework for adding ST expressions in Catalyst\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade to Junit 5.13.3\nDescription: JUnit has been upgraded to 5.13.3 so that test suites can\r\nbe parameterized at the class level (again).\r\nThis requires a matching upgrade to Surefire and some\r\ntuning of surefire options to restore existing behavior.\r\n\r\nDependency Changes:\r\n* junit.jupiter and junit.vintage => 5.13.3\r\n* junit.platform => 1.13.3\r\n* Surefire => 3.5.3.\r\n\r\nChanged Surefire Flags: \r\n* trimStackTrace => false\r\n* surefire.failIfNoSpecifiedTests => false\r\n\r\nh2. Build property trimStackTrace\r\n\r\ntrimStackTrace is set to false by default\r\n\r\nThis restores the behavior that any test failure\r\nincludes a deep stack trace rather than the first two elements.\r\nIn this version of surefire it also seems to print the full stack\r\nof JUnit test execution and the java runtime itself.\r\n\r\nThis is only an issue for when tests fail.\r\nIf the short trace is desired, enable it on the command line:\r\n{code}\r\n    mvn test -DtrimStackTrace\r\n{code}\r\n\r\nh2. Build property surefire.failIfNoSpecifiedTests\r\n\r\nsurefire.failIfNoSpecifiedTests is set to false by default    \r\n\r\nThis is required to retain the behavior where the invocation\r\n{code}\r\n    mvn verify -Dtest=unknown\r\n{code}\r\nwill skip the unit tests but still run the integration tests, and a\r\nspecific property \"it.test\" can name the specific test to run:\r\n{code}\r\n    mvn verify -Dtest=unknown -Dit.test=ITestConnection\r\n{code}", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analytics-Accelerator: Update LICENSE-binary\nDescription: update LICENSE-binary to include AAL dependency\nQ: ahmarsuhail opened a new pull request, #7982:\nURL: https://github.com/apache/hadoop/pull/7982\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Adds in AAL dependency to License-binary.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   not required.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "ahmarsuhail commented on PR #7982:\nURL: https://github.com/apache/hadoop/pull/7982#issuecomment-3306787342\n\n   @steveloughran PR to add in license binary."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fully Support Java 23, 24 and 25\nDescription: Hadoop trunk today mostly supports JDK17, but doesn't work at all on JDK23. (and conversely on JDK24 to be released in less than two weeks)\r\n\r\nWhile there are many smaller issues, the major breaking change is the SecurityManager removal (JEP411/486), and its many consequences.\r\n\r\nThe obvious change is that Subjec.doAs() and Subject.current() no longer work by default, and the replacement APIs must be used.\r\n\r\nThe more insidius change is that when SecurityManager is disabled then JDK22+ does not propapage the Subject to new Threads, which is something that Hadoop absolutely relies on.\r\n\r\n\r\nNote that Hadoop is always built with with JDK 17  (if the JDK is 17 or newer), unless the target version is specifically overriden.\r\nThis is not a problem, JDK17 class files running on a JDK 24 JVM is the expected use case for binary distributions.\r\n\r\nWe may want to run some tests where Hadoop is also compiled for the lastest JVM later. (taget Java 24 + JVM 24)\nQ: [~stoty] I originally hoped to support JDK 21 after supporting JDK 17, so it would be really awesome if it could support even newer JDK versions.", "output": "Could you also check the rest of the subtasks and dependent tasks here [~slfan1989]  ?\r\n\r\nThese are fixing varius test issues on JDK11-23 , and are a requirement for adding the big JEP411 related fixes.\r\nI've split them up into small easy-to-review patches.\r\n\r\nAs a side effect, with these fixes applied the test suite should also run cleanly with JDK17 and 21."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: enhance the documentation for `Node#isFanced`\nDescription: The `isFenced` value is only valid for broker nodes. It is always `false` for quorum controller nodes.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support TIME in the make_timestamp function in Python\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52648.", "output": "Issue resolved by pull request 52648\n[https://github.com/apache/spark/pull/52648]"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Parquet's GH-3204 patch uses hflush() just before close()\r\n\r\nthis is needless and hurts write performance on hdfs.\r\nFor s3A it will trigger a warning long (Syncable is not supported) or an actual failure if\r\nfs.s3a.downgrade.syncable.exceptions is false\r\n\r\nproposed: hflush to log at debug -only log/reject on hsync, which is the real place where semantics cannot be met", "output": "S3A: S3ABlockOutputStream to never log/reject hflush(): calls"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Rename Recovery Should Succeed When Marker File Exists with Destination Directory\nDescription: On the blob endpoint, since rename is not a direct operation but a combination of two operations—copy and delete—in the case of directory rename, we first rename all the blobs that have the source prefix and, at the end, rename the source to the destination.\r\n\r\nIn the normal rename flow, renaming is not allowed if the destination already exists. However, in the case of recovery, there is a possibility that some files have already been renamed from the source to the destination. With the recent change ([HADOOP-19474] ABFS: [FnsOverBlob] Listing Optimizations to avoid multiple iteration over list response. - ASF JIRA), where we create a marker if the path is implicit, rename recovery will fail at the end when it tries to rename the source to the destination after renaming all the files.\r\n\r\nTo fix this, while renaming the source to the destination, if we encounter an error indicating that the path already exists, we will suppress the error and mark the rename recovery as successful.\nQ: bhattmanish98 opened a new pull request, #7559:\nURL: https://github.com/apache/hadoop/pull/7559\n\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19522\r\n   On the blob endpoint, since rename is not a direct operation but a combination of two operations—copy and delete—in the case of directory rename, we first rename all the blobs that have the source prefix and, at the end, rename the source to the destination.\r\n   \r\n   In the normal rename flow, renaming is not allowed if the destination already exists. However, in the case of recovery, there is a possibility that some files have already been renamed from the source to the destination. With the recent change ([HADOOP-19474](https://issues.apache.org/jira/browse/HADOOP-19474) ABFS: [FnsOverBlob] Listing Optimizations to avoid multiple iteration over list response. - ASF JIRA), where we create a marker if the path is implicit, rename recovery will fail at the end when it tries to rename the source to the destination after renaming all the files.\r\n   \r\n   To fix this, while renaming the source to the destination, if we encounter an error indicating that the path already exists, we will suppress the error and mark the rename recovery as successful.", "output": "hadoop-yetus commented on PR #7559:\nURL: https://github.com/apache/hadoop/pull/7559#issuecomment-2766074619\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 39s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 133m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7559 |\r\n   | JIRA Issue | HADOOP-19522 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2be4af61ef21 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 71ef4361d61b6a1e8e8bcd29e1d7a913dce4d519 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/testReport/ |\r\n   | Max. process+thread count | 591 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade ASM to 9.9\nDescription: \nQ: Issue resolved by pull request 52672\n[https://github.com/apache/spark/pull/52672]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "HADOOP-19289 upgraded protobuf-java 3.25.5, we should use same version for protobuf installed in docker images.", "output": "Upgrade Protobuf 3.25.5 for docker images"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In the blob endpoint, we determine whether the path is a file, or a directory based on the metadata attribute hdi_isfolder. When creating a directory, we set hdi_isfolder to true. Currently, our method for checking if the path is a directory involves a case-sensitive equality check. Consequently, if someone configures a directory with Hdi_isfolder, the driver will not recognize that path as a directory. We need to address this issue because, in the backend, hdi_isfolder and Hdi_isfolder are considered the same metadata attribute. Therefore, the solution involves modifying our equality check to be case-insensitive, ensuring that the driver correctly identifies directories regardless of case variations in the metadata attribute.", "output": "Abfs: Fix Case Sensitivity Issue for hdi_isfolder metadata"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: KIP-1224: Add batch-linger-time and batch-flush-time metrics\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "[https://github.com/apache/kafka-site/blob/905299a0b7e2e3892d9493c7dcaaf78dce035c00/.htaccess#L13]\r\n\r\nThe Content-Security-Policy header must not be overridden.\r\n\r\nThere is now a standard way to add local exceptions to the CSP:\r\n\r\n[https://infra.apache.org/tools/csp.html]\r\n\r\nPlease update the .htaccess file accordingly.\r\n\r\nPlease note that the policy says that any exceptions must have been approved, and a reference to the approval must be commented in the .htaccess file", "output": "The Content-Security-Policy header must not be overridden"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The IP address string format has changed in JDK14\r\n[https://bugs.openjdk.org/browse/JDK-8225499|https://bugs.openjdk.org/browse/JDK-8232369]\r\n\r\n\r\n\r\nHDFS-15685 adjust some tests for it, but not all.", "output": "Handle JDK-8225499 IpAddr.toString() format changes in tests"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. \nDescription: Removed build support for the following EOL (End-of-Life) operating system versions (e.g., CentOS 7, Debian 9, etc.).\r\n\r\nCleaned up redundant code and dependency configurations in the Dockerfile related to these operating systems.\r\n\r\nOptimized the build logic to ensure that currently supported OS versions build successfully.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Enable substitution for SQLConf settings\nDescription: Values set for custom catalogs are not being substituted:\r\n\r\n{code:java}\r\nspark.sql.catalog.mssql=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog\r\nspark.sql.catalog.mssql.user=${env:MSSQL__USER}\r\n{code}\r\n\r\n\r\nFails with:\r\n{code:java}\r\nspark.sql(\"show tables in mssql\").show()\r\n\r\ncom.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '${env:MSSQL__USER}'\r\n{code}\nQ: PR: https://github.com/apache/spark/pull/52759", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update `dev/requirements.txt` to skip `torch` and `torchvision` in Python 3.14\nDescription: \nQ: Issue resolved by pull request 52543\n[https://github.com/apache/spark/pull/52543]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The AbstractContractAppendTest#testRenameFileBeingAppended() tries to rename the file while the handle is open. This isn't allowed on Windows and thus the call to rename fails.\r\n\r\nThus, we need to skip this test on Windows.", "output": "Skip testRenameFileBeingAppended on Windows"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support logging in Pandas/Arrow UDFs\nDescription: \nQ: Issue resolved by pull request 52785\n[https://github.com/apache/spark/pull/52785]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Delete the UID in the dev container that is the same as the host user\nDescription: \nQ: cnauroth closed pull request #7781: HADOOP-19606. Delete the UID in the dev container that is the same as the host user\nURL: https://github.com/apache/hadoop/pull/7781", "output": "cnauroth commented on PR #7781:\nURL: https://github.com/apache/hadoop/pull/7781#issuecomment-3050572195\n\n   I have committed this to trunk. Thank you, @pan3793 and @slfan1989 !"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When the client-side config hadoop.security.kms.client.encrypted.key.cache.expiry is set to a negative value in core-site.xml, any tool that initializes KMSClientProvider (e.g., KeyShell) fails immediately with:\r\n\r\n{code:java}\r\njava.lang.IllegalArgumentException: expiry must be > 0\r\n    at org.apache.hadoop.crypto.key.kms.ValueQueue.(ValueQueue.java:xxx)\r\n    at org.apache.hadoop.crypto.key.kms.KMSClientProvider.(KMSClientProvider.java:xxx)\r\n    ...\r\n\r\n{code}\r\n\r\nThis is a controlled failure (JVM doesn’t crash), but the error message does not mention which property and what value triggered it. Users typically see a stack trace without a clear remediation hint.\r\n\r\n*Expected behavior*\r\n\r\nFail fast with a clear configuration error that names the property and value, e.g.:\r\nInvalid configuration: hadoop.security.kms.client.encrypted.key.cache.expiry = -1 (must be > 0 ms)\r\n\r\n*Steps to Reproduce*\r\n1. In the client core-site.xml, set:\r\n\r\n{code:xml}\r\n\r\n  hadoop.security.kms.client.encrypted.key.cache.expiry\r\n  -1\r\n\r\n{code}\r\n2. Ensure the conf is active (echo $HADOOP_CONF_DIR points to this dir).\r\n3. Run:\r\n\r\n{code:java}\r\n./bin/hadoop key list -provider kms://http@localhost:9600/kms -metadata\r\n{code}", "output": "[kms] Negative hadoop.security.kms.client.encrypted.key.cache.expiry causes KMSClientProvider init failure with generic IllegalArgumentException; "}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))\nDescription: \nQ: hadoop-yetus commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3293191749\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 36s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 50s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 59s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7967 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b7ef8b2a88ce 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1dad672faf42fbfb0ada2a95456b922a50b2becf |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/testReport/ |\r\n   | Max. process+thread count | 693 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3293931499\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  15m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  50m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  6s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 156m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7967 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f1f4dba85ce3 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6faa4c9c8ad8083e0a636b6439146a24c66c0ce3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/2/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Empty Page Issue on Subsequent ListBlob call\nDescription: We came across a new behavior from server where ListBlob call can return empty list even after returning a next marker(continuation token) from previous list call.\r\n\r\nThis is to handle that case and do not infer listing to be incomplete.\nQ: anujmodi2021 opened a new pull request, #7698:\nURL: https://github.com/apache/hadoop/pull/7698\n\n   ### Description of PR\r\n   We came across a new behavior from the server where a ListBlob call can return an empty list even after returning a next marker(continuation token) from a previous list call.\r\n   This is to handle that case and do not imply listing to be incomplete.\r\n   \r\n   ### How was this patch tested?\r\n   Existing Test Suite ran, and new tests are added.", "output": "hadoop-yetus commented on PR #7698:\nURL: https://github.com/apache/hadoop/pull/7698#issuecomment-2893454757\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 11s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 1 unchanged - 0 fixed = 3 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 55s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 132m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7698 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ae738c550edc 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / dfe1e22a1c51042d53a56005de1874dcc84e6b7d |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/1/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add a new allowed value for group.coordinator.append.linger.ms and share.coordinator.append.linger.ms of -1.\r\nWhen append.linger.ms is set to -1, use the flush strategy outlined in the KIP.", "output": "KIP-1224: Implement adaptive append.linger.ms for the group coordinator and share coordinator"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Issues running Streams system tests on release candidates\nDescription: During release we now merge the RC tag into the release branch. This means the release branch has a real release number instead of -SNAPSHOT.\r\n\r\nThe Streams upgrade system tests expect -SNAPSHOT and thus fail to run.\r\n\r\nWe should either only merge RC tags once the vote pass or change the tests to use  instead of -SNAPSHOT.\r\n\r\nSee:\r\n- https://lists.apache.org/thread/v6873zlvp5bl9qf5zsr3g904nxdynr74\r\n- https://lists.apache.org/thread/y3rh1nnxqz6tc4brnyfbpbrx2gy655y6", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: SecretManager logs at INFO in bin/hadoop calls\nDescription: When I invoke a CLI command, I now get told information about HMAC keys which are\r\n* utterly meaningless to me\r\n* completely unrelated to what I am doing\r\n\r\n{code}\r\n bin/hadoop s3guard bucket-info $BUCKET\r\n\r\n2025-03-25 12:05:44,838 [main] INFO  token.SecretManager (SecretManager.java:(126)) - Selected hash algorithm: HmacSHA1\r\n2025-03-25 12:05:44,838 [main] INFO  token.SecretManager (SecretManager.java:(131)) - Selected hash key length:64\r\n{code}\r\n\r\nLooks like the changes in YARN-11738 have created this\nQ: cnauroth opened a new pull request, #7537:\nURL: https://github.com/apache/hadoop/pull/7537\n\n   ### Description of PR\r\n   \r\n   Switch to debug level for logging `SecretManager` configured algorithm and key length. I really wanted to do more improvements in this class, like resolving the configuration in a constructor instead of static initialization and potentially reusing a pre-existing `Configuration` instance. I couldn't find a way to do that while maintaining backward-compatibility, and the class is annotated as public.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   On trunk, build the distro:\r\n   \r\n   ```\r\n   mvn -o -Pdist -Dtar -DskipTests -DskipShade -Dmaven.javadoc.skip=true clean package\r\n   ```\r\n   \r\n   Reproduce the bug:\r\n   \r\n   ```\r\n   cnauroth@d9699e10bb20:/tmp/dist/hadoop-3.5.0-SNAPSHOT$ bin/hadoop s3guard -D fs.s3a.delegation.token.binding=org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenBinding bucket-info s3a://foo\r\n   ...\r\n   2025-03-25 19:57:26,731 INFO token.SecretManager: Selected hash algorithm: HmacSHA1\r\n   2025-03-25 19:57:26,732 INFO token.SecretManager: Selected hash key length:64\r\n   ...\r\n   ```\r\n   \r\n   Apply this patch, rebuild the distro and retest. The `SecretManager` logs don't appear. You can get them back by explicitly enabling debug logging:\r\n   \r\n   ```\r\n   cnauroth@d9699e10bb20:/tmp/dist/hadoop-3.5.0-SNAPSHOT$ bin/hadoop --loglevel DEBUG s3guard -D fs.s3a.delegation.token.binding=org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenBinding bucket-info s3a://foo\r\n   ...\r\n   2025-03-25 21:29:21,476 DEBUG token.SecretManager: Selected hash algorithm: HmacSHA1\r\n   2025-03-25 21:29:21,476 DEBUG token.SecretManager: Selected hash key length:64\r\n   ...\r\n   ```\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7537:\nURL: https://github.com/apache/hadoop/pull/7537#issuecomment-2753082179\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m  9s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 23s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  14m  0s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 15s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 55s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 53s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 55s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 11s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 36s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 10s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 211m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7537/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7537 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 6c2e714cad3e 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b562ce1790963766339128c1707f51df35a35e0b |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7537/1/testReport/ |\r\n   | Max. process+thread count | 1271 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7537/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [ReadAheadV2] Implement Read Buffer Manager V2 with improved aggressiveness\nDescription: ", "output": "In Progress"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade libopenssl to 3.1.4 for rsync on Windows\nDescription: * We're currently using libopenssl 3.1.2 which\r\n  is needed for rsync 3.2.7 on Windows for the\r\n  Yetus build validation.\r\n* However, libopenssl 3.1.2 is no longer\r\n  available for download on the msys2 site.\r\n* This PR upgrades libopenssl to the next\r\n  available version - 3.1.4 to mitigate this\r\n  issue.\nQ: GauthamBanasandra commented on PR #7770:\nURL: https://github.com/apache/hadoop/pull/7770#issuecomment-3058650710\n\n   @slfan1989 could you please review this PR?", "output": "slfan1989 commented on PR #7770:\nURL: https://github.com/apache/hadoop/pull/7770#issuecomment-3060807079\n\n   LGTM."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Implement SparkConnectDatabaseMetaData simple methods\nDescription: \nQ: Issue resolved by pull request 52741\n[https://github.com/apache/spark/pull/52741]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve `SparkSubmit` to invoke `exitFn` with the root cause instead of `SparkUserAppException`\nDescription: I hit this when I ran a pipeline that had no flows:\r\n```\r\norg.apache.spark.SparkUserAppException: User application exited with 1\r\n\tat org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:127)\r\n\tat org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1028)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:226)\r\n\tat org.apache.spark.deploy.Spar\nQ: I collected this as a subtask of SPARK-51727 in order to improve the visibility.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK17] Remove powermock dependency\nDescription: The powermock dependency is specified in\r\n- hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml\r\n- hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml\r\n\r\nbut not used anywhere.  We should remove it.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side\nDescription: Support create and rename idempotency on FNS Blob from client side", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use `Utils. getRootCause` instead of `Throwables.getRootCause`\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively\nDescription: It has been observed that certain requests taking more time than expected to complete hinders the performance of whole workload. Such requests are known as tailing requests. They can be taking more time due to a number of reasons and the prominent among them is a bad network connection. In Abfs driver we cache network connections and keeping such bad connections in cache and reusing them can be bad for perf.\r\n\r\nIn this effort we try to identify such connections and close them so that new good connetions can be established and perf can be improved. There are two parts of this effort.\r\n # Identifying Tailing Requests: This involves profiling all the network calls and getting percentiles value optimally. By default we consider p99 as the tail latency and all the future requests taking more than tail latency will be considere as Tailing requests.\r\n\r\n # Proactively Killing Socket Connections: With Apache client, we can now kill the socket connection and fail the tailing request. Such failur\nQ: hadoop-yetus commented on PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#issuecomment-3449602331\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 43 new + 3 unchanged - 0 fixed = 46 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 35 new + 1472 unchanged - 0 fixed = 1507 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 34 new + 1413 unchanged - 0 fixed = 1447 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 177 unchanged - 1 fixed = 181 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  60m 12s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:[line 533] |\r\n   |  |  new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:[line 55] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:[line 81] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8043 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 9b5c6baa74d5 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / de244d215362fca4d8ba16b3d01a9f39a3ff0e81 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/testReport/ |\r\n   | Max. process+thread count | 637 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "anmolanmol1234 commented on code in PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#discussion_r2464810768\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1824,4 +1860,41 @@ public int getBlobRenameDirConsumptionParallelism() {\n   public int getBlobDeleteDirConsumptionParallelism() {\n     return blobDeleteDirConsumptionParallelism;\n   }\n+\n+  public boolean isTailLatencyTrackerEnabled() {\n+    return isTailLatencyTrackerEnabled;\n+  }\n+\n+  public boolean isTailLatencyRequestTimeoutEnabled() {\n+    return isTailLatencyRequestTimeoutEnabled && isTailLatencyTrackerEnabled\n\nReview Comment:\n   first check should be for isTailLatencyTrackerEnabled"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have been disabling AQE for streaming workloads since stateful operators do not work with AQE. But applying AQE in stateless workloads is still beneficial and we have been blocking the case too aggressively.\r\n\r\nWe have been observed streaming workloads which can enjoy the benefits of AQE e.g. stream-static join with large static table, MERGE INTO in ForeachBatch sink, etc. To accelerate the workloads, we would want to support AQE in stateless streaming workloads.", "output": "Support AQE in stateless streaming workloads"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When benchmark_test is run using docker, following error is thrown in both native and JVM modes. ChatGPT suggested to use a JDK image with latest patch release({{{}17.0.12+7{}}} or {{21.0.4+7)}} and it worked. Maybe we might want to upgrade our JDK image too\r\n{code:java}\r\n[INFO:2025-09-03 09:41:49,007]: RunnerClient: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.acks=-1.topic=topic-replication-factor-three.metadata_quorum=ISOLATED_KRAFT: FAIL: RemoteCommandError({'ssh_config': {'host': 'ducker02', 'hostname': 'ducker02', 'user': 'ducker', 'port': 22, 'password': '', 'identityfile': '/home/ducker/.ssh/id_rsa', 'connecttimeout': None}, 'hostname': 'ducker02', 'ssh_hostname': 'ducker02', 'user': 'ducker', 'externally_routable_ip': 'ducker02', '_logger': , 'os': 'linux', '_ssh_client': , '_sftp_client': , '_custom_ssh_exception_checks': None}, '/opt/kafka-dev/bin/kafka-storage.sh format --ignore-formatted --config /mnt/kafka/kafka.properties --cluster-id I2eXt9rvSnyhct8BYmW6-w --feature transaction.version=0', 1, b'java.lang.NullPointerException: Cannot invoke \"jdk.internal.platform.CgroupInfo.getMountPoint()\" because \"anyController\" is null\\n\\tat java.base/jdk.internal.platform.cgroupv3.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:81)\\n\\tat java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:113)\\n\\tat java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:167)\\n\\tat java.base/jdk.internal.platform.SystemMetrics.instance(SystemMetrics.java:29)\\n\\tat java.base/jdk.internal.platform.Metrics.systemMetrics(Metrics.java:58)\\n\\tat java.base/jdk.internal.platform.Container.metrics(Container.java:43)\\n\\tat jdk.management/com.sun.management.internal.OperatingSystemImpl.(OperatingSystemImpl.java:182)\\n\\tat jdk.management/com.sun.management.internal.PlatformMBeanProviderImpl.getOperatingSystemMXBean(PlatformMBeanProviderImpl.java:280)\\n\\tat jdk.management/com.sun.management.internal.", "output": "benchmark_test is throwing `NullPointerException`"}
{"instruction": "Answer the question based on the bug.", "input": "Title: kraft Add/RemoveVoterHandlers do not check request's timeout\nDescription: \nQ: Hi [~brandboat], apologies I meant to assign it to myself, since I'm still not sure about the best way to handle this properly yet.", "output": "Gotcha, thanks for letting me know :)"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: testGenerateAssignmentWithBootstrapServer uses wrong JSON format\nDescription: In ReassignPartitiosnsCommand#generateAssignment, it uses JSON format like:\r\n\r\n \r\n{code:java}\r\n{\r\n  \"topics\": [\r\n    { \"topic\": \"foo1\" },\r\n    { \"topic\": \"foo2\" }\r\n  ],\r\n  \"version\": 1\r\n} {code}\r\nHowever, in ReassignPartitionsCommandTest#testGenerateAssignmentWithBootstrapServer, it uses input like:\r\n{code:java}\r\n{\r\n  \"version\":1,\r\n  \"partitions\": [\r\n    {\r\n      \"topic\": \"foo\",\r\n      \"partition\": 0,\r\n      \"replicas\": [3, 1, 2],\r\n      \"log_dirs\": [\"any\",\"any\",\"any\"]\r\n    }\r\n  ]\r\n}{code}\r\nThe test case can pass, but it doesn't test `generateAssignment` correctly.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix TestThrottledInputStream when bandwidth is equal to throttle limit\nDescription: Some tests in TestThrottledInputStream intermittently fail when the measured bandwidth is equal to the one set for throttling.\nQ: stoty opened a new pull request, #7517:\nURL: https://github.com/apache/hadoop/pull/7517\n\n   ### Description of PR\r\n   \r\n   Accept equals when checking if actual bandwidth conforms to the throttle value set.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran the test on various JVMs\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7517:\nURL: https://github.com/apache/hadoop/pull/7517#issuecomment-2729164087\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  25m 33s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 101m 28s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7517/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7517 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux beeb10e8f485 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1ee1fbe3af26fd7dac706815ae1c42647acab073 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7517/1/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7517/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix dependency exclusion list of hadoop-client-runtime.\nDescription: \nQ: pan3793 opened a new pull request, #7878:\nURL: https://github.com/apache/hadoop/pull/7878\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   When trying to use the Hadoop trunk version client with Spark 4.0.0, `NoClassDefFoundError` was raised.\r\n   \r\n   ```\r\n   Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/shaded/javax/ws/rs/WebApplicationException\r\n   \tat java.base/java.lang.ClassLoader.defineClass1(Native Method)\r\n   \tat java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:962)\r\n   \tat java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:144)\r\n   \tat java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:776)\r\n   \tat java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:691)\r\n   \tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:620)\r\n   \tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:578)\r\n   \tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:490)\r\n   \tat org.apache.spark.deploy.yarn.YarnRMClient.getAmIpFilterParams(YarnRMClient.scala:109)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster.addAmIpFilter(ApplicationMaster.scala:698)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster.runExecutorLauncher(ApplicationMaster.scala:555)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:265)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:942)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:941)\r\n   \tat java.base/jdk.internal.vm.ScopedValueContainer.callWithoutScope(ScopedValueContainer.java:162)\r\n   \tat java.base/jdk.internal.vm.ScopedValueContainer.call(ScopedValueContainer.java:147)\r\n   \tat java.base/java.lang.ScopedValue$Carrier.call(ScopedValue.java:419)\r\n   \tat java.base/javax.security.auth.Subject.callAs(Subject.java:331)\r\n   \tat org.apache.hadoop.util.SubjectUtil.callAs(SubjectUtil.java:134)\r\n   \tat org.apache.hadoop.util.SubjectUtil.doAs(SubjectUtil.java:166)\r\n   \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:2039)\r\n   \tat org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:941)\r\n   \tat org.apache.spark.deploy.yarn.ExecutorLauncher$.main(ApplicationMaster.scala:973)\r\n   \tat org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala)\r\n   Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.shaded.javax.ws.rs.WebApplicationException\r\n   \tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:580)\r\n   \tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:490)\r\n   \t... 24 more\r\n   ```\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Hadoop Client Runtime 3.4.2 RC2\r\n   \r\n   \r\n   Hadoop Client Runtime 3.5.0-SNAPSHOT trunk\r\n   \r\n   \r\n   Hadoop Client Runtime 3.5.0-SNAPSHOT HADOOP-19652\r\n   \r\n   \r\n   Tested by submitting a Spark application to a YARN cluster.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "pan3793 commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3194939043\n\n   cc @slfan1989 @cnauroth, this is caused by Jersey upgrading."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use `4.1.0-preview3-java21-scala` image for preview examples \nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When not enable speculation ,executor launch task failed caused by OOM or other issue, DAGScheduler won;t know task failed and won't scheduler again, causing application stuck\r\n\r\n!image-2025-10-30-16-52-22-524.png|width=589,height=233!", "output": "Spark Executor launch task failed should return task killed message"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade PyArrow to 22.0.0\nDescription: ", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Tools | Fix order of arguments to assertEquals in unit test\nDescription: This sub-task is intended to fix the order of arguments passed in assertions in test cases within the tools package.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implement cleanup mechanism for obsolete Remote Log Metadata\nDescription: see https://github.com/apache/kafka/pull/20345#issuecomment-3201413774", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Depends upon/blocked by:*\r\n * Kafka ticket: KAFKA-19174 Gradle version upgrade 8 -->> 9 (review is under way)\r\n * Spotbugs next version:\r\n ** [https://github.com/spotbugs/spotbugs/issues/3564]\r\n ** [https://github.com/spotbugs/spotbugs/issues/3569]\r\n ** https://issues.apache.org/jira/browse/BCEL-377 \r\n ** [https://github.com/spotbugs/spotbugs/pull/3712]\r\n ** [https://github.com/spotbugs/spotbugs/discussions/3380] \r\n\r\n*Related links:*\r\n * JDK 25 release date: September 16th 2025:\r\n ** [https://mail.openjdk.org/pipermail/announce/2025-September/000360.html]\r\n ** [https://openjdk.org/projects/jdk/25]\r\n * Gradle 9.1 will support Java 25: [https://docs.gradle.org/9.1.0/release-notes.html#support-for-java-25]\r\n * Scala 2.13.17 is also announced:\r\n ** [https://docs.scala-lang.org/overviews/jdk-compatibility/overview.html#jdk-25-compatibility-notes]\r\n ** [https://contributors.scala-lang.org/t/scala-2-13-17-release-planning/6994/10]\r\n ** [https://github.com/scala/scala/milestone/109]\r\n * other related links:\r\n ** [https://github.com/adoptium/temurin/issues/96]\r\n ** [https://launchpad.net/ubuntu/+source/openjdk-25]\r\n ** [https://github.com/actions/setup-java/issues/899]\r\n\r\n*Related Github PR:* [https://github.com/apache/kafka/pull/20295] _*MINOR: Run CI with Java 24*_\r\n\r\nFYI [~ijuma] [~chia7712]", "output": "Support building with Java 25 (LTS release)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Address the compileClasspath resolution warnings for the `releaseTarGz` task\nDescription: {code:java}\r\n[warn]  Resolution\r\n of the configuration :tools:tools-api:runtimeClasspath was attempted \r\nfrom a context different than the project context. Have a look at the \r\ndocumentation to understand why this is a problem and how it can be \r\nresolved. This behavior has been deprecated. (1)    - [warn]  Resolution\r\n of the configuration :tools:tools-api:runtimeClasspath was attempted \r\nfrom a context different than the project context. Have a look at the \r\ndocumentation to understand why this is a problem and how it can be \r\nresolved. This behavior has been deprecated.This will fail with an error in Gradle 9.0.        - Locations- ``- `:core:releaseTarGz` {code}\r\nThe issue was introduced by [https://github.com/apache/kafka/pull/13454]\r\n\r\n \r\n\r\n`tools-api` is already in core module runtime path, so adding it to `releaseTarGz` causes the resolution conflicts, which will be a fatal error in gradle 9\nQ: this could be resolved by gradle upgrade (see https://github.com/apache/kafka/pull/19513#issuecomment-3355314891)", "output": "[~chia7712], can I take over on this issue?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Weird behavior on Kafka Connect 4.1 class loading\nDescription: I have the [DebeziumOpenLineageEmitter|https://github.com/debezium/debezium/blob/main/debezium-openlineage/debezium-openlineage-api/src/main/java/io/debezium/openlineage/DebeziumOpenLineageEmitter.java] class in the *debezium-openlineage-api* that internally has a static map to maintain the registered emitter, the key of this map is \"connectoLogicalName-taskid\"\r\nThen there is the [OpenLineage SMT|https://github.com/debezium/debezium/blob/main/debezium-core/src/main/java/io/debezium/transforms/openlineage/OpenLineage.java], which is part of the *debezium-core.* In this SMT, I simply pass the same context to instantiate the same emitter via the connector.\r\n\r\nNow I'm running the following image\r\n{code:java}\r\nFROM quay.io/debezium/connect:3.3.0.Final\r\nENV MAVEN_REPO=\"https://repo1.maven.org/maven2\"\r\nENV GROUP_ID=\"io/debezium\"\r\nENV DEBEZIUM_VERSION=\"3.3.0.Final\"\r\nENV ARTIFACT_ID=\"debezium-openlineage-core\"\r\nENV CLASSIFIER=\"-libs\"\r\nCOPY log4j.properties /kafka/config/log4j.properties\r\n\r\nAdd OpenLineage\r\nRUN mkdir -p /tmp/openlineage-libs && \\\r\n    curl \"$MAVEN_REPO/$GROUP_ID/$ARTIFACT_ID/$DEBEZIUM_VERSION/$ARTIFACT_ID-${DEBEZIUM_VERSION}${CLASSIFIER}.tar.gz\" -o /tmp/debezium-openlineage-core-libs.tar.gz && \\\r\n    tar -xzvf /tmp/debezium-openlineage-core-libs.tar.gz -C /tmp/openlineage-libs --strip-components=1\r\n\r\nRUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-postgres/\r\nRUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-mongodb/\r\nADD openlineag", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: sendDeferedResponse should also log exception info.\nDescription: sendDeferedResponse should also log exception info.\nQ: hfutatzhanghb opened a new pull request, #7684:\nURL: https://github.com/apache/hadoop/pull/7684\n\n   ### Description of PR\r\n   Method sendDeferedResponse should also log exception info.\r\n   \r\n   ![image](https://github.com/user-attachments/assets/beb0a334-ab55-4097-b35c-02472936158b)", "output": "hadoop-yetus commented on PR #7684:\nURL: https://github.com/apache/hadoop/pull/7684#issuecomment-2876281591\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 50s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 52s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 15s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7684/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 2 new + 161 unchanged - 0 fixed = 163 total (was 161)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 45s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 246m 21s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7684/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7684 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 8b33f6f74320 5.15.0-138-generic #148-Ubuntu SMP Fri Mar 14 19:05:48 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e655ee0ccf618b2c4803d9776c34123f8c69cb9c |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7684/1/testReport/ |\r\n   | Max. process+thread count | 1294 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7684/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade SBT to 1.11.7\nDescription: We last upgraded SBT two years ago. Let's upgrade SBT to the latest version.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Its children have been converted to Junit 5, but the parent has not.\r\nThis breaks most of the child test classes (at least with the latest JUnit5+Surefire).\r\nThe breakage may not happen with the current old Surefire, but it is more likely that it is just silently ignored.", "output": "Migrate ViewFileSystemBaseTest to Junit 5"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found.\r\n\r\nI think the cause is:\r\n * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration`\r\n * However, code paths to load AWS credential provider call `S3AUtils.getInstanceFromReflection`, which uses the classloader that loaded the S3AUtils class. That's likely to be the built-in application classloader, which won't be able to load classes in a Spark job JAR.\r\n\r\nAnd the fix seems small:\r\n * Change `S3AUtils.getInstanceFromReflection` to load classes using the `Configuration`'s classloader. Luckily we already have the Configuration in this method.", "output": "S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "It has been observed that certain requests taking more time than expected to complete hinders the performance of whole workload. Such requests are known as tailing requests. They can be taking more time due to a number of reasons and the prominent among them is a bad network connection. In Abfs driver we cache network connections and keeping such bad connections in cache and reusing them can be bad for perf.\r\n\r\nIn this effort we try to identify such connections and close them so that new good connetions can be established and perf can be improved. There are two parts of this effort.\r\n # Identifying Tailing Requests: This involves profiling all the network calls and getting percentiles value optimally. By default we consider p99 as the tail latency and all the future requests taking more than tail latency will be considere as Tailing requests.\r\n\r\n # Proactively Killing Socket Connections: With Apache client, we can now kill the socket connection and fail the tailing request. Such failures will not be thrown back to user and retried immediately without any sleep but from another socket connection.", "output": "ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: some tests failing on third-party stores\nDescription: Some tests against third-party stores fail\r\n- Includes fix for the assumeStoreAwsHosted() logic.\r\n- Documents how to turn off multipart uploads with third-party stores.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove no-op `spark.shuffle.blockTransferService` configuration\nDescription: \nQ: Issue resolved by pull request 52709\n[https://github.com/apache/spark/pull/52709]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: New ConnectorClientConfigOverridePolicy with allowlist of configurations\nDescription: Jira for KIP-1188: https://cwiki.apache.org/confluence/x/2IkvFg", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implement integration test for offline migration\nDescription: In https://issues.apache.org/jira/browse/KAFKA-19570 we implemented offline migration between groups, that is, the following integration test or system test should be possible:\r\n\r\nTest A:\r\n * Start a streams application with classic protocol, process up to a certain offset and commit the offset and shut down.\r\n\r\n * Start the same streams application with streams protocol (same app ID!).\r\n\r\n * Make sure that the offsets before the one committed in the first run are not reprocessed in the second run.\r\n\r\n \r\n\r\nTest B:\r\n * Start a streams application with streams protocol, process up to a certain offset and commit the offset and shut down.\r\n\r\n * Start the same streams application with classic protocol (same app ID!).\r\n\r\n * Make sure that the offsets before the one committed in the first run are not reprocessed in the second run.\r\n\r\n \r\n\r\nWe have unit tests that make sure that non-empty groups will not be converted. This should be enough.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: hadoop-thirdparty build to update maven plugin dependencies\nDescription: github action builds of PRs for hadoopHthirdparty fail because of throttling NVE throttling of requests; needs an update to a later version with either retries or use of a github source cve list.\r\n\r\ndependency checker 11+ \r\n\r\n{code}\r\nMandatory Upgrade Notice\r\nUpgrading to 10.0.2 or later is mandatory\r\n\r\nOlder versions of dependency-check are causing numerous, duplicative requests that end in processing failures are causing unnecassary load on the NVD API. Dependency-check 10.0.2 uses an updated User-Agent header that will allow the NVD to block calls from the older client.\r\n{code}\r\n\r\n----\r\n\r\nThe upgraded dependency checker now *requires* java11+, and *prefers* the provision of an API key for the national vulnerabilities database. It also skips the sonatype check as that no longer supports anonymous checks at all", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Classify errors for AvroOptions boolean casting failure\nDescription: IllegalArgumentException is thrown when AvroOptions option requiring boolean is given to be not boolean. Should classify the error.\nQ: Issue resolved by pull request 52686\n[https://github.com/apache/spark/pull/52686]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add a config entry to make IPC.Client checkAsyncCall off by default", "output": "Add a config entry to make IPC.Client checkAsyncCall off by default"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Replace AssumptionViolatedException with TestAbortedException\nDescription: In JUnit 4, {{org.junit.internal.AssumptionViolatedException}} is used to indicate assumption failure and skip the test. However, {{AssumptionViolatedException}} is an implementation in JUnit 4, and in JUnit 5, we can use {{TestAbortedException}} to replace {{{}AssumptionViolatedException{}}}.\r\n\r\n{{TestAbortedException}} is used to indicate that a test has been aborted, and it can be used to replace {{{}AssumptionViolatedException{}}}. However, it is not directly related to assumption failure and is more commonly used in situations where the test needs to be aborted during execution.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We've observed request timeouts occurring during SASL reauthentication, and analysis suggests the issue is caused by a race condition between request handling and SASL reauthentication on the broker side. Here’s the sequence:\r\n # Client sends a request (Req1) to the broker.\r\n # Client initiates SASL reauthentication.\r\n # Broker receives Req1.\r\n # Broker also begins SASL reauthentication.\r\n # While reauth is in progress:\r\n ** Broker completes processing of Req1 and prepares a response (Res1).\r\n ** Res1 is queued via KafkaChannel.send().\r\n ** Broker sets SelectionKey.OP_WRITE to indicate write readiness.\r\n ** However, Selector.attemptWrite() does not proceed because:\r\n *** channel.hasSend() is true, but\r\n *** channel.ready() is false (reauth is still in progress).\r\n # Once reauthentication completes: Broker removes SelectionKey.OP_WRITE.\r\n # At this point:\r\n ** channel.hasSend() and channel.ready() are now true,\r\n ** But key.isWritable() is false, so the response (Res1) is never sent.\r\n # The response remains stuck in the send buffer. Client eventually hits a request timeout.\r\n\r\nThe fix is to set write readiness using SelectionKey.OP_WRITE at the end of Step 6. This is similar to [what we do on client side|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientAuthenticator.java#L422].", "output": "Request Timeout During SASL Reauthentication Due to Missed OP_WRITE  interest set "}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-streaming.\nDescription: \nQ: hadoop-yetus commented on PR #7554:\nURL: https://github.com/apache/hadoop/pull/7554#issuecomment-2763283048\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 32 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-streaming.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-streaming.txt) |  hadoop-tools/hadoop-streaming: The patch generated 18 new + 449 unchanged - 4 fixed = 467 total (was 453)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   6m  8s |  |  hadoop-streaming in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  74m 51s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7554 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d98dc63e8fb3 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f9475972a23830f75d849e36c6827e432ffbbc60 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/1/testReport/ |\r\n   | Max. process+thread count | 760 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-streaming U: hadoop-tools/hadoop-streaming |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7554:\nURL: https://github.com/apache/hadoop/pull/7554#issuecomment-2767916602\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 32 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-streaming.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-streaming.txt) |  hadoop-tools/hadoop-streaming: The patch generated 14 new + 446 unchanged - 7 fixed = 460 total (was 453)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   6m 11s |  |  hadoop-streaming in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  75m 59s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7554 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 89de1a249385 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d4b6fc6b44f8b8d88a98a9d7d14c56a5ac3e371c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/2/testReport/ |\r\n   | Max. process+thread count | 759 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-streaming U: hadoop-tools/hadoop-streaming |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix compilation error of native libraries on newer GCC\nDescription: Building with {{-Pnative}} by using GCC 14 on Fedora 40 failed due to incompatible changes of GCC.\nQ: {noformat}\r\n[WARNING] make[2]: *** [CMakeFiles/hadoop_static.dir/build.make:286: CMakeFiles/hadoop_static.dir/main/native/src/org/apache/hadoop/crypto/random/OpensslSecureRandom.c.o] Error 1\r\n[WARNING] make[2]: *** Waiting for unfinished jobs....\r\n[WARNING] /hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/random/OpensslSecureRandom.c: In function ‘locks_setup’:\r\n[WARNING] /hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/random/OpensslSecureRandom.c:250:33: error: implicit declaration of function ‘dlsym_CRYPTO_num_locks’; did you mean ‘dlsym_CRYPTO_malloc’? [-Wimplicit-function-declaration]\r\n[WARNING]   250 |   lock_cs = dlsym_CRYPTO_malloc(dlsym_CRYPTO_num_locks() *  \\\r\n[WARNING]       |                                 ^~~~~~~~~~~~~~~~~~~~~~\r\n[WARNING]       |                                 dlsym_CRYPTO_malloc\r\n[WARNING] /hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/random/OpensslSecureRandom.c:257:3: error: implicit declaration of function ‘dlsym_CRYPTO_set_id_callback’; did you mean ‘CRYPTO_set_id_callback’? [-Wimplicit-function-declaration]\r\n[WARNING]   257 |   dlsym_CRYPTO_set_id_callback((unsigned long (*)())pthreads_thread_id);\r\n[WARNING]       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n[WARNING]       |   CRYPTO_set_id_callback\r\n[WARNING] /hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/random/OpensslSecureRandom.c:258:3: error: implicit declaration of function ‘dlsym_CRYPTO_set_locking_callback’; did you mean ‘CRYPTO_set_locking_callback’? [-Wimplicit-function-declaration]\r\n[WARNING]   258 |   dlsym_CRYPTO_set_locking_callback((void (*)())pthreads_locking_callback);\r\n[WARNING]       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n[WARNING]       |   CRYPTO_set_locking_callback\r\n{noformat}", "output": "{noformat}\r\n[WARNING] /hadoop/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/uri.h:60:3: error: ‘uint16_t’ does not name a type\r\n[WARNING]    60 |   uint16_t get_port() const;\r\n[WARNING]       |   ^~~~~~~~\r\n[WARNING] /hadoop/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/uri.h:25:1: note: ‘uint16_t’ is defined in header ‘’; this is probably fixable by adding ‘#include ’\r\n[WARNING]    24 | #include \r\n[WARNING]   +++ |+#include \r\n[WARNING]    25 | #include \r\n...\r\n{noformat}"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, the {{{}AdminClient{}}}’s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does *not* include rack information, even though the underlying {{ConsumerGroupDescribeResponse}} protocol already supports a {{rackId}} field.\r\nThis causes users to be unable to retrieve member rack information through the Admin API.\r\n\r\nRack information is crucial for:\r\n * Monitoring and visualization tools\r\n\r\n * Operational analysis of rack distribution\r\n\r\n * Diagnosing rack-aware assignment issues\r\n\r\nIn addition, StreamsGroupMemberDescription already includes the rackId, so adding it here would also make the behavior more consistent.\r\n\r\nThe PR:  [https://github.com/apache/kafka/pull/20691]\r\n\r\nThe KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1227%3A+Expose+Rack+ID+in+MemberDescription+and+ShareMemberDescription", "output": "Expose Rack ID in MemberDescription"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Incorrect leaderId and leaderEpoch logged in Partition#makeFollower\nDescription: [Partition::makeFollower|https://github.com/apache/kafka/blob/be3e40c45ac9a70833ce0d5735ad141a5bd24627/core/src/main/scala/kafka/cluster/Partition.scala#L877] logs the existing leader-id and existing leader epoch if the partition registration has a leader epoch > the existing leader epoch.\r\n\r\nThere's a bug in the logging as we update the existing leaderId and leaderEpoch with the latest values from partition registration prior to logging them, thereby never logging the existing values.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Refactore DescribeTopicPartitionsRequestHandler: Improve readability and add code documentation\nDescription: My First Kafka code improvement.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: testVectoredReadAfterNormalRead() failing with 412 response from S3\nDescription: This is surfacing on a bucket using versionid for change detection: block reads are failing in the test {{ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead()}}\r\n\r\n{code}\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 412, Request ID: 0AN2EB8QXC75HH0T, Extended Request ID: U5l/UnIF4n3NO1mrZVzS2vv72F3LgUoVJxR4XodUSaTWCerfjmmpH45CbFGKkTkfgfnykwzseGo=)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:104)\r\n\r\n{code}\r\n\r\n* this is the normal readFully() call, before the vectored one\r\n* it worked last week\r\n* also found on branch-3.4 before the SDK update, so not an issue caused by the SDK unless my maven repo is badly contaminated\r\n* seems unrelated to versioning -still there when disabled.\r\n* applies on unversioned s3 express store too.\r\n\r\nAbout the main way I could see this surface is if the test file is less\nQ: ok took me a few hours but I get it now. \r\n\r\n \r\n\r\nThis happens when encryption is enabled (eg: SSE-KMS), because in that case the etag is not the md5 of the object content. When making a GET request, AAL does \r\n \r\nGetRequest.builder()\r\n.s3Uri(this.objectKey.getS3URI())\r\n.range(this.range)\r\n.etag(this.objectKey.getEtag())\r\n \r\nthe etag comes from cached HeadObjectResponse. When you run the whole test suite, head object gets cached from a previous HEAD. Then in this test, the cached value gets used, but since with SSE-KMS the value will change every time the vectored object vectored_file.txt gets created, this will fail. \r\n \r\nWhen not using SSE-KMS, the etag is always the same, as it is just the md5 of the object content which never changes.", "output": "Need to think what do here, will fix tomorrow. Maybe skip this test if encryption is enabled for AAL, as we don't support this right now: https://github.com/awslabs/analytics-accelerator-s3/issues/218"}
