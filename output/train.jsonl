{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve Error Handling for Spark Connect\nDescription: This is an umbrella Jira to migrate exceptions in Spark Connect from plain runtime exceptions to proper Spark Exceptions with error class.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce the framework for adding ST functions in Scala\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52803.", "output": "Issue resolved by pull request 52803\n[https://github.com/apache/spark/pull/52803]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `protobuf-java` to 4.33.0\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce deliveryCompleteCount in ReadShareGroupStateSummary\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: AWS4SignerType failing after S3 service changes\nDescription: With the latest change to the AWS S3 integrity, all existing releases of hadoop using the v2 SDK \r\n    fs.s3a.bucket.signing-algorithm\r\n    AWS4SignerType\r\n  \r\n{code}\r\n\r\n\r\nrejected at far end by\r\n{code}\r\n\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: Missing required header for this request: x-amz-content-sha256 (Service: S3, Status Code: 400, Request ID: 7M0MNNE8NBAGZWFH, Extended Request ID: A/0oyoZ52GLklg+GuV70vqPNEAK350ZF1rTJaNtSajWSXLT5bUCC/Gu6VbN8Pu+AeaMboIIGyGHeFLXOKUwdVQ==)\r\n{code}\r\n\r\n\r\nPresumably some change in 2.30.0 restored compatibility, but this means that all shipping 3.4.x releases do not work.\nQ: If this goes away with 2.30.0+ SDKs, then we should \r\n\r\n* include it in the release notes for HADOOP-19485\r\n* Expect it to go away with later releases\r\n* add it to the qualification test matrix", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][BugFix] IsNonEmptyDirectory Check should loop on listing using updated continuation token\nDescription: As part of recent change, loop over listing was added when checking if a directory is empty or not.\r\nContinuation token was not getting updated in subsequent listblob calls leading to infinite loop of list calls.\r\n\r\nCaused by: https://issues.apache.org/jira/browse/HADOOP-19572\nQ: anujmodi2021 commented on PR #7716:\nURL: https://github.com/apache/hadoop/pull/7716#issuecomment-2917205819\n\n   ------------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 813, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 816, Failures: 0, Errors: 0, Skipped: 116\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 655, Failures: 0, Errors: 0, Skipped: 222\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 813, Failures: 0, Errors: 0, Skipped: 175\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 659, Failures: 0, Errors: 0, Skipped: 133\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 652, Failures: 0, Errors: 0, Skipped: 224\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 656, Failures: 0, Errors: 0, Skipped: 146\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 654, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 687, Failures: 0, Errors: 0, Skipped: 171\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 652, Failures: 0, Errors: 0, Skipped: 222\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   Time taken: 375 mins 17 secs.", "output": "anujmodi2021 merged PR #7716:\nURL: https://github.com/apache/hadoop/pull/7716"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The Netty update in HADOOP-19335 required a newer grpc compiler which requires newer libraries than even the add-on C++ environment has in Centos 7.\r\n\r\nCentos 7 is EOL, and fixing Centos 7 would require rebuilding and replacing the C++ library at which point why even bother ?", "output": "Remove broken Centos 7 C++ precommit checks from CI"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-dynamometer-infra.\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Reduce default page size by LONG_ARRAY_OFFSET if ZGC or ShenandoahGC and ON_HEAP are used\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade to com.google.guava 32.1.3-jre to fix CVE-2023-2976 and CVE-2020-8908\nDescription: Upgrade com.google.guava library to 32.1.3-jre to fix:\r\n * [CVE-2023-2976|https://github.com/advisories/GHSA-7g45-4rm6-3mm3]\r\n * [CVE-2020-8908|https://github.com/advisories/GHSA-5mg8-w23w-74h3]\r\n\r\nLink to PR:\r\n[https://github.com/apache/hadoop/pull/7473]\nQ: bartoszkosiorek closed pull request #7458: HADOOP-19481 Upgrade to com.google.guava 32.1.3-jre to fix security issues\nURL: https://github.com/apache/hadoop/pull/7458", "output": "hadoop-yetus commented on PR #7473:\nURL: https://github.com/apache/hadoop/pull/7473#issuecomment-2703250363\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 47s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  31m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 54s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 55s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 32s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 32s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  50m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  30m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  5s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 41s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  18m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 35s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  51m 44s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 568m  5s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7473/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 32s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 843m 27s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.client.api.impl.TestYarnClient |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClient |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMProxy |\r\n   |   | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E |\r\n   |   | hadoop.yarn.client.api.impl.TestNMClient |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7473/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7473 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux ec095d1f3ec5 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0b041ef17a44b126f8eca900dae62940580e6543 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7473/1/testReport/ |\r\n   | Max. process+thread count | 3047 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7473/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update upgrade.html with the changes in Apache Kafka 4.2\nDescription: Add the changes from KIP-1147 into upgrade.html. Rather than have 5 PRs make tiny changes, it's probably more efficient to do this in one PR.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h3. Summary\r\n\r\nThe upgrade to Jetty 12 in Kafka 4.0 enables a strict SNI host check by default for the Connect REST API. This change breaks HTTPS request forwarding between Connect workers when they connect via IP address, causing requests to fail with a 400: Invalid SNI error.\r\nh3. The Problem\r\n\r\nPrior to Kafka 4.0, the Jetty server used for the Connect REST API did not enforce a strict match between the TLS SNI hostname and the HTTP Host header.\r\n\r\nWith the upgrade to Jetty 12, this check is now enabled by default at the HTTP level. This causes legitimate HTTPS requests to fail in environments where the client connects using an IP address or a hostname that is not listed in the server's TLS certificate.\r\n\r\nThis results in the following error:\r\n{code:java}\r\norg.eclipse.jetty.http.BadMessageException: 400: Invalid SNI\r\n{code}\r\nh3. Impacted Use Case: Inter-Node Request Forwarding\r\n\r\nThis change specifically breaks the request forwarding mechanism between Connect workers in a common deployment scenario:\r\n # A follower Connect instance needs to forward a REST request to the leader.\r\n # The follower connects directly to the leader's IP address over HTTPS.\r\n # Security is handled by mTLS certificates, often managed by a custom certificate provider.\r\n\r\nThis setup worked flawlessly before Kafka 4.0. Now, because the follower connects via IP, the SNI check fails, and the forwarding mechanism is broken.\r\nh3. Proposed Solution\r\n\r\nThis behavior cannot be disabled through any existing Kafka Connect configuration. To restore the previous functionality, a SecureRequestCustomizer must be programmatically configured in RestServer.java to disable the SNI required and the SNI host check flags. This should be driven by a configuration value that allows disabling these SNI related settings.\r\n{code:java}\r\n// In RestServer.java, when building the HTTPS connector\r\nSecureRequestCustomizer customizer = new SecureRequestCustomizer();\r\ncustomizer.setSniRequired(false);\r\ncustomizer.setSniHostC", "output": "[Connect] Provide a configuration to disable SNI required and SNI host checking for the REST API"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade Java SDK V2 to v2.31.66+\nDescription: Upgrade Java SDK to v2.31.66 or higher. \r\n\r\nFollow the qualification process defined in [https://github.com/steveloughran/engineering-proposals/blob/trunk/qualifying-an-SDK-upgrade.md] .\r\n\r\n \r\n\r\n2.31.66+ should now support third party stores, this was previously a blocker to upgrade SDK. \r\n\r\nUpgrade is needed as it has support for some S3 express features, and a fix for https://github.com/aws/aws-sdk-java-v2/issues/5247", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "macOS Tahoe includes a native container daemon and runtime and it is supposed to be near feature-compatible with docker. In principle, it should be possible to run the container build using the container command line on macOS Tahoe.\r\n\r\nIt would be great if we can add this functionality to the build script so folks can build on macOS natively without creating another VM.", "output": "make container build work on macOS Tahoe"}
{"instruction": "Answer the question based on the bug.", "input": "Title: kafka-consumer-groups.sh --describe --all-groups command times out on large clusters with many consumer groups\nDescription: Description:\r\nWhen running the following command in a Kafka cluster with a large number of consumer groups (over 380) and topics (over 500), the kafka-consumer-groups.sh --describe --all-groups operation consistently times out and fails to return results.\r\n\r\nCommand used:\r\n\r\n./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups\r\nObserved behavior:\r\nThe command fails with a TimeoutException, and no consumer group information is returned. The following stack trace is observed:\r\n\r\njava.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeConsumerGroups, deadlineMs=1753170317381, tries=1, nextAllowedTryMs=1753170317482) timed out at 1753170317382 after 1 attempt(s)\r\n    at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\r\n    at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\r\n    ...\r\nCaused by: org.apache.kafka.common.errors.Tim\nQ: hi, i am looking into this, i will give a update soon!", "output": "Thank you for looking into this."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "As JUnit6 was released [https://docs.junit.org/6.0.0/release-notes], it would be great to upgrade repo with such version. Currently {*}5.13.1{*}.", "output": "Upgrade Kafka repo to use JUnit6"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Client#call decrease asyncCallCounter incorrectlly when exceptions occur\nDescription: Client#call decrease asyncCallCounter incorrectlly when exceptions occur\nQ: hfutatzhanghb opened a new pull request, #7384:\nURL: https://github.com/apache/hadoop/pull/7384\n\n   ### Description of PR\r\n   Client#call decrease asyncCallCounter incorrectlly when exceptions occur.", "output": "hadoop-yetus commented on PR #7384:\nURL: https://github.com/apache/hadoop/pull/7384#issuecomment-2656410395\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  23m 49s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   9m 50s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 54s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m 47s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 50s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   8m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 38s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  12m 12s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 42s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 128m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7384 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 524428b2d7da 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 220e74e3e6fd5d1083bbce660942e8de08ad361f |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/1/testReport/ |\r\n   | Max. process+thread count | 3149 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.*\r\n\r\nBrokerNotFoundException was deprecated for removal, as it's not used any longer.\r\n\r\nCf https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=373886192", "output": "Remove BrokerNotFoundException"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: HADOOP-19455. S3A: Enable logging of SDK client metrics\nDescription: HADOOP-19455. S3A: Enable logging of SDK client metrics\r\n\r\nTo log the output of the AWS SDK metrics, set the log\r\n`org.apache.hadoop.fs.s3a.DefaultS3ClientFactory` to `TRACE`.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`\nDescription: Recently, `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` frequently fails.\r\n\r\n[https://github.com/apache/spark/actions/runs/18872699886/job/53854858890]\r\n\r\n \r\n{code:java}\r\n[info] - Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) have been lost during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-41, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Asser\nQ: Issue resolved by pull request 52766\n[https://github.com/apache/spark/pull/52766]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `JUnit` to 6.0.0\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Intermittent test failures when using chained emit strategy on window close\nDescription: Hi,\r\n\r\nI have a test case that contains a topology with 2 time windows and chained emitStrategy calls. The standalone reproduction use case is attached to this issue\r\nThe problem is that about 25% of the time the test fails. About 75% of the time the test succeeds. I followed the conclusions of \r\n!https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype|width=16,height=16! KAFKA-19810\r\nto make sure that I am sending events at the correct times (at least I think my calculations are correct). I really need to get to the bottom of why the test fails intermittently, as the test somewhat reflects a real life topology of a project I am working on, and we cannot have an intermittently failing test in our build pipeline. \r\nGreg\nQ: the goal, of course, is to have unit test that is reliable, i.e. doesn't fail intermittently. I need to understand whether the issue is in my test code (attached) or elsewhere in kafka test framework.", "output": "I run the test in a loop in a shell script like this:\r\n\r\n#!/bin/bash\r\nset -e\r\n\r\nfor i in \\{1..100}; do\r\n  echo ========================================================\r\n  echo ======================== $i ============================\r\n  echo ========================================================\r\n  mvn surefire:test -Dtest=ChainedEmitStrategyTopologyTest3\r\ndone\r\n\r\nI am attaching output of one such run, where first 5 iterations succeeded, but the 6th one failed"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "With *KAFKA-19817 moving all related components to `server-common`, we can rewrite and migrate `ThrottledChannelExpirationTest`. Also, the test that was moved in KAFKA-17372* should be merged into it.", "output": "Move ThrottledChannelExpirationTest to server-common module"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "follow up to HADOOP-19259", "output": "upgrade to jackson 2.18"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Running Kinesis test with {{ENABLE_KINESIS_TEST=1}} fails with {{java.lang.NoClassDefFoundError}}:\r\n\r\n{code:java}\r\nENABLE_KINESIS_TESTS=1 ./build/sbt -Pkinesis-asl\r\n...\r\nUsing endpoint URL https://kinesis.us-west-2.amazonaws.com for creating Kinesis streams for tests.\r\n[info] WithoutAggregationKinesisBackedBlockRDDSuite:\r\n[info] org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite *** ABORTED *** (1 second, 131 milliseconds)\r\n[info]   java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/PropertyNamingStrategy$PascalCaseStrategy\r\n[info]   at com.amazonaws.services.kinesis.AmazonKinesisClient.(AmazonKinesisClient.java:86)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient$lzycompute(KinesisTestUtils.scala:59)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient(KinesisTestUtils.scala:58)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:169)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.findNonExistentStreamName(KinesisTestUtils.scala:182)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.createStream(KinesisTestUtils.scala:85)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.$anonfun$beforeAll$1(KinesisBackedBlockRDDSuite.scala:45)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled(KinesisFunSuite.scala:41)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled$(KinesisFunSuite.scala:39)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.runIfTestsEnabled(KinesisBackedBlockRDDSuite.scala:26)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.beforeAll(KinesisBackedBlockRDDSuite.scala:43)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at", "output": "Kinesis tests are broken"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This implements KIP-1147 for kafka-console-producer, kafka-console-consumer and kafka-console-share-consumer.", "output": "Consistency of command-line arguments for console producer/consumer"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Hive should support IPv6\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Simplify Java Home finding for SBT unidoc\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support Geography and Geometry in SRS mappings\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump Avro 1.11.5\nDescription: \nQ: Issue resolved by pull request 52663\n[https://github.com/apache/spark/pull/52663]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SBT assembly should correct handle META-INF\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add configuration validation option to Kafka server startup\nDescription: *Problem Statement:* Currently, Kafka administrators have no way to validate server configuration files without actually starting the Kafka broker. This leads to:\r\n * Wasted time during deployments when configuration errors are discovered only at startup\r\n * Potential service disruptions in production environments\r\n * Difficulty in CI/CD pipelines to validate Kafka configurations before deployment\r\n * No quick way to test configuration changes without full broker startup overhead\r\n * *Critical cluster stability issues during rolling restarts* - misconfigured brokers can cause:\r\n ** Partition leadership imbalances\r\n ** Replication factor violations\r\n ** Network connectivity issues between brokers\r\n ** Data consistency problems\r\n ** Cascading failures across the cluster when multiple brokers restart with incompatible configurations\r\n\r\n\r\n*Proposed Solution:* Add a {*}--check-config{*}{{{}{}}} command-line option to the Kafka server startup script that would:\r\n\r\n \r\n * Parse and validate th\nQ: Hi , can I work on this ?", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "TestFcLocalFsPermission has been recently migrated to Junit 5. However, its superclass FileContextPermissionBase uses Junit 4, which causes the class initializerers (BeforeAll/BeforeClass) not to be called in certain circumstances.\r\n\r\nMigrate FileContextPermissionBase and its children fully to Junit 5.", "output": "Migrate FileContextPermissionBase to Junit 5"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The unit test failure in ITestAzureBlobFileSystemMainOperation is due to the missing {{@BeforeEach}} and {{@AfterEach}} annotations. This occurs because in JUnit 5, if a subclass overrides the parent class's {{setup}} and {{tearDown}} methods, the {{@BeforeEach}} and {{@AfterEach}} annotations must be explicitly added.", "output": "Fix Unit Test Failure in ITestAzureBlobFileSystemMainOperation"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [Connect] Provide a configuration to disable SNI required and SNI host checking for the REST API\nDescription: h3. Summary\r\n\r\nThe upgrade to Jetty 12 in Kafka 4.0 enables a strict SNI host check by default for the Connect REST API. This change breaks HTTPS request forwarding between Connect workers when they connect via IP address, causing requests to fail with a 400: Invalid SNI error.\r\nh3. The Problem\r\n\r\nPrior to Kafka 4.0, the Jetty server used for the Connect REST API did not enforce a strict match between the TLS SNI hostname and the HTTP Host header.\r\n\r\nWith the upgrade to Jetty 12, this check is now enabled by default at the HTTP level. This causes legitimate HTTPS requests to fail in environments where the client connects using an IP address or a hostname that is not listed in the server's TLS certificate.\r\n\r\nThis results in the following error:\r\n{code:java}\r\norg.eclipse.jetty.http.BadMessageException: 400: Invalid SNI\r\n{code}\r\nh3. Impacted Use Case: Inter-Node Request Forwarding\r\n\r\nThis change specifically breaks the request forwarding mechanism between Connect workers in a common depl\nQ: if you disable sni how then can you locate different host names in the same web server ?", "output": "We are routing by IP."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Implement the ST_Srid expression in SQL\nDescription: \nQ: Issue resolved by pull request 52795\n[https://github.com/apache/spark/pull/52795]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Kafka broker freezes and gets fenced during rolling restart with KRaft mode\nDescription: After upgrading our Kafka clusters to *Kafka 3.9.0* with *KRaft mode enabled* in production, we started observing strange behavior during rolling restarts of broker nodes — behavior we had never seen before.\r\n\r\nWhen a broker is {*}gracefully shut down by the KRaft controller{*}, it immediately restarts. Shortly afterward, while it is busy {*}replicating missing data{*}, the broker suddenly {*}freezes for approximately 20–50 seconds{*}. During this time, it produces {*}no logs, no metrics{*}, and *no heartbeat messages* to the controllers (see the timestamps below).\r\n\r\n \r\n{code:java}\r\n2025-07-30 09:21:27,224 INFO [Broker id=8] Skipped the become-follower state change for my-topic-215 with topic id Some(yO4CQayIRbyESrHHVPdOrQ) and partition state LeaderAndIsrPartitionState(topicName='my-topic', partitionIndex=215, controllerEpoch=-1, leader=4, leaderEpoch=54, isr=[4, 8], partitionEpoch=102, replicas=[4, 8], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since\nQ: Sharing a 5m Flame graph profiling snapshot that was taken after a broker restart that experienced this exact issue - [^flame3.html].\r\n\r\nThe Kafka cluster spec during the simulation was:\r\n * 12 Broker nodes\r\n * Using 8 vCPUs, 32G of RAM & 3.7TB local volume (im4gn.2xlarge AWS instances)\r\n * Disk utilization reached 60% during the simulation", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: kraft Add/RemoveVoterHandlers do not check request's timeout\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add tests for ConsumerRebalanceMetricsManager\nDescription: While working on KAFKA-19722  I noticed we had no test coverage for the rebalance metrics on the new consumer (this class [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/metrics/ConsumerRebalanceMetricsManager.java] )\r\n\r\nI introduced the test file and added a test related to KAFKA-19722, but we should extend coverage and test the other rebalance metrics (ex. tests like the ones for the ShareRebalanceMetrics [https://github.com/apache/kafka/blob/trunk/clients/src/test/java/org/apache/kafka/clients/consumer/internals/metrics/ShareRebalanceMetricsManagerTest.java] )\nQ: Thanks [~lianetm] . I will start working on it", "output": "[~lianetm]  PR is for review https://github.com/apache/kafka/pull/20565/files"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: AAL - Add support for stream leak detection\nDescription: AAL currently does not support leak detection. \r\n\r\nIt may not require this, as individual streams do not hold only to any connections/resources, the factory does. We should verify if it's required, and if yes, add support.", "output": "Reopened"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add dual-stack/IPv6 Support to HttpServer2\nDescription: To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp class to bind to an IPv6 address. WebApp uses the HttpServer2, and adding the IPv6 connector to this class makes the solution more elegant.\r\n\r\nTo enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host.\r\nWhen the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6.\r\nWhen java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors.\r\nTo disable IPv4, you need to configure the OS at the system level.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This patch is similar from HDDS-13014. We can add a name normalization cache between the Hadoop metrics name and the Prometheus metrics name to prevent expensive regex matchings during the metric normalization conversion.", "output": "Improve PrometheusMetricsSink#normalizeName performance"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move TransactionLogTest to transaction-coordinator module\nDescription: this is the follow-up of KAFKA-18884", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Refactor RelationResolution to enable code reuse\nDescription: Refactor RelationResolution to enable code reuse", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Close stale PRs on GitHub which updated over 100 days ago, base on the voting thread: https://lists.apache.org/thread/7x6c029fp9k62bxt2995dz6q6j6sg0bh", "output": "Close stale PRs updated over 100 days ago."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "It shows the following error.\r\n{code:java}\r\nchia7712@fedora:~/project/kafka$ ./bin/kafka-features.sh version-mapping\r\nusage: kafka-features [-h] [--command-config COMMAND_CONFIG] (--bootstrap-server BOOTSTRAP_SERVER | --bootstrap-controller BOOTSTRAP_CONTROLLER) {describe,upgrade,downgrade,disable,version-mapping,feature-dependencies} ...\r\nkafka-features: error: one of the arguments --bootstrap-server --bootstrap-controller is required\r\n{code}\r\n\r\nBy contrast, storage tool works well\r\n\r\n{code:java}\r\nchia7712@fedora:~/project/kafka$ ./bin/kafka-storage.sh version-mapping\r\nmetadata.version=27 (4.1-IV1)\r\nkraft.version=1\r\ntransaction.version=2\r\ngroup.version=1\r\neligible.leader.replicas.version=1\r\nshare.version=0\r\nstreams.version=0\r\n\r\n{code}", "output": "The `version-mapping` of kafka-features.sh should work without requiring the bootstrap"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We want to support JDK21, we better have it available in the development image for testing.", "output": "Add JDK 21 to Ubuntu 20.04 docker development images"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [ReadAheadV2] Increase Prefetch Aggressiveness to improve sequential read performance\nDescription: Various analyses done in the past have shown a need for significant improvement in the performance of sequential reads. The current implementation clearly shows the lack of parallelism that is needed to cater to high throughput sequential read workloads. \r\nMore details on updated design and results of POC benchmarking will be added here soon.\nQ: only do this is if the read policy set in openFile() is whole-file or sequential, as prefetching has minimal benefits for columnar data being processed by query engines, while adding overhead and cost. Unless you have benchmarking showing an advantage over vector io reads...", "output": "Yes, in complete agreement to your thoughts here [~stevel@apache.org] \r\nThis change will only be effective for sequential read patterns. As soon as a random read pattern is detected, we switch off trigerring prefetches.\r\n\r\nI am working on a design doc based on a POC which I did. Soon will be adding the doc here and start working on formal PRs\r\nThanks"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "failures are of the form:\r\n\r\n\r\n{{software.amazon.awssdk.core.exception.SdkClientException: Invalid configuration: region from ARN `us-east-1` does not match client region `us-east-2` and UseArnRegion is `false`}}\r\n{{ }}\r\n{{at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)}}\r\n\r\n{{ }}\r\n{{happens because when making the head request in the tests, we do }}\r\n{{HeadBucketRequest.builder().bucket(getFileSystem().getBucket()).build();}}\r\n{{ }}\r\n{{when using access points, bucket is \"arn:aws:s3:us-east-1:xxxx:accesspoint/test-bucket\", so client gets the region from the ARN which does not match the configured region. }}", "output": "S3A: ITestS3AEndpointRegion fails when using with access points"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Build {{apache/hadoop}} Docker image for both amd64 and arm64.", "output": "Build multi-arch hadoop image"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Since the `calculateAssignment` method does not provide directory inforamtion, the output assignment also omits it.", "output": "The assignment generated by ReassignPartitionsCommand should include directories"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add `build_python_3.11_macos26.yml` GitHub Action job\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: disable strong integrity checksum feature from recent AWS SDKs\nDescription: AWS SDK 2.30.0+ added checksum headers to all requests\r\nhttps://github.com/aws/aws-sdk-java-v2/discussions/5802\r\n\r\nThis apparently causes a lot of regressions, especially with third party stores, as reported in icebeg\r\n\r\nhttps://github.com/apache/iceberg/pull/12264\r\n\r\nnote the disclaimer that the SDK is for AWS S3 storage only\r\n\r\nbq. the AWS SDKs and CLI are designed for usage with official AWS services. We may introduce and enable new features by default, such as these new default integrity protections, prior to them being supported or otherwise handled by third-party service implementations.\r\n\r\nI understand why the team doesn't bother testing with other people's stores, but we have to support them and need to consider all new SDK features are potentially trouble, so disable them by default.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: sendDeferedResponse should also log exception info.\nDescription: sendDeferedResponse should also log exception info.\nQ: hadoop-yetus commented on PR #7684:\nURL: https://github.com/apache/hadoop/pull/7684#issuecomment-2876281591\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 50s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 52s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 15s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7684/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 2 new + 161 unchanged - 0 fixed = 163 total (was 161)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 45s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 246m 21s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7684/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7684 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 8b33f6f74320 5.15.0-138-generic #148-Ubuntu SMP Fri Mar 14 19:05:48 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e655ee0ccf618b2c4803d9776c34123f8c69cb9c |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7684/1/testReport/ |\r\n   | Max. process+thread count | 1294 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7684/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "github-actions[bot] commented on PR #7684:\nURL: https://github.com/apache/hadoop/pull/7684#issuecomment-3272752016\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add tests for ConsumerRebalanceMetricsManager\nDescription: While working on KAFKA-19722  I noticed we had no test coverage for the rebalance metrics on the new consumer (this class [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/metrics/ConsumerRebalanceMetricsManager.java] )\r\n\r\nI introduced the test file and added a test related to KAFKA-19722, but we should extend coverage and test the other rebalance metrics (ex. tests like the ones for the ShareRebalanceMetrics [https://github.com/apache/kafka/blob/trunk/clients/src/test/java/org/apache/kafka/clients/consumer/internals/metrics/ShareRebalanceMetricsManagerTest.java] )\nQ: Hey [~goyal.arpit.91] , this is a small one that should be a nice easy way to get to work in the new consumer area , if you are interested you can take it (I remembered you asked when working on KAFKA-19259). Thanks!\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-19259?focusedCommentId=18018434&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18018434", "output": "Thanks [~lianetm] . I will start working on it"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update byte-buddy to 1.15.11\nDescription: This is required for being able to handle recent Java bytecodes.\r\n\r\nByte-buddy is used by BOTH mockito and maven-shade-plugin.\r\n\r\n1.15.11 is the last byte-buddy version that maven-shade-plugin 3.6.0 works with.\nQ: stoty commented on PR #7687:\nURL: https://github.com/apache/hadoop/pull/7687#issuecomment-2897656712\n\n   @cnauroth @steveloughran @slfan1989 Can you please take a look ?", "output": "slfan1989 commented on code in PR #7687:\nURL: https://github.com/apache/hadoop/pull/7687#discussion_r2106550167\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -146,6 +146,7 @@\n     4.1.118.Final\n     1.1.10.4\n     1.7.1\n+    1.15.11\n\nReview Comment:\n   Thank you for your contribution. Where will we be using this package?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [SDP] Test (and fix) read/readstream options are respected for pipelines\nDescription: Add tests to verify read/readstream options are actually respected by the flow that executes the read/readstream dataframe.\r\n\r\nTrivial test example might be:\r\n{code:python}\r\n@materialized_view def mv_from_csv():\r\n   return spark.read.option(\"delimiter\", \"|\").csv(\"/my/table.csv\")\r\n{code}\r\nI suspect that today, the read/readstream options will not be respected ([1|https://github.com/apache/spark/blob/master/sql/pipelines/src/main/scala/org/apache/spark/sql/pipelines/graph/FlowAnalysis.scala#L120], [2)|#L131].\r\n\r\nIf true, a solution might be to copy over the options in the `UnresolvedRelation` into either the DataFrameReader that is constructed or the `streamingReadOptions`/`batchReadOptions` argument.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Re-enable SmokeTestDriverIntegrationTest for processing threads\nDescription: The test fails occasionally with a race condition like this:\r\n{code:java}\r\n 2_0 RUNNING StreamTask(active): org.apache.kafka.streams.errors.StreamsException: java.util.ConcurrentModificationExceptionorg.apache.kafka.streams.errors.StreamsException: java.util.ConcurrentModificationException at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:977) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:894)Caused by: java.util.ConcurrentModificationException at java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1597) at java.base/java.util.HashMap$KeyIterator.next(HashMap.java:1620) at org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer.resume(ClassicKafkaConsumer.java:979) at org.apache.kafka.clients.consumer.KafkaConsumer.resume(KafkaConsumer.java:1509) at org.apache.kafka.streams.processor.internals.StreamTask.resumePollingForPartitionsWithAvailableSpace(StreamTask.java:623) at org.apache.kafka.stre\nQ: Hi, I would like to work on this issue. Could you please assign it to me?\r\nThanks!", "output": "\"Processing threads\" are a WIP feature, but nobody is currently working on it. – This ticket cannot be worked on right now. We would first need to complete \"processing thread\" feature, to ensure it's working correctly to avoid that this test hits this bug of the incomplete \"processing threads\" feature."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Resolve build error caused by missing Checker Framework (NonNull not recognized)\nDescription: In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed.\r\n{code:java}\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[216,50] package org.checkerframework.checker.nullness.qual does not exist\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[2509,30] cannot find symbol\r\n[ERROR]   symbol:   class NonNull\r\n[ERROR]   location: class org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.AsyncThreadFactory\r\n {code}\r\nI checked the usage in the related modules, and we should use *org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.NonNull* instead of directly using {*}org.checkerframework.checker.nullness.qual.NonNull{*}.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is a follow up from KAFKA-19542(https://issues.apache.org/jira/browse/KAFKA-19542) where some sensors in the share-consumer are not closed when the consumer closes.", "output": "ShareConsumer.close() does not remove all sensors."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: RISC-V Architecture Support\nDescription: This ticket serves as an umbrella (Uber JIRA) to track all work related to enabling and improving support for the RISC-V architecture within the project.\r\n * Platform-specific optimizations and compatibility adjustments\r\n * Any follow-up bug fixes or enhancements specific to RISC-V\r\n * Test and CI coverage for RISC-V environments\r\n * Documentation updates related to RISC-V support", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, Spark converts Catalyst Expression to either Filter or Predicate and pushes it to DSV2 via SupportsPushdownFilters and SupportsPushdownV2Filters API's.\r\n\r\nHowever, some Spark filters may not convert cleanly.  For example, trim(part_col) = 'a'.  There are cases where DSV2 can return the exact partition value(s) to spark for its InputPartition, and Spark can use the original catalyst expression for filtering.", "output": "Enhance DSV2 partition filtering using catalyst expression"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, the unit tests in the project do not fully leverage modern assertion capabilities, resulting in lower readability and maintainability of the test code. To improve test clarity and extensibility, we plan to migrate the existing assertion library to {*}AssertJ{*}.\r\n\r\n \r\n\r\n*Objective:*\r\n * Migrate all assertions in existing tests from JUnit or other assertion libraries to AssertJ.\r\n\r\n * Utilize AssertJ’s rich assertion methods (e.g., fluent API, more readable assertions) to enhance the expressiveness of unit tests.\r\n\r\n * Ensure that all existing unit tests continue to run correctly after migration.\r\n\r\n \r\n\r\n*Implementation Steps:*\r\n # Analyze existing unit test code to identify assertions that need to be replaced.\r\n\r\n # Replace existing assertions with AssertJ assertion syntax.\r\n\r\n # Run unit tests to ensure the tests pass and function correctly after migration.\r\n\r\n # Update relevant documentation to ensure the team is aware of how to use AssertJ for assertions.", "output": "Migrate to AssertJ for Assertion Verification"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)\nDescription: In core-default.xml, the property hadoop.security.crypto.buffer.size is currently documented as “The buffer size used by CryptoInputStream and CryptoOutputStream.” It does not specify the legal value constraints.\r\n{code:java}\r\n  \r\nhadoop.security.crypto.buffer.size  \r\n8192  \r\nThe buffer size used by CryptoInputStream and CryptoOutputStream.  \r\n {code}\r\nThe runtime enforces two hidden constraints that are not documented:\r\n1. Minimum value is 512 bytes. Values below 512 cause IllegalArgumentException at stream construction time.\r\n2. Block-size flooring: The effective buffer size is floored to a multiple of the cipher algorithm’s block size (e.g., 16 bytes for AES/CTR/NoPadding).\r\n\r\nAs a result, users may be surprised that:\r\n1. Setting a value like 4100 results in an actual capacity of 4096.\r\n2. Setting values <512 fails fast with IllegalArgumentException.\r\n\r\n*Expected*\r\ncore-default.xml (and user-facing docs) should explicitly document:\r\n1. Minimum legal value: 512 bytes.\r\n\r\n2. The effective value is floored to the nearest multiple of the cipher algorithm block size (e.g., 16 for AES).", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Comply with ASF website policy \nDescription: Fix issue about 'Apache Project Website Checks' shows at https://whimsy.apache.org/site/project/hadoop\nQ: Confirm the ASF web site compliance checker now is most green for Hadoop now. https://whimsy.apache.org/site/project/hadoop", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix Unit Test Failure in ITestAzureBlobFileSystemMainOperation\nDescription: The unit test failure in ITestAzureBlobFileSystemMainOperation is due to the missing {{@BeforeEach}} and {{@AfterEach}} annotations. This occurs because in JUnit 5, if a subclass overrides the parent class's {{setup}} and {{tearDown}} methods, the {{@BeforeEach}} and {{@AfterEach}} annotations must be explicitly added.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Memory leak using range scans\nDescription: We introduced a regression bug in AK 4.1.0 release, when range scans are use. We are leaking some metrics/sensors.\r\n\r\nThanks to [~eduwerc] and [~hermankj] for reporting this issue.\r\n\r\nAnybody who is using iterator, ie, any type of all/rang/prefix scan etc is affected. Note that certain DSL operators like session-windows, sliding-windows, stream-stream joins, and FK-join all use range scans internally, so the number of affected users is expected to be quite high.\nQ: > also update the Kafka web page with a note that people should be\r\ncareful and maybe not even upgrade to Kafka Streams 4.1.0.\r\n\r\nHi [~mjsax], I can make a PR for the documentation changes. Would [upgrade-guide.html|https://github.com/shashankhs11/kafka/blob/trunk/docs/streams/upgrade-guide.html] be an ideal spot for this note?", "output": "Yes, the upgrade-guide would be the right place. I would also add a note to the top-level upgrade note in addition: [https://github.com/apache/kafka/blob/trunk/docs/upgrade.html#L194]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob] WASB to ABFS Migration Config Support Script\nDescription: The legacy WASB driver has been deprecated and is no longer recommended for use. To support customer onboard for migration from WASB to ABFS driver, we've introduced a script to help with the configuration changes required for the same.\r\n\r\nThe script requires the configuration file (in XML format) used for WASB and would generate configuration file required for ABFS driver respectively.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: ITestS3AEndpointRegion fails when using with access points\nDescription: failures are of the form:\r\n\r\n\r\n{{software.amazon.awssdk.core.exception.SdkClientException: Invalid configuration: region from ARN `us-east-1` does not match client region `us-east-2` and UseArnRegion is `false`}}\r\n{{ }}\r\n{{at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)}}\r\n\r\n{{ }}\r\n{{happens because when making the head request in the tests, we do }}\r\n{{HeadBucketRequest.builder().bucket(getFileSystem().getBucket()).build();}}\r\n{{ }}\r\n{{when using access points, bucket is \"arn:aws:s3:us-east-1:xxxx:accesspoint/test-bucket\", so client gets the region from the ARN which does not match the configured region. }}\nQ: a lot of these tests fail for an S3 Express bucket too, as getBucketLocation() is not allowed..\r\n\r\n \r\norg.apache.hadoop.fs.s3a.AWSUnsupportedFeatureException: getBucketLocation() on ahmar-sdk-upgrade-cse-kms--use1-az4--x-s3: software.amazon.awssdk.services.s3.model.S3Exception: The specified method is not allowed against this resource. (BgwjiHiSxWSjcmp2qn):MethodNotAllowed: The specified method is not allowed against this resource. (Service: S3, Status Code: 405, Request ID: 0128520F4A010195C9C4CF2005004583203BAAA2, Extended Request ID: BgwjiHiSxWSjcmp2qn)\r\n \r\nat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:305)\r\nat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)\r\nat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)\r\nat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\r\nat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)\r\nat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:347)", "output": "Can happen with third party stores too, i think there I just ignore them\r\noptions \r\n* add another test.fs.s3a. switch and allow for all location tests to be skipped. \r\n* add a flag which declares they'll be rejected, and then expect failures (best?)\r\n\r\nLooking at the tests, many seem to consider various exception types as valid outcomes. Maybe AWSUnsupportedFeatureException should be added to the list, so 3express is automatically handled"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Reproduced while testing system resilience and turning S3 network off (introduced a network partition to the list of IP addresses S3 uses) - but given it's seemingly timers related stack traces, I'd guess it could happen any time?\r\n\r\n\r\n{code:java}\r\nFound one Java-level deadlock:\r\n=============================\r\n\"sdk-ScheduledExecutor-2-3\":\r\n  waiting to lock monitor 0x00007f5c880a8630 (object 0x0000000315523c78, a java.lang.Object),\r\n  which is held by \"sdk-ScheduledExecutor-2-4\"\r\n\"sdk-ScheduledExecutor-2-4\":\r\n  waiting to lock monitor 0x00007f5c7c016700 (object 0x0000000327800000, a org.apache.hadoop.fs.s3a.S3ABlockOutputStream),\r\n  which is held by \"io-compute-blocker-15\"\r\n\"io-compute-blocker-15\":\r\n  waiting to lock monitor 0x00007f5c642ae900 (object 0x00000003af0001d8, a java.lang.Object),\r\n  which is held by \"sdk-ScheduledExecutor-2-3\"\r\nJava stack information for the threads listed above:\r\n===================================================\r\n\"sdk-ScheduledExecutor-2-3\":\r\n        at java.lang.Thread.interrupt(java.base@21/Thread.java:1717)\r\n        - waiting to lock  (a java.lang.Object)\r\n        at software.amazon.awssdk.core.internal.http.timers.SyncTimeoutTask.run(SyncTimeoutTask.java:60)\r\n        - locked  (a java.lang.Object)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(java.base@21/Executors.java:572)\r\n        at java.util.concurrent.FutureTask.run(java.base@21/FutureTask.java:317)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(java.base@21/ScheduledThreadPoolExecutor.java:304)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@21/ThreadPoolExecutor.java:1144)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@21/ThreadPoolExecutor.java:642)\r\n        at java.lang.Thread.runWith(java.base@21/Thread.java:1596)\r\n        at java.lang.Thread.run(java.base@21/Thread.java:1583)\r\n\"sdk-ScheduledExecutor-2-4\":\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.ge", "output": "S3A: Deadlock in multipart upload"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "After upgrading our Kafka clusters to *Kafka 3.9.0* with *KRaft mode enabled* in production, we started observing strange behavior during rolling restarts of broker nodes — behavior we had never seen before.\r\n\r\nWhen a broker is {*}gracefully shut down by the KRaft controller{*}, it immediately restarts. Shortly afterward, while it is busy {*}replicating missing data{*}, the broker suddenly {*}freezes for approximately 20–50 seconds{*}. During this time, it produces {*}no logs, no metrics{*}, and *no heartbeat messages* to the controllers (see the timestamps below).\r\n\r\n \r\n{code:java}\r\n2025-07-30 09:21:27,224 INFO [Broker id=8] Skipped the become-follower state change for my-topic-215 with topic id Some(yO4CQayIRbyESrHHVPdOrQ) and partition state LeaderAndIsrPartitionState(topicName='my-topic', partitionIndex=215, controllerEpoch=-1, leader=4, leaderEpoch=54, isr=[4, 8], partitionEpoch=102, replicas=[4, 8], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 54. (state.change.logger) [kafka-8-metadata-loader-event-handler]\r\n\r\n2025-07-30 09:21:51,887 INFO [Broker id=8] Transitioning 427 partition(s) to local followers. (state.change.logger) [kafka-8-metadata-loader-event-handler]       {code}\r\n \r\n\r\nAfter this “hanging” period, the broker *resumes normal operation* without emitting any error or warning messages — as if nothing happened. However, during this gap, because the broker fails to send heartbeats to the KRaft controllers, it gets *fenced out of the cluster* ([9s timeout|https://kafka.apache.org/documentation/#brokerconfigs_broker.session.timeout.ms]), which leads to {*}partitions going offline and, ultimately, data loss{*}.\r\n{code:java}\r\n2025-07-30 09:21:40,325 INFO [QuorumController id=300] Fencing broker 8 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager) [quorum-controller-300-event-handler]{code}\r\n \r\n\r\nWe were able to re-produce this behaviour w", "output": "Kafka broker freezes and gets fenced during rolling restart with KRaft mode"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve Netty usage patterns\nDescription: \nQ: Hi, [~yao]. As a recognition of your contribution, I made this umbrella JIRA issue with `releasenotes` label.\r\nAlthough I have more ideas for this topic and will add some, I believe we are able to mark this as a *resolved* item for Apache Spark 4.1.0 by containing only what we did and what we want to add until the feature freeze. WDYT?", "output": "Thank you for making this umbrella, [~dongjoon]. Making it resolved sounds good to me"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve handling of long processing times\nDescription: If the share consumer application takes a long time to process records, the share consume request manager can do a better job of time-outs and liveness.", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix site CSP issue which to match Infra team requirement.\nDescription: The Infra team has issued a requirement to comply with CSP standards, prohibiting the use of external resource files on the site. Currently, we have found that the Hadoop website has lost its styling. The goal of this PR is to comply with the CSP standards by changing the references to external CSS resources to local ones.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [RISC-V]  Add rv bulk CRC32 (non-CRC32C) optimized path\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "[Partition::makeFollower|https://github.com/apache/kafka/blob/be3e40c45ac9a70833ce0d5735ad141a5bd24627/core/src/main/scala/kafka/cluster/Partition.scala#L877] logs the existing leader-id and existing leader epoch if the partition registration has a leader epoch > the existing leader epoch.\r\n\r\nThere's a bug in the logging as we update the existing leaderId and leaderEpoch with the latest values from partition registration prior to logging them, thereby never logging the existing values.", "output": "Incorrect leaderId and leaderEpoch logged in Partition#makeFollower"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove AppInfoParser deprecated metrics\nDescription: Ref: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1120%3A+AppInfo+metrics+don%27t+contain+the+client-id]\r\n\r\nWe should remove the metrics whose tags do not contain a client-id in Kafka 5.0.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Frequency estimation functions\nDescription: Introduce the following frequency estimation functions:\r\n * *APPROX_TOP_K(expr[, k[, maxItemsTracked]])* – Returns an approximate list of the top _k_ most frequent values in the input expression using a probabilistic algorithm.\r\n\r\n * *APPROX_TOP_K_ACCUMULATE(expr[, maxItemsTracked])* – Creates a state object that accumulates frequency statistics for use in later estimation or combination.\r\n\r\n * *APPROX_TOP_K_ESTIMATE(state [, k] ])* – Extracts and returns the approximate top _k_ values and their estimated frequencies from an accumulated state.\r\n\r\n * *APPROX_TOP_K_COMBINE(expr[, maxItemsTracked])* – Merges intermediate APPROX_TOP_K state objects, allowing distributed or parallel computation.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve the put/merge operation in ListState when t here are multiple values\nDescription: In SS TWS, when we do the {{put(array)}} operation in liststate, we put the first element and then merge the remaining elements one by one. so if we want to put an array with 100 elements, it means we need do 1 put + 99 merges. This can result in worse performance than a single put operation for the entire array.\r\n\r\n \r\n\r\nSimilar, we have the same issue in {{merge(array)}}\nQ: Issue resolved by pull request 52820\n[https://github.com/apache/spark/pull/52820]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Extract common methods to KafkaOffsetReaderBase\nDescription: When reviewing https://github.com/apache/spark/pull/52729, I found that KafkaOffsetReaderConsumer and KafkaOffsetReaderAdmin have the exactly same getOffsetRangesFromResolvedOffsets method. The method is actually long so seems good to extract them to common one.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: pull out new configuration load/probes under S3AStore\nDescription: S3AFS i's too full of switches; it is why initialize() is so big, and there are lots and lots of fields to record the values.\r\n\r\nMany of these need to go down into S3AStoreImpl, but replicating the same design just pushes the mess down.\r\n\r\nProposed: a child service StoreConfigurationService, which reads in the config during serviceInit(), and set the state in there, from where it can be probed.\r\n\r\nI am initially doing this purely for new configuration flags for the conditional write feature\r\n\r\n* moving other flags in there would be separate work\r\n* new boolean config options should go in here\r\n* we also need to think about integers, units of scale and durations.\r\n\r\nIdeally, we should  the annotations and reflection code from ABFS and\r\norg.apache.hadoop.fs.azurebfs.contracts.annotations.ConfigurationValidationAnnotations,\r\n\r\n* copy it into common\r\n* adding duration and size attributes\r\n* move reflection code from AbfsConfiguration#AbfsConfiguration into there too,\r\n* use in s3a\r\n* migra\nQ: steveloughran opened a new pull request, #7463:\nURL: https://github.com/apache/hadoop/pull/7463\n\n   \r\n   New service under S3AStoreImpl: StoreConfigurationService\r\n   \r\n   This is just the draft design; it is intended to be a place to move most of our configuration options:\r\n   \r\n   flags, durations etc, ideally lifting some of the work from org.apache.hadoop.fs.azurebfs.contracts.annotations.ConfigurationValidationAnnotations,\r\n   \r\n   * putting it into common\r\n   * adding duration and size attributes\r\n   * move reflection code from AbfsConfiguration#AbfsConfiguration into there too, and apply\r\n   \r\n   \r\n   Lifted from #7362, where it will be cut. Because of that lift: doesn't yet compile\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7463:\nURL: https://github.com/apache/hadoop/pull/7463#issuecomment-2697709668\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 20s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 22s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 20s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-aws in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | -1 :x: |  javac  |   0m 20s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-aws in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) |  hadoop-tools/hadoop-aws: The patch generated 8 new + 4 unchanged - 0 fixed = 12 total (was 4)  |\r\n   | -1 :x: |  mvnsite  |   0m 22s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 28s | [/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/patch-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-aws in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | -1 :x: |  spotbugs  |   0m 20s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 37s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 24s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 117m  2s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7463 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f81f972f2167 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 881353edd69002a23bcd2b3bc5ab9c484458ef98 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/testReport/ |\r\n   | Max. process+thread count | 674 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7463/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support TIME in the make_timestamp and try_make_timestamp functions in Scala\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Optimize Boost dependency in build images\nDescription: At least in the Windows Docker image, vspkg currently builds 171 packages the vast majority of which are varios boost modules and their dependencies.\r\n\r\nHadoop only uses a handful of those modules.\r\n\r\nOptimize the Windows image by only installing the boost modules that Hadoop actually needs. This would save time, disk space, bandwidth.\r\n\r\nThis is less relevant in the Linux images, where the Boost build simply skips the parts with missing dependencies.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: AssertDataframeEqual carries rows when showing differences\nDescription: When we try to do assertDataFrameEqual, on two dataframes that have not the same amount of rows, the output gets cascading from the difference till the end. Why this is happening:\r\n\r\n[https://github.com/apache/spark/blob/067969ff946712eeabf47040415f25000837cd87/python/pyspark/testing/utils.py#L1036]\r\n{code:java}\r\n    def assert_rows_equal(\r\n        rows1: List[Row], rows2: List[Row], maxErrors: int = None, showOnlyDiff: bool = False\r\n    ):\r\n        __tracebackhide__ = True\r\n        zipped = list(zip_longest(rows1, rows2))\r\n        diff_rows_cnt = 0\r\n        diff_rows = []\r\n        has_diff_rows = False        \r\n        rows_str1 = \"\"\r\n        rows_str2 = \"\"        \r\n        \r\n        # count different rows\r\n        for r1, r2 in zipped:\r\n            if not compare_rows(r1, r2):\r\n                diff_rows_cnt += 1\r\n                has_diff_rows = True\r\n                if includeDiffRows:\r\n                    diff_rows.append((r1, r2))\r\n                rows_str1 += str(r1) + \"\\n\"\r\n                rows_str2 += str(r2) + \"\\n\"\r\n                if maxErrors is not None and diff_rows_cnt >= maxErrors:\r\n                    break\r\n            elif not showOnlyDiff:\r\n                rows_str1 += str(r1) + \"\\n\"\r\n                rows_str2 += str(r2) + \"\\n\"        \r\n        generated_diff = _context_diff(\r\n            actual=rows_str1.splitlines(), expected=rows_str2.splitlines(), n=len(zipped)\r\n        )        \r\n        if has_diff_rows:\r\n            error_msg = \"Results do not match: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Drop temporary functions in regular UDF tests\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fully Support Java 23, 24 and 25\nDescription: Hadoop trunk today mostly supports JDK17, but doesn't work at all on JDK23. (and conversely on JDK24 to be released in less than two weeks)\r\n\r\nWhile there are many smaller issues, the major breaking change is the SecurityManager removal (JEP411/486), and its many consequences.\r\n\r\nThe obvious change is that Subjec.doAs() and Subject.current() no longer work by default, and the replacement APIs must be used.\r\n\r\nThe more insidius change is that when SecurityManager is disabled then JDK22+ does not propapage the Subject to new Threads, which is something that Hadoop absolutely relies on.\r\n\r\n\r\nNote that Hadoop is always built with with JDK 17  (if the JDK is 17 or newer), unless the target version is specifically overriden.\r\nThis is not a problem, JDK17 class files running on a JDK 24 JVM is the expected use case for binary distributions.\r\n\r\nWe may want to run some tests where Hadoop is also compiled for the lastest JVM later. (taget Java 24 + JVM 24)\nQ: Could you also check the rest of the subtasks and dependent tasks here [~slfan1989]  ?\r\n\r\nThese are fixing varius test issues on JDK11-23 , and are a requirement for adding the big JEP411 related fixes.\r\nI've split them up into small easy-to-review patches.\r\n\r\nAs a side effect, with these fixes applied the test suite should also run cleanly with JDK17 and 21.", "output": "Hi All, would you please set expectations as to when we will be able to run on Java 24/25-ea without getting failures like:\r\n{noformat}\r\n[ERROR] org.apache.commons.vfs2.provider.hdfs.HdfsFileProviderTestCase$HdfsProviderTestSuite -- Time elapsed: 0.006 s (FSNamesystem.java:887)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.(FSNamesystem.java:851)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1396)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:495)\r\n        at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:256)\r\n        at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1158)\r\n        at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1042)\r\n        at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:974)\r\n        at org.apache.hadoop.hdfs.MiniDFSCluster.(MiniDFSCluster.java:888)\r\n        at org.apache.commons.vfs2.provider.hdfs.HdfsFileProviderTestCase$HdfsProviderTestSuite.setUp(HdfsFileProviderTestCase.java:113)\r\n        at org.apache.commons.vfs2.AbstractTestSuite.lambda$run$0(AbstractTestSuite.java:234)\r\n        at junit.framework.TestResult.runProtected(TestResult.java:142)\r\n        at org.apache.commons.vfs2.AbstractTestSuite.run(AbstractTestSuite.java:239)\r\n{noformat}"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Support for new auth type: User-bound SAS\nDescription: Adding support for new authentication type: user bound SAS\nQ: manika137 opened a new pull request, #8051:\nURL: https://github.com/apache/hadoop/pull/8051\n\n   ### Description of PR\r\n   Adding support for new authentication type: user bound SAS\r\n   \r\n   ### How was this patch tested?\r\n   Test suite will be run for the patch", "output": "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3442714485\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 45s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 23s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 34s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/blanks-eol.txt) |  The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 36 new + 4 unchanged - 0 fixed = 40 total (was 4)  |\r\n   | -1 :x: |  mvnsite  |   0m 37s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 32s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 2 new + 1472 unchanged - 0 fixed = 1474 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 2 new + 1413 unchanged - 0 fixed = 1415 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 34s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  29m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 41s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 119m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux 88abe69cd96c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 05b52e40f1bc99edc039fc0d2ab5f83d1ceb0da9 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/testReport/ |\r\n   | Max. process+thread count | 779 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve RebalanceProtocolMigrationIntegrationTest\nDescription: With KAFKA-19647, we added RebalanceProtocolMigrationIntegrationTest, that needs to wait for a \"classic\" consumer group to timeout.\r\n\r\nAfter KAFKA-18193 is completed, we should consider to use the new API to send a leave-group request in the test, to reduce test runtime.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce Geography and Geometry data types to Java API\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Fix Case Sensitivity Issue for hdi_isfolder metadata\nDescription: In the blob endpoint, we determine whether the path is a file, or a directory based on the metadata attribute hdi_isfolder. When creating a directory, we set hdi_isfolder to true. Currently, our method for checking if the path is a directory involves a case-sensitive equality check. Consequently, if someone configures a directory with Hdi_isfolder, the driver will not recognize that path as a directory. We need to address this issue because, in the backend, hdi_isfolder and Hdi_isfolder are considered the same metadata attribute. Therefore, the solution involves modifying our equality check to be case-insensitive, ensuring that the driver correctly identifies directories regardless of case variations in the metadata attribute.\nQ: bhattmanish98 opened a new pull request, #7496:\nURL: https://github.com/apache/hadoop/pull/7496\n\n   Jira: https://issues.apache.org/jira/browse/HADOOP-19494\r\n   ------------------------------------------------------------------\r\n   In the blob endpoint, we determine whether the path is a file, or a directory based on the metadata attribute hdi_isfolder. When creating a directory, we set hdi_isfolder to true. Currently, our method for checking if the path is a directory involves a case-sensitive equality check. Consequently, if someone configures a directory with Hdi_isfolder, the driver will not recognize that path as a directory. We need to address this issue because, in the backend, hdi_isfolder and Hdi_isfolder are considered the same metadata attribute. Therefore, the solution involves modifying our equality check to be case-insensitive, ensuring that the driver correctly identifies directories regardless of case variations in the metadata attribute.", "output": "hadoop-yetus commented on PR #7496:\nURL: https://github.com/apache/hadoop/pull/7496#issuecomment-2714703867\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  13m  3s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  37m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  1s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  6s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 133m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7496/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7496 |\r\n   | JIRA Issue | HADOOP-19494 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 48a24b66f245 5.15.0-134-generic #145-Ubuntu SMP Wed Feb 12 20:08:39 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0cbaae8d373e56ef3914349b03c6171e5bfcf38c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7496/1/testReport/ |\r\n   | Max. process+thread count | 699 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7496/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Clean up integration tests related to state-updater\nDescription: This is the first step in the parent task: To clean up all the integration tests related to state-updater flag\r\n\r\nSee [https://github.com/apache/kafka/pull/20392#issuecomment-3240659815] for more details", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Streaming Query RecentProgress performance regression in Classic Pyspark\nDescription: We have identified a significant performance regression in Apache Spark's streaming recentProgress method in python notebook starting from version 4.0.0. The time required to fetch recentProgress increases substantially as the number of progress records grows, creating a linear or worse scaling issue. We only observe this behavior in classic pyspark.\r\n\r\nWith the following code, it output charts for time it takes to get recentProgress before and after changes in [this commit|https://github.com/apache/spark/commit/22eb6c4b0a82b9fcf84fc9952b1f6c41dde9bd8d#diff-4d4ed29d139877b160de444add7ee63cfa7a7577d849ab2686f1aa2d5b4aae64]\r\n\r\n```\r\n%python\r\nfrom datetime import datetime\r\nimport time\r\n\r\ndf = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\r\nq = df.writeStream.format(\"noop\").start()\r\nprint(\"begin waiting for progress\")\r\n\r\nprogress_list = []\r\ntime_diff_list = []\r\n\r\nnumProgress = len(q.recentProgress)\r\nwhile numProgress < 70 and q.exception() is None:\r\ntime.sleep(1)\r\nbeforeTime = datetime.now()\r\nprint(beforeTime.strftime(\"%Y-%m-%d %H:%M:%S\") +\": before we got those progress: \"+str(numProgress))\r\nrep = q.recentProgress\r\nnumProgress = len(rep)\r\nafterTime = datetime.now()\r\nprint(afterTime.strftime(\"%Y-%m-%d %H:%M:%S\") +\": after we got those progress: \"+str(numProgress))\r\ntime_diff = (afterTime - beforeTime).total_seconds()\r\nprint(\"Total Time: \"+str(time_diff) +\" seconds\")\r\nprogress_list.append(numProgress)\r\ntime_diff_list.append(time_diff)\r\n\r\nq.stop()\r\nq.awaitTermina", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade os-maven-plugin to 1.7.1 to support riscv64\nDescription: *Problem*\r\n\r\nWhen building Hadoop on the  riscv64 architecture, the current version of os-maven-plugin (1.7.0) fails with the following error:\r\n\r\n    os.detected.arch: unknown\r\n\r\n    org.apache.maven.MavenExecutionException: unknown os.arch: riscv64\r\n\r\nThis indicates that version 1.7.0 does not recognize or support the riscv64 target, causing native or plugin-based build failures.\r\n\r\n*Resolution*\r\n\r\nUpgrading os-maven-plugin to version 1.7.1 resolves the issue. The newer version includes updated architecture detection logic and supports riscv64 without error.\nQ: hadoop-yetus commented on PR #7796:\nURL: https://github.com/apache/hadoop/pull/7796#issuecomment-3060513576\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   9m 46s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  20m  1s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 25s | [/branch-compile-hadoop-project-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/artifact/out/branch-compile-hadoop-project-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-project in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 25s | [/branch-compile-hadoop-project-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/artifact/out/branch-compile-hadoop-project-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-project in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 12s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  22m 36s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m  9s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m  7s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m  8s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m  8s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  32m  3s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 24s | [/patch-unit-hadoop-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/artifact/out/patch-unit-hadoop-project.txt) |  hadoop-project in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 25s |  |  ASF License check generated no output?  |\r\n   |  |   |  66m 56s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7796 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux e4e5439ea320 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a55ccaf8c85912a95820a71e54c528862a4fe660 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/testReport/ |\r\n   | Max. process+thread count | 114 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "leiwen2025 commented on PR #7796:\nURL: https://github.com/apache/hadoop/pull/7796#issuecomment-3067494397\n\n   > Some issue with the build, can you rebase and push again to trigger the CI again. I am +1 if the build comes clean\r\n   \r\n   Thank you for the review and the +1. I’ll rebase and push again to re-trigger the CI."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `JaCoCo` to 0.8.14 for official Java 25 support\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We aim to successfully compile Apache Hadoop on CentOS Stream 9 to verify the feasibility of using this platform in future development and deployment environments.\r\n\r\nThis task may involve addressing potential compatibility issues related to GCC and libc versions. Initial attempts will focus on resolving compilation challenges and ensuring the platform meets the necessary requirements for Hadoop builds.", "output": "[JDK17] Attempted to build on CentOS Stream 9."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Create lean docker image\nDescription: Create a new docker image based on the lean tarball.\r\n\r\nhadoop-3.4.2-lean.tar.gz", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use `MacOS 26` in `build_maven_java21_macos15.yml`\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: AbfsConfiguration should store account type information (HNS or FNS)\nDescription: Currently, both {{AbfsClient}} and {{AzureBlobFileSystemStore}} store information about whether the account is FNS or HNS (i.e., whether namespace is enabled). This information should instead be stored at the {{AbfsConfiguration}} level, allowing both the client and the store to retrieve it from there when needed.\nQ: bhattmanish98 commented on code in PR #7765:\nURL: https://github.com/apache/hadoop/pull/7765#discussion_r2174762509\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestGetNameSpaceEnabled.java:\n##########\n@@ -341,6 +344,42 @@ public void testAccountSpecificConfig() throws Exception {\n     }\n   }\n \n+  /**\n+   * Tests the behavior of AbfsConfiguration when the namespace-enabled\n+   * configuration set based on config provided.\n+   *\n+   * Expects the namespace value based on config provided.\n+   *\n+   * @throws Exception if any error occurs during configuration setup or evaluation\n+   */\n+  @Test\n+  public void testNameSpaceConfig() throws Exception {\n\nReview Comment:\n   Added.", "output": "anujmodi2021 commented on code in PR #7765:\nURL: https://github.com/apache/hadoop/pull/7765#discussion_r2174789793\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1525,8 +1529,8 @@ void setMaxBackoffIntervalMilliseconds(int maxBackoffInterval) {\n   }\n \n   @VisibleForTesting\n-  void setIsNamespaceEnabledAccount(String isNamespaceEnabledAccount) {\n-    this.isNamespaceEnabledAccount = isNamespaceEnabledAccount;\n+  public void setIsNamespaceEnabledAccount(Trilean isNamespaceEnabledAccount) {\n\nReview Comment:\n   We should be setting this value on abfsconfiguration object only when we know the exact value. Do we really need this to be Trilean? I think we should pass the definitive argument and do the Boolean to Trilean conversion here just to make sure that no one accidently sets UNKNOWN here.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -443,7 +428,7 @@ private synchronized boolean getNamespaceEnabledInformationFromServer(\n    */\n   @VisibleForTesting\n   boolean isNamespaceEnabled() throws TrileanConversionException {\n-    return this.isNamespaceEnabled.toBoolean();\n+    return abfsConfiguration.getIsNamespaceEnabledAccount().toBoolean();\n\nReview Comment:\n   Better to use getter here and other places for abfsconfiguration. This will make sure same object is being referred everywhere and can easily be mocked if needed in future."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade spotbug to 4.9.4\nDescription: see discussion https://github.com/apache/kafka/pull/20295#issuecomment-3146551515\nQ: I've raised https://github.com/apache/kafka/pull/20333 to fix this.\r\n\r\n[~gongxuanzhang] Sorry, I didn't notice your comment until now, I didn't mean to steal this issue, it just kind of happened since I did the previous spotbugs-related work too. Feel free to let me know if you have a better fix than the one I linked, and I can close my PR in favor of yours.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: NoSuchElementException (again) in Kafka Streams iterator metrics\nDescription: Since upgrading to Kafka Streams 4.1.0, we again see in our metrics collection:\r\n{code:java}\r\nException thrown from GraphiteReporter#report. Exception was suppressed.\r\njava.util.NoSuchElementException: null\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:1863)\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:393)\r\n\tat org.apache.kafka.streams.internals.metrics.OpenIterators.lambda$add$0(OpenIterators.java:57)\r\n\tat org.apache.kafka.common.metrics.KafkaMetric.metricValue(KafkaMetric.java:81) {code}\r\nIt seems that unfortunately the dynamic metric registration introduced in [https://github.com/apache/kafka/pull/20022] undid the fix from [https://github.com/apache/kafka/pull/18771]\r\n\r\nThere is still a race between the metric gauge being removed, and the last open iterator being removed from the tracking set. The same fix from #18771 would fix the issue again.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hi, I got this example by using the following prompt in Google:\r\n # kafka streams unit testing with chained \"emitStrategy\"\r\n # Provide an example of testing chained suppress with different grace periods\r\n\r\n[https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d]\r\n\r\nCompiled and ran the example using latest kafka jars only to get\r\n\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s  expected:  but was: \r\nat org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:158)\r\nat org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:139)\r\nat org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:201)\r\nat org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:168)\r\nat org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:694)\r\nat com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopologyTest.java:123)\r\n\r\nIt appears that the test is not able to drive the kafka stream to emit the 2nd event.\r\nCould be a bug in test code/test driver/kafka streams?\r\nThanks in advance\r\nGreg", "output": "Kafka streams with chained emitStrategy(onWindowClose) example does not work"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{{globalStreamThread}} in {{KafkaStreams}} class ignores all exceptions and fails to apply the user-provided {{StreamsUncaughtExceptionHandler}} in:\r\n{code:java}\r\npublic void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler userStreamsUncaughtExceptionHandler)\r\n{code}\r\nThis can lead to streams being stuck in faulty state after an exception is thrown during their initialization phase (e.g. failure to load the RocksDB native library). The exception isn't logged, so debugging such problem is difficult.\r\n\r\nFrom my understanding of the {{b62d8b97}} commit message, the following code is unnecessary as the {{globalStreamThread}} can't be replaced and the issue description from KAFKA-12699 doesn't apply to it. Removing it should help.\r\n{code:java}\r\nif (globalStreamThread != null) {\r\n    globalStreamThread.setUncaughtExceptionHandler((t, e) -> { }\r\n    );\r\n}\r\n{code}", "output": "Global stream thread ignores all exceptions"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add detailed error message for corrupted view metadata with mismatched column counts\nDescription: Enhance error reporting for corrupted view metadata by adding a detailed, user-friendly assertion message when the number of view query column names and the number of columns in the view schema mismatch.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When running benchmark test, some of the producer perf tests are failing in statistics parsing. Logs attached\r\n\r\ncmd: `TC_PATHS=\"tests/kafkatest/benchmarks/core/benchmark_test.py\" bash tests/docker/run_tests.sh`\r\n\r\nJDK: `azul/zulu-openjdk:17`(used azul's jdk because with openjdk tests were failing to run)", "output": "Producer performance test failing in parsing aggregate statistics"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Jackson to 2.16.0\nDescription: \nQ: why 2.16.0? - you are much better using the latest release (2.19.2) or if there is something in 2.16 that you want but something in 2.17 that you don't like, you should at least use the latest 2.16.x release.\r\n\r\nHADOOP-19544 is already open - it is usually not a good idea not raise issues without checking for open issues in the area", "output": "Oh didn't check existing jira, will close this as duplicate. Upgraded to this version as it was non-vulnerable and did not have any breaking changes."}
{"instruction": "Answer the question based on the bug.", "input": "Title: hadoop binary distribution to move cloud connectors to hadoop common/lib\nDescription: Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed.\r\n\r\n* filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. \r\n* Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. \r\n\r\nThis adds a lot more stuff into the distribution, so I'm doing the following design\r\n* all hadoop-* modules in common/lib\r\n* minimal dependencies for hadoop-azure and hadoop-gcs (once we get those right!)\r\n* hadoop-aws: everything except bundle.jar\r\n* other connectors: only included with explicit profiles.\r\n\r\nASF releases will support azure out the box, the others once you add the dependencies. And anyone can build their own release with everything\r\n\r\nOne concern here, we make hadoop-cloud-storage artifact incomplete at pulling in things when depended on. We may need a separate module for the distro setup.\r\n\r\nNoticed during thi\nQ: steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303127263\n\n   \r\n   * This puts the hadoop-azure, hadoop-aws &c binaries into common/lib and so on the classpath everywhere\r\n   * some problem with gcs instantiation during enum (will file later, as while it surfaces here, I think it's unrelated)\r\n   * my local builds end up (today) with some versioned jars as well as the -SNAPSHOT. I think this is from me tainting my maven repo, would like to see what others see\r\n   \r\n   ```\r\n   total 1401704\r\n   -rw-r--r--@ 1 stevel  staff     106151 Sep 17 12:57 aliyun-java-core-0.2.11-beta.jar\r\n   -rw-r--r--@ 1 stevel  staff     194215 Sep 17 12:57 aliyun-java-sdk-core-4.5.10.jar\r\n   -rw-r--r--@ 1 stevel  staff     163698 Sep 17 12:57 aliyun-java-sdk-kms-2.11.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     220800 Sep 17 12:57 aliyun-java-sdk-ram-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     928456 Sep 17 12:57 aliyun-sdk-oss-3.18.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    2470776 Sep 17 12:57 analyticsaccelerator-s3-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      27006 Sep 17 13:11 aopalliance-repackaged-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      20891 Sep 17 13:11 audience-annotations-0.12.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     651391 Sep 17 13:11 avro-1.11.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     113966 Sep 17 12:57 azure-data-lake-store-sdk-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff      10288 Sep 17 12:57 azure-keyvault-core-1.0.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     815331 Sep 17 12:57 azure-storage-7.0.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    8324412 Sep 17 13:11 bcprov-jdk18on-1.78.1.jar\r\n   -rw-r--r--@ 1 stevel  staff  641534749 Sep 17 12:57 bundle-2.29.52.jar\r\n   -rw-r--r--@ 1 stevel  staff     223979 Sep 17 13:11 checker-qual-3.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      75479 Sep 17 13:11 commons-cli-1.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     353793 Sep 17 13:11 commons-codec-1.15.jar\r\n   -rw-r--r--@ 1 stevel  staff     751914 Sep 17 13:11 commons-collections4-4.4.jar\r\n   -rw-r--r--@ 1 stevel  staff    1079377 Sep 17 13:11 commons-compress-1.26.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     657516 Sep 17 13:11 commons-configuration2-2.10.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      24239 Sep 17 13:11 commons-daemon-1.0.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     508826 Sep 17 13:11 commons-io-2.16.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     673587 Sep 17 13:11 commons-lang3-3.17.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      70816 Sep 17 13:11 commons-logging-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    2213560 Sep 17 13:11 commons-math3-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     316431 Sep 17 13:11 commons-net-3.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     238400 Sep 17 13:11 commons-text-1.10.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    8661164 Sep 17 12:57 cos_api-bundle-5.6.19.jar\r\n   -rw-r--r--@ 1 stevel  staff    2983237 Sep 17 13:11 curator-client-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     336384 Sep 17 13:11 curator-framework-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     315569 Sep 17 13:11 curator-recipes-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     583996 Sep 17 13:11 dnsjava-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     324655 Sep 17 12:57 dom4j-2.1.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     670059 Sep 17 12:57 esdk-obs-java-3.20.4.2.jar\r\n   -rw-r--r--@ 1 stevel  staff       4617 Sep 17 13:11 failureaccess-1.0.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     249277 Sep 17 13:11 gson-2.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    3037368 Sep 17 13:11 guava-32.0.1-jre.jar\r\n   -rw-r--r--@ 1 stevel  staff      94013 Sep 17 12:57 hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      14456 Sep 17 13:11 hadoop-annotations-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     114335 Sep 17 13:11 hadoop-auth-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     930516 Sep 17 12:57 hadoop-aws-3.5.0-20250916.124028-686.jar\r\n   -rw-r--r--@ 1 stevel  staff     827349 Sep 17 12:57 hadoop-azure-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      33363 Sep 17 12:57 hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      70007 Sep 17 12:57 hadoop-cos-3.5.0-20250916.124028-683.jar\r\n   -rw-r--r--@ 1 stevel  staff     138447 Sep 17 12:57 hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     142274 Sep 17 12:57 hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff    3519516 Sep 17 13:11 hadoop-shaded-guava-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1952967 Sep 17 13:11 hadoop-shaded-protobuf_3_25-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    4019589 Sep 17 12:57 hadoop-tos-3.5.0-20250916.124028-202.jar\r\n   -rw-r--r--@ 1 stevel  staff     200223 Sep 17 13:11 hk2-api-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     203358 Sep 17 13:11 hk2-locator-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     131590 Sep 17 13:11 hk2-utils-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     780321 Sep 17 13:11 httpclient-4.5.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     328593 Sep 17 13:11 httpcore-4.4.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     102220 Sep 17 12:57 ini4j-0.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff      29807 Sep 17 13:11 istack-commons-runtime-3.0.12.jar\r\n   -rw-r--r--@ 1 stevel  staff       9301 Sep 17 13:11 j2objc-annotations-2.8.jar\r\n   -rw-r--r--@ 1 stevel  staff      76636 Sep 17 13:11 jackson-annotations-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     473081 Sep 17 13:11 jackson-core-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff    1617187 Sep 17 13:11 jackson-databind-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      68453 Sep 17 13:11 jakarta.activation-1.2.2.jar\r\n   -rw-r--r--@ 1 stevel  staff      44399 Sep 17 13:11 jakarta.activation-api-1.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      25058 Sep 17 13:11 jakarta.annotation-api-1.3.5.jar\r\n   -rw-r--r--@ 1 stevel  staff      18140 Sep 17 13:11 jakarta.inject-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      82973 Sep 17 13:11 jakarta.servlet-api-4.0.4.jar\r\n   -rw-r--r--@ 1 stevel  staff      53683 Sep 17 13:11 jakarta.servlet.jsp-api-2.3.6.jar\r\n   -rw-r--r--@ 1 stevel  staff      91930 Sep 17 13:11 jakarta.validation-api-2.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     140376 Sep 17 13:11 jakarta.ws.rs-api-2.1.6.jar\r\n   -rw-r--r--@ 1 stevel  staff     115638 Sep 17 13:11 jakarta.xml.bind-api-2.3.3.jar\r\n   -rw-r--r--@ 1 stevel  staff       7771 Sep 17 12:57 java-trace-api-0.2.11-beta.jar\r\n   -rw-r--r--@ 1 stevel  staff      18432 Sep 17 12:57 java-xmlbuilder-1.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     794714 Sep 17 13:11 javassist-3.30.2-GA.jar\r\n   -rw-r--r--@ 1 stevel  staff      95806 Sep 17 13:11 javax.servlet-api-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1019097 Sep 17 13:11 jaxb-runtime-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff       4722 Sep 17 13:11 jcip-annotations-1.0-1.jar\r\n   -rw-r--r--@ 1 stevel  staff     327806 Sep 17 12:57 jdom2-2.0.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     311826 Sep 17 13:11 jersey-client-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff    1267957 Sep 17 13:11 jersey-common-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      32929 Sep 17 13:11 jersey-container-servlet-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      75742 Sep 17 13:11 jersey-container-servlet-core-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      80272 Sep 17 13:11 jersey-hk2-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff     964550 Sep 17 13:11 jersey-server-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      90184 Sep 17 12:57 jettison-1.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     249911 Sep 17 13:11 jetty-http-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     183011 Sep 17 13:11 jetty-io-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     118496 Sep 17 13:11 jetty-security-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     739348 Sep 17 13:11 jetty-server-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     146064 Sep 17 13:11 jetty-servlet-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     588962 Sep 17 13:11 jetty-util-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff      66643 Sep 17 13:11 jetty-util-ajax-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     140308 Sep 17 13:11 jetty-webapp-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff      68894 Sep 17 13:11 jetty-xml-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     282591 Sep 17 13:11 jsch-0.1.55.jar\r\n   -rw-r--r--@ 1 stevel  staff      19936 Sep 17 13:11 jsr305-3.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff       4519 Sep 17 13:11 jul-to-slf4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff     223129 Sep 17 13:11 kerb-core-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     115065 Sep 17 13:11 kerb-crypto-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      36361 Sep 17 13:11 kerb-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     100095 Sep 17 13:11 kerby-asn1-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      30190 Sep 17 13:11 kerby-config-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     200581 Sep 17 13:11 kerby-pkix-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      40787 Sep 17 13:11 kerby-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff       2199 Sep 17 13:11 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar\r\n   -rw-r--r--@ 1 stevel  staff     136314 Sep 17 13:11 metrics-core-3.2.4.jar\r\n   -rw-r--r--@ 1 stevel  staff       4554 Sep 17 13:11 netty-all-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     339045 Sep 17 13:11 netty-buffer-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     355199 Sep 17 13:11 netty-codec-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      67192 Sep 17 13:11 netty-codec-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      37789 Sep 17 13:11 netty-codec-haproxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     674362 Sep 17 13:11 netty-codec-http-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     490985 Sep 17 13:11 netty-codec-http2-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      44736 Sep 17 13:11 netty-codec-memcache-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     113699 Sep 17 13:11 netty-codec-mqtt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      46015 Sep 17 13:11 netty-codec-redis-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      21344 Sep 17 13:11 netty-codec-smtp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     121032 Sep 17 13:11 netty-codec-socks-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      34636 Sep 17 13:11 netty-codec-stomp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      19823 Sep 17 13:11 netty-codec-xml-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     719225 Sep 17 13:11 netty-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     580162 Sep 17 13:11 netty-handler-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      25650 Sep 17 13:11 netty-handler-proxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      26833 Sep 17 13:11 netty-handler-ssl-ocsp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      37842 Sep 17 13:11 netty-resolver-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     188360 Sep 17 13:11 netty-resolver-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff       9145 Sep 17 13:11 netty-resolver-dns-classes-macos-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      19825 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      19629 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff     521428 Sep 17 13:11 netty-transport-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     147621 Sep 17 13:11 netty-transport-classes-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     108558 Sep 17 13:11 netty-transport-classes-kqueue-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      42321 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      36594 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar\r\n   -rw-r--r--@ 1 stevel  staff      40644 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff       6193 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      25741 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      25170 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      44157 Sep 17 13:11 netty-transport-native-unix-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      18241 Sep 17 13:11 netty-transport-rxtx-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      50814 Sep 17 13:11 netty-transport-sctp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      32189 Sep 17 13:11 netty-transport-udt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     779369 Sep 17 13:11 nimbus-jose-jwt-9.37.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     425763 Sep 17 12:57 okhttp-3.14.2.jar\r\n   -rw-r--r--@ 1 stevel  staff      91980 Sep 17 12:57 okio-1.17.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     141734 Sep 17 12:57 opentelemetry-api-1.38.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      47252 Sep 17 12:57 opentelemetry-context-1.38.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      18189 Sep 17 12:57 opentracing-api-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      10542 Sep 17 12:57 opentracing-noop-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff       7504 Sep 17 12:57 opentracing-util-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     281989 Sep 17 12:57 org.jacoco.agent-0.8.5-runtime.jar\r\n   -rw-r--r--@ 1 stevel  staff      19479 Sep 17 13:11 osgi-resource-locator-1.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     128414 Sep 17 13:11 re2j-1.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      11369 Sep 17 12:57 reactive-streams-1.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     332398 Sep 17 13:11 reload4j-1.2.22.jar\r\n   -rw-r--r--@ 1 stevel  staff      41125 Sep 17 13:11 slf4j-api-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff       9824 Sep 17 13:11 slf4j-reload4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff    2112099 Sep 17 13:11 snappy-java-1.1.10.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     195909 Sep 17 13:11 stax2-api-4.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      72007 Sep 17 13:11 txw2-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff     443788 Sep 17 12:57 wildfly-openssl-2.1.4.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     522679 Sep 17 13:11 woodstox-core-5.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1323991 Sep 17 13:11 zookeeper-3.8.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     254932 Sep 17 13:11 zookeeper-jute-3.8.4.jar\r\n   \r\n   ```", "output": "steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303133677\n\n   for contrast, here's 3.4.2\r\n   ```\r\n   ../hadoop-3.4.2/:\r\n   total 272\r\n   drwxr-xr-x@ 13 stevel  staff    416 Aug  7 12:58 bin\r\n   -rw-r--r--@  1 stevel  staff    824 Aug 27 16:58 binding.xml\r\n   drwxr-xr-x@  3 stevel  staff     96 Aug 25 17:09 downloads\r\n   drwxr-xr-x@  3 stevel  staff     96 Aug  7 12:16 etc\r\n   drwxr-xr-x@  7 stevel  staff    224 Aug  7 12:58 include\r\n   drwxr-xr-x@  3 stevel  staff     96 Aug  7 12:58 lib\r\n   drwxr-xr-x@ 14 stevel  staff    448 Aug  7 12:58 libexec\r\n   -rw-r--r--@  1 stevel  staff  23682 Aug  7 10:40 LICENSE-binary\r\n   -rw-r--r--@  1 stevel  staff  15791 Aug  7 10:39 LICENSE.txt\r\n   drwxr-xr-x@ 45 stevel  staff   1440 Aug  7 12:58 licenses-binary\r\n   -rw-r--r--@  1 stevel  staff  45514 Aug 27 17:12 log.txt\r\n   -rw-r--r--@  1 stevel  staff  27373 Aug  7 10:39 NOTICE-binary\r\n   -rw-r--r--@  1 stevel  staff   1541 Aug  7 10:39 NOTICE.txt\r\n   -rw-r--r--@  1 stevel  staff    175 Aug  7 10:39 README.txt\r\n   drwxr-xr-x@ 29 stevel  staff    928 Aug  7 12:16 sbin\r\n   -rw-r--r--@  1 stevel  staff    438 Aug 25 16:59 secrets.bin\r\n   drwxr-xr-x@  4 stevel  staff    128 Aug  7 13:23 share\r\n   -rw-r--r--@  1 stevel  staff    275 Aug 27 17:12 system.properties\r\n   \r\n   share/hadoop/common/lib:\r\n   total 1401704\r\n   -rw-r--r--@ 1 stevel  staff     106151 Sep 17 12:57 aliyun-java-core-0.2.11-beta.jar\r\n   -rw-r--r--@ 1 stevel  staff     194215 Sep 17 12:57 aliyun-java-sdk-core-4.5.10.jar\r\n   -rw-r--r--@ 1 stevel  staff     163698 Sep 17 12:57 aliyun-java-sdk-kms-2.11.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     220800 Sep 17 12:57 aliyun-java-sdk-ram-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     928456 Sep 17 12:57 aliyun-sdk-oss-3.18.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    2470776 Sep 17 12:57 analyticsaccelerator-s3-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      27006 Sep 17 13:11 aopalliance-repackaged-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      20891 Sep 17 13:11 audience-annotations-0.12.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     651391 Sep 17 13:11 avro-1.11.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     113966 Sep 17 12:57 azure-data-lake-store-sdk-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff      10288 Sep 17 12:57 azure-keyvault-core-1.0.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     815331 Sep 17 12:57 azure-storage-7.0.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    8324412 Sep 17 13:11 bcprov-jdk18on-1.78.1.jar\r\n   -rw-r--r--@ 1 stevel  staff  641534749 Sep 17 12:57 bundle-2.29.52.jar\r\n   -rw-r--r--@ 1 stevel  staff     223979 Sep 17 13:11 checker-qual-3.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      75479 Sep 17 13:11 commons-cli-1.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     353793 Sep 17 13:11 commons-codec-1.15.jar\r\n   -rw-r--r--@ 1 stevel  staff     751914 Sep 17 13:11 commons-collections4-4.4.jar\r\n   -rw-r--r--@ 1 stevel  staff    1079377 Sep 17 13:11 commons-compress-1.26.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     657516 Sep 17 13:11 commons-configuration2-2.10.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      24239 Sep 17 13:11 commons-daemon-1.0.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     508826 Sep 17 13:11 commons-io-2.16.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     673587 Sep 17 13:11 commons-lang3-3.17.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      70816 Sep 17 13:11 commons-logging-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    2213560 Sep 17 13:11 commons-math3-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     316431 Sep 17 13:11 commons-net-3.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     238400 Sep 17 13:11 commons-text-1.10.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    8661164 Sep 17 12:57 cos_api-bundle-5.6.19.jar\r\n   -rw-r--r--@ 1 stevel  staff    2983237 Sep 17 13:11 curator-client-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     336384 Sep 17 13:11 curator-framework-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     315569 Sep 17 13:11 curator-recipes-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     583996 Sep 17 13:11 dnsjava-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     324655 Sep 17 12:57 dom4j-2.1.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     670059 Sep 17 12:57 esdk-obs-java-3.20.4.2.jar\r\n   -rw-r--r--@ 1 stevel  staff       4617 Sep 17 13:11 failureaccess-1.0.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     249277 Sep 17 13:11 gson-2.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    3037368 Sep 17 13:11 guava-32.0.1-jre.jar\r\n   -rw-r--r--@ 1 stevel  staff      94013 Sep 17 12:57 hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      14456 Sep 17 13:11 hadoop-annotations-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     114335 Sep 17 13:11 hadoop-auth-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     930516 Sep 17 12:57 hadoop-aws-3.5.0-20250916.124028-686.jar\r\n   -rw-r--r--@ 1 stevel  staff     827349 Sep 17 12:57 hadoop-azure-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      33363 Sep 17 12:57 hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      70007 Sep 17 12:57 hadoop-cos-3.5.0-20250916.124028-683.jar\r\n   -rw-r--r--@ 1 stevel  staff     138447 Sep 17 12:57 hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     142274 Sep 17 12:57 hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff    3519516 Sep 17 13:11 hadoop-shaded-guava-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1952967 Sep 17 13:11 hadoop-shaded-protobuf_3_25-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    4019589 Sep 17 12:57 hadoop-tos-3.5.0-20250916.124028-202.jar\r\n   -rw-r--r--@ 1 stevel  staff     200223 Sep 17 13:11 hk2-api-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     203358 Sep 17 13:11 hk2-locator-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     131590 Sep 17 13:11 hk2-utils-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     780321 Sep 17 13:11 httpclient-4.5.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     328593 Sep 17 13:11 httpcore-4.4.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     102220 Sep 17 12:57 ini4j-0.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff      29807 Sep 17 13:11 istack-commons-runtime-3.0.12.jar\r\n   -rw-r--r--@ 1 stevel  staff       9301 Sep 17 13:11 j2objc-annotations-2.8.jar\r\n   -rw-r--r--@ 1 stevel  staff      76636 Sep 17 13:11 jackson-annotations-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     473081 Sep 17 13:11 jackson-core-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff    1617187 Sep 17 13:11 jackson-databind-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      68453 Sep 17 13:11 jakarta.activation-1.2.2.jar\r\n   -rw-r--r--@ 1 stevel  staff      44399 Sep 17 13:11 jakarta.activation-api-1.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      25058 Sep 17 13:11 jakarta.annotation-api-1.3.5.jar\r\n   -rw-r--r--@ 1 stevel  staff      18140 Sep 17 13:11 jakarta.inject-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      82973 Sep 17 13:11 jakarta.servlet-api-4.0.4.jar\r\n   -rw-r--r--@ 1 stevel  staff      53683 Sep 17 13:11 jakarta.servlet.jsp-api-2.3.6.jar\r\n   -rw-r--r--@ 1 stevel  staff      91930 Sep 17 13:11 jakarta.validation-api-2.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     140376 Sep 17 13:11 jakarta.ws.rs-api-2.1.6.jar\r\n   -rw-r--r--@ 1 stevel  staff     115638 Sep 17 13:11 jakarta.xml.bind-api-2.3.3.jar\r\n   -rw-r--r--@ 1 stevel  staff       7771 Sep 17 12:57 java-trace-api-0.2.11-beta.jar\r\n   -rw-r--r--@ 1 stevel  staff      18432 Sep 17 12:57 java-xmlbuilder-1.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     794714 Sep 17 13:11 javassist-3.30.2-GA.jar\r\n   -rw-r--r--@ 1 stevel  staff      95806 Sep 17 13:11 javax.servlet-api-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1019097 Sep 17 13:11 jaxb-runtime-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff       4722 Sep 17 13:11 jcip-annotations-1.0-1.jar\r\n   -rw-r--r--@ 1 stevel  staff     327806 Sep 17 12:57 jdom2-2.0.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     311826 Sep 17 13:11 jersey-client-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff    1267957 Sep 17 13:11 jersey-common-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      32929 Sep 17 13:11 jersey-container-servlet-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      75742 Sep 17 13:11 jersey-container-servlet-core-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      80272 Sep 17 13:11 jersey-hk2-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff     964550 Sep 17 13:11 jersey-server-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      90184 Sep 17 12:57 jettison-1.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     249911 Sep 17 13:11 jetty-http-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     183011 Sep 17 13:11 jetty-io-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     118496 Sep 17 13:11 jetty-security-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     739348 Sep 17 13:11 jetty-server-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     146064 Sep 17 13:11 jetty-servlet-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     588962 Sep 17 13:11 jetty-util-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff      66643 Sep 17 13:11 jetty-util-ajax-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     140308 Sep 17 13:11 jetty-webapp-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff      68894 Sep 17 13:11 jetty-xml-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     282591 Sep 17 13:11 jsch-0.1.55.jar\r\n   -rw-r--r--@ 1 stevel  staff      19936 Sep 17 13:11 jsr305-3.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff       4519 Sep 17 13:11 jul-to-slf4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff     223129 Sep 17 13:11 kerb-core-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     115065 Sep 17 13:11 kerb-crypto-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      36361 Sep 17 13:11 kerb-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     100095 Sep 17 13:11 kerby-asn1-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      30190 Sep 17 13:11 kerby-config-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     200581 Sep 17 13:11 kerby-pkix-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      40787 Sep 17 13:11 kerby-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff       2199 Sep 17 13:11 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar\r\n   -rw-r--r--@ 1 stevel  staff     136314 Sep 17 13:11 metrics-core-3.2.4.jar\r\n   -rw-r--r--@ 1 stevel  staff       4554 Sep 17 13:11 netty-all-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     339045 Sep 17 13:11 netty-buffer-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     355199 Sep 17 13:11 netty-codec-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      67192 Sep 17 13:11 netty-codec-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      37789 Sep 17 13:11 netty-codec-haproxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     674362 Sep 17 13:11 netty-codec-http-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     490985 Sep 17 13:11 netty-codec-http2-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      44736 Sep 17 13:11 netty-codec-memcache-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     113699 Sep 17 13:11 netty-codec-mqtt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      46015 Sep 17 13:11 netty-codec-redis-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      21344 Sep 17 13:11 netty-codec-smtp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     121032 Sep 17 13:11 netty-codec-socks-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      34636 Sep 17 13:11 netty-codec-stomp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      19823 Sep 17 13:11 netty-codec-xml-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     719225 Sep 17 13:11 netty-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     580162 Sep 17 13:11 netty-handler-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      25650 Sep 17 13:11 netty-handler-proxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      26833 Sep 17 13:11 netty-handler-ssl-ocsp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      37842 Sep 17 13:11 netty-resolver-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     188360 Sep 17 13:11 netty-resolver-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff       9145 Sep 17 13:11 netty-resolver-dns-classes-macos-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      19825 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      19629 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff     521428 Sep 17 13:11 netty-transport-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     147621 Sep 17 13:11 netty-transport-classes-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     108558 Sep 17 13:11 netty-transport-classes-kqueue-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      42321 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      36594 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar\r\n   -rw-r--r--@ 1 stevel  staff      40644 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff       6193 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      25741 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      25170 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      44157 Sep 17 13:11 netty-transport-native-unix-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      18241 Sep 17 13:11 netty-transport-rxtx-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      50814 Sep 17 13:11 netty-transport-sctp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      32189 Sep 17 13:11 netty-transport-udt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     779369 Sep 17 13:11 nimbus-jose-jwt-9.37.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     425763 Sep 17 12:57 okhttp-3.14.2.jar\r\n   -rw-r--r--@ 1 stevel  staff      91980 Sep 17 12:57 okio-1.17.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     141734 Sep 17 12:57 opentelemetry-api-1.38.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      47252 Sep 17 12:57 opentelemetry-context-1.38.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      18189 Sep 17 12:57 opentracing-api-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      10542 Sep 17 12:57 opentracing-noop-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff       7504 Sep 17 12:57 opentracing-util-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     281989 Sep 17 12:57 org.jacoco.agent-0.8.5-runtime.jar\r\n   -rw-r--r--@ 1 stevel  staff      19479 Sep 17 13:11 osgi-resource-locator-1.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     128414 Sep 17 13:11 re2j-1.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      11369 Sep 17 12:57 reactive-streams-1.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     332398 Sep 17 13:11 reload4j-1.2.22.jar\r\n   -rw-r--r--@ 1 stevel  staff      41125 Sep 17 13:11 slf4j-api-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff       9824 Sep 17 13:11 slf4j-reload4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff    2112099 Sep 17 13:11 snappy-java-1.1.10.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     195909 Sep 17 13:11 stax2-api-4.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      72007 Sep 17 13:11 txw2-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff     443788 Sep 17 12:57 wildfly-openssl-2.1.4.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     522679 Sep 17 13:11 woodstox-core-5.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1323991 Sep 17 13:11 zookeeper-3.8.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     254932 Sep 17 13:11 zookeeper-jute-3.8.4.jar\r\n   ```"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade commons-lang3 to 3.19.0 \nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When opening a {{{}RocksDbStateStore{}}}, {{.position}} offsets should be migrated into the offsets column family.", "output": "Auto migrate .position offsets"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: MetricsException thrown since miniClusterMode off\nDescription: In `DefaultMetricsSystem`, if `miniClusterMode` is set to false by other functions, `newSourceName` exits with a MetricsException", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The first step to supporting JDK23/24 is being able to test it.\r\n\r\nAdd JDK24 to the default docker images, so that both manual and automated testing is possible.", "output": "Add JDK 24 to Ubuntu 20.04 docker development images"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump guava to  33.4.8-jre due to EOL\nDescription: We can use the latest 33.4.8-jre version as the current one is quite old.\nQ: When I do a thirdparty build I now get a warning of duplicate module 9 info.\r\n{code}\r\n[INFO] No artifact matching filter org.checkerframework:checker-qual\r\n[WARNING] error_prone_annotations-2.36.0.jar, guava-33.4.8-jre.jar, failureaccess-1.0.3.jar, jspecify-1.0.0.jar, j2objc-annotations-3.0.0.jar define 1 overlapping classes: \r\n[WARNING]   - META-INF.versions.9.module-info\r\n[WARNING] maven-shade-plugin has detected that some class files are\r\n[WARNING] present in two or more JARs. When this happens, only one\r\n[WARNING] single version of the class is copied to the uber jar.\r\n[WARNING] Usually this is not harmful and you can skip these warnings,\r\n[WARNING] otherwise try to manually exclude artifacts based on\r\n[WARNING] mvn dependency:tree -Ddetail=true and the above output.\r\n[WARNING] See http://maven.apache.org/plugins/maven-shade-plugin/\r\n{code}\r\nThis is new.\r\n\r\ngiven we want this to work on java17+ , we need to come up with a way of resolving the conflict, at the very least by making the guava one dominant.", "output": "ahh, the later versions of guava exclude a dependency on checkerframework. So any references in our code (there are four) fail. Which means we have to manually add it if we want a drop in replacement. PITA."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] WASB to ABFS Migration Config Support Script\nDescription: The legacy WASB driver has been deprecated and is no longer recommended for use. To support customer onboard for migration from WASB to ABFS driver, we've introduced a script to help with the configuration changes required for the same.\r\n\r\nThe script requires the configuration file (in XML format) used for WASB and would generate configuration file required for ABFS driver respectively.\nQ: hadoop-yetus commented on PR #7564:\nURL: https://github.com/apache/hadoop/pull/7564#issuecomment-2768581697\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7564/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 33s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 37s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7564/1/artifact/out/results-asflicense.txt) |  The patch generated 2 ASF License warnings.  |\r\n   |  |   | 122m 36s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7564/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7564 |\r\n   | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs xmllint markdownlint |\r\n   | uname | Linux 88dd9936b8a7 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3a1a2ebc5a14860824ad9ce035668d08fc1565cc |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7564/1/testReport/ |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7564/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "anujmodi2021 commented on code in PR #7564:\nURL: https://github.com/apache/hadoop/pull/7564#discussion_r2026199497\n\n\n##########\nhadoop-tools/hadoop-azure/dev-support/testrun-scripts/config_support.sh:\n##########\n@@ -0,0 +1,140 @@\n+#!/usr/bin/env bash\n+\n+set -eo pipefail\n+\n+FILE=$1\n+\n+if [ ! -f \"$FILE\" ]; then\n+    echo \"Error: File '$FILE' not found. Exiting....\"\n+    exit 1\n+fi\n+\n+if [[ \"$1\" != *.xml ]]; then\n+    echo \"The file provided is not an XML file. Exiting....\"\n+    exit 1\n+fi\n+\n+OUTPUT_FILE=\"abfs-converted-config.xml\"\n+cp \"$FILE\" \"$OUTPUT_FILE\"\n+\n+contactTeamMsg=\"For any queries or support, kindly reach out to us at 'askabfs@microsoft.com'.\"\n+endpoint=\".dfs.\"\n+printf \"Select 'HNS' if you're migrating to ABFS driver with Hierarchical Namespace enabled account,\n+            or 'Non-HNS' if you're migrating with Non-Hierarchical Namespace (FNS) account. \\n\"\n+printf \"WARNING: Please ensure the correct option is chosen as it will affect the configuration changes made to the file. \\n\"\n+printf \"If you are unsure, follow the instructions below to check from Azure Portal: \\n\"\n+printf \"* Go to the Azure Portal and navigate to your storage account. \\n\"\n+printf \"* In the left-hand menu, select 'Overview' section and look for 'Properties'. \\n\"\n+printf \"* Under 'Blob service', check if 'Hierarchical namespace' is enabled or disabled. \\n\"\n+echo \"$contactTeamMsg\"\n+select namespaceType in \"HNS\" \"NonHNS\"\n+do\n+    case $namespaceType in\n+        HNS)\n+            xmlstarlet ed -L -i '//configuration/property[1]' -t elem -n property -v '' \\\n+              -s '//configuration/property[1]' -t elem -n name -v 'fs.azure.account.hns.enabled' \\\n+              -s '//configuration/property[1]' -t elem -n value -v 'true' \"$OUTPUT_FILE\"\n+            break;\n+            ;;\n+        NonHNS)\n+            endpoint=\".blob.\"\n+            break;\n+            ;;\n+          *)\n+            echo \"Invalid selection. Please try again. Exiting...\"\n+            exit 1;\n+            ;;\n+    esac\n+done\n+\n+# Mapping for renaming configurations\n+declare -A rename_configs_map=(\n+    [\"autothrottling.enable\"]=\"enable.autothrottling\" #fs.azure.autothrottling.enable  to fs.azure.enable.autothrottling\n+    [\"rename.dir\"]=\"rename.key\" # fs.azure.atomic.rename.dir to fs.azure.atomic.rename.key\n+    [\"block.blob.buffered.pread.disable\"]=\"buffered.pread.disable\" #fs.azure.block.blob.buffered.pread.disable to fs.azure.buffered.pread.disable\n+    [\"fs.azure.sas\"]=\"fs.azure.sas.fixed.token.\" #fs.azure.sas.CONTAINER_NAME.ACCOUNT_NAME to fs.azure.sas.fixed.token.CONTAINER_NAME.ACCOUNT_NAME\n+    [\"check.block.md5\"]=\"enable.checksum.validation\" #fs.azure.check.block.md5 to fs.azure.enable.checksum.validation\n+)\n+\n+# Configs not supported in ABFS\n+unsupported_configs_list=(\n+    \"fs.azure.page.blob.dir\"\n+    \"fs.azure.block.blob.with.compaction.dir\"\n+    \"fs.azure.store.blob.md5\"\n+)\n+\n+# Configurations not required in ABFS Driver and can be removed\n+obsolete_configs_list=(\n+    \"azure.authorization\" #fs.azure.authorization, fs.azure.authorization.caching.enable , fs.azure.authorization.caching.maxentries, fs.azure.authorization.cacheentry.expiry.period, fs.azure.authorization.remote.service.urls\n+    \"azure.selfthrottling\" #fs.azure.selfthrottling.enable, fs.azure.selfthrottling.read.factor, fs.azure.selfthrottling.write.factor\n+    \"azure.saskey\" #fs.azure.saskey.cacheentry.expiry.period , fs.azure.saskey.usecontainersaskeyforallaccess\n+    \"copyblob.retry\" #fs.azure.io.copyblob.retry.min.backoff.interval, fs.azure.io.copyblob.retry.max.backoff.interval, fs.azure.io.copyblob.retry.backoff.interval, fs.azure.io.copyblob.retry.max.retries\n+    \"service.urls\" #fs.azure.cred.service.urls , fs.azure.delegation.token.service.urls, fs.azure.authorization.remote.service.urls\n+    \"blob.metadata.key.case.sensitive\" #fs.azure.blob.metadata.key.case.sensitive\n+    \"cacheentry.expiry.period\" #fs.azure.cacheentry.expiry.period\n+    \"chmod.allowed.userlist\" #fs.azure.chmod.allowed.userlist\n+    \"chown.allowed.userlist\" #fs.azure.chown.allowed.userlist\n+    \"daemon.userlist\" #fs.azure.daemon.userlist\n+    \"delete.threads\" #fs.azure.delete.threads\n+    \"enable.kerberos.support\" #fs.azure.enable.kerberos.support\n+    \"flatlist.enable\" #fs.azure.flatlist.enable\n+    \"fsck.temp.expiry.seconds\" #fs.azure.fsck.temp.expiry.seconds\n+    \"local.sas.key.mode\" #fs.azure.local.sas.key.mode\n+    \"override.canonical.service.name\" #fs.azure.override.canonical.service.name\n+    \"permissions.supergroup\" #fs.azure.permissions.supergroup\n+    \"rename.threads\" #fs.azure.rename.threads\n+    \"secure.mode\" #fs.azure.secure.mode\n+    \"skip.metrics\" #fs.azure.skip.metrics\n+    \"storage.client.logging\" #fs.azure.storage.client.logging\n+    \"storage.emulator.account.name\" #fs.azure.storage.emulator.account.name\n+    \"storage.timeout\" #fs.azure.storage.timeout\n+)\n+\n+# Stop the script if any unsupported config is found\n+for key in \"${unsupported_configs_list[@]}\"; do\n+    if grep -q \"$key\" \"$OUTPUT_FILE\"; then\n+        echo \"FAILURE: Remove the following configuration from file and rerun: '$key'\"\n\nReview Comment:\n   Tell user why to remove?\r\n   \"Unsupported Config found\"\n\n\n\n##########\nhadoop-tools/hadoop-azure/dev-support/testrun-scripts/config_support.sh:\n##########\n@@ -0,0 +1,140 @@\n+#!/usr/bin/env bash\n+\n+set -eo pipefail\n+\n+FILE=$1\n+\n+if [ ! -f \"$FILE\" ]; then\n+    echo \"Error: File '$FILE' not found. Exiting....\"\n+    exit 1\n+fi\n+\n+if [[ \"$1\" != *.xml ]]; then\n+    echo \"The file provided is not an XML file. Exiting....\"\n+    exit 1\n+fi\n+\n+OUTPUT_FILE=\"abfs-converted-config.xml\"\n+cp \"$FILE\" \"$OUTPUT_FILE\"\n+\n+contactTeamMsg=\"For any queries or support, kindly reach out to us at 'askabfs@microsoft.com'.\"\n+endpoint=\".dfs.\"\n+printf \"Select 'HNS' if you're migrating to ABFS driver with Hierarchical Namespace enabled account,\n+            or 'Non-HNS' if you're migrating with Non-Hierarchical Namespace (FNS) account. \\n\"\n+printf \"WARNING: Please ensure the correct option is chosen as it will affect the configuration changes made to the file. \\n\"\n+printf \"If you are unsure, follow the instructions below to check from Azure Portal: \\n\"\n+printf \"* Go to the Azure Portal and navigate to your storage account. \\n\"\n+printf \"* In the left-hand menu, select 'Overview' section and look for 'Properties'. \\n\"\n+printf \"* Under 'Blob service', check if 'Hierarchical namespace' is enabled or disabled. \\n\"\n+echo \"$contactTeamMsg\"\n+select namespaceType in \"HNS\" \"NonHNS\"\n+do\n+    case $namespaceType in\n+        HNS)\n+            xmlstarlet ed -L -i '//configuration/property[1]' -t elem -n property -v '' \\\n+              -s '//configuration/property[1]' -t elem -n name -v 'fs.azure.account.hns.enabled' \\\n+              -s '//configuration/property[1]' -t elem -n value -v 'true' \"$OUTPUT_FILE\"\n+            break;\n+            ;;\n+        NonHNS)\n+            endpoint=\".blob.\"\n+            break;\n+            ;;\n+          *)\n+            echo \"Invalid selection. Please try again. Exiting...\"\n+            exit 1;\n+            ;;\n+    esac\n+done\n+\n+# Mapping for renaming configurations\n+declare -A rename_configs_map=(\n+    [\"autothrottling.enable\"]=\"enable.autothrottling\" #fs.azure.autothrottling.enable  to fs.azure.enable.autothrottling\n+    [\"rename.dir\"]=\"rename.key\" # fs.azure.atomic.rename.dir to fs.azure.atomic.rename.key\n+    [\"block.blob.buffered.pread.disable\"]=\"buffered.pread.disable\" #fs.azure.block.blob.buffered.pread.disable to fs.azure.buffered.pread.disable\n+    [\"fs.azure.sas\"]=\"fs.azure.sas.fixed.token.\" #fs.azure.sas.CONTAINER_NAME.ACCOUNT_NAME to fs.azure.sas.fixed.token.CONTAINER_NAME.ACCOUNT_NAME\n+    [\"check.block.md5\"]=\"enable.checksum.validation\" #fs.azure.check.block.md5 to fs.azure.enable.checksum.validation\n+)\n+\n+# Configs not supported in ABFS\n+unsupported_configs_list=(\n+    \"fs.azure.page.blob.dir\"\n+    \"fs.azure.block.blob.with.compaction.dir\"\n+    \"fs.azure.store.blob.md5\"\n+)\n+\n+# Configurations not required in ABFS Driver and can be removed\n+obsolete_configs_list=(\n+    \"azure.authorization\" #fs.azure.authorization, fs.azure.authorization.caching.enable , fs.azure.authorization.caching.maxentries, fs.azure.authorization.cacheentry.expiry.period, fs.azure.authorization.remote.service.urls\n+    \"azure.selfthrottling\" #fs.azure.selfthrottling.enable, fs.azure.selfthrottling.read.factor, fs.azure.selfthrottling.write.factor\n+    \"azure.saskey\" #fs.azure.saskey.cacheentry.expiry.period , fs.azure.saskey.usecontainersaskeyforallaccess\n+    \"copyblob.retry\" #fs.azure.io.copyblob.retry.min.backoff.interval, fs.azure.io.copyblob.retry.max.backoff.interval, fs.azure.io.copyblob.retry.backoff.interval, fs.azure.io.copyblob.retry.max.retries\n+    \"service.urls\" #fs.azure.cred.service.urls , fs.azure.delegation.token.service.urls, fs.azure.authorization.remote.service.urls\n+    \"blob.metadata.key.case.sensitive\" #fs.azure.blob.metadata.key.case.sensitive\n+    \"cacheentry.expiry.period\" #fs.azure.cacheentry.expiry.period\n+    \"chmod.allowed.userlist\" #fs.azure.chmod.allowed.userlist\n+    \"chown.allowed.userlist\" #fs.azure.chown.allowed.userlist\n+    \"daemon.userlist\" #fs.azure.daemon.userlist\n+    \"delete.threads\" #fs.azure.delete.threads\n+    \"enable.kerberos.support\" #fs.azure.enable.kerberos.support\n+    \"flatlist.enable\" #fs.azure.flatlist.enable\n+    \"fsck.temp.expiry.seconds\" #fs.azure.fsck.temp.expiry.seconds\n+    \"local.sas.key.mode\" #fs.azure.local.sas.key.mode\n+    \"override.canonical.service.name\" #fs.azure.override.canonical.service.name\n+    \"permissions.supergroup\" #fs.azure.permissions.supergroup\n+    \"rename.threads\" #fs.azure.rename.threads\n+    \"secure.mode\" #fs.azure.secure.mode\n+    \"skip.metrics\" #fs.azure.skip.metrics\n+    \"storage.client.logging\" #fs.azure.storage.client.logging\n+    \"storage.emulator.account.name\" #fs.azure.storage.emulator.account.name\n+    \"storage.timeout\" #fs.azure.storage.timeout\n+)\n+\n+# Stop the script if any unsupported config is found\n+for key in \"${unsupported_configs_list[@]}\"; do\n+    if grep -q \"$key\" \"$OUTPUT_FILE\"; then\n+        echo \"FAILURE: Remove the following configuration from file and rerun: '$key'\"\n+        failure=true\n+    fi\n+done\n+\n+if [ \"$failure\" = true ]; then\n+    echo \"$contactTeamMsg\"\n+    echo \"Exiting...\"\n+    exit 1\n+fi\n+\n+# Renaming the configs\n+for old in \"${!rename_configs_map[@]}\"; do\n+    new=\"${rename_configs_map[$old]}\"\n+    xmlstarlet ed -L -u \"//property/name[contains(., '$old')]\" -x \"concat(substring-before(., '$old'),\n+     '$new', substring-after(., '$old'))\" \"$OUTPUT_FILE\"\n+done\n+\n+# Remove the obsolete configs\n+for key in \"${obsolete_configs_list[@]}\"; do\n+    xmlstarlet ed -L -d \"//property[name[contains(text(), '$key')]]\" \"$OUTPUT_FILE\"\n+done\n+\n+# Change the endpoints to DFS if migrating to HNS\n+if [ \"$endpoint\" = \".dfs.\" ]; then\n+    xmlstarlet ed -L -u \"//property/name[contains(., '.blob.')]\" -x \"concat(substring-before(., '.blob.'),\n+     '$endpoint', substring-after(., '.blob.'))\" \"$OUTPUT_FILE\"\n+fi\n+\n+# Change the value of fs.defaultFS\n+if xmlstarlet sel -t -v \"//property[name='fs.defaultFS']/value\" \"$OUTPUT_FILE\" | grep -q \".\"; then\n+    if xmlstarlet sel -t -v \"//property[name='fs.defaultFS']/value\" \"$OUTPUT_FILE\" | grep -q \".blob.\"; then\n+        xmlstarlet ed -L -u \"//property[name='fs.defaultFS']/value\" -x \"concat('abfs', substring-before(substring-after(., 'wasb'), '@'),\n+         '@', substring-before(substring-after(., '@'), '.blob.'), '$endpoint', 'core.windows.net')\" \"$OUTPUT_FILE\"\n+    else\n+        echo \"ERROR: 'fs.defaultFS' does not have 'Blob' as endpoint. Exiting...\"\n+        echo \"$contactTeamMsg\"\n+        exit 1\n+    fi\n+fi\n+\n+# Remove the property block if any name tag is empty\n+xmlstarlet ed -L -d \"//property[not(name) or name='']\" \"$OUTPUT_FILE\"\n+\n+echo \"Updated file: $OUTPUT_FILE\"\n\nReview Comment:\n   Is this script tested?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/site/markdown/wasbToAbfsMigration.md:\n##########\n@@ -0,0 +1,64 @@\n+# WASB to ABFS Configuration Conversion Script \n+\n+To support customer onboard for migration from WASB to ABFS driver, we've\n+introduced a script to help with the configuration changes required\n+for the same.\n+\n+## Introduction\n+\n+ABFS driver has now built support for\n+FNS accounts (over BlobEndpoint that WASB Driver uses) using the ABFS scheme.\n\nReview Comment:\n   Adda  link here to fns_blob.md file\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/site/markdown/wasbToAbfsMigration.md:\n##########\n@@ -0,0 +1,64 @@\n+# WASB to ABFS Configuration Conversion Script \n+\n+To support customer onboard for migration from WASB to ABFS driver, we've\n+introduced a script to help with the configuration changes required\n+for the same.\n+\n+## Introduction\n+\n+ABFS driver has now built support for\n+FNS accounts (over BlobEndpoint that WASB Driver uses) using the ABFS scheme.\n+\n+The legacy WASB driver has been **deprecated** and is no longer recommended for\n\nReview Comment:\n   Add a link to deprecated_wasb.md here.\r\n   May be first documentation PR needs to be merged\n\n\n\n##########\nhadoop-tools/hadoop-azure/dev-support/testrun-scripts/config_support.sh:\n##########\n@@ -0,0 +1,140 @@\n+#!/usr/bin/env bash\n+\n+set -eo pipefail\n+\n+FILE=$1\n+\n+if [ ! -f \"$FILE\" ]; then\n+    echo \"Error: File '$FILE' not found. Exiting....\"\n+    exit 1\n+fi\n+\n+if [[ \"$1\" != *.xml ]]; then\n+    echo \"The file provided is not an XML file. Exiting....\"\n+    exit 1\n+fi\n+\n+OUTPUT_FILE=\"abfs-converted-config.xml\"\n+cp \"$FILE\" \"$OUTPUT_FILE\"\n+\n+contactTeamMsg=\"For any queries or support, kindly reach out to us at 'askabfs@microsoft.com'.\"\n+endpoint=\".dfs.\"\n+printf \"Select 'HNS' if you're migrating to ABFS driver with Hierarchical Namespace enabled account,\n+            or 'Non-HNS' if you're migrating with Non-Hierarchical Namespace (FNS) account. \\n\"\n\nReview Comment:\n   Nit: Statement construction should be same for both the options.\r\n   `Select 'HNS' if you're migrating to ABFS driver for Hierarchical Namespace enabled account,\r\n               or 'Non-HNS' if you're migrating to ABFS driver for Non-Hierarchical Namespace (FNS) account. \\n`\n\n\n\n##########\nhadoop-tools/hadoop-azure/dev-support/testrun-scripts/config_support.sh:\n##########\n@@ -0,0 +1,140 @@\n+#!/usr/bin/env bash\n+\n+set -eo pipefail\n+\n+FILE=$1\n+\n+if [ ! -f \"$FILE\" ]; then\n+    echo \"Error: File '$FILE' not found. Exiting....\"\n+    exit 1\n+fi\n+\n+if [[ \"$1\" != *.xml ]]; then\n+    echo \"The file provided is not an XML file. Exiting....\"\n+    exit 1\n+fi\n+\n+OUTPUT_FILE=\"abfs-converted-config.xml\"\n+cp \"$FILE\" \"$OUTPUT_FILE\"\n+\n+contactTeamMsg=\"For any queries or support, kindly reach out to us at 'askabfs@microsoft.com'.\"\n+endpoint=\".dfs.\"\n+printf \"Select 'HNS' if you're migrating to ABFS driver with Hierarchical Namespace enabled account,\n+            or 'Non-HNS' if you're migrating with Non-Hierarchical Namespace (FNS) account. \\n\"\n+printf \"WARNING: Please ensure the correct option is chosen as it will affect the configuration changes made to the file. \\n\"\n+printf \"If you are unsure, follow the instructions below to check from Azure Portal: \\n\"\n+printf \"* Go to the Azure Portal and navigate to your storage account. \\n\"\n+printf \"* In the left-hand menu, select 'Overview' section and look for 'Properties'. \\n\"\n+printf \"* Under 'Blob service', check if 'Hierarchical namespace' is enabled or disabled. \\n\"\n+echo \"$contactTeamMsg\"\n+select namespaceType in \"HNS\" \"NonHNS\"\n+do\n+    case $namespaceType in\n+        HNS)\n+            xmlstarlet ed -L -i '//configuration/property[1]' -t elem -n property -v '' \\\n+              -s '//configuration/property[1]' -t elem -n name -v 'fs.azure.account.hns.enabled' \\\n+              -s '//configuration/property[1]' -t elem -n value -v 'true' \"$OUTPUT_FILE\"\n+            break;\n+            ;;\n+        NonHNS)\n+            endpoint=\".blob.\"\n+            break;\n+            ;;\n+          *)\n+            echo \"Invalid selection. Please try again. Exiting...\"\n+            exit 1;\n+            ;;\n+    esac\n+done\n+\n+# Mapping for renaming configurations\n+declare -A rename_configs_map=(\n+    [\"autothrottling.enable\"]=\"enable.autothrottling\" #fs.azure.autothrottling.enable  to fs.azure.enable.autothrottling\n+    [\"rename.dir\"]=\"rename.key\" # fs.azure.atomic.rename.dir to fs.azure.atomic.rename.key\n+    [\"block.blob.buffered.pread.disable\"]=\"buffered.pread.disable\" #fs.azure.block.blob.buffered.pread.disable to fs.azure.buffered.pread.disable\n+    [\"fs.azure.sas\"]=\"fs.azure.sas.fixed.token.\" #fs.azure.sas.CONTAINER_NAME.ACCOUNT_NAME to fs.azure.sas.fixed.token.CONTAINER_NAME.ACCOUNT_NAME\n+    [\"check.block.md5\"]=\"enable.checksum.validation\" #fs.azure.check.block.md5 to fs.azure.enable.checksum.validation\n+)\n+\n+# Configs not supported in ABFS\n+unsupported_configs_list=(\n+    \"fs.azure.page.blob.dir\"\n+    \"fs.azure.block.blob.with.compaction.dir\"\n+    \"fs.azure.store.blob.md5\"\n+)\n+\n+# Configurations not required in ABFS Driver and can be removed\n+obsolete_configs_list=(\n\nReview Comment:\n   Where is the real mapping for supported configs defined?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add examples for function unwrap_udt\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add `StatefulSet`-based SparkApp example\nDescription: \nQ: Issue resolved by pull request 390\n[https://github.com/apache/spark-kubernetes-operator/pull/390]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Pass down SSE-C key to AAL so it can be attached while making the GET request.", "output": "S3A Analytics-Accelerator: Add support SSE-C"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: pull out new configuration load/probes under S3AStore\nDescription: S3AFS i's too full of switches; it is why initialize() is so big, and there are lots and lots of fields to record the values.\r\n\r\nMany of these need to go down into S3AStoreImpl, but replicating the same design just pushes the mess down.\r\n\r\nProposed: a child service StoreConfigurationService, which reads in the config during serviceInit(), and set the state in there, from where it can be probed.\r\n\r\nI am initially doing this purely for new configuration flags for the conditional write feature\r\n\r\n* moving other flags in there would be separate work\r\n* new boolean config options should go in here\r\n* we also need to think about integers, units of scale and durations.\r\n\r\nIdeally, we should  the annotations and reflection code from ABFS and\r\norg.apache.hadoop.fs.azurebfs.contracts.annotations.ConfigurationValidationAnnotations,\r\n\r\n* copy it into common\r\n* adding duration and size attributes\r\n* move reflection code from AbfsConfiguration#AbfsConfiguration into there too,\r\n* use in s3a\r\n* migra\nQ: update: not going to do it in the conditional create...to complex and it will delay merging of that.", "output": "steveloughran opened a new pull request, #7463:\nURL: https://github.com/apache/hadoop/pull/7463\n\n   \r\n   New service under S3AStoreImpl: StoreConfigurationService\r\n   \r\n   This is just the draft design; it is intended to be a place to move most of our configuration options:\r\n   \r\n   flags, durations etc, ideally lifting some of the work from org.apache.hadoop.fs.azurebfs.contracts.annotations.ConfigurationValidationAnnotations,\r\n   \r\n   * putting it into common\r\n   * adding duration and size attributes\r\n   * move reflection code from AbfsConfiguration#AbfsConfiguration into there too, and apply\r\n   \r\n   \r\n   Lifted from #7362, where it will be cut. Because of that lift: doesn't yet compile\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support TIME in the make_timestamp function in Python\nDescription: \nQ: Issue resolved by pull request 52648\n[https://github.com/apache/spark/pull/52648]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Just because a controller node sets `--no-initial-controllers` flag does not mean it is necessarily running kraft.version=1. The more precise meaning is that the controller node does not know what kraft version the cluster should be in.\r\n\r\nIt is a valid configuration to run a static quorum defined by `controller.quorum.voters` but have all the controllers format with `--no-initial-controllers`.", "output": "setting --no-initial-controllers flag should not validate kraft version against metadata version"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "1. `MetadataVersionConfigValidator`\r\n2. `MetadataVersionConfigValidatorTest`\r\n3. `LogConfigTest#testValidateWithMetadataVersionJbodSupport`", "output": "Move MetadataVersionConfigValidator and related test code to metadata module"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [SDP] Sinks\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This change is to make sure the RocksDB snapshot zip file doesn't contain empty files (except RocksDB log file). Other files such as OPTION, manifest, metadata are not expected to be empty in any situation.\r\n\r\nIntroduced a conf {{verifyNonEmptyFilesInZip}} to make sure we can turn this off if needed.", "output": "Make sure we don't upload empty files in RocksDB snapshot"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The current GitHub Actions workflows in the Hadoop repository are using outdated versions of GitHub Actions, such as {{{}actions/checkout@v3{}}}. To ensure better security, performance, and compatibility, we should update them to the latest stable versions.", "output": "Update GitHub Actions workflow to use the latest versions of actions"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default\nDescription: Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Dockerfile_windows_10 cannot be build\nDescription: The following command from BUILDING.txt does not work anymore:\r\n{code:java}\r\ndocker build -t hadoop-windows-10-builder -f .\\dev-support\\docker\\Dockerfile_windows_10 .\\dev-support\\docker\\ {code}\r\nSeveral dependencies are missing and vcpkg points to an outdated 7-zip package.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Ref: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1120%3A+AppInfo+metrics+don%27t+contain+the+client-id]\r\n\r\nWe should remove the metrics whose tags do not contain a client-id in Kafka 5.0.", "output": "Remove AppInfoParser deprecated metrics"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add JDK 21 to Ubuntu 20.04 docker development images\nDescription: We want to support JDK21, we better have it available in the development image for testing.\nQ: hadoop-yetus commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3275227945\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   1m 32s |  |  Docker failed to build run-specific yetus/hadoop:tp-5188}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7947 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "pan3793 commented on code in PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#discussion_r2340621524\n\n\n##########\ndev-support/docker/pkg-resolver/packages.json:\n##########\n@@ -268,17 +268,20 @@\n     ],\n     \"ubuntu:focal\": [\n       \"temurin-24-jdk\",\n+      \"temurin-21-jdk\",\n\nReview Comment:\n   the ubuntu official apt repo provides `openjdk-21-jdk`, it's unnecessary to install from 3rd party. let's use the official one and put it at the end of the list."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Display a detailed error message when table metadata is corrupted\nDescription: Currently, no meaningful error message appears when the table metadata is corrupted due to a mismatch between the partition column names in the table schema and the declared partition columns. To facilitate debugging, a detailed error message should be shown in this situation.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Turn on Gradle reproducible builds feature\nDescription: *Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923] \r\n\r\n*Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future)\r\n\r\n*Related Gradle issues and links:*\r\n *  [https://github.com/gradle/gradle/issues/34643]\r\n * [https://github.com/gradle/gradle/issues/30871]  \r\n * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives]\r\n * [https://docs.gradle.org/9.1.0/userguide/upgrading_major_version_9.html#reproducible_archives_by_default] \r\n * [https://docs.gradle.org/9.1.0/dsl/org.gradle.api.tasks.bundling.Tar.html#org.gradle.api.tasks.bundling.Tar:preserveFileTimestamps]  \r\n\r\n \r\n\r\n*Definition of done (at the minimum):*\r\n * *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc) : [https://kafka.apache.org/quickstart]", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Switch direct ctors to IoHandlerFactories for EventLoopGroups\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-kafka.\nDescription: \nQ: slfan1989 opened a new pull request, #7547:\nURL: https://github.com/apache/hadoop/pull/7547\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19437. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-kafka.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7547:\nURL: https://github.com/apache/hadoop/pull/7547#issuecomment-2761357293\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 46s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 10s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 27s |  |  hadoop-kafka in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 37s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7547/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7547 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux d080f1a81a92 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2cf275bb168fb3efbaa33b3c80e29aff6452e273 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7547/2/testReport/ |\r\n   | Max. process+thread count | 530 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-kafka U: hadoop-tools/hadoop-kafka |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7547/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Flaky test ShareConsumerTest. testShareGroupMaxSizeConfigExceeded\nDescription: The test has been observed as flaky in the recent runs: [https://develocity.apache.org/scans/tests?search.names=CI%20workflow%2CGit%20repository&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI%2Chttps:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.consumer.ShareConsumerTest&tests.sortField=FLAKY&tests.test=testShareGroupMaxSizeConfigExceeded()%5B1%5D] \r\n\r\n \r\nAfter tracing the failures, the root cause was narrowed down to commit [https://github.com/apache/kafka/commit/87657fdfc721055835f5b1f22151c461e85eab4a]. This change introduced a new try/catch around {{handleCompletedAcknowledgements()}} in {{{}ShareConsumerImpl.java{}}}. The side effect is that all exceptions coming from the commit callback — including GroupMaxSizeReachedException — are now swallowed and only logged, preventing the test from ever receiving the exception.\r\n - If the ack callback fires while {{poll()}} is executing → exception is caught & swallowed → test times out\r\n - If the callback fires outside that path → exception escapes → test passes\r\n\r\nSo the same test randomly passes/fails depending on scheduling of the ack callback.\r\n\r\nAlso, the test testAcknowledgementCommitCallbackThrowsException has been marked as Flaky for the same Root cause.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "There is a typo in config name added as part of fns-blob documentation.\r\nThis will fix that typo", "output": "HADOOP-19467: [ABFS][FnsOverBlob] Fixing Config Name in Documenatation"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Disallow create too many partitions also in createPartitions\nDescription: Similarly to \r\nhttps://issues.apache.org/jira/browse/KAFKA-17870\r\n\r\nExtending existing topics with a request that asks to create too many partitions can lead to OOM error and the client receiving an UnlknownServerException.\r\n\r\n \r\n\r\nWe should limit the number of new partitions similarly as when creating new topics.\nQ: https://github.com/apache/kafka/pull/20485", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Performance overhead in Client.java due to Increased threads creation\nDescription: In client.java before HADOOP-18324 , rpc connection class used newCachedThreadPool, which reuses previously constructed threads if they are available else create new. \r\n\r\nIn the HADOOP-18324, This has been modified to create new single thread every time and threads will not be reused for other tasks once it completes its execution.\r\n\r\nBecause of this change, there is overhead in the total number threads created over the lifespan of a client jvm, for example a Hive Metastore. This overhead leads to slight performance latency in our applications.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK 17] Implementation of JAXB-API has not been found on module path or classpath\nDescription: When i try to run the *org.apache.hadoop.yarn.webapp.TestWebApp* tests over JDK17 i am facing the following errors:\r\n\r\n{noformat}\r\nCaused by: A MultiException has 2 exceptions.  They are:\r\n1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver\r\n{noformat}\r\n\r\nRepro steps:\r\n- run: ./start-build-env.sh ubuntu_24\r\n- in docker run: mvn clean install -Pjdk17+ -T 32 -DskipTests \r\n- in hadoop-yarn-common modul run: mvn test -Dtest=org.apache.hadoop.yarn.webapp.TestWebApp\r\n\r\nI found a similar error here:\r\nhttps://issues.apache.org/jira/browse/HDDS-5068\r\n\r\nBased on my understanding the problem is the JDK11+ environments does not have the JAXB runtime, so we have to explicit provide them. Maybe this is just a temporal solution, till the whole Jakarta upgrade can be done.\r\n\r\n{panel:title=Full error log}\r\n{code}\r\n[ER\nQ: K0K0V0K opened a new pull request, #7928:\nURL: https://github.com/apache/hadoop/pull/7928\n\n   https://issues.apache.org/jira/browse/HADOOP-19674\r\n   \r\n   ### Description of PR\r\n   \r\n   When we try to create an instance of `JettisonJaxbContext` a `JAXBContext.newInstance` call will happen. With my understanding JAXB is removed since JDK11 ([source](https://docs.oracle.com/en/java/javase/24/migrate/removed-tools-and-components.html#GUID-11F78105-D735-430D-92DD-6C37958FCBC3)) and if we would like to access JAXB, then we should explicit include them to the jars. So a new **jaxb-runtime** dependency is included in the code base. I used test scope where ever it was possible but for example in the `MRClientService` the `JAXBContextResolver` is in use, so i left the test scope there.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Unit tests\r\n   - I created a JDK8 build and run the `org.apache.hadoop.yarn.webapp.TestWebApp` test\r\n   - I created a JDK17 build and run the` org.apache.hadoop.yarn.webapp.TestWebApp` test\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "Hi [~stoty]!\r\n\r\nYes, just now.\r\nI am not 100% sure this is a good solution in point of architecture, security or long term maintenance, so i am grateful to every review.\r\nThanks!"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Migrate FileContextPermissionBase to Junit 5\nDescription: TestFcLocalFsPermission has been recently migrated to Junit 5. However, its superclass FileContextPermissionBase uses Junit 4, which causes the class initializerers (BeforeAll/BeforeClass) not to be called in certain circumstances.\r\n\r\nMigrate FileContextPermissionBase and its children fully to Junit 5.\nQ: stoty opened a new pull request, #7630:\nURL: https://github.com/apache/hadoop/pull/7630\n\n   ### Description of PR\r\n   \r\n   Make sure that FileContextPermissionBase and its childer are all migrated to Junit 5.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran the children on my JDK24 + Surefire 3.5.3 branch\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "[~stoty]  I am responsible for handling the upgrade from Junit4 to Junit5. Currently, some parts of Hadoop-common have not been fully upgraded due to its complex dependencies."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Customers love PySpark and the flexibility of using several python libraries as part of our workflows. I've a unique scenario where this specific usecase has multiple tables with around 10k columns and some of those columns have array datatype that when exploded, contain ~1k columns each.\r\n\r\n*Issues that we are facing:*\r\n * Frequent driver OOM depending on the use case and how many columns are involved in the logic and how many array type columns are exploded. There is frequent GC, slowing down the workflows.\r\n * We tried equivalent scala apis and the performance as well latency seemed a lot better (no OOM and significantly less GC overheads).\r\n\r\n*Here is what we understand so far from thread and memory dumps:*\r\n * driver ends up having open references for every pyspark object created in the python vm because of py4j bridge-based communication implementation for pyspark apis. and the garbage keeps accumulating on driver ultimately leading to OOM\r\n * even if we delete python references in pyspark code (for example:  del df_dummy1) and run \"gc.collect()\" specifically, we are not able to ease the memory pressure. Python gc or python triggered gc via py4j bridge in the driver doesn't seem to be that good.\r\n\r\nThis is not a typical workload but we have multiple such usecases and we are debating if it's worth changing existing workflows to scala just like that (existing DEs are more comfortable with PySpark, there is cost of migration as well that we will have to convince our management to approve)\r\n\r\n*ASK:*\r\n\r\nThis Jira includes a sample notebook that reproduces what our usecases see. We are seeking community feedback on such a usecase and if there are ideas to improve this situation further other than migrating to Scala apis. Any PySpark improvement ideas that could help?", "output": "Performance degradation for PySpark apis at scale as compared to Scala apis"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "[S3 Dual-stack|https://docs.aws.amazon.com/AmazonS3/latest/userguide/dual-stack-endpoints.html] allows a client to access an S3 bucket through a dual-stack endpoint. When clients request a dual-stack endpoint, the bucket URL resolves to an IPv6 address if possible, otherwise fallback to IPv4.\r\n\r\nFor more details on using S3 Dual-stack, please refer [Using dual-stack endpoints from the AWS CLI and the AWS SDKs|https://docs.aws.amazon.com/AmazonS3/latest/userguide/dual-stack-endpoints.html#dual-stack-endpoints-cli]", "output": "S3A : Add option fs.s3a.dual-stack.enabled to enable Amazon S3 dual-stack endpoints"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When a user invokes an API like `spark.sql` inside a query function that triggers analysis, we want to include information that allows us to do SDP-specific special handling. Adding a PipelineAnalysisContext will allow us to include information along with related RPCs so that the server knows to do this special handling.", "output": "Add PipelineAnalysisContext message to support pipeline analysis during Spark Connect query execution"}
{"instruction": "Answer the question based on the bug.", "input": "Title: HADOOP-19455. S3A: Enable logging of SDK client metrics\nDescription: HADOOP-19455. S3A: Enable logging of SDK client metrics\r\n\r\nTo log the output of the AWS SDK metrics, set the log\r\n`org.apache.hadoop.fs.s3a.DefaultS3ClientFactory` to `TRACE`.\nQ: It'd be nicest to wire up the logging publisher so it'd automatically to the pretty logs at trace level -so could be turned on when needed. \r\n\r\nHowever, that'd require some reflection to invoke a constructor which, in the shaded bundle.jar, needs a reference to the shaded slf4j enum.\r\n\r\nInstead, it'll be set to wire up to log at INFO, but only if DefaultS3ClientFactory is set to log at trace\r\n{code}\r\nlog4j.logger.org.apache.hadoop.fs.s3a.DefaultS3ClientFactory=TRACE\r\n{code}\r\n\r\n\r\n{code}\r\n2025-03-07 17:34:26,086 [s3a-transfer-stevel-london-bounded-pool2-t1] DEBUG impl.S3AStoreImpl (DurationInfo.java:(80)) - Starting: deleting test/testXAttrFile\r\n2025-03-07 17:34:26,154 [s3a-transfer-stevel-london-bounded-pool2-t1] INFO  metrics.LoggingMetricPublisher (LoggerAdapter.java:info(165)) - Metrics published: MetricCollection(name=ApiCall, metrics=[MetricRecord(metric=MarshallingDuration, value=PT0.000092041S), MetricRecord(metric=RetryCount, value=0), MetricRecord(metric=ApiCallSuccessful, value=true), MetricRecord(metric=OperationName, value=DeleteObject), MetricRecord(metric=EndpointResolveDuration, value=PT0.000132792S), MetricRecord(metric=ApiCallDuration, value=PT0.064890875S), MetricRecord(metric=CredentialsFetchDuration, value=PT0.000017458S), MetricRecord(metric=ServiceEndpoint, value=https://stevel-london.s3.eu-west-2.amazonaws.com), MetricRecord(metric=ServiceId, value=S3)], children=[MetricCollection(name=ApiCallAttempt, metrics=[MetricRecord(metric=TimeToFirstByte, value=PT0.06260225S), MetricRecord(metric=SigningDuration, value=PT0.000293083S), MetricRecord(metric=ReadThroughput, value=0.0), MetricRecord(metric=ServiceCallDuration, value=PT0.06260225S), MetricRecord(metric=AwsExtendedRequestId, value=zvfoe8Vr8JFB42aDPawtJ3VfqV/xm0zyJ2jt9GG6027aVErpWdQ3UYdLEG8TVIAldxUovsyVJdw=), MetricRecord(metric=HttpStatusCode, value=204), MetricRecord(metric=BackoffDelayDuration, value=PT0S), MetricRecord(metric=TimeToLastByte, value=PT0.064313667S), MetricRecord(metric=AwsRequestId, value=RKZD44SE5DW91K1G)], children=[MetricCollection(name=HttpClient, metrics=[MetricRecord(metric=AvailableConcurrency, value=1), MetricRecord(metric=LeasedConcurrency, value=0), MetricRecord(metric=ConcurrencyAcquireDuration, value=PT0S), MetricRecord(metric=PendingConcurrencyAcquires, value=0), MetricRecord(metric=MaxConcurrency, value=512), MetricRecord(metric=HttpClientName, value=Apache)], children=[])])])\r\n{code}", "output": "the upgrade SDK work will include this as part of debugging some stuff"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When you close an s3a filesystem there is a lot of ERROR level stack traces about a CancellationException -despite that being exactly what is wanted.\r\n\r\nCore of it comes from netty.\r\n{code}\r\n        Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 1 failure: Unable to execute HTTP request: The connection was closed during the request. The request will usually succeed on a retry, but if it does not: consider disabling any proxies you have configured, enabling debug logging, or performing a TCP dump to identify the root cause. If this is a streaming operation, validate that data is being read or written in a timely manner. Channel Information: ChannelDiagnostics(channel=[id: 0x801baead, L:0.0.0.0/0.0.0.0:59534 ! R:bucket.vpce-0117f6033eaf7aee5-2hxd9fg4.s3.us-west-2.vpce.amazonaws.com/10.80.134.179:443], channelAge=PT0.676S, requestCount=1, responseCount=0, lastIdleDuration=PT0.006284125S)\r\nCaused by: java.lang.IllegalStateException: executor not accepting a task\r\n{code}", "output": "S3A: error stack traces printed on analytics stream factory close"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is required for being able to handle recent Java bytecodes.\r\n\r\nByte-buddy is used by BOTH mockito and maven-shade-plugin.\r\n\r\n1.15.11 is the last byte-buddy version that maven-shade-plugin 3.6.0 works with.", "output": "Update byte-buddy to 1.15.11"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add RegexpSingleline module to checkstyle.xml", "output": "Add RegexpSingleline module to checkstyle.xml"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: testIfMatchOverwriteWithOutdatedEtag() fails when not using SSE-KMS\nDescription: ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteWithOutdatedEtag() fails when no encryption method is set. \r\n \r\nThis is because it does  \r\ncreateFileWithFlags(fs, path, SMALL_FILE_BYTES, true, null);\r\n \r\nand then to overwrite the file, also does\r\n \r\ncreateFileWithFlags(fs, path, SMALL_FILE_BYTES, true, null);\r\n \r\nWhen no encryption is used, the eTAG is the md5 of the object, and so will always be the same, and won't result in the 412 conditional write failure. \r\n \r\nTest passes when using SSE-KMS, as when using encryption, eTag is no longer the md5 of the object content, and changes on every write. \r\n \r\n \r\nFix is simple enough, change the object content on the second write.\nQ: ahmarsuhail opened a new pull request, #7816:\nURL: https://github.com/apache/hadoop/pull/7816\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   We were previously creating second file with the same object content, so the etag will always be the same, leading to test failures, except when testing with an encryption method set, as in that case eTag is no longer the md5. \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Can't test on trunk as the all tests in this class fail due to the Junit5 migration. Will test on 3.4.", "output": "ahmarsuhail commented on PR #7816:\nURL: https://github.com/apache/hadoop/pull/7816#issuecomment-3089345707\n\n   @steveloughran @mukund-thakur small one to fix  a test failure I noticed."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Volcano to 1.13.0\nDescription: \nQ: Issue resolved by pull request 52722\n[https://github.com/apache/spark/pull/52722]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Support of CSE-KMS in version 3.4.1 seems to be broken\nDescription: Seem like on Commit c54bf19 (moving to aws sdk v2) the support of CSE-KMS was dropped or the way to configure it was changed without updating documentation. \r\n\r\n[https://github.com/apache/hadoop/commit/81d90fd65b3c0c7ac66ebae78e22dcb240a0547b#diff-a8dabc9bdb3ac3b04f92eadd1e3d9a7076d8983ca4fb7d1d146a1ac725caa309]\r\n\r\nin file: DefaultS3ClientFactory.java\r\n\r\nfor example code that was removed:\r\n\r\n!image-2025-07-21-18-13-16-765.png!\r\n\r\n \r\nFYI: [~stevel@apache.org]", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve PrometheusMetricsSink#normalizeName performance\nDescription: This patch is similar from HDDS-13014. We can add a name normalization cache between the Hadoop metrics name and the Prometheus metrics name to prevent expensive regex matchings during the metric normalization conversion.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Create a CI to verify all gradle tasks\nDescription: see https://github.com/apache/kafka/pull/19513#issuecomment-3390047209", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Fix hadoop site incorrect link which reported by https://lists.apache.org/thread/ozvkh0x7qdr8d3vr0tbm5g7jx1jprbct.", "output": "Fix incorrect link from current3 of hadoop-site"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update script `free_disk_space`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\"\nDescription: Experienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests.\r\n\r\nHypothesis: a transient failure meant the server receiving the POST calls to complete the uploads was mistakenly reporting no upload IDs.\r\n\r\nOutcome: all active write operations failed, without any retry attempts. This can lose data and fail jobs, even though the store may recover.\r\n\r\nProposed. The multipart uploads, especially block output stream, retry on this error; treat it as a connectivity issue.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add dependency checks for Python-related tests in the connect module\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Model DSV2 Commit Operation Metrics API\nDescription: SPARK-52689 added a DataSourceV2 API that sends operation metrics along with the commit, via a map of string, long.  It would be cleaner to model it as a proper object so that it is more clear what metrics Spark sends, and to handle future cases where metrics may not be long values.\r\n\r\nSuggestion from [~aokolnychyi]\nQ: Issue resolved by pull request 52595\n[https://github.com/apache/spark/pull/52595]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "`SPARK-40492: maintenance before unload` in StateStoreSuite is flaky due to timing issues. Fix this by adding hooks to pause maintenance.", "output": "Deflake StateStoreSuite `SPARK-40492: maintenance before unload`"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, LdapAuthenticationHandler supports only a single ldap server url, this can obviously cause issues if the ldap instance goes down. This JIRA attempts to improve this by allowing users to list multiple ldap server urls, and performing a failover if we detect any issues.", "output": "LdapAuthenticationHandler supports configuring multiple ldapUrls."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: leader of the __consumer_offsets topic is not balanced\nDescription: h3. {color:#172b4d}I have already set [auto.leader.rebalance.enable|https://kafka.apache.org/documentation/#brokerconfigs_auto.leader.rebalance.enable] to true, but the leader of the __consumer_offsets topic is not balanced at all, remaining on only one node or two nodes.{color}\r\nh3. {color:#172b4d}Only this internal topic is unbalanced, resulting in an uneven number of connections and uneven CPU pressure.{color}\r\nh3. {color:#172b4d}Except for this topic{color}{color:#172b4d}__consumer_offsets{color}{color:#172b4d}, other topics are normal, the leader is balanced to three nodes.{color}\r\n\r\n \r\n\r\n*3.5.0 version,The leader is only on nodes 1 and 0 and will not be balanced to node 2. :*\r\nTopic: __consumer_offsets TopicId: 480x8Ow1Qwyb-YvuWWqwVQ PartitionCount: 50 ReplicationFactor: 3 Configs: compression.type=producer,deletion.filterDeadGroup.enable=true,min.insync.replicas=1,cleanup.policy=compact,segment.bytes=104857600,max.message.bytes=15728658,index.interval.bytes=4096,unclean.leader.election.enable=false,retention.bytes=1073741824\r\nTopic: __consumer_offsetsPartition: 0Leader: 0Replicas: 0,1,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 1Leader: 1Replicas: 1,0,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 2Leader: 0Replicas: 0,1,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 3Leader: 1Replicas: 1,0,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 4Leader: 1Replicas: 1,0,2Isr: 0,2,1\r\nTopic: __consumer_offsetsPartition: 5Leader: 0Replicas: 0,1,2Isr: 0,2,1\r\nTopic: __cons", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [ABFS] Fix logging in FSDataInputStream buffersize as that is not used and confusing the customer\nDescription: Fix debug logs here. \r\n\r\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java#L350\nQ: Thanks for reporting this [~mthakur] \r\nWe will plan for this work as soon as possible.", "output": "manika137 opened a new pull request, #7642:\nURL: https://github.com/apache/hadoop/pull/7642\n\n   ### Description of PR\r\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19548\r\n   Small change to fix logging in FSDataInputStream and mention the correct buffersize being used. \r\n   \r\n   ### How was this patch tested?\r\n   Test suite was run, results of which are added in the comments below."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Flaky Test org.apache.kafka.streams.integration.SmokeTestDriverIntegrationTest\nDescription: The test `SmokeTestDriverIntegrationTest` failed many time in recent runs:\r\n\r\n[https://develocity.apache.org/scans/tests?search.names=Git%20branch&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&search.values=trunk&tests.container=org.apache.kafka.streams.integration.SmokeTestDriverIntegrationTest&tests.sortField=FLAKY]\r\n\r\n \r\n\r\nSample run: https://github.com/apache/kafka/actions/runs/17471808549/job/49622305615?pr=20266", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This sub-task is intended to fix the order of arguments passed in assertions in test cases within the generator package.", "output": "Generator | Fix order of arguments to assertEquals in unit tests"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `Spotless` to 8.0.0\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://lists.apache.org/thread/xv04txcy4wwtpkn1zfqxqf95098ls61h", "output": "Fix the \"6.6 Java Version\" for branch 3.9"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Allow to configure custom `ReplicaPlacer` implementation\nDescription: Replica assignment is a complex issue, as it depends on how a kafka cluster is run, maintained, and used. KAFKA-19507 aims to enhance the default assignment policy, and in my opinion, the best approach is to make the system flexible enough to allow users to customize the policy according to their specific needs\nQ: it looks like https://cwiki.apache.org/confluence/display/KAFKA/KIP-660%3A+Pluggable+ReplicaPlacer, which was rejected. This was a while ago so maybe things have changed.\r\n\r\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-1066%3A+Mechanism+to+cordon+brokers+and+log+directories is the replacement KIP I'm currently trying to get voted.", "output": "We could make it pluggable by exposing the internal config at the very least, if there are concerns or negative experiences with exposing such internal functionality"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Regenerate benchmark results after upgrading to Scala 2.13.17\nDescription: \nQ: Issue resolved by pull request 52600\n[https://github.com/apache/spark/pull/52600]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add automatic commit offset caching in subscribe mode\nDescription: Suppose that no new messages have been written to the target topic for a long time, for example, the producer has not generated new data for nearly an hour. On the consumer side, when the user sets enable.auto.commit=true, the  OFFSET_COMMIT request will still be repeatedly submitted every 5 seconds. This not only wastes network resources but also increases the burden on __consumer_offset.\r\n\r\n \r\nTherefore, maybe we can add a cache for committed offsets to check if the currently committed offset is consistent with the most recently successfully committed offset. If they are the same, this offset commit can be ignored. Of course, all the above applies to the subscribe mode.\r\n\r\n \r\nAdvantages:\r\n * Reduce unnecessary network requests\r\n * Alleviate the processing pressure on the broker side\r\n * Relieve the pressure of log cleaning for __consumer_offset\r\n\r\n \r\nDisadvantage:\r\n * There may be hidden bugs that need to be discussed and identified\nQ: Hey, in case it helps:\r\n * I filed something related a while back here https://issues.apache.org/jira/browse/KAFKA-16233  \r\n * why subscribe mode only? I expect that whatever improvement we want to consider in this area would be for consumers with auto.commit enabled + group Id (regardless of using subscribe or not). A consumer using assign can auto commit too (if it has group ID + auto.commit enabled)", "output": "[~lianetm]   Sorry for the late reply. I think in assign mode, apart from the current consumer being able to modify the offset of the corresponding TopicPartition, the Admin can also make modifications. Let’s assume such a scenario: the offset cached by the consumer is 10, and all subsequent requests to commit offset 10 will return successfully quickly without being sent to the broker. At this point, if the Admin is used to set the offset to 11, the consumer will not be aware of this change. As a result, the consumer caches an invalid offset, which is inconsistent with expectations.   WDYT ?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The currentlly used Boost 1.72.0 does not compile with recent C++ compilers, which prevents compiling on recent systems (at least without replacing the C++ compiler)\r\n\r\nhdfs-native does not compile with 1.87.0, but does with 1.86.0.", "output": "Update Boost to 1.86.0"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "S3AFS i's too full of switches; it is why initialize() is so big, and there are lots and lots of fields to record the values.\r\n\r\nMany of these need to go down into S3AStoreImpl, but replicating the same design just pushes the mess down.\r\n\r\nProposed: a child service StoreConfigurationService, which reads in the config during serviceInit(), and set the state in there, from where it can be probed.\r\n\r\nI am initially doing this purely for new configuration flags for the conditional write feature\r\n\r\n* moving other flags in there would be separate work\r\n* new boolean config options should go in here\r\n* we also need to think about integers, units of scale and durations.\r\n\r\nIdeally, we should  the annotations and reflection code from ABFS and\r\norg.apache.hadoop.fs.azurebfs.contracts.annotations.ConfigurationValidationAnnotations,\r\n\r\n* copy it into common\r\n* adding duration and size attributes\r\n* move reflection code from AbfsConfiguration#AbfsConfiguration into there too,\r\n* use in s3a\r\n* migrate abfs to the moved code", "output": "S3A: pull out new configuration load/probes under S3AStore"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "when closing a process during a large upload, and NPE is triggered in the abort call. This is because the S3 client has already been released.\r\n\r\n{code}\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$abortMultipartUpload$41(S3AFileSystem.java:5337)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.abortMultipartUpload(S3AFileSystem.java:5336)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$abortMultipartUpload$4(WriteOperationHelper.java:392)\r\n{code}\r\n\r\n* close() in small writes also fails, just with a different exception\r\n* and on some large writes, the output stream hangs as it awaits the end of the queued writes. This is a problem inside the semaphore executor", "output": "S3A: stream write/close fails badly once FS is closed"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-streaming.\nDescription: \nQ: slfan1989 opened a new pull request, #7554:\nURL: https://github.com/apache/hadoop/pull/7554\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19441. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-streaming.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   mvn clean test & junit test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7554:\nURL: https://github.com/apache/hadoop/pull/7554#issuecomment-2763283048\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 32 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-streaming.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-streaming.txt) |  hadoop-tools/hadoop-streaming: The patch generated 18 new + 449 unchanged - 4 fixed = 467 total (was 453)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   6m  8s |  |  hadoop-streaming in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  74m 51s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7554 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d98dc63e8fb3 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f9475972a23830f75d849e36c6827e432ffbbc60 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/1/testReport/ |\r\n   | Max. process+thread count | 760 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-streaming U: hadoop-tools/hadoop-streaming |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7554/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add RegexpSingleline module to checkstyle.xml\nDescription: Add RegexpSingleline module to checkstyle.xml\nQ: hadoop-yetus commented on PR #7505:\nURL: https://github.com/apache/hadoop/pull/7505#issuecomment-2724481281\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  71m 24s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 19s |  |  hadoop-build-tools in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 109m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7505/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7505 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 243e431c62db 5.15.0-134-generic #145-Ubuntu SMP Wed Feb 12 20:08:39 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 91ab935d0061373208673c6054ec0265310b0c2b |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7505/1/testReport/ |\r\n   | Max. process+thread count | 623 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-build-tools U: hadoop-build-tools |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7505/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hfutatzhanghb commented on PR #7505:\nURL: https://github.com/apache/hadoop/pull/7505#issuecomment-2724518674\n\n   Hi, @slfan1989 . Could you please help review this PR when have free time? Thanks ahead!"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Empty Page Issue on Subsequent ListBlob call\nDescription: We came across a new behavior from server where ListBlob call can return empty list even after returning a next marker(continuation token) from previous list call.\r\n\r\nThis is to handle that case and do not infer listing to be incomplete.\nQ: hadoop-yetus commented on PR #7698:\nURL: https://github.com/apache/hadoop/pull/7698#issuecomment-2893454757\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 11s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 1 unchanged - 0 fixed = 3 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 55s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 132m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7698 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ae738c550edc 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / dfe1e22a1c51042d53a56005de1874dcc84e6b7d |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/1/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7698:\nURL: https://github.com/apache/hadoop/pull/7698#issuecomment-2894175410\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 56s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 3 new + 1 unchanged - 0 fixed = 4 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 44s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 24s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  72m 36s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7698 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3d9996fd7371 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 67a71bdced89ddc2b190d97fbba25ff205c55a48 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/2/testReport/ |\r\n   | Max. process+thread count | 560 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7698/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-dynamometer-workload.\nDescription: \nQ: hadoop-yetus commented on PR #7587:\nURL: https://github.com/apache/hadoop/pull/7587#issuecomment-2788155635\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m  7s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 52s |  |  hadoop-dynamometer-workload in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  77m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7587/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7587 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f3c6312400bb 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8e75e22d749c761d646c0dc2863b82bf1f0bd8f2 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7587/1/testReport/ |\r\n   | Max. process+thread count | 678 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload U: hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7587/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 merged PR #7587:\nURL: https://github.com/apache/hadoop/pull/7587"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Expose consumer CorruptRecordException\nDescription: As part of our analysis around KAFKA-19430, we realized it's not possible to handle a {{CorruptRecordException}} in Streams because it's not exposed to the client; instead, a generic {{KafkaException}} ** is thrown\r\n\r\nThe proposed solution is to expose {{CorruptRecordException}} with information about affected TopicPartition and offset we're trying read from\r\n\r\nKIP: https://cwiki.apache.org/confluence/x/NQrxFg\nQ: [~mjsax]\r\nGood evening. Sorry for ping, but I wasn't sure if you've got a notification. \r\nPath is available, it's simple", "output": "Since the clients are supposed to skip a corrupted message, it would also be beneficial if the partition and maybe the offset is added to the `CorruptRecordException`. Parsing the exception message is uncool and currently I don't see another way to get the partition?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Relex Py4J requirement to 0.10.9.7+\nDescription: JVM are compatible with  0.10.9.7+ and above versions have some correctness fixes\nQ: Issue resolved by pull request 52721\n[https://github.com/apache/spark/pull/52721]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support resetting offsets in kafka-share-groups.sh for topics which are not currently subscribed\nDescription: The kafka-share-groups.sh tool can be used to reset the start offset for consumption if the share group is empty. One use for this is to initialise the start offset before starting to use the share group. Currently, the tool only lets the user reset the start offset for a topic which already has offsets in the share group. Instead, it should permit setting the SPSO for any topic, subject to auth checking.\r\n\r\nThis brings the tool's behaviour in line with kafka-consumer-groups.sh.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "AbfsInputStream should take in account the Read Policy set by user with Open File Options. Based on the read policy set, appropriate optimizations should be enabled and kicked in.", "output": "ABFS: Read Policy set in openFileOptions should be considered for enabling various optimizations"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "After the 3.7 release of Kafka, the java client will emit many warning messages and more objects deployed due to an open telemetry object instantiation (possibly due to KIP-714). It appears the open telemetry feature was enabled by default and there's no way for a user to disable it. \r\n\r\nUsers receive the INFO message defined [here |https://github.com/apache/kafka/blob/3.7.2/clients/src/main/java/org/apache/kafka/common/telemetry/internals/ClientTelemetryReporter.java#L479] after activating INFO log level for \"org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter\" class. This log line appears lots of times.\r\n\r\nIt appears that the occurrence of this info messages shows ClientTelemetry objects are being created if the feature is not explicitly disabled. Users would like this feature to be opt-in and not enabled by default as it is.", "output": "Make open telemetry object instantiation configurable"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip `test_profile_pandas_*` tests if pandas or pyarrow are unavailable\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Some features require a JavaScript engine, which has been removed from JDK.\r\n\r\nSkip those tests if JavaScript is not available.", "output": "Skip tests that require JavaScript engine when it's not available"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Streams open iterator tracking has high contention on metrics lock\nDescription: We run Kafka Streams 4.1.0 with custom processors that heavily use state store range iterators.\r\n\r\nWhile investigating disappointing performance, we found a surprising source of lock contention.\r\n\r\nOver the course of about a 1 minute profiler sample, the {{org.apache.kafka.common.metrics.Metrics}} lock is taken approximately 40,000 times and blocks threads for about 1 minute.\r\n\r\nThis appears to be because our state stores generally have no iterators open, except when their processor is processing a record, in which case it opens an iterator (taking the lock through {{OpenIterators.add}} into {{{}Metrics.registerMetric{}}}), does a tiny bit of work, and then closes the iterator (again taking the lock through {{OpenIterators.remove}} into {{{}Metrics.removeMetric{}}}).\r\n\r\nSo, stream processing threads takes a globally shared lock twice per record, for this subset of our data. I've attached a profiler thread state visualization with our findings - the red bar indicates the thread was bloc\nQ: Thanks for reporting this issue. – I am wondering what your application is doing exactly, and if it might be possible to avoid creating an iterator per record?\r\n\r\nOne hacky work-around I could think of, would be to create a \"dummy iterator\", so you always have at least one-open iterators, and the metric won't be removed/added over and over again?", "output": "Thanks [~mjsax] for taking a look.\r\n\r\nWe have a product requirement to compute a streaming-min and streaming-max operation over a grouped aggregate. For example, \"earliest record due date for each user\" or \"latest record created date for each user\".\r\n\r\nTo do this, we take the input stream,\r\n{code:java}\r\nK1 = U1 V1\r\nK2 = U1 V2\r\nK3 = U2 V3\r\nK4 = U2 V4 {code}\r\nand reorganize the records so the group-key and value are the key prefix, like\r\n{code:java}\r\nU1 V1 K1 = K1\r\nU1 V2 K2 = K2\r\nU2 V3 K3 = K3\r\nU2 V4 K4 = K4{code}\r\nand put it in a state store. Then, to determine the minimum or maximum, we do a prefix range scan to take the first or last record for the group U1 or U2.\r\n\r\nIt might be possible to reduce the number of range scans by caching the minimum and maximum values by key, to know if the max or min possibly changed and skip the iterator if not, but then we need a second state store duplicating the winning record per user. We assumed the cost of opening an iterator is roughly equal to the cost of a key lookup, but maybe this is not a good assumption.\r\n\r\nRegardless, to me, the current semantics for this metric seems wrong. If the store is open, with no iterators currently, the correct value for the metric is explicitly \"0\" not \"null / unregister\". The current setup makes it difficult to graph, since our dashboards will interpret \"null\" as \"missing data\" which is distinct from a present 0.\r\n\r\nI would expect the metric to be unregistered only when the state store is closed or otherwise we are sure no new iterators will ever be created."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The Jenkins CI for Windows Nightly build is failing since it's unable to download Apache Maven -\r\n\r\n{code}\r\n00:32:18  Step 14/78 : RUN powershell Invoke-WebRequest -URI https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.zip -OutFile $Env:TEMP\\apache-maven-3.8.8-bin.zip\r\n00:32:18   ---> Running in a47cc5638ee5\r\n00:32:22  \u001b[91mInvoke-WebRequest : \r\n00:32:22  \u001b[0m\u001b[91m\r\n00:32:22  \u001b[0m\u001b[91m404 Not Found\r\n00:32:22  \u001b[0m\u001b[91m\r\n00:32:22  \u001b[0m\u001b[91mNot Found\r\n00:32:22  \u001b[0m\u001b[91mThe requested URL was not found on this server.\r\n00:32:22  \u001b[0m\u001b[91m\r\n00:32:22  \u001b[0m\u001b[91mAt line:1 char:1\r\n00:32:22  + Invoke-WebRequest -URI https://downloads.apache.org/maven/maven-3/3.8 ...\r\n00:32:22  \u001b[0m\u001b[91m+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n00:32:22  \u001b[0m\u001b[91m    + CategoryInfo          : InvalidOperation: (System.Net.HttpWebRequest:Htt \r\n00:32:22  \u001b[0m\u001b[91m   pWebRequest) [Invoke-WebRequest], WebException\r\n00:32:22  \u001b[0m\u001b[91m    + FullyQualifiedErrorId : WebCmdletWebResponseException,Microsoft.PowerShe \r\n00:32:22     ll.Commands.InvokeWebRequestCommand\r\n00:32:38  \u001b[0mThe command 'cmd /S /C powershell Invoke-WebRequest -URI https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.zip -OutFile $Env:TEMP\\apache-maven-3.8.8-bin.zip' returned a non-zero code: 1\r\n[Pipeline] }\r\n[Pipeline] // withCredentials\r\n{code}\r\n\r\nWe need to update the URL to https://archive.apache.org/dist/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.zip", "output": "Fix Maven download link"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Debian 10 EOL, and the apt repo is unavailable\r\n{code:bash}\r\ndocker run --rm -it debian:10 bash\r\nroot@bc2a4c509cb3:/# apt update\r\nIgn:1 http://deb.debian.org/debian buster InRelease\r\nIgn:2 http://deb.debian.org/debian-security buster/updates InRelease\r\nIgn:3 http://deb.debian.org/debian buster-updates InRelease\r\nErr:4 http://deb.debian.org/debian buster Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:5 http://deb.debian.org/debian-security buster/updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:6 http://deb.debian.org/debian buster-updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nReading package lists... Done\r\nE: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian-security buster/updates Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian buster-updates Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nroot@bc2a4c509cb3:/#\r\n{code}", "output": "Upgrade Debian 10 to 11 in build env Dockerfile"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is continue with the effort to move the clients test to {{clients-integration-tests}} a la KAFKA-19042.", "output": "Move ClientOAuthIntegrationTest to clients-integration-tests"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix `pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather` in Python 3.14\nDescription: {code}\r\n======================================================================\r\nERROR [1.960s]: test_to_feather (pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/pandas/tests/io/test_feather.py\", line 43, in test_to_feather\r\n    self.psdf.to_feather(path2)\r\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/pandas/frame.py\", line 2702, in to_feather\r\n    return validate_arguments_and_invoke_function(\r\n        self._to_internal_pandas(), self.to_feather, pd.DataFrame.to_feather, args\r\n    )\r\n  File \"/__w/spark/spark/python/pyspark/pandas/utils.py\", line 592, in validate_arguments_and_invoke_function\r\n    return pandas_func(**args)\r\n  File \"/usr/local/lib/python3.14/dist-packages/pandas/core/frame.py\", line 2949, in to_feather\r\n    to_feather(self, path, **kwargs)\r\n    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.14/dist-packages/pandas/io/feather_format.py\", line 65, in to_feather\r\n    feather.write_feather(df, handles.handle, **kwargs)\r\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/feather.py\", line 156, in write_feather\r\n    table = Table.from_pandas(df, preserve_index=preserve_index)\r\n  File \"pyarrow/table.pxi\", line 4795, in pyarrow.lib.Table.from_pandas\r\n  File \"/usr/local/lib/pyt", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Publish Hadoop Docker Image to DockerHub for amd64 and arm64 \nDescription: Update GitHub Actions workflow to publish the apache/hadoop Docker image to DockerHub for multiple arch's: amd64 and arm64.\nQ: kumaab opened a new pull request, #7795:\nURL: https://github.com/apache/hadoop/pull/7795\n\n   ### Description of PR\r\n   Update GitHub Actions workflow to publish the `apache/hadoop` Docker image to GitHub Container Registry and DockerHub for multiple architectures: `amd64` and `arm64`.\r\n   \r\n   Image is tagged based on branch name, e.g. docker-hadoop-X.Y is tagged as X.Y.\r\n   \r\n   Pull request only triggers build (for CI), but image is not published.\r\n   \r\n   https://issues.apache.org/jira/browse/HADOOP-19614\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Pending, waiting on INFRA-27002\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "smengcl commented on PR #7795:\nURL: https://github.com/apache/hadoop/pull/7795#issuecomment-3090703143\n\n   > tag step is untested\r\n   \r\n   Is it possible to test it on your own fork and Docker Hub (registry), just to verify if it works?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hadoop build fails to find pthreads on SLES 15 builds while linking rpc. It looks like it checks for SunRPC library via rpc/rpc.h, but instead finds tirpc and sets it up incorrectly.\r\n\r\n{code}\r\nPerforming C SOURCE FILE Test CMAKE_HAVE_LIBC_PTHREAD failed with the following output:\r\nChange Dir: /grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/gmake cmTC_4ddc0/fast && /usr/bin/gmake  -f CMakeFiles/cmTC_4ddc0.dir/build.make CMakeFiles/cmTC_4ddc0.dir/build\r\ngmake[1]: Entering directory '/grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeTmp'\r\nBuilding C object CMakeFiles/cmTC_4ddc0.dir/src.c.o\r\n/usr/bin/gcc-8 -DCMAKE_HAVE_LIBC_PTHREAD   -o CMakeFiles/cmTC_4ddc0.dir/src.c.o -c /grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeTmp/src.c\r\nLinking C executable cmTC_4ddc0\r\n/grid/0/jenkins/tools/cmake/3.19.3/bin/cmake -E cmake_link_script CMakeFiles/cmTC_4ddc0.dir/link.txt --verbose=1\r\n/usr/bin/gcc-8 -rdynamic CMakeFiles/cmTC_4ddc0.dir/src.c.o -o cmTC_4ddc0\r\n/usr/lib64/gcc/x86_64-suse-linux/8/../../../../x86_64-suse-linux/bin/ld: CMakeFiles/cmTC_4ddc0.dir/src.c.o: in function `main':\r\nsrc.c:(.text+0x2d): undefined reference to `pthread_create'\r\n/usr/lib64/gcc/x86_64-suse-linux/8/../../../../x86_64-suse-linux/bin/ld: src.c:(.text+0x39): undefined reference to `pthread_detach'\r\n/usr/lib64/gcc/x86_64-suse-linux/8/../../../../x86_64-suse-linux/bin/ld: src.c:(.text+0x45): undefined reference to `pthread_cancel'\r\n/usr/lib64/gcc/x86_64-suse-linux/8/../../../../x86_64-suse-linux/bin/ld: src.c:(.text+0x56): undefined reference to `pthread_join'\r\ncollect2: error: ld returned 1 exit status\r\ngmake[1]: *** [CMakeFiles/cmTC_4ddc0.dir/build.make:106: cmTC_4ddc0] Error 1\r\ngmake[1]: Leaving directory '/grid/0/jenkins/workspace/wo", "output": "Native profile fails to build on SLES 15"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add LegacyCheckpointingStateStore\nDescription: Wrapping StateStore that implements legacy {{.checkpoint}} behaviour, moving its logic out of the {{ProcessorStateManager.}}", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1\nDescription: The project is currently using {{restrict-imports-enforcer-rule}} version {*}2.0.0{*}, which was released in *2021* and is now outdated. Since then, several new versions have been released with important bug fixes, performance improvements, and enhanced compatibility. To maintain a modern and stable build environment, it is necessary to upgrade to version {*}2.6.1{*}.", "output": "In Progress"}
{"instruction": "Answer the question based on the bug.", "input": "Title:  Upgrade buf plugins to v29.5\nDescription: \nQ: Issue resolved by pull request 52677\n[https://github.com/apache/spark/pull/52677]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Restore Compatibility with EMRFS FileSystem\nDescription: After HADOOP-19278 , The S3N folder marker *_$folder$* is not skipped during listing of S3 directories leading to S3A filesystem not able to read data written by legacy Hadoop S3N filesystem and AWS EMR's EMRFS (S3 filesystem) leading to compatibility issues and possible migration risks to S3A filesystem.\nQ: steveloughran commented on code in PR #7410:\nURL: https://github.com/apache/hadoop/pull/7410#discussion_r1963335389\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestEMRFSCompatibility.java:\n##########\n@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a;\n+\n+import org.assertj.core.api.Assertions;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.touch;\n+import static org.apache.hadoop.fs.s3a.Constants.S3N_FOLDER_SUFFIX;\n+\n+/**\n+ * This test verifies that the EMRFS or legacy S3N filesystem compatibility with\n+ * S3A works as expected.\n+ */\n+public class ITestEMRFSCompatibility extends AbstractS3ATestBase {\n\nReview Comment:\n   are there other tests here? I think I'd like\r\n   \r\n   * list parent path reports an empty dir\r\n   * delete parent dir results in a getFileStatus(parent) => 404, and same for marker.\r\n   \r\n   \r\n   What does dir rename do? I know for normal / markers we only create a dir marker if there's nothing underneath, but that's just an optimisation. Here we'd want:\r\n   \r\n   ```\r\n   touch parent/src/subdir/$folder$\r\n   mv parent/src parent/dest\r\n   isDir(parent/dest/subdir)\r\n   isNotFound(parent/src)\r\n   ```", "output": "hadoop-yetus commented on PR #7410:\nURL: https://github.com/apache/hadoop/pull/7410#issuecomment-2671320170\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 53s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 17s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 117m 22s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7410/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7410 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d28f75e7ac98 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / be2055603cdd6a57161146a7cdef214a6cd0617d |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7410/1/testReport/ |\r\n   | Max. process+thread count | 547 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7410/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "According to [Develocity|https://develocity.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&tests.container=org.apache.kafka.tools.streams.DescribeStreamsGroupTest&tests.sortField=FLAKY], {{testDescribeStreamsGroupWithShortTimeout}} is very flaky.", "output": "Fix flaky DescribeStreamsGroupTest testDescribeStreamsGroupWithShortTimeout()"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add tests for ConsumerRebalanceMetricsManager\nDescription: While working on KAFKA-19722  I noticed we had no test coverage for the rebalance metrics on the new consumer (this class [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/metrics/ConsumerRebalanceMetricsManager.java] )\r\n\r\nI introduced the test file and added a test related to KAFKA-19722, but we should extend coverage and test the other rebalance metrics (ex. tests like the ones for the ShareRebalanceMetrics [https://github.com/apache/kafka/blob/trunk/clients/src/test/java/org/apache/kafka/clients/consumer/internals/metrics/ShareRebalanceMetricsManagerTest.java] )", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "OffsetsRequestManager lacks the same level of testing compared to the code it eventually intends to replace, OffsetFetcher. The addition of new unit tests could potentially surface issues with its implementation and assist with the resolution of KAFKA-18216 and KAFKA-18217", "output": "Improve testing of OffsetsRequestManager to better match OffsetFetcher"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Bump Avro 1.11.5\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add JDK 21 to Ubuntu 20.04 docker development images\nDescription: We want to support JDK21, we better have it available in the development image for testing.\nQ: stoty opened a new pull request, #7947:\nURL: https://github.com/apache/hadoop/pull/7947\n\n   ### Description of PR\r\n   \r\n   Add JDK 21 to Ubuntu 20.04 and 24.04  docker development images\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Built the default image locally and started JDK21.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7947:\nURL: https://github.com/apache/hadoop/pull/7947#issuecomment-3275227945\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   1m 32s |  |  Docker failed to build run-specific yetus/hadoop:tp-5188}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7947 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7947/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve heartbeat request manager initial HB interval \nDescription: With KIP-848, consumer HB interval config moved to the broker, so currently, the consumer HB request manager starts with a 0ms interval (mainly to send a first HB right away after the consumer subscribe + poll). Once a response is received, the consumer takes the interval from the response and starts using it. \r\n\r\nThat 0ms initial interval makes that the HB mgr poll continuously executes logic on a tight loop that may not really be needed. It mostly has to wait for a response (or a failure).\r\n\r\nProbably worse than this, is the impact on the app thread, given that pollTimeout takes into account the maxTimeToWait from the network thread, that is directly impacted by the timeToNextHeartbeat\r\n * [https://github.com/apache/kafka/blob/388739f5d847d7a16e389d9891f806547f023476/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1764-L1766]\r\n * [https://github.com/apache/kafka/blob/781bc7a54b8c4f7c86f0d6bb9ef8399d86d0735e/clients/src/main/java/org/apache/k\nQ: [~lianetm], are you planning to work on this issue? If not, I’d be happy to take it over. Thanks!", "output": "Sure, feel free to take it and I can help with reviews. Thanks for your help!"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-resourceestimator.\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module\nDescription: Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution:\r\n{code:java}\r\njava.lang.IllegalStateException: Failed to set environment variable\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:120)\tat org.apache.hadoop.fs.tosfs.object.ObjectStorageTestBase.setUp(ObjectStorageTestBase.java:59)\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.util.Map java.util.Collections$UnmodifiableMap.m accessible: module java.base does not \"opens java.util\" to unnamed module @69eee410\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)\tat java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)\tat java.base/java.lang.reflect.Field.setAccessible(Field.java:172)\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:116)\t... 4 more\r\n {code}\r\nThis error occurs due to the module system restrictions in JDK 17, where reflection cannot access private fields in the java.util.Collections$UnmodifiableMap class.\r\n\r\n \r\n\r\nTo resolve this issue, ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: LSO movement archives only the first available batch not all the ones prior to the new LSO\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SHOW VIEWS AS JSON\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use batch.num_columns instead of len(batch.columns).\nDescription: \nQ: Issue resolved by pull request 52639\n[https://github.com/apache/spark/pull/52639]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK17] Remove JUnit4 Dependency\nDescription: Due to the extensive JUnit4 dependencies in the Hadoop modules, we will attempt to remove JUnit4 dependencies on a module-by-module basis.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob] Rename Recovery Should Succeed When Marker File Exists with Destination Directory\nDescription: On the blob endpoint, since rename is not a direct operation but a combination of two operations—copy and delete—in the case of directory rename, we first rename all the blobs that have the source prefix and, at the end, rename the source to the destination.\r\n\r\nIn the normal rename flow, renaming is not allowed if the destination already exists. However, in the case of recovery, there is a possibility that some files have already been renamed from the source to the destination. With the recent change ([HADOOP-19474] ABFS: [FnsOverBlob] Listing Optimizations to avoid multiple iteration over list response. - ASF JIRA), where we create a marker if the path is implicit, rename recovery will fail at the end when it tries to rename the source to the destination after renaming all the files.\r\n\r\nTo fix this, while renaming the source to the destination, if we encounter an error indicating that the path already exists, we will suppress the error and mark the rename recovery as successful.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Migrate from Java to Jakarta - Upgrade to Jersey 3.x, Jetty 12.x\nDescription: This Jira is to track the migration of Javax to Jakarta in Hadoop and identify all the transitive dependencies which need to be migrated as well. Currently, I see the following major upgrades required:\r\n # Jersey: 2.46 to 3.x\r\n # Jetty: 9.x to 12.x", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Custom S3 object [tags|https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html] can be added to S3 objects while writing and deleting.\r\n\r\n*Use Case:*\r\n\r\nS3 tags can be used to categorize the [objects|https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html] and potentially apply bucket level polices to take some actions.\r\n\r\nFor example : objects can be marked as \"to-be-glacier\" and based on some bucket policy the written objects can be moved to Glacier tier after sometime for cost savings.\r\n\r\nApache iceberg's [S3FileIO|#s3-tags]] also uses S3 Tags for soft deletes.", "output": "S3A : Add option for custom S3 tags while writing and deleting S3 objects"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix `CheckpointSuite.'get correct spark.driver.[host|port] from checkpoint'` test flakiness\nDescription: \nQ: Issue resolved by pull request 52565\n[https://github.com/apache/spark/pull/52565]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Log warning message on every set/get of a deprecated configuration property\nDescription: Currently, the warning message is logged once (or at most twice after HADOOP-8865) when we first use (set/get) a deprecated configuration key and most of the time this happens early on during system startup. \r\n\r\nUsers tend to set/get properties on their job scripts/applications  that keep running on a hourly/daily/etc basis. When a problem comes up the user will package the latest logs and send them to us (developers/support) for further analysis and troubleshooting. However, it's very likely that these logs will not contain information about the deprecated usages which might be crucial for advancing the investigation.\r\n\r\nOn the other end, a warning that appears just once is not that worrisome so even if the users/customers/developers see it, they can easily ignore it, and move on, thinking that there is no action needed on their end.\r\n\r\nThe above scenarios are based on applications such as the Hivemetastore, HiveServer2, which use the Hadoop Configuration, and usually run for weeks/mo\nQ: zabetak commented on PR #7766:\nURL: https://github.com/apache/hadoop/pull/7766#issuecomment-3022702343\n\n   Thanks for the review @slfan1989 ! I fixed the SpotBug warning in https://github.com/apache/hadoop/pull/7766/commits/7412e89112970f53a1682452d6a771703fcd022c.", "output": "hadoop-yetus commented on PR #7766:\nURL: https://github.com/apache/hadoop/pull/7766#issuecomment-3023207799\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 38s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 58s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 10s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 37s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  hadoop-common-project/hadoop-common: The patch generated 0 new + 210 unchanged - 1 fixed = 210 total (was 211)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  19m 52s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 43s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 134m 39s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7766/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7766 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 76c3b62ca2b4 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7412e89112970f53a1682452d6a771703fcd022c |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7766/2/testReport/ |\r\n   | Max. process+thread count | 1262 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7766/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove deprecate metrics in AssignmentsManager and RemoteStorageThreadPool\nDescription: Following KIP-1100, the metrics with incorrect component names were deprecated. These deprecated metrics should be removed in Kafka 5.0.\r\n\r\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-1100%3A+Rename+org.apache.kafka.server%3Atype%3DAssignmentsManager+and+org.apache.kafka.storage.internals.log.RemoteStorageThreadPool+metrics", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Enhancing ABFS Driver Metrics for Analytical Usability\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce type encoders for Geography and Geometry types\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [Bug Report] Thread leak in ABFS AbfsClientThrottlingAnalyzer\nDescription: Bug reported by Matt over common-dev discussion.\r\n\r\n> What seems to be the issue is that the timer tasks are cleaned up but\r\n> the timer threads themselves are never actually cleaned up. This will\r\n> eventually lead to an OOM since nothing is collecting these. I was\r\n> able to reproduce this locally in 3.3.6 and 3.4.1 but I believe that\r\n> it would affect any version that relies on autothrottling for ABFS.\r\n>\r\n> I was also able to make a quick fix as well as confirm a workaround --\r\n> the long term fix would be to include `timer.cancel()` and\r\n> `timer.purge()` in a method for AbfsClientThrottlingAnalyzer.java. The\r\n> short term workaround is to disable autothrottling and rely on Azure\r\n> to throttle the connections as needed with the below configuration.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The S3 Express response to create conflicts of multipart uploads is not a 412/419, it is 200 + an xml error.\r\n\r\n\r\n{code}\r\n\r\nError \r\n\r\nCode: PreconditionFailed\r\nMessage: At least one of the pre-conditions you specified did not hold\r\nCondition: If-None-Match\r\nRequestId: 01e17cb4f5000197899662aa050968218250cedb\r\nHostId: K1OApth\r\n\r\n{code}\r\n\r\nThis is raised an exception in the SDK, but it isn't mapping to a 419/412, the way we expect, so is mapped to\r\na generic AWSS3IOException.\r\n\r\n{code}\r\n\r\nAWSS3IOException: Completing multipart upload on test/testIfNoneMatchConflictOnMultipartUpload: software.amazon.awssdk.services.s3.model.S3Exception: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 200, Request ID: 0110480881000197899bc814050902966399617b, Extended Request ID: y1E4oDawYBQ7XsAVFA):PreconditionFailed: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 200, Request ID: 0110480881000197899bc814050902966399617b, Extended Request ID: y1E4oDawYBQ7XsAVFA)\r\n{code}\r\n\r\nIf this is expected behaviour of S3 express:\r\n* docs need updating\r\n* unless it is on the SDK roadmap, we should catch and map to condition failures, presumably RemoteFileChangedException,", "output": "S3A: S3 Express bucket failure of conditional overwrite of multiparts is 200 + error, not 412"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implement group-level initial rebalance delay\nDescription: During testing, an artifact of the new rebalance protocol showed up. In some cases, the first joining member gets all active tasks assigned, and is slow to revoke the tasks after more member has joined the group. This affects in particular cases where the first member is slow (possibly overloaded in the case of cloudlimits benchmarks) and there are a lot of tasks to be assigned.\r\n\r\nTo help with this situation, we want to introduce a new group-specific configuration to delay the initial rebalance.", "output": "Patch Available"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: TopologyTestDriver spends most of its time forcing changes to disk with persistent state stores\nDescription: We are using TopologyTestDriver \r\n\r\nIt looks like the test driver is syncing the entire state directory to disk for every record processed:\r\n\r\n!image-2025-09-08-14-07-05-768.png!", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move MetadataVersionConfigValidator and related test code to metadata module\nDescription: 1. `MetadataVersionConfigValidator`\r\n2. `MetadataVersionConfigValidatorTest`\r\n3. `LogConfigTest#testValidateWithMetadataVersionJbodSupport`", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In {{ConsumerPerformance}} (used by {{kafka-consumer-perf-test.sh}}), the metrics are shown, but only after the {{Consumer}} has been closed. Because metrics are removed from the {{Metrics}} object on {{Consumer.close()}}, this means that the complete set of metrics is not displayed when the performance tool outputs the metrics.", "output": "Close Consumer in ConsumerPerformance only after metrics displayed"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Retrieve dependency version from pom.xml for DependencyOverrides in SparkBuild.scala\nDescription: Currently, version strings for some dependencies are hard coded in DependencyOverrides in SparkBuild, so developers need to keep consistent with version strings specified in pom.xml manually.\nQ: Issue resolved by pull request 52760\n[https://github.com/apache/spark/pull/52760]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Migrate to AssertJ for Assertion Verification\nDescription: Currently, the unit tests in the project do not fully leverage modern assertion capabilities, resulting in lower readability and maintainability of the test code. To improve test clarity and extensibility, we plan to migrate the existing assertion library to {*}AssertJ{*}.\r\n\r\n \r\n\r\n*Objective:*\r\n * Migrate all assertions in existing tests from JUnit or other assertion libraries to AssertJ.\r\n\r\n * Utilize AssertJ’s rich assertion methods (e.g., fluent API, more readable assertions) to enhance the expressiveness of unit tests.\r\n\r\n * Ensure that all existing unit tests continue to run correctly after migration.\r\n\r\n \r\n\r\n*Implementation Steps:*\r\n # Analyze existing unit test code to identify assertions that need to be replaced.\r\n\r\n # Replace existing assertions with AssertJ assertion syntax.\r\n\r\n # Run unit tests to ensure the tests pass and function correctly after migration.\r\n\r\n # Update relevant documentation to ensure the team is aware of how to use AssertJ for assertions.\nQ: [~stevel@apache.org] The JUnit 5 upgrade for Hadoop is nearing completion, and I will soon initiate a new upgrade plan to migrate the unit tests to AssertJ in order to improve test readability and maintainability.\r\n \r\n We need to establish a new set of unit testing standards. If you have any relevant suggestions or rules, feel free to share.", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "With the latest change to the AWS S3 integrity, all existing releases of hadoop using the v2 SDK \r\n    fs.s3a.bucket.signing-algorithm\r\n    AWS4SignerType\r\n  \r\n{code}\r\n\r\n\r\nrejected at far end by\r\n{code}\r\n\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: Missing required header for this request: x-amz-content-sha256 (Service: S3, Status Code: 400, Request ID: 7M0MNNE8NBAGZWFH, Extended Request ID: A/0oyoZ52GLklg+GuV70vqPNEAK350ZF1rTJaNtSajWSXLT5bUCC/Gu6VbN8Pu+AeaMboIIGyGHeFLXOKUwdVQ==)\r\n{code}\r\n\r\n\r\nPresumably some change in 2.30.0 restored compatibility, but this means that all shipping 3.4.x releases do not work.", "output": "S3A: AWS4SignerType failing after S3 service changes"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add support for KLL quantiles functions based on DataSketches\nDescription: Documentation reference: [https://datasketches.apache.org/docs/KLL/KLLSketch.html].\r\n\r\nDataSketches code API reference: [https://apache.github.io/datasketches-java/6.1.0/org/apache/datasketches/kll/KllLongsSketch.html] \r\n\r\nReference PR for recently adding Theta sketches: [https://github.com/apache/spark/pull/51298]", "output": "In Progress"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Normally, we only update `gradle-wrapper.properties` and `dependencies.gradle`, but that process is incomplete. The correct steps are shown below.\r\n # upgrade gradle-wrapper.properties to the latest gradle\r\n # upgrade dependencies.gradle as well\r\n # use latest gradle to run command `gradle wrapper`to update gradlew \r\n # update wrapper.gradle to ensure the generated \"download command\" works well", "output": "Write down the steps for upgrading Gradle"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update the command usage of NNThroughputBenchmark by adding the \"-blockSize\" option.\nDescription: In HDFS-15652, make block size from NNThroughputBenchmark configurable. Benchmarking.md should also be updated.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This implements KIP-1147 for kafka-consumer-perf-test.sh and kafka-share-consumer-perf-test.sh.", "output": "Consistency of command-line arguments for consumer performance tests"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Tweak org.apache.kafka.clients.consumer.OffsetAndMetadata\nDescription: 1. Using `leaderEpoch=\" + leaderEpoch` feels a bit odd, since not every value of `leaderEpoch` is invalid\r\n\r\n2. `leaderEpoch=null` should be treated as same as `leaderEpoch=-1`, but `OffsetAndMetadata#equals` does not respect it.\r\n\r\n3. `OffsetAndMetadata#metadata` currently has neither integration nor unit tests", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*How to reproduce:* \r\n * checkout trunk (Swagger version: 2.2.25)\r\n * change `swaggerVersion` in `gradle.properties` to some more recent version (something in between of 2.2.27 and 2.2.39)\r\n * execute *./gradlew clean releaseTarGz*\r\n * build fails with an error _*Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String.*_\r\n * see details below\r\n * (!) note: build works for swagger version 2.2.26 (although reporter had some issues once or twice with that particular version, so please keep that on your mind)\r\n\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew clean releaseTarGz \r\n\r\n> Configure project :\r\nStarting build with version 4.2.0-SNAPSHOT (commit id 34581dff) using Gradle 9.1.0, Java 17 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\n\r\n[Incubating] Problems report is available at: file:///home/dejan/kafka/build/reports/problems/problems-report.html\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* Where:\r\nBuild file '/home/dejan/kafka/build.gradle' line: 3728\r\n\r\n* What went wrong:\r\nA problem occurred evaluating root project 'kafka'.\r\n> Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String.\r\n\r\n* Try:\r\n> Run with --stacktrace option to get the stack trace.\r\n> Run with --info or --debug option to get more log output.\r\n> Get more help at https://help.gradle.org.\r\n\r\nDeprecated Gradle features were used in this build, making it incompatible with Gradle 10.\r\n\r\nYou can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.\r\n\r\nFor more on this, please refer to https://docs.gradle.org/9.1.0/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation.\r\n\r\nBUILD FAILED in 5s\r\ndejan", "output": "Gradle build fails after Swagger patch version update"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Migrate usage of old Java date/time classes to java.time\nDescription: Many pieces of code still use old Java date/time classes, and can benefit from migrating to the \"new\" (Java 8) {{java.time}} APIs for several reasons, including:\r\n* Non thread-safe SimpleDateFormat - forces ThreadLocal usages and is generally bad practice these days (see YARN-11116)\r\n* Redundant object creation - e.g. creating a {{java.util.Date}} just to get an epoch millis value\r\n* Use of {{Calendar}}, {{TimeZone}}, etc.\r\n* All sorts of date/time arithmetics\r\n\r\nI propose 3 types of changes in this area:\r\n1) Internal changes - should be trivial to change without user-facing changes\r\n2) User-facing APIs - should be done _extending_ current code, with the question of deprecating existing code discussed separately\r\n3) External dependencies - value of change vs. cost (complexity/performance) should be discussed on a case-by-case basis.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When loading a large poorly compacted __consumer_offsets partition with a transactional workload (alternating transaction offset commits and commit markers), we create and discard lots of TimelineHashMaps. These accumulate in the SnapshotRegistry's rollback mechanism. Overall, memory usage is linear in the size of the partition.\r\n\r\nWe can commit the snapshot every N offsets in {{{}CoordinatorLoaderImpl{}}}, to allow the memory to be reclaimed.", "output": "OOM when loading large uncompacted __consumer_offsets partitions with transactional workload"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Persist deliveryCompleteCount in ShareSnapshot and ShareUpdate records\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve producer and consumer config files\nDescription: Improve consumer and producer config files that are shipped with Kafka binary.\r\nThis may include additional important config to consider or simply improved comments.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Publish Hadoop Docker Image to DockerHub for amd64 and arm64 \nDescription: Update GitHub Actions workflow to publish the apache/hadoop Docker image to DockerHub for multiple arch's: amd64 and arm64.", "output": "In Progress"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The acknowledgement commit callback in the share consumer gets called on the application thread at the start of the poll, commitSync and commitAsync methods. Specifically in the peculiar case of using the callback together with commitSync, the acknowledgement callback for the committed records is called at the start of the next eligible call, even though the information is already known at the end of the commitSync's execution. The results are correct already, but the timing could be improved in some situations.", "output": "Call acknowledgement commit callback at end of waiting calls"}
{"instruction": "Answer the question based on the bug.", "input": "Title: hadoop-thirdparty build to update maven plugin dependencies\nDescription: github action builds of PRs for hadoopHthirdparty fail because of throttling NVE throttling of requests; needs an update to a later version with either retries or use of a github source cve list.\r\n\r\ndependency checker 11+ \r\n\r\n{code}\r\nMandatory Upgrade Notice\r\nUpgrading to 10.0.2 or later is mandatory\r\n\r\nOlder versions of dependency-check are causing numerous, duplicative requests that end in processing failures are causing unnecassary load on the NVD API. Dependency-check 10.0.2 uses an updated User-Agent header that will allow the NVD to block calls from the older client.\r\n{code}\r\n\r\n----\r\n\r\nThe upgraded dependency checker now *requires* java11+, and *prefers* the provision of an API key for the national vulnerabilities database. It also skips the sonatype check as that no longer supports anonymous checks at all\nQ: latest pr does it, just needs java11 for that action. And I've turned off the sonatype checking", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support for Sketch family in ThetaSketch Aggregates\nDescription: Theta sketch aggregate currently supports only quick select.\r\n\r\nConsumers like Iceberg{^}[1][2]{^} might benefit will benefit from the sketch aggregate if has the ability to specify `ALPHA family`\r\n\r\n[1] [Iceberg specification to use ALPHA sketches|https://iceberg.apache.org/puffin-spec/#apache-datasketches-theta-v1-blob-type]\r\n\r\n[2] [Custom implementation of theta sketch aggregates in Iceberg|https://github.com/apache/iceberg/blob/2f6e7e6371902bcb72f21deeaea8889d4768004e/spark/v3.5/spark/src/main/scala/org/apache/spark/sql/stats/ThetaSketchAgg.scala#L67] that can be replaced with Spark Theta aggregates", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{code}\r\n======================================================================\r\nERROR [1.960s]: test_to_feather (pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/pandas/tests/io/test_feather.py\", line 43, in test_to_feather\r\n    self.psdf.to_feather(path2)\r\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/pandas/frame.py\", line 2702, in to_feather\r\n    return validate_arguments_and_invoke_function(\r\n        self._to_internal_pandas(), self.to_feather, pd.DataFrame.to_feather, args\r\n    )\r\n  File \"/__w/spark/spark/python/pyspark/pandas/utils.py\", line 592, in validate_arguments_and_invoke_function\r\n    return pandas_func(**args)\r\n  File \"/usr/local/lib/python3.14/dist-packages/pandas/core/frame.py\", line 2949, in to_feather\r\n    to_feather(self, path, **kwargs)\r\n    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.14/dist-packages/pandas/io/feather_format.py\", line 65, in to_feather\r\n    feather.write_feather(df, handles.handle, **kwargs)\r\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/feather.py\", line 156, in write_feather\r\n    table = Table.from_pandas(df, preserve_index=preserve_index)\r\n  File \"pyarrow/table.pxi\", line 4795, in pyarrow.lib.Table.from_pandas\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/pandas_compat.py\", line 663, in dataframe_to_arrays\r\n    pandas_metadata = construct_metadata(\r\n        columns_to_convert, df, column_names, index_columns, index_descriptors,\r\n        preserve_index, types, column_field_names=column_field_names\r\n    )\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/pandas_compat.py\", line 281, in construct_metadata\r\n    b'pandas': json.dumps({\r\n               ~~~~~~~~~~^^\r\n        'index_columns': index_descriptors,\r\n     ", "output": "Fix `pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather` in Python 3.14"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add `cluster-preview.yaml` and `spark-connect-server-preview.yaml`\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: `SparkAppDriverConf` should respect `sparkVersion` of `SparkApplication` CRD\nDescription: \nQ: Issue resolved by pull request 385\n[https://github.com/apache/spark-kubernetes-operator/pull/385]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\"\nDescription: Experienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests.\r\n\r\nHypothesis: a transient failure meant the server receiving the POST calls to complete the uploads was mistakenly reporting no upload IDs.\r\n\r\nOutcome: all active write operations failed, without any retry attempts. This can lose data and fail jobs, even though the store may recover.\r\n\r\nProposed. The multipart uploads, especially block output stream, retry on this error; treat it as a connectivity issue.\nQ: And stack on a write failure. \r\n{code}\r\n[ERROR] org.apache.hadoop.fs.s3a.scale.ITestS3AHugeFilesArrayBlocks.test_010_CreateHugeFile -- Time elapsed: 2.870 s <<< ERROR!\r\norg.apache.hadoop.fs.s3a.AWSBadRequestException: Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1)\r\n        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:265)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)\r\n        at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)\r\n        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\r\n        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.finalizeMultipartUpload(WriteOperationHelper.java:318)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.completeMPUwithRetries(WriteOperationHelper.java:370)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.lambda$complete$3(S3ABlockOutputStream.java:1227)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:493)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:1225)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$1500(S3ABlockOutputStream.java:876)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:545)\r\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\r\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\r\n        at org.apache.hadoop.fs.s3a.scale.AbstractSTestS3AHugeFiles.test_010_CreateHugeFile(AbstractSTestS3AHugeFiles.java:276)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at java.util.ArrayList.forEach(ArrayList.java:1259)\r\n        at java.util.ArrayList.forEach(ArrayList.java:1259)\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:113)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:61)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.retryPolicyDisallowedRetryException(RetryableStageHelper.java:168)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:73)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:53)\r\n        at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:35)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:82)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:43)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\n        at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)\r\n        at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\n        at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)\r\n        at software.amazon.awssdk.services.s3.DefaultS3Client.completeMultipartUpload(DefaultS3Client.java:801)\r\n        at software.amazon.awssdk.services.s3.DelegatingS3Client.lambda$completeMultipartUpload$1(DelegatingS3Client.java:611)\r\n        at software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67)\r\n        at software.amazon.awssdk.services.s3.DelegatingS3Client.completeMultipartUpload(DelegatingS3Client.java:611)\r\n        at org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.completeMultipartUpload(S3AStoreImpl.java:906)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelperCallbacksImpl.completeMultipartUpload(S3AFileSystem.java:1953)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$finalizeMultipartUpload$1(WriteOperationHelper.java:324)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)\r\n        ... 17 more\r\n\r\n{code}\r\n\r\nwe'd have to map 400 + the error text to a \"MultipartUploadCompleteFailed\" exception and add a policy for it, leaving other 400s as unrecoverable.", "output": "+ any tracking in block output stream should record when the POST to initiate the MPU was issued. That way if an error still surfaces but the output stream has been open for three days, we have a good cause \"stream open too long\""}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support TIME in the make_timestamp and try_make_timestamp functions in Scala\nDescription: \nQ: Issue resolved by pull request 52631\n[https://github.com/apache/spark/pull/52631]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve PrometheusMetricsSink#normalizeName performance\nDescription: This patch is similar from HDDS-13014. We can add a name normalization cache between the Hadoop metrics name and the Prometheus metrics name to prevent expensive regex matchings during the metric normalization conversion.\nQ: hadoop-yetus commented on PR #7692:\nURL: https://github.com/apache/hadoop/pull/7692#issuecomment-2889108549\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  19m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 39s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 17s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  6s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 39s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 39s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 36s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   1m 13s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/1/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-common in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 56s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 55s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 27s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 220m 37s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7692 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 8de9ba3edbb2 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / de9fa0ca2163b6a1c8cf43888a1f02ec1e555c84 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/1/testReport/ |\r\n   | Max. process+thread count | 1266 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7692:\nURL: https://github.com/apache/hadoop/pull/7692#issuecomment-2889630866\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  1s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 53s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 58s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  2s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 54s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 54s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 12s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 55s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 24s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 203m 28s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7692 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3d6ffc297392 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ba85667767669ec53779219032490e1d0a8efabc |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/2/testReport/ |\r\n   | Max. process+thread count | 2152 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update Java 24 to 25 in docker images\nDescription: Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25 in the docker images.\nQ: stoty opened a new pull request, #7991:\nURL: https://github.com/apache/hadoop/pull/7991\n\n   ### Description of PR\r\n   \r\n   Update Java 24 to 25 in docker images\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Built ubuntu_20 and ubuntu_24 x64 images, and ran java 25 in them.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "stoty commented on PR #7991:\nURL: https://github.com/apache/hadoop/pull/7991#issuecomment-3322708226\n\n   PTAL @slfan1989"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Part OF [KAFKA-19097|https://issues.apache.org/jira/browse/KAFKA-19097]\r\n\r\nThis sub-task is intended to fix the order of arguments passed in assertions in test cases within the streams package.", "output": "Streams | Fix order of arguments to assertEquals in unit test"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A : Add option for custom S3 tags while writing and deleting S3 objects\nDescription: Custom S3 object [tags|https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html] can be added to S3 objects while writing and deleting.\r\n\r\n*Use Case:*\r\n\r\nS3 tags can be used to categorize the [objects|https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html] and potentially apply bucket level polices to take some actions.\r\n\r\nFor example : objects can be marked as \"to-be-glacier\" and based on some bucket policy the written objects can be moved to Glacier tier after sometime for cost savings.\r\n\r\nApache iceberg's [S3FileIO|#s3-tags]] also uses S3 Tags for soft deletes.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Problem*\r\n\r\nWhen building Hadoop on the  riscv64 architecture, the current version of os-maven-plugin (1.7.0) fails with the following error:\r\n\r\n    os.detected.arch: unknown\r\n\r\n    org.apache.maven.MavenExecutionException: unknown os.arch: riscv64\r\n\r\nThis indicates that version 1.7.0 does not recognize or support the riscv64 target, causing native or plugin-based build failures.\r\n\r\n*Resolution*\r\n\r\nUpgrading os-maven-plugin to version 1.7.1 resolves the issue. The newer version includes updated architecture detection logic and supports riscv64 without error.", "output": "Upgrade os-maven-plugin to 1.7.1 to support riscv64"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hadoop's {{Clock}} interface was recently moved from {{org.apache.hadoop.yarn.util}} (in hadoop-yarn) to {{org.apache.hadoop.util}} (in hadoop-common) as part of YARN-11765.\r\n\r\nI propose to seize the opportunity of this being targeted done for 3.5.0 to modernize it usage:\r\n# Deprecate {{org.apache.hadoop.util.Clock}}\r\n# Replace all of its usages with {{java.time.Clock}}\r\n# Replace existing usages of its simple implementations, e.g. {{SystemClock}}/{{UTCClock}} with standard {{java.time.Clock}} subclasses, e.g. {{Clock.systemUTC()}}\r\n# Re-implement other implementations, e.g. {{MonotonicClock}}/{{ControllerClock}}, as {{java.time.Clock}} subclasses.\r\n\r\nThe standard {{java.time.Clock}} has a richer API supports modern {{java.time}} classes such as {{Instant}} and {{ZoneId}}, and migration would be straightforward:\r\nJust changing {{org.apache.hadoop.util.Clock.getTime()}} to {{java.time.Clock.millis()}}", "output": "Use java.time.Clock instead of org.apache.hadoop.util.Clock"}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade bouncycastle to 1.82 due to CVE-2025-8916\nDescription: https://github.com/advisories/GHSA-4cx2-fc23-5wg6\r\n\r\nThought it was tidier to upgrade to latest version even if the fix was a while ago.\nQ: pjfanning opened a new pull request, #8039:\nURL: https://github.com/apache/hadoop/pull/8039\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   HADOOP-19730\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #8039:\nURL: https://github.com/apache/hadoop/pull/8039#issuecomment-3420366545\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 51s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  28m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 12s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 28s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  mvnsite  |   8m 58s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 36s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 36s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  27m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 49s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 37s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   7m  1s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 42s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 36s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  45m 37s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 807m 40s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1064m 10s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList |\r\n   |   | hadoop.hdfs.tools.TestDFSAdmin |\r\n   |   | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |\r\n   |   | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes |\r\n   |   | hadoop.hdfs.TestRollingUpgrade |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   |   | hadoop.yarn.service.TestYarnNativeServices |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8039 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint compile javac javadoc mvninstall unit shadedclient xmllint shellcheck shelldocs |\r\n   | uname | Linux 66cf96c27f49 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 695a0a30232b143ec8837d6a6648344ffd4efec0 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/testReport/ |\r\n   | Max. process+thread count | 4498 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-cloud-storage-project/hadoop-cos . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Re-enable SmokeTestDriverIntegrationTest for processing threads\nDescription: The test fails occasionally with a race condition like this:\r\n{code:java}\r\n 2_0 RUNNING StreamTask(active): org.apache.kafka.streams.errors.StreamsException: java.util.ConcurrentModificationExceptionorg.apache.kafka.streams.errors.StreamsException: java.util.ConcurrentModificationException at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:977) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:894)Caused by: java.util.ConcurrentModificationException at java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1597) at java.base/java.util.HashMap$KeyIterator.next(HashMap.java:1620) at org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer.resume(ClassicKafkaConsumer.java:979) at org.apache.kafka.clients.consumer.KafkaConsumer.resume(KafkaConsumer.java:1509) at org.apache.kafka.streams.processor.internals.StreamTask.resumePollingForPartitionsWithAvailableSpace(StreamTask.java:623) at org.apache.kafka.stre\nQ: \"Processing threads\" are a WIP feature, but nobody is currently working on it. – This ticket cannot be worked on right now. We would first need to complete \"processing thread\" feature, to ensure it's working correctly to avoid that this test hits this bug of the incomplete \"processing threads\" feature.", "output": "Hi, \r\n\r\nI'm currently reviewing the code related to `org.apache.kafka.clients.consumer.KafkaConsumer#resume`.  \r\nCould you please let me know which Kafka versions are affected by this issue?  \r\n\r\nThanks!"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Enable all disabled tests on Linux\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update 3.4.2 docs landing page to highlight changes shipped in the release\nDescription: The landing page for the 3.4.2 docs mostly lists changes that already shipped in 3.4.0. Update this to highlight 3.4.2 changes.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "KIP-1229: [https://cwiki.apache.org/confluence/x/ZQteFw]", "output": "Add Idle Thread Ratio Metric to MetadataLoader"}
{"instruction": "Answer the question based on the bug.", "input": "Title: always disallow min.insync.replicas at the broker level\nDescription: In [https://github.com/apache/kafka/pull/17952], if ELR is enabled, we (1) disallow min.insync.replicas at the broker level; (2) automatically add min.insync.replicas at the cluster level, if not present; (3) disallow removing min.insync.replicas at the cluster level.  The reason for this is that if brokers disagree about which partitions are under min ISR, it breaks the KIP-966 replication invariants.\r\n\r\nHowever, even if ELR is not enabled, it's bad to have different min.insync.replicas on different brokers since if a leader is moved to a different broker, it will behave differently on the min.insync.replicas semantic. So, it's probably better to always enforce the above regardless whether ELR is enabled or not. Similarly, we probably want to do the same for at least unclean.leader.election.enable.\r\n\r\nSince this is a public facing change, it requires a KIP.\nQ: I'm working on this, thanks :)", "output": "Thanks, [~yung] !"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Support Fixed SAS token at container level\nDescription: The ABFS driver currently lacks support for multiple SAS tokens for the same storage account across different containers.\r\n\r\nWe are now introducing this support.\r\n\r\nTo use fixed SAS token at container level the configuration to be used is:\r\n{quote}fs.azure.sas.fixed.token..\r\n{quote}\nQ: hadoop-yetus commented on PR #7461:\nURL: https://github.com/apache/hadoop/pull/7461#issuecomment-2696603323\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 6 new + 4 unchanged - 0 fixed = 10 total (was 4)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 48s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 47s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 140m 41s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7461 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 1292e9229efb 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8c4413e89ff27f0cfd1f426be65e92c5c49f0b02 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/testReport/ |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7461:\nURL: https://github.com/apache/hadoop/pull/7461#issuecomment-2697109535\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 54s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 4 unchanged - 0 fixed = 6 total (was 4)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 19s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 23s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  71m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7461 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a33d76a9617b 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4a182acf271e2c390663440d7e8e02d43b09d412 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/3/testReport/ |\r\n   | Max. process+thread count | 671 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Consumer throughput drops by 10 times with Kafka v3.9.0 in ZK mode\nDescription: Kafka consumer throughput in best-case drops by ~10 times after upgrading to kafka v3.9.0 from v3.5.1. Note that this is in ZK mode and KRAFT migration is not done yet.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Rename `spark.executor.(log -> logs).redirectConsoleOutputs`\nDescription: \nQ: Issue resolved by pull request 52624\n[https://github.com/apache/spark/pull/52624]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*  kafka server version is 2.5.1\r\n *  kafka-client version bigger than 3.1.1 \r\n\r\n \r\n{code:java}\r\nimport org.apache.kafka.clients.producer.KafkaProducer;\r\nimport org.apache.kafka.clients.producer.Producer;\r\nimport org.apache.kafka.clients.producer.ProducerRecord;\r\n\r\nimport java.util.Properties;\r\n\r\n\r\npublic class KafkaSendTest {\r\n\r\n  public static void main(String[] args) {\r\n    Properties props = new Properties();\r\n    props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\");\r\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"transaction.timeout.ms\", \"300000\");\r\n    props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully\r\n    props.put(\"compression.type\", \"lz4\");\r\n    props.put(\"security.protocol\", \"SASL_PLAINTEXT\");\r\n    props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\");\r\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"xxx\\\" password=\\\"xxxx\\\";\");\r\n\r\n    Producer producer = new KafkaProducer<>(props);\r\n\r\n    try {\r\n      String topic = \"topic1\";\r\n      byte[] value = new byte[]{1,2}; \r\n      ProducerRecord record = new ProducerRecord<>(topic, null, value);\r\n      producer.send(record, (metadata, exception) -> {\r\n        if (exception == null) {\r\n          System.out.printf(\"Sent record(key=%s value=%s) meta(partition=%d, offset=%d)\\n\",\r\n                  record.key(), new String(record.value()), metadata.partition(), metadata.offset());\r\n        } else {\r\n          exception.printStackTrace();\r\n        }\r\n      });\r\n      producer.close();\r\n    } catch (Exception e) {\r\n      e.printStackTrace();\r\n    }\r\n  }\r\n} {code}\r\npom.xml config\r\n\r\n \r\n{code:java}\r\n\r\n  org.apache.kafka\r\n  kafka-clients\r\n  3.4.0\r\n {code}\r\n         When kafka producer acks=-1, It will throw exception.\r\n\r\n \r\n{code:java}\r\norg.apache.kafka.common.KafkaException: Cannot execute ", "output": "kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0"}
{"instruction": "Answer the question based on the bug.", "input": "Title: always disallow min.insync.replicas at the broker level\nDescription: In [https://github.com/apache/kafka/pull/17952], if ELR is enabled, we (1) disallow min.insync.replicas at the broker level; (2) automatically add min.insync.replicas at the cluster level, if not present; (3) disallow removing min.insync.replicas at the cluster level.  The reason for this is that if brokers disagree about which partitions are under min ISR, it breaks the KIP-966 replication invariants.\r\n\r\nHowever, even if ELR is not enabled, it's bad to have different min.insync.replicas on different brokers since if a leader is moved to a different broker, it will behave differently on the min.insync.replicas semantic. So, it's probably better to always enforce the above regardless whether ELR is enabled or not. Similarly, we probably want to do the same for at least unclean.leader.election.enable.\r\n\r\nSince this is a public facing change, it requires a KIP.\nQ: Thanks, [~yung] !", "output": "Nice idea, Perhaps we could do the same for `message.max.bytes`, `log.message.timestamp.type` and `log.cleanup.policy` as well. Those configs should be set only at topic-level and cluster-level."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Kinesis tests are broken\nDescription: Running Kinesis test with {{ENABLE_KINESIS_TEST=1}} fails with {{java.lang.NoClassDefFoundError}}:\r\n\r\n{code:java}\r\nENABLE_KINESIS_TESTS=1 ./build/sbt -Pkinesis-asl\r\n...\r\nUsing endpoint URL https://kinesis.us-west-2.amazonaws.com for creating Kinesis streams for tests.\r\n[info] WithoutAggregationKinesisBackedBlockRDDSuite:\r\n[info] org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite *** ABORTED *** (1 second, 131 milliseconds)\r\n[info]   java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/PropertyNamingStrategy$PascalCaseStrategy\r\n[info]   at com.amazonaws.services.kinesis.AmazonKinesisClient.(AmazonKinesisClient.java:86)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient$lzycompute(KinesisTestUtils.scala:59)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient(KinesisTestUtils.scala:58)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:169)\r\n[in\nQ: Using maven leads to the same error.", "output": "Issue resolved in https://github.com/apache/spark/pull/52630"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: ITestS3AEndpointRegion fails when using with access points\nDescription: failures are of the form:\r\n\r\n\r\n{{software.amazon.awssdk.core.exception.SdkClientException: Invalid configuration: region from ARN `us-east-1` does not match client region `us-east-2` and UseArnRegion is `false`}}\r\n{{ }}\r\n{{at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)}}\r\n\r\n{{ }}\r\n{{happens because when making the head request in the tests, we do }}\r\n{{HeadBucketRequest.builder().bucket(getFileSystem().getBucket()).build();}}\r\n{{ }}\r\n{{when using access points, bucket is \"arn:aws:s3:us-east-1:xxxx:accesspoint/test-bucket\", so client gets the region from the ARN which does not match the configured region. }}\nQ: Can happen with third party stores too, i think there I just ignore them\r\noptions \r\n* add another test.fs.s3a. switch and allow for all location tests to be skipped. \r\n* add a flag which declares they'll be rejected, and then expect failures (best?)\r\n\r\nLooking at the tests, many seem to consider various exception types as valid outcomes. Maybe AWSUnsupportedFeatureException should be added to the list, so 3express is automatically handled", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove `blacklist` alternative config names\nDescription: \nQ: Issue resolved by pull request 52558\n[https://github.com/apache/spark/pull/52558]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A : Add option for custom S3 tags while writing and deleting S3 objects\nDescription: Custom S3 object [tags|https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html] can be added to S3 objects while writing and deleting.\r\n\r\n*Use Case:*\r\n\r\nS3 tags can be used to categorize the [objects|https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html] and potentially apply bucket level polices to take some actions.\r\n\r\nFor example : objects can be marked as \"to-be-glacier\" and based on some bucket policy the written objects can be moved to Glacier tier after sometime for cost savings.\r\n\r\nApache iceberg's [S3FileIO|#s3-tags]] also uses S3 Tags for soft deletes.\nQ: * would this be a bucket setting?\r\n* or in file creation\r\n\r\ntags can be changed, cant they? which means the xattr api should be used to get at them.\r\n\r\n* we already use xattr to read headers (though we got the naming of those attrs wrong in a way which complicates some apps)\r\n* we could extend this to allow get *and set* of tags", "output": "Hey [~stevel@apache.org] , I will be working on this ticket.\r\n\r\nTo answer your questions this will be a feature of S3A which will be operated through configs. Any component when interacting to S3 using S3A, if provided with relevant configs (which will be newly created) then the tags will be applied in S3. These configs can be added while creating an object or deleting an object in S3. \r\n\r\nConfig for adding the tag : fs.s3a.object.tag.* (comma separated key and values) or fs.s3a.object.tag.\\{TAG_NAME} (separate configs)\r\n\r\nConfig for deleting the tag : {{fs.s3a.soft.delete.enabled=true}}\r\n\r\nExample : \r\n\r\n1. hadoop fs -Dfs.s3a.object.tag.department=finance,project=alpha -put file.txt s3a://bucket/path/\r\n\r\nor adding conf in different lines : --conf spark.hadoop.fs.s3a.object.tag.department=finance \\\r\n--conf spark.hadoop.fs.s3a.object.tag.project=alpha \\\r\n\r\nThis command will add tags with key as department and project and value as finance and alpha respectively.\r\n\r\n2. hadoop fs \\\r\n-Dfs.s3a.soft.delete.enabled=true \\\r\n-Dfs.s3a.soft.delete.tag.key=archive \\\r\n-Dfs.s3a.soft.delete.tag.value=true \\\r\n-rm s3a://ayshukla-emr-dev/tagged-file27.txt\r\n\r\nIn this tagged-file27.txt will not be deleted. Instead a tag will be added with key as archive and value as true (since those are defined by user).\r\n\r\nHere if no key and value tag are added then a default delete tag can be added. For example key is status and value as deleted.\r\n\r\nDocumented this in the attached pdf."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A : Add Support For Multi-Region S3 Access Points\nDescription: [Amazon S3 Multi-Region Access Points|https://aws.amazon.com/s3/features/multi-region-access-points/] provide a global endpoint for applications that need to access data stored in S3 buckets located in multiple AWS regions. This feature simplifies how applications access data distributed across different geographic locations, providing a single endpoint that automatically routes requests to the bucket with the lowest latency.\r\n\r\nThe current implementation in S3A only allows single region access point and the goal is to expand the scope/implementation to include multi-region-access points as well.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "https://www.cve.org/CVERecord?id=CVE-2025-48924\r\n\r\nWill update commons-text to 1.14.0 which was released with commons-lang3 3.18.0. Due to https://issues.apache.org/jira/browse/HADOOP-19532 - seems best to upgrade them together.", "output": "Bump commons-lang3 to 3.18.0 due to CVE-2025-48924"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix `CheckpointSuite.'get correct spark.driver.[host|port] from checkpoint'` test flakiness\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add RegexpSingleline module to checkstyle.xml\nDescription: Add RegexpSingleline module to checkstyle.xml", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Request Timeout During SASL Reauthentication Due to Missed OP_WRITE  interest set \nDescription: We've observed request timeouts occurring during SASL reauthentication, and analysis suggests the issue is caused by a race condition between request handling and SASL reauthentication on the broker side. Here’s the sequence:\r\n # Client sends a request (Req1) to the broker.\r\n # Client initiates SASL reauthentication.\r\n # Broker receives Req1.\r\n # Broker also begins SASL reauthentication.\r\n # While reauth is in progress:\r\n ** Broker completes processing of Req1 and prepares a response (Res1).\r\n ** Res1 is queued via KafkaChannel.send().\r\n ** Broker sets SelectionKey.OP_WRITE to indicate write readiness.\r\n ** However, Selector.attemptWrite() does not proceed because:\r\n *** channel.hasSend() is true, but\r\n *** channel.ready() is false (reauth is still in progress).\r\n # Once reauthentication completes: Broker removes SelectionKey.OP_WRITE.\r\n # At this point:\r\n ** channel.hasSend() and channel.ready() are now true,\r\n ** But key.isWritable() is false, so the response (Res1) is never sent.\r\n # The response remains stuck in the send buffer. Client eventually hits a request timeout.\r\n\r\nThe fix is to set write readiness using SelectionKey.OP_WRITE at the end of Step 6. This is similar to [what we do on client side|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientAuthenticator.java#L422].", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is related to https://issues.apache.org/jira/browse/KAFKA-19259. If an empty ShareFetch response is returned before the poll delay has elapsed, it seems possible that the application thread will continue to wait rather than promptly sending another ShareFetch request.", "output": "Investigate stalling share consumer poll when empty response is returned early"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update non-thirdparty Guava version to  33.4.8-jre\nDescription: Keep in sync with recently upgraded thirdparty Guava\nQ: stoty opened a new pull request, #7994:\nURL: https://github.com/apache/hadoop/pull/7994\n\n   ### Description of PR\r\n   \r\n   Update non-thirdparty Guava version to 33.4.8-jre\r\n   \r\n   The motivation is the same as for updating the thirdparty one.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   CI\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7994:\nURL: https://github.com/apache/hadoop/pull/7994#issuecomment-3326537486\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  31m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  33m 33s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m  9s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m  9s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m  8s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 10s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |   1m 22s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 10s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  45m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7994 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 9c1032fe6d33 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bd6bdde979214be1291cda6a340e8e629a848d7a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/testReport/ |\r\n   | Max. process+thread count | 98 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Share consumer changes to support renew ack.\nDescription: \nQ: [~goyarpit] I'm afraid the work on this issue is already in progress so I'm going to take it. If we get some subtasks, you can certainly help.", "output": "Thanks [~schofielaj] ."}
{"instruction": "Answer the question based on the bug.", "input": "Title: kafka-broker-api tool should support to get controller api version\nDescription: like tool kafka-broker-api-versions.sh, we can get all RPC version from broker.\r\n\r\nIt should also support for controller.\r\n\r\nKIP: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1220%3A+kafka-broker-api-versions+tool+support+controllers]\nQ: I've been meaning to do this for a while. Please fix this. When adding new APIs and testing the upgrades, this would be a really useful tool.", "output": "I will prepare KIP ASAP."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: add sbt-dependency-graph to SBT plugins\nDescription: The plugin adds few useful commands to browser dependencies tree in SBT.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h4. *Problem*\r\n\r\nApache Spark currently allows only *global compression configuration* for Parquet files using:\r\n\r\n{{spark.sql.parquet.compression.codec = snappy | gzip | zstd | uncompressed}}\r\n\r\nHowever, many production datasets contain heterogeneous columns — for example:\r\n * text or categorical columns that compress better with {*}ZSTD{*},\r\n\r\n * numeric columns that perform better with {*}SNAPPY{*}.\r\n\r\nToday, Spark applies a single codec to the entire file, preventing users from optimizing storage and I/O performance per column.\r\n\r\n\r\nh4. Proposed Improvement\r\n\r\nIntroduce a new configuration key to define *per-column compression codecs* in a map format:\r\n\r\n{{spark.sql.parquet.column.compression.map = colA:zstd,colB:snappy,colC:gzip}}\r\n\r\n*Behavior:*\r\n * The global codec ({{{}spark.sql.parquet.compression.codec{}}}) remains the default for all columns.\r\n\r\n * Any column listed in {{spark.sql.parquet.column.compression.map}} will use its specified codec.\r\n\r\n * Unspecified columns continue to use the global codec.\r\n\r\n*Example:*\r\n\r\n{{--conf spark.sql.parquet.compression.codec=snappy \\}}\r\n\r\n{{--conf spark.sql.parquet.column.compression.map=\"country:zstd,price:snappy,comment:gzip\"\r\n\r\nEffect:-\r\n\r\n}}\r\n||Column||Codec||\r\n|country|zstd|\r\n|price|snappy|\r\n|comment|gzip|\r\n|all others|snappy (global default)|\r\n\r\n{{}}", "output": "Addition of column-level Parquet compression preference in Spark"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add RealTimeScanExec and ability to execute long running batches\nDescription: \nQ: Issue resolved by pull request 52620\n[https://github.com/apache/spark/pull/52620]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Failures in the StateUpdater thread may lead to inability to shut down a stream thread\nDescription: If during rebalance a failure occurs in the StateUpdater thread, and this failure leads to the thread shutdown, the Stream thread may get into an infinite wait state during it's shutdown.\r\n\r\nSee the attached test that reproduces the issue.\nQ: Hi [~nikita-shupletsov]  if you have not started ,can i pick this up ?", "output": "Hi [~goyarpit] \r\nI am already working on it.\r\nI will let you know if I need help. thanks!"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension\nDescription: This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc\r\ninstruction sets, with full functional verification and performance testing completed.\r\n\r\nThe implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction.\r\n\r\nKey Features: \r\n1. Runtime Hardware Detection\r\nThe PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime.\r\n\r\n2. Performance Improvement\r\nHardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation.\nQ: hadoop-yetus commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3232576752\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  13m  5s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   7m 30s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 29s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  56m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |   7m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |   7m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   7m  3s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/blanks-eol.txt) |  The patch has 20 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-tabs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/blanks-tabs.txt) |  The patch 2 line(s) with tabs.  |\r\n   | -1 :x: |  mvnsite  |   1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  19m 44s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7912 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 3c797fab6900 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 67c832e1dc930b52b9a68c261f372b14e6cf1639 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/testReport/ |\r\n   | Max. process+thread count | 1262 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3233497529\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   7m 37s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 30s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  57m 17s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   6m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |   6m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |   6m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   6m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  19m 38s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 111m 26s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7912 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 9af82b84cde2 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / be2c9c5dfefc7f4ad7591c58f7857b177bef5b0e |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/testReport/ |\r\n   | Max. process+thread count | 3150 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/2/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: tests against third party stores failing after latest update & conditional creation \nDescription: some test regressions when testing with third party stores and the 3.4.2 branch\r\n\r\n* ITestS3ACopyFromLocalFile \" software.amazon.awssdk.services.s3.model.S3Exception: The Content-SHA256 you specified did not match what we received (Service: S3, Status Code: 400,\"\r\n\r\n* conditional overwrite tests fail because it's not there. Even when disabled in the config, some tests still try to use it (create cost), and find assertions about overwrites not met.\r\n\r\n* ITestS3ABucketExistence for bucket existence probes failures; assumption is that bucket probes succeed for all bucket names, it's when you actually try to work with one you get a failure.\r\n\r\nThe copy from local failure an unwelcome change, but maybe could be addressed with doc changes.\r\n\r\nThe conditional overwrite thing is something to flag in release notes as possible regression.\nQ: doing this in HADOOP-19654", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Deduplicate the variables in PythonArrowInput\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-gridmix.\nDescription: \nQ: slfan1989 opened a new pull request, #7578:\nURL: https://github.com/apache/hadoop/pull/7578\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19436. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-gridmix.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   mvn clean test & junit test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7578:\nURL: https://github.com/apache/hadoop/pull/7578#issuecomment-2777558635\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 19 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/artifact/out/blanks-eol.txt) |  The patch has 11 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 18s | [/results-checkstyle-hadoop-tools_hadoop-gridmix.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-gridmix.txt) |  hadoop-tools/hadoop-gridmix: The patch generated 65 new + 148 unchanged - 2 fixed = 213 total (was 150)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 44s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  16m 50s | [/patch-unit-hadoop-tools_hadoop-gridmix.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/artifact/out/patch-unit-hadoop-tools_hadoop-gridmix.txt) |  hadoop-gridmix in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 133m  6s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapred.gridmix.TestGridmixSummary |\r\n   |   | hadoop.mapred.gridmix.TestGridmixMemoryEmulation |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7578 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 915eeb034a41 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / adb77fea3c43c5a5e67a7661e1a8764a882b781a |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/testReport/ |\r\n   | Max. process+thread count | 1027 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-gridmix U: hadoop-tools/hadoop-gridmix |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update script `free_disk_space_container`\nDescription: \nQ: Issue resolved by pull request 52545\n[https://github.com/apache/spark/pull/52545]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove invalid `licenses` field of `hadoop-aliyun` SBOM by upgradding `cyclonedx` to 2.9.1\nDescription: \nQ: dongjoon-hyun opened a new pull request, #7990:\nURL: https://github.com/apache/hadoop/pull/7990\n\n   …\r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "dongjoon-hyun commented on PR #7990:\nURL: https://github.com/apache/hadoop/pull/7990#issuecomment-3322518878\n\n   Thank you, @slfan1989 ."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Add ubuntu:noble as a build platform with JDK-17 as default\nDescription: Add a new Dockerfile to compile Hadoop on latest ubuntu:noble (24.04) with JDK17 as the default compiler.\nQ: vinayakumarb opened a new pull request, #7608:\nURL: https://github.com/apache/hadoop/pull/7608\n\n   ### Description of PR\r\n   1. Added \"ubuntu:noble\" to supported docker platforms.\r\n   2. Modified `start-build-env.sh` to use expected Dockerfiles dynamically. ex: `bash start-build-env.sh ubuntu_24` where `ubuntu_24` is the extra suffix in the name of the Dockerfile.\r\n   3. Added `jdk17` profile activated when jdk17 is available.\r\n   4. Had to add `#include ` to header files which misses it to successfully compile with g++ (13.3) in ubuntu 24.\r\n   \r\n   ### How was this patch tested?\r\n   Successfully built the entire Hadoop tar with native support on env created using `bash start-build-env.sh ubuntu_24`\r\n   \r\n   ### For code changes:\r\n   \r\n   - [y] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [NA] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [NA] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [NA] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7608:\nURL: https://github.com/apache/hadoop/pull/7608#issuecomment-2799568628\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  45m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  1s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  1s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  1s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 16s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  36m 34s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |  13m  0s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/branch-compile-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  mvnsite  |   4m 48s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   6m 47s | [/branch-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/branch-javadoc-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  45m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |  35m 57s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  compile  |  13m  2s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  cc  |  13m  2s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  golang  |  13m  2s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javac  |  13m  2s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   4m 59s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  xmllint  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   6m 29s | [/patch-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-javadoc-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  45m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 598m  9s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 33s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 855m 27s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.util.TestNativeCodeLoader |\r\n   |   | hadoop.crypto.TestCryptoCodec |\r\n   |   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerMultiNodes |\r\n   |   | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMProxy |\r\n   |   | hadoop.yarn.client.api.impl.TestYarnClient |\r\n   |   | hadoop.yarn.client.api.impl.TestNMClient |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClient |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7608 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile cc mvnsite javac unit golang javadoc mvninstall shadedclient xmllint |\r\n   | uname | Linux ed869f1b88db 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 877f79282a0789d17255605b1b3ee94b5cf269c5 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_412-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/testReport/ |\r\n   | Max. process+thread count | 2370 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-hdfs-project/hadoop-hdfs-native-client . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/console |\r\n   | versions | git=2.9.5 maven=3.6.3 xmllint=20901 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade libopenssl to 3.5.2-1 needed for rsync\nDescription: The currently used libopenssl-3.1.4-1 is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n\r\nThe Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows.\r\n\r\n{code}\r\n00:25:33 SUCCESS: Specified value was saved.\r\n00:25:46 Removing intermediate container 5ce7355571a1\r\n00:25:46 ---> a13a4bc69545\r\n00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n00:25:46 ---> Running in d2dafad446f9\r\n00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n00:25:54 \u001b[0m\u001b[91mAt line:1 char:1\r\n00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ...\r\n00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.Ht\nQ: hadoop-yetus commented on PR #7875:\nURL: https://github.com/apache/hadoop/pull/7875#issuecomment-3192521324\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7875/1/console in case of problems.", "output": "hadoop-yetus commented on PR #7875:\nURL: https://github.com/apache/hadoop/pull/7875#issuecomment-3192810626\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  25m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 21s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  54m 29s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7875/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7875 |\r\n   | Optional Tests | dupname asflicense |\r\n   | uname | Linux 7a9ce5519195 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 988bb50936dcb2b9808ed2ffaa95851a4c228fd6 |\r\n   | Max. process+thread count | 556 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7875/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Restore Compatibility with EMRFS FileSystem\nDescription: After HADOOP-19278 , The S3N folder marker *_$folder$* is not skipped during listing of S3 directories leading to S3A filesystem not able to read data written by legacy Hadoop S3N filesystem and AWS EMR's EMRFS (S3 filesystem) leading to compatibility issues and possible migration risks to S3A filesystem.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip doctest `pyspark.sql.pandas.functions` without pyarrow/pandas\nDescription: \nQ: Issue resolved by pull request 52572\n[https://github.com/apache/spark/pull/52572]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update Boost to 1.86.0 in Windows build image\nDescription: HADOOP-19475 has updated Boost to 1.86.0, but it missed the Windows docker image, which doesn't use the same mechanism for dependencies as the Linux ones.\r\n\r\nUpdate Boost in the Windows build Docker image to the same version.\nQ: stoty opened a new pull request, #7601:\nURL: https://github.com/apache/hadoop/pull/7601\n\n   ### Description of PR\r\n   \r\n   Update Boost to 1.86.0 in the Windows Build image\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Built Hadoop in the modified Docker image\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "stoty closed pull request #7601: HADOOP-19538. Update Boost to 1.86.0 in Windows build image\nURL: https://github.com/apache/hadoop/pull/7601"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve repartition optimization\nDescription: Kafka Streams supports \"merge.repartition.topic\" optimization. This optimization tries to push repartition topics further upstream, to be able to merge multiple repartition topics from different downstream branches into a single repartition topic.\r\n\r\nIn the current implementation, the optimization stops to push repartition topics upstream, if the parent node is a value-changing operation. We cannot push-up further, because the data type might change, and thus we won't have the correct serde at hand.\r\n\r\nHowever, we could actually also inherit the correct serde from further upstream. Thus, we could actually push a repartition-step upstream (and at the same time, switch the serdes), if we can find the right serde before the value-changing operations further upstream.\nQ: Hi [~mjsax] - I'd like to work on this issue, if no one is currently working on it. Thanks.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Duplicated batches should be logged\nDescription: When writing records with idempotent producer, the broker can help de-duplicate records. But when records being de-duplicated, there is no log in the broker side. This will confuse the producer because the records sent going to nowhere. Sometimes it's caused by the buggy producer that keeps using the wrong sequence number to cause the duplicated records. We should at least log the duplicated records info instead of pretending nothing happened.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Each Spark Connect session that uses Python UDFs seems to leak one PySpark `daemon` process. Over time, these can accumulate in the 100s until the corresponding node or container goes OOM (although it can take a while as each process doesn't use a lot of memory and some of those pages are shared).\r\n{code:java}\r\nspark        263  0.0  0.0 121424 59504 ?        S    05:00   0:01  \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark       1515  0.0  0.0 121324 60148 ?        S    05:04   0:01  \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark       1525  0.0  0.0 121324 60400 ?        S    05:04   0:01  \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark       1568  0.0  0.0 121324 60280 ?        S    05:04   0:01  \\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon{code}\r\nIn addition there are also threads leaking - e.g., here is a thread dump histogram from a sample executor with 200+ leaked daemon processes:\r\n{code:java}\r\n# These threads seem to be leaking\r\n226 threads   Idle Worker Monitor for /opt/spark/.venv/bin/python3\r\n226 threads   process reaper\r\n226 threads   stderr reader for /opt/spark/.venv/bin/python3\r\n226 threads   stdout reader for /opt/spark/.venv/bin/python3\r\n250 threads   Worker Monitor for /opt/spark/.venv/bin/python3\r\n\r\n# These threads seem fine, Spark is configured with 24 cores/executor\r\n21 threads    stdout writer for /opt/spark/.venv/bin/python3\r\n21 threads    Writer Monitor for /opt/spark/.venv/bin/python3{code}\r\nWith Spark Connect, each session always has a `SPARK_JOB_ARTIFACT_UUID`, even if there are no artifacts, so the UDF environment built by `BasePythonRunner.compute` is always different, and each session ends up with its own `PythonWorkerFactory` and hence its own daemon process.\r\n\r\n`PythonWorkerFactory` has a `stop` method that stops the daemon, but there does not seem to be anyone that calls `PythonWorkerFactory.stop`, except at shutdown in `SparkEnv.stop`.\r\n\r\n \r\n\r\nThis can be reproduced by running a bunch of Spark Conn", "output": "Spark Connect sessions leak pyspark UDF daemon processes and threads"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Enable messageTemplate propagation to SparkThrowable \nDescription: The goal is to add a new *default* method, getDefaultMessageTemplate, to the public SparkThrowable interface. This gives clients a consistent, machine-readable *default* template for error rendering, while leaving them free to localize or otherwise transform the message.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use difference error message when kill on idle timeout\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: RunJar throws UnsupportedOperationException on Windows\nDescription: On Windows, run {{hadoop jar}} (with any jar). One immediately gets the following exception:\r\n\r\n{code}\r\nException in thread \"main\" java.lang.UnsupportedOperationException: 'posix:permissions' not supported as initial attribute\r\n    at java.base/sun.nio.fs.WindowsSecurityDescriptor.fromAttribute(WindowsSecurityDescriptor.java:358)\r\n    at java.base/sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:497)\r\n    at java.base/java.nio.file.Files.createDirectory(Files.java:690)\r\n    at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:135)\r\n    at java.base/java.nio.file.TempFileHelper.createTempDirectory(TempFileHelper.java:172)\r\n    at java.base/java.nio.file.Files.createTempDirectory(Files.java:966)\r\n    at org.apache.hadoop.util.RunJar.run(RunJar.java:296)\r\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:245)\r\n{code}\r\n\r\nI'm running Windows 11 with OpenJDK 11.0.26.\r\n\r\nThis bug does not exist in 3.3.6.\nQ: [~hexiaoqiao] could you please review this and comment? Thanks.", "output": "sjlee opened a new pull request, #7511:\nURL: https://github.com/apache/hadoop/pull/7511\n\n   Use ACL-based permissions for Windows instead of POSIX permissions.\r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   Fix Hadoop RunJar for Windows. The Windows filesystem does not recognize the POSIX permissions, and it was causing the directory creation failure. If the filesystem does not support POSIX permissions, switch to using the user-based, ACL-based permissions that work on Windows.\r\n   \r\n   ### How was this patch tested?\r\n   I added a test to TestRunJar and ran it first to reproduce the bug on Windows. After I changed RunJar.java, I ran TestRunJar on Windows and confirmed it runs successfully.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Reorder Gradle tasks (in order to bump Shadow plugin version)\nDescription: *Prologue:*\r\n * JIRA ticket: KAFKA-19174\r\n * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027]\r\n\r\n*Scenario:*\r\n * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+\r\n * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x)\r\n\r\n*Action points (what needs to be done):*\r\n * use `com.gradleup.shadow` recent version (9+)\r\n * reorder Gradle tasks so that Gradle command mentioned above can work\r\n\r\n*Definition of done (at the minimum):*\r\n * Gradle command mentioned above works as expected\r\n * also: *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc): [https://kafka.apache.org/quickstart]\nQ: Hi [~nish4_nth] \r\n\r\nAlthough I don't actually decide who can work on this (because I'm not a maintainer but only a contributor) I guess you are free to start if you want to contribute :)", "output": "Sure Thanks Dejan for updating the notes as well."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade kafka to 3.9.0 to fix CVE-2024-31141\nDescription: Upgrade kafka-clients to 3.9.0 to fix [https://nvd.nist.gov/vuln/detail/CVE-2024-31141]", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When I invoke a CLI command, I now get told information about HMAC keys which are\r\n* utterly meaningless to me\r\n* completely unrelated to what I am doing\r\n\r\n{code}\r\n bin/hadoop s3guard bucket-info $BUCKET\r\n\r\n2025-03-25 12:05:44,838 [main] INFO  token.SecretManager (SecretManager.java:(126)) - Selected hash algorithm: HmacSHA1\r\n2025-03-25 12:05:44,838 [main] INFO  token.SecretManager (SecretManager.java:(131)) - Selected hash key length:64\r\n{code}\r\n\r\nLooks like the changes in YARN-11738 have created this", "output": "SecretManager logs at INFO in bin/hadoop calls"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Deflake StateStoreSuite `SPARK-40492: maintenance before unload`\nDescription: `SPARK-40492: maintenance before unload` in StateStoreSuite is flaky due to timing issues. Fix this by adding hooks to pause maintenance.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: JDK8: RawLocalFileSystem calls ByteBuffer.flip\nDescription: running 3.4.2 rc1 on older JDK 8 runtimes can fail\r\n{code}\r\n\r\n\r\n[INFO] -------------------------------------------------------\r\n[INFO] Running org.apache.parquet.hadoop.TestParquetReader\r\nException in thread \"Thread-8\" java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;\r\n        at org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler.completed(RawLocalFileSystem.java:428)\r\n        at org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler.completed(RawLocalFileSystem.java:362)\r\n        at sun.nio.ch.Invoker.invokeUnchecked(Invoker.java:126)\r\n        at sun.nio.ch.SimpleAsynchronousFileChannelImpl$2.run(SimpleAsynchronousFileChannelImpl.java:335)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:750)\r\n{code}\r\n\r\n\r\nthis can be fixed trivially, casting to java.io.Buffer first: ((Byffer)buff).\nQ: hadoop-yetus commented on PR #7725:\nURL: https://github.com/apache/hadoop/pull/7725#issuecomment-2941341314\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 39s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 56s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 17s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  9s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 46s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   5m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 41s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  3s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  4s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 39s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 38s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 39s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 256m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.50 ServerAPI=1.50 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7725/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7725 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f34bb0af7b0b 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1014e42f534268f1b45bc5a2e7963818fb154864 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7725/1/testReport/ |\r\n   | Max. process+thread count | 3134 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7725/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "zeekling commented on code in PR #7725:\nURL: https://github.com/apache/hadoop/pull/7725#discussion_r2129126382\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java:\n##########\n@@ -425,7 +426,7 @@ public void completed(Integer result, Integer rangeIndex) {\n           channel.read(buffer, range.getOffset() + buffer.position(), rangeIndex, this);\n         } else {\n           // Flip the buffer and declare success.\n-          buffer.flip();\n+          ((Buffer)(buffer)).flip();\n\nReview Comment:\n   Do you use jdk17 build hadoop, and run on jdk8?\r\n   If so, you need check in all project like this, I think it has so many."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve logging about task readiness\nDescription: Kafka Streams processes its assigned tasks in a round robin fashion, however, if a task is considered not ready for processing, it might get skipped. We have observed cases, in which Kafka Streams seems to get stuck on a partition, and restarting the application instance resolves this issue. We suspect, it's related to considering the task for the stuck partition as not ready for processing. (As the ready/not-ready decision is based on in-memory state of KS runtime, bouncing the instance would reset KS runtime into a clean state, unblocking the stuck task.)\r\n\r\nHowever, we currently don't have sufficient logging to reason about this case, and to understand if and why a task might be skipped. We should add more log statement (DEBUG and/or TRACE) to get better visibility. There might be a lurking bug in this logic that we cannot narrow down w/o the corresponding information logging could provide.\nQ: Sure, you can pick it up. I haven't had a chance to start yet to work on it yet.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix NPE messages in ConsoleShareConsumer.\nDescription: If there is any failure in construction of KafkaShareConsumer, then it logs a few NPEs while closing.\r\n{code:java}\r\n[2025-07-31 21:45:46,484] ERROR [ShareConsumer clientId=console-share-consumer, groupId=test_1] Failed to release assignment before closing consumer (org.apache.kafka.clients.consumer.internals.ShareConsumerImpl) java.lang.NullPointerException: Cannot invoke \"org.apache.kafka.clients.consumer.internals.events.ApplicationEventHandler.add(org.apache.kafka.clients.consumer.internals.events.ApplicationEvent)\" because \"this.applicationEventHandler\" is null at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.sendAcknowledgementsAndLeaveGroup(ShareConsumerImpl.java:936) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.lambda$close$4(ShareConsumerImpl.java:882) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.common.utils.Utils.swallow(Utils.java:1042) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.close(ShareConsumerImpl.java:881) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.(ShareConsumerImpl.java:335) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.(ShareConsumerImpl.java:209) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator.create(ShareConsumerDelegateCreator.java:49) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.cli", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Store decimal precision loss conf in arithmetic expressions\nDescription: \nQ: Issue resolved by pull request 52681\n[https://github.com/apache/spark/pull/52681]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-V platforms supporting the Zbc extension (CLMUL instructions) in bulk_crc32_riscv.c.\r\nKey changes:\r\n * Implements optimized CRC32 and CRC32C routines using CLMUL instructions for zlib and Castagnoli polynomials.\r\n * Automatically switches to hardware acceleration when Zbc is available, otherwise falls back to generic table-based software implementation.\r\n * Maintains compatibility with platforms lacking Zbc support.\r\n\r\nThis optimization improves CRC performance on RISC-V CPUs with Zbc extension.", "output": "Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve perfomance Authorizer.authorizeByResourceType by using Prefix Trie\nDescription: Class Authorizer.authorizeByResourceType under the hood uses HashMap for \r\ncheck if the caller is authorized to perform the given ACL operation on at least one resource of the given type.\r\n\r\nIt check each character in allowPatterns and pass to deny patterns\r\n\r\n{code:java}\r\n// For any literal allowed, if there's no dominant literal and prefix denied, return allow.\r\n        // For any prefix allowed, if there's no dominant prefix denied, return allow.\r\n        for (Map.Entry> entry : allowPatterns.entrySet()) {\r\n            for (String allowStr : entry.getValue()) {\r\n                if (entry.getKey() == PatternType.LITERAL\r\n                        && denyPatterns.get(PatternType.LITERAL).contains(allowStr))\r\n                    continue;\r\n                StringBuilder sb = new StringBuilder();\r\n                boolean hasDominatedDeny = false;\r\n                for (char ch : allowStr.toCharArray()) {\r\n                    sb.append(ch);\r\n                    if (denyPatterns.get(PatternTy\nQ: [~evkuvardin] can I take up this issue?", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "update LICENSE-binary to include AAL dependency", "output": "S3A Analytics-Accelerator: Update LICENSE-binary"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use BOM for AWS Java SDK V2 dependency management\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "With KAFKA-19647, we added RebalanceProtocolMigrationIntegrationTest, that needs to wait for a \"classic\" consumer group to timeout.\r\n\r\nAfter KAFKA-18193 is completed, we should consider to use the new API to send a leave-group request in the test, to reduce test runtime.", "output": "Improve RebalanceProtocolMigrationIntegrationTest"}
{"instruction": "Answer the question based on the bug.", "input": "Title: NPE in dependency-check of hadoop-thirdparty\nDescription: the dependency checker of hadoop-thirdparty PRs fails with an NPE in the plugin.\r\n{code}\r\nError:  Failed to execute goal org.owasp:dependency-check-maven:6.1.5:aggregate (default-cli) on project hadoop-thirdparty: Fatal exception(s) analyzing Apache Hadoop Third-party Libs: One or more exceptions occurred during analysis:\r\nError:  \tUpdateException: java.util.concurrent.ExecutionException: java.lang.NullPointerException\r\nError:  \t\tcaused by ExecutionException: java.lang.NullPointerException\r\nError:  \t\tcaused by NullPointerException: null\r\n{code}\r\n\r\n\r\nThere is a much newer version of the library, but it is a java11 release. we will need to find the most recent release without this issue that is java8 only.\r\n\r\nAgain, this highlights why trying to continue build on java8 is futile\nQ: the checker now runs on java17, even if we release on java8.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Add ubuntu:noble as a build platform with JDK-17 as default\nDescription: Add a new Dockerfile to compile Hadoop on latest ubuntu:noble (24.04) with JDK17 as the default compiler.\nQ: hadoop-yetus commented on PR #7608:\nURL: https://github.com/apache/hadoop/pull/7608#issuecomment-2799568628\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  45m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  1s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  1s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  1s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 16s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  36m 34s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |  13m  0s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/branch-compile-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  mvnsite  |   4m 48s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   6m 47s | [/branch-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/branch-javadoc-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  45m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |  35m 57s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  compile  |  13m  2s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  cc  |  13m  2s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  golang  |  13m  2s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javac  |  13m  2s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   4m 59s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  xmllint  |   0m  0s |  |  No new issues.  |\r\n   | -1 :x: |  javadoc  |   6m 29s | [/patch-javadoc-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-javadoc-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  45m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 598m  9s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 33s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 855m 27s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.util.TestNativeCodeLoader |\r\n   |   | hadoop.crypto.TestCryptoCodec |\r\n   |   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerMultiNodes |\r\n   |   | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMProxy |\r\n   |   | hadoop.yarn.client.api.impl.TestYarnClient |\r\n   |   | hadoop.yarn.client.api.impl.TestNMClient |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClient |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7608 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint compile cc mvnsite javac unit golang javadoc mvninstall shadedclient xmllint |\r\n   | uname | Linux ed869f1b88db 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 877f79282a0789d17255605b1b3ee94b5cf269c5 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_412-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/testReport/ |\r\n   | Max. process+thread count | 2370 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-hdfs-project/hadoop-hdfs-native-client . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7608/1/console |\r\n   | versions | git=2.9.5 maven=3.6.3 xmllint=20901 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "vinayakumarb commented on PR #7608:\nURL: https://github.com/apache/hadoop/pull/7608#issuecomment-2800682551\n\n   All the failures in compilation are due to below. Nothing to do with changes in this PR.\r\n   ```\r\n   [ERROR] PROTOC FAILED: /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe)\r\n   /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe)\r\n   /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe)\r\n   --grpc-java_out: protoc-gen-grpc-java: Plugin failed with status code 1.\r\n   \r\n   [ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/proto/csi.proto [0:0]: /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe)\r\n   /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe)\r\n   /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-7608/centos-7/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/target/protoc-plugins/protoc-gen-grpc-java-1.69.0-linux-x86_64.exe)\r\n   --grpc-java_out: protoc-gen-grpc-java: Plugin failed with status code 1.\r\n   ```"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update SpotBugs version and enable Spotbugs Gradle tasks on Java 25\nDescription: *Prologue:* KAFKA-19664 \r\n\r\n*In more details:* Apache commons bcel Java 25 compatible version will be created soon (and SpotBugs version will follow immediately): https://issues.apache.org/jira/browse/BCEL-377?focusedCommentId=18028106&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18028106\r\n\r\n*Action points:*\r\n - upgrade SpotBugs version (use Java 25 compatible version)\r\n - enable SpotBugs checks for Java 25 Github actions build\r\n\r\n*Related links: *\r\n- [https://issues.apache.org/jira/projects/BCEL/versions/12354966] \r\n- [https://github.com/spotbugs/spotbugs/issues/3564]", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add `FAIR` schedule examples with Spark (Connect|Thrift) Servers\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [ABFS] Enable rename and create recovery from client transaction id over DFS endpoint\nDescription: We have implemented create and rename recovery using client transaction IDs over the DFS endpoint ([HADOOP-19450] [ABFS] Rename/Create path idempotency client-level resolution - ASF JIRA). Since the backend changes were not fully rolled out, we initially implemented the changes with the flag disabled. With this update, we aim to enable the flag, which will start sending client transaction IDs. In case of a failure, we will attempt to recover from the failed state if possible. Here are the detailed steps and considerations for this process:\r\n\r\n1. **Implementation Overview**: We introduced a mechanism for create and rename recovery via client transaction IDs to enhance reliability and data integrity over the DFS endpoint. This change was initially flagged as disabled due to incomplete backend rollouts.\r\n\r\n2. **Current Update**: With the backend changes now in place, we are ready to enable the flag. This will activate the sending of client transaction IDs, allowing us to track and manage \nQ: bhattmanish98 opened a new pull request, #7509:\nURL: https://github.com/apache/hadoop/pull/7509\n\n   We have implemented create and rename recovery using client transaction IDs over the DFS endpoint ([HADOOP-19450](https://issues.apache.org/jira/browse/HADOOP-19450) [ABFS] Rename/Create path idempotency client-level resolution - ASF JIRA). Since the backend changes were not fully rolled out, we initially implemented the changes with the flag disabled. With this update, we aim to enable the flag, which will start sending client transaction IDs. In case of a failure, we will attempt to recover from the failed state if possible. Here are the detailed steps and considerations for this process:\r\n   \r\n   1. *Implementation Overview*: We introduced a mechanism for create and rename recovery via client transaction IDs to enhance reliability and data integrity over the DFS endpoint. This change was initially flagged as disabled due to incomplete backend rollouts.\r\n   \r\n   2. *Current Update*: With the backend changes now in place, we are ready to enable the flag. This will activate the sending of client transaction IDs, allowing us to track and manage transactions more effectively.\r\n   \r\n   3. *Failure Recovery*: The primary advantage of enabling this flag is the potential for recovery from failed states. If a transaction fails, we can use the client transaction ID to attempt a recovery, minimizing data loss and ensuring continuity.\r\n   \r\n   4. *Next Steps*: We will proceed with enabling the flag and closely monitor the system's performance. Any issues or failures will be documented and addressed promptly to ensure a smooth transition.", "output": "bhattmanish98 commented on PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#issuecomment-2726292282\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 796, Failures: 0, Errors: 0, Skipped: 152\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 799, Failures: 0, Errors: 0, Skipped: 106\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 638, Failures: 0, Errors: 0, Skipped: 214\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 796, Failures: 0, Errors: 0, Skipped: 163\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 2\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 641, Failures: 0, Errors: 0, Skipped: 144\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 635, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 638, Failures: 0, Errors: 0, Skipped: 145\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 636, Failures: 0, Errors: 0, Skipped: 163\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 670, Failures: 0, Errors: 0, Skipped: 159\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 635, Failures: 0, Errors: 0, Skipped: 213\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-fs2img.\nDescription: \nQ: slfan1989 opened a new pull request, #7579:\nURL: https://github.com/apache/hadoop/pull/7579\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19435. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-fs2img.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7579:\nURL: https://github.com/apache/hadoop/pull/7579#issuecomment-2777507518\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 18s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 11s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 11s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/1/artifact/out/blanks-eol.txt) |  The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-fs2img.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-fs2img.txt) |  hadoop-tools/hadoop-fs2img: The patch generated 42 new + 6 unchanged - 0 fixed = 48 total (was 6)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 48s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 26s |  |  hadoop-fs2img in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  71m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7579 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2af45b81c6c1 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / cf959f3fc7818c076de1540d5e481d667c0dc6e0 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/1/testReport/ |\r\n   | Max. process+thread count | 560 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-fs2img U: hadoop-tools/hadoop-fs2img |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Implement the ST_Srid expression in SQL\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52795.", "output": "Issue resolved by pull request 52795\n[https://github.com/apache/spark/pull/52795]"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Observation class has been evolved a few times during Spark 3.5 to Spark 4.0.0. Previously it uses locking mechanism (synchronized) between get and onFinish methods to coordinate metrics update and retrieval.\r\n\r\nBut it has a potential deadlocking bug. If get is called before ObservationListener is triggered to call onFinish, get will forever be waiting for metrics because it locks the observation object by synchronized so later onFinish call is locked out from updating the metrics.\r\n\r\nThis locking mechanism was replaced by a promise by SPARK-49423 which is a large refactoring on the observation feature. But in the PR, I don’t see the deadlock bug was mentioned, and there is no bug fix PR proposed to earlier versions. So I think that the bug was not known and the fix is unintentional in Spark 4.0.0. The bug is still in Spark 3.5 branch.", "output": "Fix deadlock in Observation"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: tests against third party stores failing after latest update & conditional creation \nDescription: some test regressions when testing with third party stores and the 3.4.2 branch\r\n\r\n* ITestS3ACopyFromLocalFile \" software.amazon.awssdk.services.s3.model.S3Exception: The Content-SHA256 you specified did not match what we received (Service: S3, Status Code: 400,\"\r\n\r\n* conditional overwrite tests fail because it's not there. Even when disabled in the config, some tests still try to use it (create cost), and find assertions about overwrites not met.\r\n\r\n* ITestS3ABucketExistence for bucket existence probes failures; assumption is that bucket probes succeed for all bucket names, it's when you actually try to work with one you get a failure.\r\n\r\nThe copy from local failure an unwelcome change, but maybe could be addressed with doc changes.\r\n\r\nThe conditional overwrite thing is something to flag in release notes as possible regression.", "output": "In Progress"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add `spark.kubernetes.driver.pod.excludedFeatureSteps` usage example\nDescription: \nQ: Issue resolved by pull request 379\n[https://github.com/apache/spark-kubernetes-operator/pull/379]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [pb-upgrade] RouterAdminProtocolTranslatorPB use ShadedProtobufHelper\nDescription: RouterAdminProtocolTranslatorPB should also use ShadedProtobufHelper.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Deadlock in Kafka Streams when processing Interactive Queries and state store updates concurrently\nDescription: We are using a Kafka Streams topology that continuously writes large volumes of data into a RocksDB state store with stable throughput. In parallel, another thread executes Interactive Query (IQ) requests against the same local state store.\r\n\r\nWhen the number of IQ requests in the queue grows (≈50+), the application enters a {*}deadlock state{*}.\r\n\r\n*Investigation:*\r\nUsing a thread dump, we discovered a lock inversion between RocksDB operations:\r\n * {{RocksDBStore.put}}\r\n\r\n ** blocked on {{org.apache.kafka.streams.query.Position@4ba00b6c}}\r\n\r\n ** holding {{org.apache.kafka.streams.state.internals.RocksDBStore@414cff0e}}\r\n\r\n * {{RocksDBStore.range}}\r\n\r\n ** blocked on {{org.apache.kafka.streams.state.internals.RocksDBStore@414cff0e}}\r\n\r\n ** holding {{org.apache.kafka.streams.query.Position@4ba00b6c}}\r\n\r\nThis indicates that {*}{{put}} and {{range}} acquire the same locks but in different order{*}, which leads to deadlock under concurrent load.\r\n\r\n*Expected Behavior:*\r\nKafka Streams API sh\nQ: Thanks for filing this ticket. Seems the changes of KAFKA-15770 introduced this issue. Not 100% sure yet, what the right fix is, but not allocating locks in the same order on all code path is for sure incorrect.\r\n\r\nWe do lock `Position` object inside `StoreQueryUtils#handleBasicQueries(...)` – maybe we would need to lock the passed in `store`, first?", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Race condition between log segment flush and file deletion causing log dir to go offline\nDescription: h1. Context\r\n\r\nWe are using Kafka v3.7.1 with Zookeeper, our brokers are configured with multiple disks in a JBOD setup, routine intra-broker data rebalancing is performed using Cruise Control to manage disk utilization. During these rebalance operations, a race condition between a log segment flush operation and the file deletion that is part of the replica's directory move. This race leads to a `NoSuchFileException` when the flush operation targets a file path that has just been deleted by the rebalance process. This exception incorrectly forces the broker to take the entire log directory offline.\r\nh1. Logs / Stack trace\r\n{code:java}\r\n2025-07-23 19:03:30,114 WARN Failed to flush file /var/lib/kafka-08/topic_01-12/00000000024420850595.snapshot (org.apache.kafka.\r\ncommon.utils.Utils)\r\njava.nio.file.NoSuchFileException: /var/lib/kafka-08/topic_01-12/00000000024420850595.snapshot\r\n        at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\r\n        at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\r\n        at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\r\n        at java.base/sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:182)\r\n        at java.base/java.nio.channels.FileChannel.open(FileChannel.java:292)\r\n        at java.base/java.nio.channels.FileChannel.open(FileChannel.java:345)\r\n        at org.apache.kafka.common.utils.Utils.flushFileIfExists(Util", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add bitmap_and_agg aggregation function\nDescription: Introduce a function analogous to bitmap_or_agg, but performing a bitwise AND operation instead of OR.\r\n\r\nSpecifically, the bitmap_and_agg function should output a bitmap that represents the bitwise AND of all bitmaps in the input column. The input column must contain bitmaps generated from bitmap_construct_agg(). \r\n\r\nExample:\r\n{code:java}\r\n>>> from pyspark.sql import functions as sf\r\n>>> df = spark.createDataFrame([(\"30\",),(\"70\",),(\"F0\",)], [\"a\"])\r\n>>> df.select(sf.bitmap_and_agg(sf.to_binary(df.a, sf.lit(\"hex\")))).show()\r\n\r\n+--------------------------------+\r\n|bitmap_and_agg(to_binary(a, hex))|\r\n+--------------------------------+\r\n|            [30 00 00 00 00 0...|\r\n+--------------------------------+{code}\nQ: Issue resolved by pull request 52586\n[https://github.com/apache/spark/pull/52586]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: WithAggregationKinesisBackedBlockRDDSuite fails on JDK 9 and above\nDescription: Running {{ENABLE_KINESIS_TESTS=1 build/mvn test -Pkinesis-asl -pl connector/kinesis-asl}} on JDK 9 or above fails with\r\n{code}\r\nWithAggregationKinesisBackedBlockRDDSuite:\r\n*** RUN ABORTED ***\r\n  java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter\r\n  at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.(KinesisProducer.java:245)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer(KPLBasedKinesisTestUtils.scala:45)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.$anonfun$sendData$1(KPLBasedKinesisTestUtils.scala:60)\r\n  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\r\n  at scala.collection.immutable.Range.foreach(Range.scala:158)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.sendData(KPLBasedKinesisTestUtils.scala:57)\r\n  at org.apache.spark.streaming.kinesis.KinesisTestUtils.pushData(KinesisTestUtils.scala:134)\r\n  ...\r\n  Cause: java.lang.ClassNotFoundException: javax.xml.bind.DatatypeConverter\r\n  at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n  at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce the framework for adding ST expressions in Catalyst\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52784.", "output": "Issue resolved by pull request 52784\n[https://github.com/apache/spark/pull/52784]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Addition of column-level Parquet compression preference in Spark\nDescription: h4. *Problem*\r\n\r\nApache Spark currently allows only *global compression configuration* for Parquet files using:\r\n\r\n{{spark.sql.parquet.compression.codec = snappy | gzip | zstd | uncompressed}}\r\n\r\nHowever, many production datasets contain heterogeneous columns — for example:\r\n * text or categorical columns that compress better with {*}ZSTD{*},\r\n\r\n * numeric columns that perform better with {*}SNAPPY{*}.\r\n\r\nToday, Spark applies a single codec to the entire file, preventing users from optimizing storage and I/O performance per column.\r\n\r\n\r\nh4. Proposed Improvement\r\n\r\nIntroduce a new configuration key to define *per-column compression codecs* in a map format:\r\n\r\n{{spark.sql.parquet.column.compression.map = colA:zstd,colB:snappy,colC:gzip}}\r\n\r\n*Behavior:*\r\n * The global codec ({{{}spark.sql.parquet.compression.codec{}}}) remains the default for all columns.\r\n\r\n * Any column listed in {{spark.sql.parquet.column.compression.map}} will use its specified codec.\r\n\r\n * Unspecified columns continue to use the global codec.\r\n\r\n*Example:*\r\n\r\n{{--conf spark.sql.parquet.compression.codec=snappy \\}}\r\n\r\n{{--conf spark.sql.parquet.column.compression.map=\"country:zstd,price:snappy,comment:gzip\"\r\n\r\nEffect:-\r\n\r\n}}\r\n||Column||Codec||\r\n|country|zstd|\r\n|price|snappy|\r\n|comment|gzip|\r\n|all others|snappy (global default)|\r\n\r\n{{}}", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update PR template to ask about AI contribution; other AI hardening\nDescription: Add a section at the bottom of the PR template to ask which AI tooling was used, if any, \r\n\r\n--- \r\n## AI\r\n\r\nIf an AI tool was used: \r\n[ ] The PR includes the phrase \"Generated by  where tool is  the AI tool used\r\n[ ] My use of AI contributions follows the ASF legal policy\r\nhttps://www.apache.org/legal/generative-tooling.html\nQ: + add ignore files for jetbrains and cursor to ignore .auth-keys.xml files which may contain credentials\r\n\r\nhttps://www.jetbrains.com/help/ai-assistant/disable-ai-assistant.html\r\nhttps://docs.github.com/en/copilot/managing-copilot/configuring-and-auditing-content-exclusion\r\nhttps://docs.github.com/en/copilot/managing-copilot/configuring-and-auditing-content-exclusion/excluding-content-from-github-copilot\r\n\r\nmore I read the security aspects of this I realise how dangerous it can be to add any module to an IDEA/vscode project without explicitly locking down its policies `otherwise copilot can opt to send data to github", "output": "and someone with the permissions to edit apache/hadoop should exclude auth-keys.xml files."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-fs2img.\nDescription: \nQ: hadoop-yetus commented on PR #7579:\nURL: https://github.com/apache/hadoop/pull/7579#issuecomment-2777507518\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 18s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 11s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 11s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/1/artifact/out/blanks-eol.txt) |  The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-fs2img.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-fs2img.txt) |  hadoop-tools/hadoop-fs2img: The patch generated 42 new + 6 unchanged - 0 fixed = 48 total (was 6)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 48s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 26s |  |  hadoop-fs2img in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  71m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7579 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2af45b81c6c1 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / cf959f3fc7818c076de1540d5e481d667c0dc6e0 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/1/testReport/ |\r\n   | Max. process+thread count | 560 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-fs2img U: hadoop-tools/hadoop-fs2img |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7579:\nURL: https://github.com/apache/hadoop/pull/7579#issuecomment-2782205770\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 11s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 10s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 10s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/2/artifact/out/blanks-eol.txt) |  The patch has 2 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m  7s | [/results-checkstyle-hadoop-tools_hadoop-fs2img.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-fs2img.txt) |  hadoop-tools/hadoop-fs2img: The patch generated 43 new + 6 unchanged - 0 fixed = 49 total (was 6)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 11s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 11s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 23s |  |  hadoop-fs2img in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  73m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7579 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 450a69bd5621 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a618166265856c6192b3ff8f2b14426b76d794e2 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/2/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-fs2img U: hadoop-tools/hadoop-fs2img |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7579/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade kafka to 3.9.0 to fix CVE-2024-31141\nDescription: Upgrade kafka-clients to 3.9.0 to fix [https://nvd.nist.gov/vuln/detail/CVE-2024-31141]\nQ: seems reasonable. Are you in a position to do a PR before we start doing a new release this month?", "output": "Sure [~stevel@apache.org], working on it."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Enable ProducerPerformance to abort transaction randomly\nDescription: While testing KAFKA-18884, I noticed there are no official tools available to measure the performance of aborted transactions. `ProducerPerformance` should serve this purpose by allowing us to configure a ratio of transactions to abort", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Parallel remote reads causes memory leak in broker\nDescription: This issue is caused with changes from https://issues.apache.org/jira/browse/KAFKA-14915\r\n\r\nBroker heap memory gets filled up and throws OOM error when remote reads are triggered for multiple partitions within a FETCH request. \r\n\r\nSteps to reproduce: \r\n\r\n1. Start a one node broker and configure LocalTieredStorage as remote storage. \r\n2. Create a topic with 5 partitions. \r\n3. Produce message and ensure that few segments are uploaded to remote.\r\n4. Start a consumer to read from those 5 partitions. Seek the offset to beginning for 4 partitions and to end for 1 partition. This is to simulate that the FETCH request read from both remote-log and local-log.\r\n5. The broker crashes with the OOM error.\r\n6. The DelayedRemoteFetch / RemoteLogReadResult references are being held by the purgatory, so the broker crashes.\r\n\r\ncc [~showuon] [~satish.duggana]\nQ: [~showuon]\r\nAble to reproduce the issue consistently. Uploaded the RemoteReadMemoryLeakReproducer. The leak was due to that the DelayedRemoteFetchPurgatory holding the references of previously completed DelayedRemoteFetch objects. DelayedRemoteFetch contains the RemoteReadResult internally.\r\n\r\n> Have you figured it out where do we leak the memory? \r\nIn a given FETCH request, if 1 out of 5 partition, read the data from local log, then the watcherKey for that partition holds the reference of the DelayedRemoteFetch in the purgatory; if there are no other remote-read happens for that partition, then it won't get removed until the reaper thread cleans it up after the purgeInterval (entries) of 1000. \r\n{code:java}\r\n% sh kafka-topics.sh --create --topic apple --partitions 5 --replication-factor 1 --bootstrap-server localhost:9092 --config remote.storage.enable=true --config local.retention.ms=60000 --config retention.ms=7200000 --config segment.bytes=104857600 --config file.delete.delay.ms=1000\r\n\r\n% for i in `seq 1 100`; do echo $i; sleep 1; sh kafka-producer-perf-test.sh --topic apple --num-records 1200000000 --record-size 1024 --throughput 1000 --producer-props bootstrap.servers=localhost:9092; done {code}", "output": "This patch fixes the problem. But it slows down the read throughput because it takes time to clone the buffer. There should be other better solutions.\r\n\r\n \r\n{code:java}\r\n--- a/core/src/main/scala/kafka/server/DelayedRemoteFetch.scala\r\n+++ b/core/src/main/scala/kafka/server/DelayedRemoteFetch.scala\r\n@@ -22,6 +22,7 @@ import kafka.utils.Logging\r\n import org.apache.kafka.common.TopicIdPartition\r\n import org.apache.kafka.common.errors._\r\n import org.apache.kafka.common.protocol.Errors\r\n+import org.apache.kafka.common.record.MemoryRecords\r\n import org.apache.kafka.server.LogReadResult\r\n import org.apache.kafka.server.metrics.KafkaMetricsGroup\r\n import org.apache.kafka.server.purgatory.DelayedOperation\r\n@@ -121,7 +122,8 @@ class DelayedRemoteFetch(remoteFetchTasks: util.Map[TopicIdPartition, Future[Voi\r\n             result.error,\r\n             result.highWatermark,\r\n             result.leaderLogStartOffset,\r\n-            info.records,\r\n+            // clone the record buffer to release the memory\r\n+            MemoryRecords.readableRecords(info.records.asInstanceOf[MemoryRecords].buffer()),\r\n             Optional.empty(),\r\n             if (result.lastStableOffset.isPresent) OptionalLong.of(result.lastStableOffset.getAsLong) else OptionalLong.empty(),\r\n             info.abortedTransactions,\r\n@@ -132,7 +134,8 @@ class DelayedRemoteFetch(remoteFetchTasks: util.Map[TopicIdPartition, Future[Voi\r\n         tp -> result.toFetchPartitionData(false)\r\n       }\r\n     }\r\n-\r\n+    // clear the map to avoid memory leak\r\n+    remoteFetchResults.clear()\r\n     responseCallback(fetchPartitionData)\r\n   }\r\n }\r\n{code}"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hello,\r\n\r\nI'm trying to upgrade my version of ParquetJava and I'm seeing the following error locally\r\n{code:java}\r\n    java.lang.UnsupportedOperationException: getSubject is not supported\r\n\r\n\r\n        at java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\r\n\r\n        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.(FileSystem.java:3852)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.(FileSystem.java:3842)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3630)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541)\r\n\r\n\r\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\r\n\r\n        at org.apache.parquet.hadoop.util.HadoopOutputFile.fromPath(HadoopOutputFile.java:58)\r\n\r\n\r\n        at com.amazon.networkvalidator.parquet.ParquetWriter.write(ParquetWriter.kt:75)\r\n\r\n\r\n        at com.amazon.networkvalidator.parquet.ParquetWriterTest$test \r\nwriting to parquet$1.invokeSuspend(ParquetWriterTest.kt:88)\r\n\r\n\r\n        at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt)\r\n\r\n\r\n        at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt)\r\n\r\n\r\n        at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt$runTest$2$1$1.invokeSuspend(TestBuilders.kt:318)\r\n\r\n\r\n        at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n\r\n\r\n        at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:101)\r\n\r\n\r\n        at kotlinx.coroutines.test.TestDispatcher.processEvent$kotlinx_coroutines_test(TestDispatcher.kt:24)\r\n\r\n\r\n        at \r\nkotlinx.coroutines.test.TestCoroutineScheduler.tryRunNextTaskUnless$kotlinx_", "output": "UserGroupInformation.java is using a non-support operation in JDK25"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-rumen.\nDescription: \nQ: slfan1989 opened a new pull request, #7552:\nURL: https://github.com/apache/hadoop/pull/7552\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19439. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-rumen.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   mvn clean test & junit test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7552:\nURL: https://github.com/apache/hadoop/pull/7552#issuecomment-2763304720\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 16s | [/results-checkstyle-hadoop-tools_hadoop-rumen.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-rumen.txt) |  hadoop-tools/hadoop-rumen: The patch generated 4 new + 5 unchanged - 0 fixed = 9 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 54s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 32s |  |  hadoop-rumen in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 132m  3s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7552 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7083e5353efc 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f09b946cd84d837717ad417c71edfe32bd9fedce |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/1/testReport/ |\r\n   | Max. process+thread count | 524 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-rumen U: hadoop-tools/hadoop-rumen |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{panel:title=Notes:|borderStyle=dashed|borderColor=#cccccc|titleBGColor=#f7d6c1|bgColor=#ffffce}\r\n * (!) this ticket blocks KAFKA-19664 *_Support building with Java 25 (LTS release)_*\r\n * (on) looks similar to: KAFKA-15117 *_SslTransportLayerTest.testValidEndpointIdentificationCN fails with Java 20 & 21_*{panel}\r\n*Reproducer:*\r\n * use this branch: [https://github.com/dejan2609/kafka/tree/KAFKA-19664] (i.e. this PR: [https://github.com/apache/kafka/pull/20561])\r\n * execute: *./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -i* and change Java versions\r\n * test results:\r\n ** {*}Java 17{*}: (/)\r\n ** J{*}ava 24{*}: (/)\r\n ** {*}Java 25{*}: 159 tests completed, *8 failed* (x)\r\n\r\n*Test results on Github CI:*\r\n!Screenshot from 2025-10-07 19-59-32.png!\r\n\r\n*Test results locally:*\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ git log -3 --oneline \r\na37616a1eb (HEAD -> KAFKA-19664, origin/KAFKA-19664) KAFKA-19664 Support building with Java 25 (LTS release)\r\nc6bbbbe24d KAFKA-19174 Gradle version upgrade 8 -->> 9 (#19513)\r\nf5a87b3703 KAFKA-19748: Add a note in docs about memory leak in Kafka Streams 4.1.0  (#20639)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version \r\nopenjdk version \"17.0.16\" 2025-07-15\r\nOpenJDK Runtime Environment Temurin-17.0.16+8 (build 17.0.16+8)\r\nOpenJDK 64-Bit Server VM Temurin-17.0.16+8 (build 17.0.16+8, mixed mode, sharing)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -q\r\nStarting build with version 4.2.0-SNAPSHOT (commit id a37616a1) using Gradle 9.1.0, Java 17 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\nMessageGenerator: processed 197 Kafka message JSON file(s).\r\nMessageGenerator: processed 4 Kafka message JSON file(s).\r\nNote: Some input files use or override a deprecated API.\r\nNote: Recompile with -Xlint:deprecation for details.\r\nOpenJDK 64-Bit Server", "output": "[Java 25] few tests are failling (class: clients/org.apache.kafka.common.network.SslTransportLayerTest)"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Deprecate ClientQuotaCallback#updateClusterMetadata\nDescription: In KAFKA-18225, we discovered that {{ClientQuotaCallback#updateClusterMetadata}} is not supported in KRaft mode, unlike in Zookeeper mode. Kafka 4.0 addressed this gap by implementing the method, but limitations remain:\r\n * A new {{Cluster}} object (immutable and heavyweight) is passed on every metadata update, which may cause memory pressure in large clusters (see KAFKA-18239).\r\n\r\n * Some {{Cluster}} fields are confusing or irrelevant in KRaft, such as {{controller()}} returning a random node for compatibility. Also, listener parsing differs between modes, potentially causing inconsistent partition info (see KAFKA-19122).\r\n\r\nTo resolve these issues, *[KIP-1162|https://cwiki.apache.org/confluence/x/zInoF]* proposes a redesign of the method. However, given that this method remained unimplemented for years without user complaints, we believe it's not worth the complexity to fix it. Instead, we propose deprecating {{updateClusterMetadata}} now and removing it in Kafka 5.0.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip doctests in pyspark.sql.functions.builtin if pyarrow is not installe\nDescription: \nQ: Issue resolved by pull request 52569\n[https://github.com/apache/spark/pull/52569]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use empty schema when altering a view which is not Hive compatible\nDescription: Spark attempts to save views in a Hive-compatible format and only sets the schema to empty if the save operation fails.\r\n\r\nHowever, due to certain Hive compatibility issues, the save operation may succeed while subsequent read operations fail. This issue arises after the change introduced in SPARK-46934, which removed the {{verifyColumnDataType}} check during the {{ALTER TABLE}} operation.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve metavariable names for verifiable producer/consumer\nDescription: The metavariable names in the usage messages for the verifiable producer, consumer and share consumer are a bit of a mess which makes the usage messages hard to read. It is trivial to improve and only affects the usage messages.\r\n\r\nFor example:\r\n\r\nusage: verifiable-share-consumer\r\n\r\n       [-h] --topic TOPIC --group-id GROUP_ID [--max-messages MAX-MESSAGES] [--verbose]\r\n\r\n       [--acknowledgement-mode ACKNOWLEDGEMENTMODE] [--offset-reset-strategy OFFSETRESETSTRATEGY]\r\n\r\n       [--command-config CONFIG_FILE] --bootstrap-server HOST1:PORT1[,HOST2:PORT2[...]]\r\n\r\nwould be better as:\r\n\r\nusage: verifiable-share-consumer\r\n\r\n       [-h] --topic TOPIC --group-id GROUP-ID [--max-messages MAX-MESSAGES] [--verbose]\r\n\r\n       [--acknowledgement-mode ACKNOWLEDGEMENT-MODE] [--offset-reset-strategy OFFSET-RESET-STRATEGY]\r\n\r\n       [--command-config CONFIG_FILE] --bootstrap-server HOST1:PORT1[,HOST2:PORT2[...]]", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Support for new auth type: User-bound SAS\nDescription: Adding support for new authentication type: user bound SAS\nQ: hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3442714485\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  34m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 45s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   1m 23s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 34s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 35s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/blanks-eol.txt) |  The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 36 new + 4 unchanged - 0 fixed = 40 total (was 4)  |\r\n   | -1 :x: |  mvnsite  |   0m 37s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 32s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 2 new + 1472 unchanged - 0 fixed = 1474 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 2 new + 1413 unchanged - 0 fixed = 1415 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 34s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  29m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 41s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 119m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint |\r\n   | uname | Linux 88abe69cd96c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 05b52e40f1bc99edc039fc0d2ab5f83d1ceb0da9 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/testReport/ |\r\n   | Max. process+thread count | 779 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #8051:\nURL: https://github.com/apache/hadoop/pull/8051#issuecomment-3454395745\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   4m 52s |  |  Docker failed to build run-specific yetus/hadoop:tp-4947}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8051 |\r\n   | JIRA Issue | HADOOP-19736 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8051/2/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade SpotBugs Version to Support JDK 17 Compilation\nDescription: The current SpotBugs version used in the project is 4.2.2 (SpotBugs) and 4.2.0 (SpotBugs Maven Plugin), which is not fully compatible with JDK 17 compilation. To ensure proper functionality of the code quality check tool in a JDK 17 environment, SpotBugs needs to be upgraded to the latest version that supports JDK 17.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `netty-tcnative` to 2.0.74.Final\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{code:java}\r\nException in thread \"main\" org.apache.kafka.common.InvalidRecordException: The baseOffset of the record batch in the append to kafka-test-0 should be 0, but it is 9\r\n{code}\r\n\r\nWe could simplify reset the offset or create the new records in each write", "output": "The records appended to the log are illegal because of an incorrect base offset during TestLinearWriteSpeed"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip query execution for DESCRIBE QUERY\nDescription: DESCRIBE QUERY command only needs the schema of the query, so we only need to do analysis. Therefore, we can skip the unnecessary execution portion for the command.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is surfacing on a bucket using versionid for change detection: block reads are failing in the test {{ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead()}}\r\n\r\n{code}\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 412, Request ID: 0AN2EB8QXC75HH0T, Extended Request ID: U5l/UnIF4n3NO1mrZVzS2vv72F3LgUoVJxR4XodUSaTWCerfjmmpH45CbFGKkTkfgfnykwzseGo=)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:104)\r\n\r\n{code}\r\n\r\n* this is the normal readFully() call, before the vectored one\r\n* it worked last week\r\n* also found on branch-3.4 before the SDK update, so not an issue caused by the SDK unless my maven repo is badly contaminated\r\n* seems unrelated to versioning -still there when disabled.\r\n* applies on unversioned s3 express store too.\r\n\r\nAbout the main way I could see this surface is if the test file is less than the actual length of file created, so the GET is rejected for reading off the end (the openfile passes in the length to save the HEAD)", "output": "S3A: testVectoredReadAfterNormalRead() failing with 412 response from S3"}
{"instruction": "Answer the question based on the bug.", "input": "Title: setting --no-initial-controllers flag should not validate kraft version against metadata version\nDescription: Just because a controller node sets `--no-initial-controllers` flag does not mean it is necessarily running kraft.version=1. The more precise meaning is that the controller node does not know what kraft version the cluster should be in.\r\n\r\nIt is a valid configuration to run a static quorum defined by `controller.quorum.voters` but have all the controllers format with `--no-initial-controllers`.\nQ: trunk: https://github.com/apache/kafka/commit/857b1e92cc5c75eb178e48613e5963755bc1b03b", "output": "4.1: https://github.com/apache/kafka/commit/012e4ca6d8fcd9a76dcb60480d9ba9cb7827816e"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analytics-Accelerator: Add support SSE-C\nDescription: Pass down SSE-C key to AAL so it can be attached while making the GET request.\nQ: ahmarsuhail commented on code in PR #7738:\nURL: https://github.com/apache/hadoop/pull/7738#discussion_r2242908029\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsStream.java:\n##########\n@@ -205,6 +209,12 @@ private OpenStreamInformation buildOpenStreamInformation(ObjectReadParameters pa\n           .etag(parameters.getObjectAttributes().getETag()).build());\n     }\n \n+    if(parameters.getEncryptionSecrets().getEncryptionMethod() == S3AEncryptionMethods.SSE_C) {\n\nReview Comment:\n   On the GET, you only need it for SSE_C i think, this is what the current S3A implementation does as well here: https://github.com/apache/hadoop/blob/636d822682715dbc05af9483e91a9f0ee72f83b8/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java#L639\n   \n   On writing you need to set it for KMS as well, so the write operation knows what key to use. Think on the GET, the right key just gets picked up automatically from KMS.", "output": "steveloughran commented on code in PR #7738:\nURL: https://github.com/apache/hadoop/pull/7738#discussion_r2301169051\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/ObjectReadParameters.java:\n##########\n@@ -69,6 +70,29 @@ public final class ObjectReadParameters {\n    */\n   private LocalDirAllocator directoryAllocator;\n \n+  /**\n+   * Encryption secrets for this stream\n\nReview Comment:\n   add a . at the end\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsStream.java:\n##########\n@@ -205,6 +209,12 @@ private OpenStreamInformation buildOpenStreamInformation(ObjectReadParameters pa\n           .etag(parameters.getObjectAttributes().getETag()).build());\n     }\n \n+    if(parameters.getEncryptionSecrets().getEncryptionMethod() == S3AEncryptionMethods.SSE_C) {\n\nReview Comment:\n   nit: add a space before the {\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEC.java:\n##########\n@@ -327,6 +325,65 @@ public void testChecksumRequiresReadAccess() throws Throwable {\n         () -> fsKeyB.getFileChecksum(path));\n   }\n \n+\n+  /**\n+   * Tests the creation and reading of a file using a different encryption key\n+   * when Analytics Accelerator is enabled.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void testCreateFileAndReadWithDifferentEncryptionKeyWithAnalyticsAcceleratorEnabled() throws Exception {\n\nReview Comment:\n   rather than do it this way, make the entire test suite (i.e. class) something which runs with both the normal and accelerated options."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix hadoop-client-minicluster\nDescription: \nQ: can you please fill the description of the ticket, it isn't conclusive from the ticket, like what is broken which needs to be fixed", "output": "I should close this ticket, the issue was fixed by HADOOP-19652"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In ReassignPartitiosnsCommand#generateAssignment, it uses JSON format like:\r\n\r\n \r\n{code:java}\r\n{\r\n  \"topics\": [\r\n    { \"topic\": \"foo1\" },\r\n    { \"topic\": \"foo2\" }\r\n  ],\r\n  \"version\": 1\r\n} {code}\r\nHowever, in ReassignPartitionsCommandTest#testGenerateAssignmentWithBootstrapServer, it uses input like:\r\n{code:java}\r\n{\r\n  \"version\":1,\r\n  \"partitions\": [\r\n    {\r\n      \"topic\": \"foo\",\r\n      \"partition\": 0,\r\n      \"replicas\": [3, 1, 2],\r\n      \"log_dirs\": [\"any\",\"any\",\"any\"]\r\n    }\r\n  ]\r\n}{code}\r\nThe test case can pass, but it doesn't test `generateAssignment` correctly.", "output": "testGenerateAssignmentWithBootstrapServer uses wrong JSON format"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Running {{ENABLE_KINESIS_TESTS=1 build/mvn test -Pkinesis-asl -pl connector/kinesis-asl}} on JDK 9 or above fails with\r\n{code}\r\nWithAggregationKinesisBackedBlockRDDSuite:\r\n*** RUN ABORTED ***\r\n  java.lang.NoClassDefFoundError: javax/xml/bind/DatatypeConverter\r\n  at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.(KinesisProducer.java:245)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer(KPLBasedKinesisTestUtils.scala:45)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.$anonfun$sendData$1(KPLBasedKinesisTestUtils.scala:60)\r\n  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\r\n  at scala.collection.immutable.Range.foreach(Range.scala:158)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.sendData(KPLBasedKinesisTestUtils.scala:57)\r\n  at org.apache.spark.streaming.kinesis.KinesisTestUtils.pushData(KinesisTestUtils.scala:134)\r\n  ...\r\n  Cause: java.lang.ClassNotFoundException: javax.xml.bind.DatatypeConverter\r\n  at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n  at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n  at com.amazonaws.services.kinesis.producer.HashedFileCopier.copyFileFrom(HashedFileCopier.java:56)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.extractBinaries(KinesisProducer.java:893)\r\n  at com.amazonaws.services.kinesis.producer.KinesisProducer.(KinesisProducer.java:245)\r\n  at org.apache.spark.streaming.kinesis.KPLDataGenerator.producer$lzycompute(KPLBasedKinesisTestUtils.scala:52)\r\n  at org.apache.spark.s", "output": "WithAggregationKinesisBackedBlockRDDSuite fails on JDK 9 and above"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider\nDescription: Externally Reported Enhancement:\r\n\r\n*Current Limitation*\r\nThe current WorkloadIdentityTokenProvider implementation works well for file-based token scenarios, but it's tightly coupled to file system operations and cannot be easily extended for alternative token sources\r\n\r\n{*}Use Case{*}: *Kubernetes TokenRequest API* \r\nIn modern Kubernetes environments, the recommended approach is to use the TokenRequest API to generate short-lived, on-demand service account tokens rather than relying on projected volume mounts.\r\n\r\n*Proposed Enhancement* \r\nI propose modifying WorkloadIdentityTokenProvider to accept a Supplier for token retrieval instead of being hardcoded to file operations:", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [Java 25] few tests are failling (class: clients/org.apache.kafka.common.network.SslTransportLayerTest)\nDescription: {panel:title=Notes:|borderStyle=dashed|borderColor=#cccccc|titleBGColor=#f7d6c1|bgColor=#ffffce}\r\n * (!) this ticket blocks KAFKA-19664 *_Support building with Java 25 (LTS release)_*\r\n * (on) looks similar to: KAFKA-15117 *_SslTransportLayerTest.testValidEndpointIdentificationCN fails with Java 20 & 21_*{panel}\r\n*Reproducer:*\r\n * use this branch: [https://github.com/dejan2609/kafka/tree/KAFKA-19664] (i.e. this PR: [https://github.com/apache/kafka/pull/20561])\r\n * execute: *./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -i* and change Java versions\r\n * test results:\r\n ** {*}Java 17{*}: (/)\r\n ** J{*}ava 24{*}: (/)\r\n ** {*}Java 25{*}: 159 tests completed, *8 failed* (x)\r\n\r\n*Test results on Github CI:*\r\n!Screenshot from 2025-10-07 19-59-32.png!\r\n\r\n*Test results locally:*\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ git log -3 --oneline \r\na37616a1eb (HEAD -> KAFKA-19664, origin/KAFKA-19664) KAFKA-19664 Support building with Java 25 (LTS rele\nQ: [~dejan2609] Thanks for reporting this. I think this is because [TestUtils::CertificateBuilder|https://github.com/dejan2609/kafka/blob/a21a1b0a84b577724d0030a02b2ad7f2d08890e2/clients/src/test/java/org/apache/kafka/test/TestSslUtils.java#L402]'s constructor uses {{SHA1withRSA}} as the signature algorithm which is likely disabled by default. Please try updating the signature algorithm to say {{SHA256withRSA}}", "output": "You got that right [~gnarula] :) !\r\n\r\nThank you for the tip (y)"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: MetadataShell read 0000-0000.checkpoint will cause infinite loop\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Delete dynamic config that were removed by Kafka\nDescription: [KIP-724: Drop support for message formats v0 and v1|https://cwiki.apache.org/confluence/display/KAFKA/KIP-724%3A+Drop+support+for+message+formats+v0+and+v1]  and Kafka 4.0.0 removed support for the dynamic configs like message.format.version.\r\n\r\nWhen the user upgrades from 3.x to 4.x it can cause the dynamic configuration validation to fail if there are dynamic configurations that are no longer supported.\r\n{code:java}\r\nCaused by: org.apache.kafka.common.errors.InvalidConfigurationException: Unknown topic config name: message.format.version {code}\r\nOne solution for solving this problem is to have a well define list of allowed dynamic configuration. Any config that doesn't match this list will be automatically deleted.\r\n\r\nThis deletion can be done implicitly when applying the ConfigRecord record. This behavior can be further guarded by checking that the metadata version is greater than 4.0. This also means that upgrading the metadata version to 4.0 would cause all of the removed config to get deleted.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix flaky DescribeStreamsGroupTest testDescribeStreamsGroupWithShortTimeout()\nDescription: According to [Develocity|https://develocity.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&tests.container=org.apache.kafka.tools.streams.DescribeStreamsGroupTest&tests.sortField=FLAKY], {{testDescribeStreamsGroupWithShortTimeout}} is very flaky.\nQ: There is already a PR this this: [https://github.com/apache/kafka/pull/20320] – we did not have a ticket, so it was filed a MINOR... \\cc [~alisa23]", "output": "Thanks for the reminder! I didn't notice this PR before."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce deliveryCompleteCount in DescribeShareGroupStateOffsets\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `docker-java` to 3.6.0\nDescription: \nQ: Issue resolved by pull request 52601\n[https://github.com/apache/spark/pull/52601]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When a failure occurs with a push telemetry request, any exception is treated as fatal, causing telemetry to be turned off.  We can enhance this error handling to check if the exception is a transient one with expected recovery, and not turn off telemetry in those cases.", "output": "Improve handling of failed push telemetry request"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview3-java21`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce Geometry and Geography in-memory wrapper formats\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade to hadoop-thirdparty 1.4.0\nDescription: HADOOP-19483 releases thirdparty 1.4.0; upgrade hadoop to it\nQ: steveloughran opened a new pull request, #7560:\nURL: https://github.com/apache/hadoop/pull/7560\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Upgrade to the newly released 1.4.0 library\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Been running it locally for a week\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "cnauroth commented on code in PR #7560:\nURL: https://github.com/apache/hadoop/pull/7560#discussion_r2021689285\n\n\n##########\nLICENSE-binary:\n##########\n@@ -243,7 +243,7 @@ com.google.guava:listenablefuture:9999.0-empty-to-avoid-conflict-with-guava\n com.microsoft.azure:azure-storage:7.0.0\n com.nimbusds:nimbus-jose-jwt:9.37.2\n com.zaxxer:HikariCP:4.0.3\n-commons-beanutils:commons-beanutils:1.9.4\n+commons-beanutils:commons-beanutils:1.9.4j\n\nReview Comment:\n   Typo?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The --property argument in kafka-console-producer.sh should be deprecated and replaced with --reader-property.", "output": "Deprecate --property and replace with --reader-property in console producer"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A Analytics-Accelerator: AAL stream factory not being closed\nDescription: When the Factory service is stopped, we're currently missing the code to close the factory. My miss, this got lost in the move to the new factory code.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "As part of our analysis around KAFKA-19430, we realized it's not possible to handle a {{CorruptRecordException}} in Streams because it's not exposed to the client; instead, a generic {{KafkaException}} ** is thrown\r\n\r\nThe proposed solution is to expose {{CorruptRecordException}} with information about affected TopicPartition and offset we're trying read from\r\n\r\nKIP: https://cwiki.apache.org/confluence/x/NQrxFg", "output": "Expose consumer CorruptRecordException"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Missing assigned-partitions metric in new consumer\nDescription: The metric assigned-partitions is missing in the new async consumer.\r\n\r\nRegistered by the coordinator in the old consumer here [https://github.com/apache/kafka/blob/dbd2b527d0fa5eda4ec603ee449a47c69ac41852/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L1640]", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade commons-beanutils to 1.11.0 due to CVE-2025-48734.\nDescription: Upgrade commons-beanutils to 1.11.0 due to CVE-2025-48734.", "output": "Reopened"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add Caching Mechanism to HostResolver to Avoid Redundant Hostname Resolutions\nDescription: *Background:*\r\n\r\n \r\n\r\nCurrently, the two implementations of org.apache.hadoop.security.SecurityUtil.HostResolver, *StandardHostResolver and QualifiedHostResolver* in Hadoop performs hostname resolution each time it is called. *Each heartbeat between the AM and RM causes the RM to invoke the* HostResolver#getByName {*}method once{*}. In large-scale clusters running numerous applications, this results in *a high frequency of redundant hostname resolutions.*\r\n\r\n \r\n\r\n*Proposal:*\r\n\r\n \r\n\r\nIntroduce a caching mechanism in HostResolver to store resolved hostnames for a configurable duration. This would:\r\n\r\n•Reduce redundant DNS queries.\r\n\r\n•Improve performance for frequently used hostnames.\r\n\r\n•Allow configuration options for cache size and TTL (Time-to-Live).\r\n\r\n \r\n\r\n*Suggested Implementation:*\r\n\r\n1.{*}Leverage Existing CachedResolver{*}:\r\n\r\nThe NodesListManager.CachedResolver class in Hadoop already implements a caching mechanism for hostname resolution. Instead of introducing an entirely ne\nQ: this issue depends on the changes introduced in [YARN-11765]. The modifications in YARN-11765 should be completed first to ensure a smooth integration of this enhancement.", "output": "HostnameCache depends on the Clock interface, which is currently defined in hadoop-yarn-project. However, this creates an unavoidable circular dependency, as hadoop-common-project cannot depend on hadoop-yarn-project.\r\n\r\nTo resolve this issue, we need to move the Clock interface and its implementation to hadoop-common-project, making it a shared utility. Therefore, a new issue YARN-11765 has been created to refactor and relocate the Clock interface.\r\n\r\nThis refactor ensures that hadoop-common-project remains independent while allowing all projects to use the same Clock implementation without duplication."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support `spark.logConf` configuration\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: NPE Cannot invoke org.apache.kafka.connect.runtime.ConnectMetrics$MetricGroup.close()\nDescription: Several tasks in multiple sink connectors in a long-running connect cluster broke spontaneously (within a couple of hours) with the following stacktrace:\r\n{code:java}\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.kafka.connect.runtime.ConnectMetrics$MetricGroup.close()\" because the return value of \"java.util.concurrent.ConcurrentMap.get(Object)\" is null\r\n    at org.apache.kafka.connect.runtime.Worker$ConnectorStatusMetricsGroup.recordTaskRemoved(Worker.java:2333)\r\n    at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:707)\r\n    at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:568)\r\n    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2009)\r\n    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$39(DistributedHerder.java:2059)\r\n    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)\r\n    at java.base/java.util.concurrent.ThreadPoolEx\nQ: I also encountered this issue. The NPE is due to multiple uninitialized tasks unregistering ConnectorStatus Metrics Group. Created a PR to fix it.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-gridmix.\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support atomic rename() for S3 express\nDescription: S3 Express now supports atomic renames: [https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-s3-express-one-zone-atomic-renaming-objects-api/]\r\n\r\n \r\n\r\nRename API is available as of SDK version 2.31.66.\nQ: * we can bypass xfer manager and just issue rename commands to the thread pool, with some throttling\r\n* source etag should be passed in\r\n* auditing to attach referrer header\r\n\r\n\r\nI see the comment about idempotency and want to know what it means if the request works, but we get a network error on the response, so don't see the 200\r\n\r\ns3a will have to retry, and the source will have been copied. What then?\r\n\r\nIf the dest etag == source then s3 should be able to consider this a no-op. does it?\r\n\r\notherwise we repeat, catch whatever error comes back from missing source, probe dest for having etag and the interpret as success.\r\n\r\n* Lots of tests of failure and race conditions, with the fault injection stuff we've added for other tests.\r\n\r\n\r\nnote also the manifest committer could then run on s3, org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.ManifestStoreOperationsThroughFileSystem\r\nno tangible benefit over the magic committer, just an interesting fact. Not worth the effort to support.\r\n\r\nBe more relevant to hive commits, FWIW.", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Integration tests for Stream Admin related API\r\n\r\nPrevious one: https://issues.apache.org/jira/browse/KAFKA-19550\r\n\r\nThis one adds:\r\n * Integration test for {{Admin#listStreamsGroupOffsets}} API\r\n * Integration test fo {{Admin#deleteStreamsGroupOffsets}} API\r\n * Integration test fo {{Admin#alterStreamsGroupOffsets}} API", "output": "Integration test for Streams-related Admin APIs[2/N]"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `ZSTD-JNI` to 1.5.7-5\nDescription: \nQ: Issue resolved by pull request 52591\n[https://github.com/apache/spark/pull/52591]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Share Partition Lag Persistence and Retrieval\nDescription: Ticket for KIP-1226. The KIP proposes introducing the concept of lag for share partitions, which will be persisted in the __share_group_state topic and can be retrieved using admin.listShareGroupOffsets", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Write down the steps for upgrading Gradle\nDescription: Normally, we only update `gradle-wrapper.properties` and `dependencies.gradle`, but that process is incomplete. The correct steps are shown below.\r\n # upgrade gradle-wrapper.properties to the latest gradle\r\n # upgrade dependencies.gradle as well\r\n # use latest gradle to run command `gradle wrapper`to update gradlew \r\n # update wrapper.gradle to ensure the generated \"download command\" works well", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-kafka.\nDescription: \nQ: hadoop-yetus commented on PR #7547:\nURL: https://github.com/apache/hadoop/pull/7547#issuecomment-2761357293\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 46s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 10s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 27s |  |  hadoop-kafka in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 37s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7547/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7547 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux d080f1a81a92 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2cf275bb168fb3efbaa33b3c80e29aff6452e273 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7547/2/testReport/ |\r\n   | Max. process+thread count | 530 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-kafka U: hadoop-tools/hadoop-kafka |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7547/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "jojochuang merged PR #7547:\nURL: https://github.com/apache/hadoop/pull/7547"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The mvnsite compilation step needs python3. Although we're installing python3 in the build environment, the python3 executable is missing.\r\nThus, we need to create a symbolic link python3 pointing to python.exe needed for mvnsite.\r\n\r\nFailure -\r\n{code}\r\n[INFO] -------------------------------------\r\n[INFO] Building Apache Hadoop Common 3.5.0-SNAPSHOT                    [11/115]\r\n[INFO]   from hadoop-common-project\\hadoop-common\\pom.xml\r\n[INFO] --------------------------------[ jar ]---------------------------------\r\n[INFO] \r\n[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ hadoop-common ---\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\target\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\src\\site\\markdown (includes = [UnixShellAPI.md], excludes = [])\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\src\\site\\resources (includes = [configuration.xsl, core-default.xml], excludes = [])\r\n[INFO] \r\n[INFO] --- exec-maven-plugin:1.3.1:exec (shelldocs) @ hadoop-common ---\r\n/usr/bin/env: 'python3': No such file or directory\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Reactor Summary for Apache Hadoop Main 3.5.0-SNAPSHOT:\r\n[INFO] \r\n[INFO] Apache Hadoop Main ................................. SUCCESS [21:20 min]\r\n[INFO] Apache Hadoop Build Tools .......................... SUCCESS [  0.017 s]\r\n[INFO] Apache Hadoop Project POM .......................... SUCCESS [  2.107 s]\r\n[INFO] Apache Hadoop Annotations .......................... SUCCESS [  2.018 s]\r\n[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.411 s]\r\n[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  0.326 s]\r\n[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  5.055 s]\r\n[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  2.848 s]\r\n[INFO] Apache Hadoop Auth ................................. SUCCESS [  9.377 s]\r\n[INFO] Apache Hadoop Auth Examp", "output": "Create python3 symlink needed for mvnsite"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip query execution for DESCRIBE QUERY\nDescription: DESCRIBE QUERY command only needs the schema of the query, so we only need to do analysis. Therefore, we can skip the unnecessary execution portion for the command.\nQ: Issue resolved by pull request 52713\n[https://github.com/apache/spark/pull/52713]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add ContinuousMemorySink for RTM testing\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix decimal rescaling in LocalDataToArrowConversion\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation\nDescription: This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-V platforms supporting the Zbc extension (CLMUL instructions) in bulk_crc32_riscv.c.\r\nKey changes:\r\n * Implements optimized CRC32 and CRC32C routines using CLMUL instructions for zlib and Castagnoli polynomials.\r\n * Automatically switches to hardware acceleration when Zbc is available, otherwise falls back to generic table-based software implementation.\r\n * Maintains compatibility with platforms lacking Zbc support.\r\n\r\nThis optimization improves CRC performance on RISC-V CPUs with Zbc extension.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The powermock dependency is specified in\r\n- hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml\r\n- hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml\r\n\r\nbut not used anywhere.  We should remove it.", "output": "[JDK17] Remove powermock dependency"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update build instructions for Windows\nDescription: We recently upgraded vcpkg to install Boost 1.86. We need to update the documentation as well.\nQ: GauthamBanasandra opened a new pull request, #7673:\nURL: https://github.com/apache/hadoop/pull/7673\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   * We recently upgraded vcpkg to install Boost 1.86 in #7601.\r\n   * This PR updates the build documentation for the same.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Jenkins CI validation.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7673:\nURL: https://github.com/apache/hadoop/pull/7673#issuecomment-2849038608\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  37m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  73m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7673/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7673 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets |\r\n   | uname | Linux 15bb8d6521d4 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 90268e02346b3cab22d79071e7ba7b763d046214 |\r\n   | Max. process+thread count | 671 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7673/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Extract common Generate resolution logic\nDescription: I'm planning to add Generate node resolution to the single-pass analyzer. Before that, I need to do minor refactoring to extract common logic from ResolveGenerate rule.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Complex sql with expand operator and code gen enabled, very slow\r\n\r\nsql format like select keya,keyb,count(distinct case when),...,count(distinct case when),sum(a),sum(b) from x group by keya,keyb\r\n\r\nwhen disable whole stage code gen, run will speed up 20x times\r\n\r\nwhen add executor jvm parameter -XX:-TieredCompilation, run will speed up 20x times\r\n\r\nreduce select column count, such as 28 -> 27, can speed up 10x times", "output": "Complex sql with expand operator and code gen enabled, very slow"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Support WebIdentityTokenFileCredentialsProvider\nDescription: The current default s3 credential provider chain is set in the order of \r\n{code:java}\r\norg.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,software.amazon.awssdk.auth.credentials.EnvironmentVariableCredentialsProvider,org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider{code}\r\nRefer [code ref |https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml#L1450]for more details.\r\n\r\n \r\n\r\nThis works perfectly fine when used in AWS EC2, EMR Serverless, but not with AWS EKS pods.\r\n\r\n \r\n\r\nFor EKS pods, It is recommended to use\r\n{code:java}\r\nsoftware.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider , software.amazon.awssdk.auth.credentials.ContainerCredentialsProvider (PodIdentity is enabled){code}\r\nWebIdentityTokenFileCredentialsProvider is an AWS credentials provider that enables applications to obtain temporary AWS credentials by assuming an IAM role using a web identity token (like OAuth or OIDC tokens). It's particularly important in EKS as it's the underlying mechanism that makes IRSA (IAM Roles for Service Accounts) work.\r\n\r\n \r\n\r\n \r\n\r\nContainerCredentialsProvider is already part of org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve Netty usage patterns\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Exclude junit 4 transitive dependency\nDescription: HADOOP-19617 removed direct junit 4 dependency.  However, junit 4 is still pulled transitively by other dependencies.\nQ: {code}\r\n[INFO] -------------------------------------\r\n[INFO] Building Apache Hadoop Common 3.5.0-SNAPSHOT                    [11/117]\r\n[INFO]   from hadoop-common-project/hadoop-common/pom.xml\r\n...\r\n[INFO] +- com.squareup.okhttp3:mockwebserver:jar:4.11.0:test\r\n[INFO] |  +- com.squareup.okhttp3:okhttp:jar:4.11.0:test\r\n[INFO] |  |  \\- com.squareup.okio:okio:jar:3.2.0:test\r\n[INFO] |  |     \\- com.squareup.okio:okio-jvm:jar:3.2.0:test\r\n[INFO] |  \\- junit:junit:jar:4.13:test\r\n[INFO] |     \\- org.hamcrest:hamcrest-core:jar:1.3:test\r\n{code}", "output": "{code}\r\n[INFO] --------------------------------\r\n[INFO] Building Apache Hadoop HttpFS 3.5.0-SNAPSHOT                    [19/117]\r\n[INFO]   from hadoop-hdfs-project/hadoop-hdfs-httpfs/pom.xml\r\n...\r\n[INFO] +- com.googlecode.json-simple:json-simple:jar:1.1.1:compile\r\n[INFO] |  \\- junit:junit:jar:4.10:compile\r\n{code}"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade com.huaweicloud:esdk-obs-java for CVE-2023-3635\nDescription: The {{com.huaweicloud:esdk-obs-java}} dependency , used exclusively by the {{hadoop-huaweicloud}} uses {{com.squareup.okio:okio:1.17.2}} which has [CVE-2023-3635|https://nvd.nist.gov/vuln/detail/cve-2023-3635].\r\nUpgrading it will use a newer fixed version of {{okio}}, which will mitigate the vulnerability.\nQ: YanivKunda opened a new pull request, #7707:\nURL: https://github.com/apache/hadoop/pull/7707\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Upgrade esdk-obs-java (in hadoop-huaweicloud) to resolve CVE-2023-3635\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran existing tests.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?", "output": "hadoop-yetus commented on PR #7707:\nURL: https://github.com/apache/hadoop/pull/7707#issuecomment-2907769746\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 12s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  71m 56s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 24s | [/patch-mvninstall-hadoop-cloud-storage-project_hadoop-huaweicloud.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7707/1/artifact/out/patch-mvninstall-hadoop-cloud-storage-project_hadoop-huaweicloud.txt) |  hadoop-huaweicloud in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 24s |  |  hadoop-huaweicloud in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 132m  1s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7707/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7707 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 75c9a3d9b31f 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c3f593b8a1667b38b8c9074b4324a14ba0cd584f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7707/1/testReport/ |\r\n   | Max. process+thread count | 557 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-huaweicloud U: hadoop-cloud-storage-project/hadoop-huaweicloud |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7707/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Create lean docker image\nDescription: Create a new docker image based on the lean tarball.\r\n\r\nhadoop-3.4.2-lean.tar.gz\nQ: adoroszlai opened a new pull request, #8013:\nURL: https://github.com/apache/hadoop/pull/8013\n\n   ### Description of PR\r\n   \r\n   Create a new docker image based on `hadoop-3.4.2-lean.tar.gz`, which omits AWS `bundle-2.29.52.jar`.\r\n   \r\n   This PR should not be merged.  I will push it from CLI as a new branch to publish the image with Docker tag `3.4.2-lean`, rather than overwrite the existing image `3.4.2`.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   [Workflow run](https://github.com/adoroszlai/hadoop/actions/runs/18277349554/job/52032416240) in my fork created the [image](https://github.com/adoroszlai/hadoop/pkgs/container/hadoop/535792302?tag=3.4.2-lean).", "output": "steveloughran commented on PR #8013:\nURL: https://github.com/apache/hadoop/pull/8013#issuecomment-3371636931\n\n   I did check the url resolved, BTW.\r\n   \r\n   Note that in #7980 packaging will change where we move hadoop-aws and hadoop azure to common/lib, with all dependencies except bundle.jar; that'll come iff you do a \"-Paws-sdk\" build. And the other cloud modules will come in if you explicitly ask for them.\r\n   \r\n   Still a WiP; hope to be done ASAP with hadoop 3.4.3 like this. No more \"let's strip the build\" work, instead just choose the build options for a release."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve PrometheusMetricsSink#normalizeName performance\nDescription: This patch is similar from HDDS-13014. We can add a name normalization cache between the Hadoop metrics name and the Prometheus metrics name to prevent expensive regex matchings during the metric normalization conversion.\nQ: ivandika3 opened a new pull request, #7692:\nURL: https://github.com/apache/hadoop/pull/7692\n\n   ### Description of PR\r\n   \r\n   This patch is similar from [HDDS-13014](https://issues.apache.org/jira/browse/HDDS-13014). We can add a name normalization cache between the Hadoop metrics name and the Prometheus metrics name to prevent expensive regex matchings during the metric normalization conversion.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Simple microbenchmark (see https://issues.apache.org/jira/secure/attachment/13076557/TestPrometheusMetricsSinkPerformance.java)\r\n   \r\n   ```\r\n   # With cache\r\n   \r\n   Warming up...\r\n   WARNING: A terminally deprecated method in sun.misc.Unsafe has been called\r\n   WARNING: sun.misc.Unsafe::objectFieldOffset has been called by org.apache.hadoop.thirdparty.com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper (file:/Users/ivan.andika/.m2/repository/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.4.0/hadoop-shaded-guava-1.4.0.jar)\r\n   WARNING: Please consider reporting this to the maintainers of class org.apache.hadoop.thirdparty.com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper\r\n   WARNING: sun.misc.Unsafe::objectFieldOffset will be removed in a future release\r\n   \r\n   Running performance test...\r\n   \r\n   Performance Test Results:\r\n   Total test cases: 5\r\n   Total iterations: 100000\r\n   Total operations: 500000\r\n   Total time: 46.00 ms\r\n   Average time per operation: 0.000 ms\r\n   \r\n   Process finished with exit code 0\r\n   \r\n   # Without cache\r\n   \r\n   Warming up...\r\n   \r\n   Running performance test...\r\n   \r\n   Performance Test Results:\r\n   Total test cases: 5\r\n   Total iterations: 100000\r\n   Total operations: 500000\r\n   Total time: 1181.00 ms\r\n   Average time per operation: 0.002 ms\r\n   ```", "output": "hadoop-yetus commented on PR #7692:\nURL: https://github.com/apache/hadoop/pull/7692#issuecomment-2889108549\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  19m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 39s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 17s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  6s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 39s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 39s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 36s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   1m 13s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/1/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-common in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 56s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 55s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 27s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 220m 37s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7692 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 8de9ba3edbb2 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / de9fa0ca2163b6a1c8cf43888a1f02ec1e555c84 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/1/testReport/ |\r\n   | Max. process+thread count | 1266 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7692/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "[stagedArtifacts|https://github.com/apache/spark/blob/master/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala#L48-L49] buffer is never cleared when artifacts are flushed/on error", "output": "Address memory leak in SparkConnectAddArtifactsHandler"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Rename Operation\nDescription: We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Rename File and Rename directory operations on blob Endpoint\r\n\r\nThis Jira tracks implementing these tests.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Due to JEP411, depending on the Java version, SecurityManager either has to be explicitly enabled, or is completely disabled.", "output": "Skip tests that depend on SecurityManager if the JVM does not support it"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Audit test dependencies in Spark 4.1.0\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Prologue:*\r\n * JIRA ticket: KAFKA-19174\r\n * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027]\r\n\r\n*Scenario:*\r\n * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+\r\n * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x)\r\n\r\n*Action points (what needs to be done):*\r\n * use `com.gradleup.shadow` recent version (9+)\r\n * reorder Gradle tasks so that Gradle command mentioned above can work\r\n\r\n*Definition of done (at the minimum):*\r\n * Gradle command mentioned above works as expected\r\n * also: *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc): [https://kafka.apache.org/quickstart]", "output": "Reorder Gradle tasks (in order to bump Shadow plugin version)"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Clover breaks on double semicolon\nDescription: Building with {{-Pclover}} fails with\r\n{code}\r\n[INFO] Instrumentation error\r\ncom.atlassian.clover.api.CloverException: /home/jenkins/hadoop/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ;\r\n...\r\nFailed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory\r\n{code}\r\n\r\nIt doesn't seem to like a double semicolon in ITestS3APutIfMatchAndIfNoneMatch.java that was added in HADOOP-19256.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Spark Structured Streaming Filesink can not generate open lineage with output details\nDescription: h2. Environment details\r\nh3. OpenLineage version\r\n\r\n{quote}io.openlineage:openlineage-spark_2.13:1.39.0\r\nTechnology and package versions\r\n\r\nPython: 3.13.3\r\nScala: 2.13.16\r\nJava: OpenJDK 64-Bit Server VM, 17.0.16\r\n\r\npip freeze\r\npy4j==0.10.9.9\r\npyspark==4.0.1{quote}\r\n\r\n\r\nFor the openlineage set up, I used the default setting:\r\n\r\n\r\n{quote}$ git clone https://github.com/MarquezProject/marquez.git && cd marquez\r\n\r\n$ ./docker/up.sh{quote}\r\n\r\nh3. Spark Deployment details\r\n\r\nI used native spark on local machine. There is no managed services involved.\r\nProblem details\r\n\r\nh2. Issue details\r\nWhen using Spark structured streaming to write parquet file to file systems,\r\n\r\n*     File sink will only generate openlineage event with streaming processing type with output information as empty.\r\n*     Foreachbatch sink will generate openlineage event with both streaming processing type and batch processing type. The batch processing type will have valid output information.\r\n\r\nThe bug is that File sink in Spark structured streaming does not generate open lineage event with output details.\r\n\r\nMore details about the sample code and sample events are following.\r\nFile sink:\r\nSample code:\r\n{quote}\r\nquery = streaming_df.writeStream \\\r\n    .format('parquet') \\\r\n    .outputMode('append') \\\r\n    .partitionBy('year', 'month', 'day') \\\r\n    .option('checkpointLocation', checkpoint_path) \\\r\n    .option('path', output_path) \\\r\n    .queryName('filesink') \\\r\n    .start()\r\n{quote}\r\nSample event for \"processingTy", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade to netty 4.1.118 due to CVE-2025-24970\nDescription: https://github.com/advisories/GHSA-4g8c-wm8x-jfhw\nQ: pjfanning opened a new pull request, #7413:\nURL: https://github.com/apache/hadoop/pull/7413\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   CVE-2025-24970\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7413:\nURL: https://github.com/apache/hadoop/pull/7413#issuecomment-2672872624\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  17m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  3s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  33m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 29s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 41s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  50m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 42s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  30m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 39s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  18m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 21s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 44s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  51m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  | 394m 40s |  |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 686m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7413/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7413 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux 43c8b5ca518e 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4b03a24469f9bcc0e16dd73ccaeae07fe75c12ba |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7413/1/testReport/ |\r\n   | Max. process+thread count | 3416 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7413/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image to GitHub Container Registry.  Build for both amd64 and arm64.", "output": "Publish multi-arch hadoop-runner image to GitHub"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support Java Modularity\nDescription: This is an umbrella JIRA for supporting Java 9 Modularity.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "A bug in ExecuteGrpcResponseSender causes RPC streams to hang indefinitely when the configured deadline passes. The bug was introduced in [[PR|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253]|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253] during migration from System.currentTimeMillis() to System.nanoTime(), where an integer division error converts sub-millisecond timeout values to 0, triggering Java's wait(0) behavior (infinite wait).\r\nh2. Root Cause\r\nexecutionObserver.responseLock.wait(timeoutNs / NANOS_PER_MILLIS)  // ← BUG\r\n{*}The Problem{*}: When deadlineTimeNs < System.nanoTime() (deadline has passed):\r\n # Math.max(1, negative_value) clamps to 1 nanosecond\r\n\r\n # Math.min(progressInterval_ns, 1) remains 1 nanosecond\r\n\r\n # Integer division: 1 / 1,000,000 = 0 milliseconds\r\n\r\n # wait(0) in Java means *wait indefinitely until notified*\r\n\r\n # No notification arrives (execution already completed), thread hangs forever\r\n\r\nWhile one the loop conditions guards against deadlineTimeNs < System.nanoTime(), it isn’t sufficient as the deadline can elapse while inside the loop (the time is freshly fetched in the latter timeout calculation). The probability of occurence can exacerbated by GC pauses\r\nh2. Conditions Required for Bug to Trigger\r\n\r\nThe bug manifests when *all* of the following conditions are met:\r\n # *Reattachable execution enabled* (CONNECT_EXECUTE_REATTACHABLE_ENABLED = true)\r\n\r\n # *Execution completes prior* to the deadline within the inner loop\r\n\r\n # (all responses sent before deadline)\r\n\r\n # *Deadline passes* within the inner loop\r\n\r\nh2. Proposed fix\r\n\r\nHave timeoutNs always contain a positive value.\r\nexecutionObserver.responseLock.wait(Math.max(1, timeoutNs / NANOS_PER_MILLIS))", "output": "Thread.wait(0) unintentionally called under rare conditions in ExecuteGrpcResponseSender"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently Desc As Json is supported only for v1 table (see [error|https://github.com/apache/spark/blob/3d292dc7b1c5b5ff977c178a88f8ee73deaee88f/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DescribeRelationJsonCommand.scala#L99] thrown for v2 tables). \r\n\r\nAdd support v2 table path (ex: DescribeRelation v2 table [path)|https://github.com/apache/spark/blob/3d292dc7b1c5b5ff977c178a88f8ee73deaee88f/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala#L385]", "output": "DESCRIBE TABLE AS JSON: v2 table"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Limit Arrow batch sizes in SQL_GROUPED_AGG_ARROW_UDF\nDescription: \nQ: Issue resolved by pull request 52605\n[https://github.com/apache/spark/pull/52605]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Parsing of the scope claim does not comply with RFC-8693\nDescription: I notice that the code in Kafka for handling of the scopes claim does not comply with the RFC.\r\n\r\n[https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim |https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim]says:\r\n{quote}The value of the {{scope}} claim is a JSON string containing a space-separated list of scopes associated with the token, in the format described in [Section 3.3|https://www.rfc-editor.org/rfc/rfc6749#section-3.3] of [[RFC6749|https://datatracker.ietf.org/doc/html/rfc6749]]\r\n{quote}\r\n \r\n\r\nHowever the code in Kafka that parses the JWT payload does not permit a space separated list.  It would treat a value like \"email phone address\" as a single scope \"email phone address\" rather than a three separate scopes of \"email\", \"phone\", \"address\".\r\n\r\nThe affected code is here:\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/BrokerJwtValidator.java#L166]\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredJws.java#L343]\r\n\r\nImpact:\r\n\r\nLooking at the production code in Apache Kafka itself, I think the defect currently harmless.  As far as I can tell, there's no production code that makes use of  org.apache.kafka.common.security.oauthbearer.internals.secured.BasicOAuthBearerToken#scope.\r\n\r\nI think there would be a potential for impact for a user writing their own OAuthBearerVal", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SHOW TABLES AS JSON\nDescription: Support new SQL syntax: `Show Table As JSON`", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Addition of column-level Parquet compression preference in Spark\nDescription: h4. *Problem*\r\n\r\nApache Spark currently allows only *global compression configuration* for Parquet files using:\r\n\r\n{{spark.sql.parquet.compression.codec = snappy | gzip | zstd | uncompressed}}\r\n\r\nHowever, many production datasets contain heterogeneous columns — for example:\r\n * text or categorical columns that compress better with {*}ZSTD{*},\r\n\r\n * numeric columns that perform better with {*}SNAPPY{*}.\r\n\r\nToday, Spark applies a single codec to the entire file, preventing users from optimizing storage and I/O performance per column.\r\n\r\n\r\nh4. Proposed Improvement\r\n\r\nIntroduce a new configuration key to define *per-column compression codecs* in a map format:\r\n\r\n{{spark.sql.parquet.column.compression.map = colA:zstd,colB:snappy,colC:gzip}}\r\n\r\n*Behavior:*\r\n * The global codec ({{{}spark.sql.parquet.compression.codec{}}}) remains the default for all columns.\r\n\r\n * Any column listed in {{spark.sql.parquet.column.compression.map}} will use its specified codec.\r\n\r\n * Unspecified columns continu\nQ: Apache Spark community has a policy which manages `Fix Version` and `Target Version` like the following. So, please don't set it when you file a JIRA issue.\r\nhttps://spark.apache.org/contributing.html\r\n{quote}Do not set the following fields:\r\n- Fix Version. This is assigned by committers only when resolved.\r\n- Target Version. This is assigned by committers to indicate a PR has been accepted for possible fix by the target version.{quote}", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "[KIP-1188|https://cwiki.apache.org/confluence/x/2IkvFg] deprecated the Principal connector client override policy. This policy should be removed in 5.0.0. Also the default policy should switch to Allowlist in 5.0.0.", "output": "Follow up for KIP-1188"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: EpochState should override close to avoid throwing IOException\nDescription: All subclasses of EpochState do not throw an IOException when closing, so catching it is unnecessary. We could override close to remove the IOException declaration and streamline the code\r\n\r\n{code:java}\r\n        if (state != null) {\r\n            try {\r\n                state.close();\r\n            } catch (IOException e) {\r\n                throw new UncheckedIOException(\r\n                    \"Failed to transition from \" + state.name() + \" to \" + newState.name(), e);\r\n            }\r\n        }\r\n{code}", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When ASF projects copy 3rd party code into their code bases, they are meant to:\r\n* check the orginal license is Category A - https://www.apache.org/legal/resolved.html\r\n* keep the original source code headers\r\n* add something to their LICENSE that mentions the source file and what license is on it\r\n* if the 3rd party project is Apache licensed but has a NOTICE file, we need to copy its contents to our NOTICE\r\n* these requirements are only negated if the original code is submitted to the ASF project by the code's copyright holder (the individual or company that wrote the original code).\r\n\r\n* Hadoop copy https://github.com/apache/hadoop/blob/c357e435fd691b4c184b82325bef6d7c65e5f32b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LimitInputStream.java#L39\r\n* Original code has a Guava copyright that we probably need to keep https://github.com/google/guava/blob/master/guava/src/com/google/common/io/ByteStreams.java", "output": "acknowledge Guava license on LimitInputStream"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Suppress `StatusRecorder` warning messages\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Clean up TaskManagerTest\nDescription: See https://github.com/apache/kafka/pull/20392#issuecomment-3241457533", "output": "In Progress"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [Gradle 8 --> 9 upgrade] GitHub Actions build issues: JUnit tests parsing fails (for a new/flaky steps)\nDescription: (!) {*}Note{*}: this blocks Gradle upgrade from 8 to 9 (KAFKA-19174) - _Gradle upgrade related stuff are done, but can't be merged due to CI build issues._\r\n\r\nSee here for more details: [https://github.com/apache/kafka/pull/19513#issuecomment-3236158518]\r\n\r\nInterestingly enough: only parsing fails (not tests) and on top of that it happens for flaky/new matrix {*}_only_{*}.\r\n_______________________________________________________________________________________________________________________________\r\n\r\n -{*}(i) Update{*}: after so many testing on Gradle upgrade PR ([https://github.com/apache/kafka/pull/19513]) I can confirm that something strange is going on between GitHub Actions and Develocity instances.-\r\n\r\n-Let me describe just a tip of the issues here; will use 'com.gradle.develocity' plugin version as an example (and please keep in mind that no classes/test are added or changed):-\r\n * Gradle 9/PR branch (with plugin version 3.19, i.e. trunk version): *parsing step fails for JUnit\nQ: Separate related issue is created here: KAFKA-19707", "output": "fixed by KAFKA-19174"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0\nDescription: My team recently updated spark dependency version from 3.5.5 to 4.0.0\r\nThis included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic).\r\n\r\nAfter this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and 2. read from database, write to database.\r\n\r\nI have performed comparison between application versions with spark 3.5.5 and 4.0.0 in cluster mode, local mode and one comparison (with use of synthetic file) using spark-shell only.\r\nIn case of spark-shell I had more or less the same throughput for 3.5.5 and 4.0.0 but in case of our app used in cluster / local mode - both of these scenarios had better throughput with 3.5.5.\r\n\r\nI have observed that with 4.0.0 there are longer delays (when compared with 3.5.5) between log lines\r\n\"Running task x in stage y\"\r\nand\r\n\"Finished task x in stage y\".\r\n\r\nIs this throughput degradation a known issue? Could it be related to this task - [SPARK-48456] [M1] Performance benchmark - ASF JIRA ?\r\n\r\n(I'll also mention that we are using checkpointing (in case it might be important here))", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Optimizing the null check logic of Lists#addAll method", "output": "Optimizing the null check logic of Lists#addAll method"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Make HadoopArchives support human-friendly units about blocksize and partsize.\nDescription: You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: google gs connector registration failing\nDescription: Surfaced during HADOOP-19696 and work with all the cloud connectors on the classpath.\r\n\r\nThere's a missing dependency causing the gcs connector to fail to register *when the first filesystem is instantiated*, because the service registration process loads the gcs connector class, instantiates one and asks for its schema.\r\n\r\nAs well as a sign of a problem, it's better to just add an entry in core-default.xml as this saves all classloading overhead. It's what the other asf bundled ones do.\nQ: Hi [~stevel@apache.org]. Not fully caught up on this one, but there is a {{fs.gs.impl}} entry in core-default.xml now. Was that all we needed?\r\n\r\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml#L4508", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "* Each broker rotates the segment independently.\r\n * When the leadership changes to the next replica, then the highest offset uploaded to the remote storage might already be higher than the baseOffset of the current leader active segment.\r\n * During this case, the {{onlyLocalLogSegmentsSize}} and {{onlyLocalLogSegmentsCount}} returns 0, so the metrics shows negative value (0 - activeSegment.size).", "output": "Fix the negative remote bytesLag and segmentsLag metric"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Include cipher feature for HttpServer2 and SSLFactory\nDescription: Currently, we have a feature to exclude weak ciphers from *HttpServer2* and *SSLFactory* using the *ssl.server.exclude.cipher.list property*. \r\nWith this feature, we can also define an inclusion list of ciphers using the *ssl.server.include.cipher.list property*. \r\nIf the inclusion list is populated, any cipher not present in the list will not be allowed. \r\nIf a cipher is present in both the exclusion and inclusion lists, it will be excluded.\r\nNote that SSLFactory does not support regex-based cipher patterns, unlike HttpServer2.\nQ: hadoop-yetus commented on PR #7629:\nURL: https://github.com/apache/hadoop/pull/7629#issuecomment-2813004052\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 48s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  19m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 25s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 13s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 44s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 20s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  1s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 32s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 58s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7629/1/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 1 new + 87 unchanged - 0 fixed = 88 total (was 87)  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  7s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  13m 10s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   4m  8s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   4m 59s |  |  hadoop-yarn-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 153m 41s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7629/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7629 |\r\n   | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets compile javac javadoc mvninstall shadedclient spotbugs checkstyle |\r\n   | uname | Linux bc4595305705 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 75e187206e997f125bda2b0fecfd87216094616e |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7629/1/testReport/ |\r\n   | Max. process+thread count | 1262 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7629/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "Hean-Chhinling commented on code in PR #7629:\nURL: https://github.com/apache/hadoop/pull/7629#discussion_r2049182917\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java:\n##########\n@@ -608,12 +615,19 @@ private ServerConnector createHttpsChannelConnector(\n           sslContextFactory.setTrustStorePassword(trustStorePassword);\n         }\n       }\n-      if(null != excludeCiphers && !excludeCiphers.isEmpty()) {\n+\n+      if (null != excludeCiphers && !excludeCiphers.isEmpty()) {\n         sslContextFactory.setExcludeCipherSuites(\n             StringUtils.getTrimmedStrings(excludeCiphers));\n         LOG.info(\"Excluded Cipher List:\" + excludeCiphers);\n       }\n \n+      if (null != includeCiphers && !includeCiphers.isEmpty()) {\n\nReview Comment:\n   Same here as well\n\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java:\n##########\n@@ -608,12 +615,19 @@ private ServerConnector createHttpsChannelConnector(\n           sslContextFactory.setTrustStorePassword(trustStorePassword);\n         }\n       }\n-      if(null != excludeCiphers && !excludeCiphers.isEmpty()) {\n+\n+      if (null != excludeCiphers && !excludeCiphers.isEmpty()) {\n\nReview Comment:\n   Thank you, @K0K0V0K for the pull request.\r\n   At first glance, this line seems odd to me. Perhaps, we could set `exlcudeCiphers != null` instead?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The ABFS driver currently lacks support for multiple SAS tokens for the same storage account across different containers.\r\n\r\nWe are now introducing this support.\r\n\r\nTo use fixed SAS token at container level the configuration to be used is:\r\n{quote}fs.azure.sas.fixed.token..\r\n{quote}", "output": "ABFS: [FnsOverBlob] Support Fixed SAS token at container level"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade to Junit 5.13.3\nDescription: JUnit has been upgraded to 5.13.3 so that test suites can\r\nbe parameterized at the class level (again).\r\nThis requires a matching upgrade to Surefire and some\r\ntuning of surefire options to restore existing behavior.\r\n\r\nDependency Changes:\r\n* junit.jupiter and junit.vintage => 5.13.3\r\n* junit.platform => 1.13.3\r\n* Surefire => 3.5.3.\r\n\r\nChanged Surefire Flags: \r\n* trimStackTrace => false\r\n* surefire.failIfNoSpecifiedTests => false\r\n\r\nh2. Build property trimStackTrace\r\n\r\ntrimStackTrace is set to false by default\r\n\r\nThis restores the behavior that any test failure\r\nincludes a deep stack trace rather than the first two elements.\r\nIn this version of surefire it also seems to print the full stack\r\nof JUnit test execution and the java runtime itself.\r\n\r\nThis is only an issue for when tests fail.\r\nIf the short trace is desired, enable it on the command line:\r\n{code}\r\n    mvn test -DtrimStackTrace\r\n{code}\r\n\r\nh2. Build property surefire.failIfNoSpecifiedTests\r\n\r\nsurefire.failIfNoSpecifiedTests i\nQ: ayushtkn commented on PR #7785:\nURL: https://github.com/apache/hadoop/pull/7785#issuecomment-3074040788\n\n   > Surefire rejects me running without a named test\r\n   \r\n   that is a known issue some surefire upgrade maybe did that. We can put that in pom & get rid of this problem\r\n   \r\n   https://github.com/apache/hadoop/pull/5567/files#diff-9c5fb3d1b7e3b0f54bc5c4182965c4fe1f9023d449017cece3005d3f90e8e4d8R126-R128\r\n   \r\n   Looking at the previous build results, I think the Jenkins didn't run any test, we might need to touch something here & there to trigger all the tests", "output": "hadoop-yetus commented on PR #7785:\nURL: https://github.com/apache/hadoop/pull/7785#issuecomment-3075916579\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  85m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 125m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |   3m 27s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 12s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 30s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 132m 55s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7785/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7785 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 200bb3fe4862 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ac5300cfbf822a579168bec04f9029875b1289f2 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7785/3/testReport/ |\r\n   | Max. process+thread count | 549 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7785/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Auto migrate .checkpoint offsets\nDescription: When opening a {{StateStore}} that {{{}managesOffsets{}}}, existing {{.checkpoint}} offsets should be \"migrated\" to the {{StateStore}} using {{{}StateStore#commit{}}}.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: NoSuchElementException (again) in Kafka Streams iterator metrics\nDescription: Since upgrading to Kafka 4.1.0, we again see in our metrics collection:\r\n{code:java}\r\nException thrown from GraphiteReporter#report. Exception was suppressed.\r\njava.util.NoSuchElementException: null\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:1863)\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:393)\r\n\tat org.apache.kafka.streams.internals.metrics.OpenIterators.lambda$add$0(OpenIterators.java:57)\r\n\tat org.apache.kafka.common.metrics.KafkaMetric.metricValue(KafkaMetric.java:81) {code}\r\nI think unfortunately the dynamic registration introduced in KAFKA-19398 ([https://github.com/apache/kafka/pull/20022]) might have reverted the fix from [https://github.com/apache/kafka/pull/18771]\r\n\r\nThere is still a race between removing the metric gauge, and the last iterator being removed from the tracking set.\nQ: Oh dear... :(", "output": "[~stevenschlansker] – Can you test the fix and verify if it does work as expected, and let us know?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We can use the latest 33.4.8-jre version as the current one is quite old.", "output": "Bump guava to  33.4.8-jre due to EOL"}
{"instruction": "Answer the question based on the bug.", "input": "Title: make container build work on macOS Tahoe\nDescription: macOS Tahoe includes a native container daemon and runtime and it is supposed to be near feature-compatible with docker. In principle, it should be possible to run the container build using the container command line on macOS Tahoe.\r\n\r\nIt would be great if we can add this functionality to the build script so folks can build on macOS natively without creating another VM.\nQ: this means you can run linux in it? cool. puts it on a par with windows -though that has the advantage you can just dual boot the machine to linux or just replace windows entirely", "output": "Correct. This is a container daemon/runtime that runs natively on Apple (Silicon), which does pretty much all the things that a Docker runtime would do without involving VMs. Also, I understand you can install this on macOS before Tahoe. Here's one article (among many out there): [https://www.infoq.com/news/2025/06/apple-container-linux/]"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Recently, `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` frequently fails.\r\n\r\n[https://github.com/apache/spark/actions/runs/18872699886/job/53854858890]\r\n\r\n \r\n{code:java}\r\n[info] - Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) have been lost during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-41, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info] \tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)\r\n[info] \tat org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.waitUntilMetadataIsPropagated(KafkaTestUtils.scala:613)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$createTopic$1(KafkaTestUtils.scala:378)\r\n[info]", "output": "Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add PipelineAnalysisContext message to support pipeline analysis during Spark Connect query execution\nDescription: When a user invokes an API like `spark.sql` inside a query function that triggers analysis, we want to include information that allows us to do SDP-specific special handling. Adding a PipelineAnalysisContext will allow us to include information along with related RPCs so that the server knows to do this special handling.\nQ: PR: https://github.com/apache/spark/pull/52685", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add Connect JDBC module\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Model DSV2 Commit Operation Metrics API\nDescription: SPARK-52689 added a DataSourceV2 API that sends operation metrics along with the commit, via a map of string, long.  It would be cleaner to model it as a proper object so that it is more clear what metrics Spark sends, and to handle future cases where metrics may not be long values.\r\n\r\nSuggestion from [~aokolnychyi]", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Upgrade AWS V2 SDK to 2.29.52\nDescription: Upgrade to 2.29.52 -the last version compatible with third party stores until there are fixes in the AWS SDK or workarounds added in the S3A connector\r\n\r\nThis SDK update doesn't need to come with some changes to disable some new features (default integrity protections),\r\nand to apply critical changes related to the SDK\r\n\r\nDefault integrity protection came with 2.30, and is on unless disabled.\r\nhttps://github.com/aws/aws-sdk-java-v2/issues/5801\r\n\r\nAs well as being incompatible with third party stores, it has also affected S3 multiregion Access Points: https://github.com/aws/aws-sdk-java-v2/issues/5878\r\n\r\nThis has broken most interaction with third party stores, hence fixes in Iceberg https://github.com/apache/iceberg/pull/12264 and Trinio https://github.com/trinodb/trino/pull/24954\r\n\r\nThere's also [AWS v2.30 SDK InputStream behavior changes #5859](AWS v2.30 SDK InputStream behavior changes).\r\nIt looks like our code is safer from that, but it did require code review.\r\n\r\nSDK 2.30.19 seems good with this", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add `word-count-preview.yaml` Example\nDescription: \nQ: Issue resolved by pull request 408\n[https://github.com/apache/spark-kubernetes-operator/pull/408]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Update GitHub Actions workflow to publish the apache/hadoop Docker image to DockerHub for multiple arch's: amd64 and arm64.", "output": "Publish Hadoop Docker Image to DockerHub for amd64 and arm64 "}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: StreamsRebalanceListener is not triggered on Consumer.close()\nDescription: In the consumer, we invoke the consumer rebalance {{onPartitionRevoked}} or {{onPartitionLost}} callbacks, when the consumer closes. The point is that the application may want to commit, or wipe the state if we are closing unsuccessfully.\r\n\r\nIn the {{{}StreamsRebalanceListener{}}}, we did not implement this behavior, which means when closing the consumer we may lose some progress, and in the worst case also miss that we have to wipe our local state state since we got fenced.\r\n\r\nWe should implement something like the {{ConsumerRebalanceListenerInvoker}} and invoke it in {{{}Consumer.close{}}}.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title:  Change log.segment.bytes configuration type from int to long to support segments larger than 2GB\nDescription: h2. Description\r\nh3. Summary\r\n\r\nChange the data type of *{{log.segment.bytes}}* configuration from *{{int}}* to *{{long}}* to allow segment sizes beyond the current 2GB limit imposed by the integer maximum value.\r\nh3. Current Limitation\r\n\r\nThe {{*log.segment.bytes*}} configuration currently uses an *{{int}}* data type, which limits the maximum segment size to ~2GB (2,147,483,647 bytes). This constraint becomes problematic for modern high-capacity storage deployments.\r\nh3. Background: Kafka Log Segment Structure\r\n\r\nEach Kafka topic partition consists of multiple log segments stored as separate files on disk. For each segment, Kafka maintains three core files:\r\n * {*}{{.log}} files{*}: Contain the actual message data\r\n * {*}{{.index}} files{*}: Store mappings between message offsets and their physical positions within the log file, allowing Kafka to quickly locate messages by their offset without scanning the entire log file\r\n * {*}{{.timeindex}} files{*}: Store mappings between message \nQ: [~proggga] you might want to create a KIP and add details there. Then, you can trigger disc on mailing list. Guidelines here: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm.", "output": "Introduce APPROX_PERCENTILE_ACCUMULATE"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade SBT to 1.11.7\nDescription: We last upgraded SBT two years ago. Let's upgrade SBT to the latest version.\nQ: Issue resolved by pull request 52653\n[https://github.com/apache/spark/pull/52653]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ServiceStateException thrown due to unexpected state\nDescription: In `ServiceStateModel`, `enterState` can set `state` to a state that is different from `expectedState`, which causes an exception in `ensureCurrentState`.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview3-java21`\nDescription: \nQ: Issue resolved by pull request 259\n[https://github.com/apache/spark-connect-swift/pull/259]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Kafka Streams supports \"merge.repartition.topic\" optimization. This optimization tries to push repartition topics further upstream, to be able to merge multiple repartition topics from different downstream branches into a single repartition topic.\r\n\r\nIn the current implementation, the optimization stops to push repartition topics upstream, if the parent node is a value-changing operation. We cannot push-up further, because the data type might change, and thus we won't have the correct serde at hand.\r\n\r\nHowever, we could actually also inherit the correct serde from further upstream. Thus, we could actually push a repartition-step upstream (and at the same time, switch the serdes), if we can find the right serde before the value-changing operations further upstream.", "output": "Improve repartition optimization"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add integration test similar to `ShareGroupHeartbeatRequestTest` and `ConsumerGroupHeartbeatRequestTest` for `StreamsGroupHeartbeat`", "output": "Add RPC-level integration tests for StreamsGroupHeartbeat"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "There is a bug in the broker logic of splitting bytes in {{PartitionMaxBytesStrategy.java}} due to which we are setting {{partitionMaxBytes}} as 0 in case requestMaxBytes is lesser than \r\nacquiredPartitionsSize", "output": "PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When submitting jobs to standalone cluster using restful api, we get errors like:\r\n\r\n```\r\n25/10/30 16:02:59 INFO DriverRunner: Killing driver process!\r\n25/10/30 16:02:59 INFO CommandUtils: Redirection to /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stdout closed: Stream closed\r\n25/10/30 16:02:59 WARN Worker: Driver driver-20251030160259-0020 failed with unrecoverable exception: java.nio.file.NoSuchFileException: /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stderr\r\n\r\n```\r\n\r\nWe need to fix the usage of `Files.writeString` in DriverRunner", "output": "Fix `initialize` to add `CREATE` option additionally in `DriverRunner`"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade hadoop-runner to Ubuntu 24.04\nDescription: Latest {{hadoop-runner}} images are based on Ubuntu 22.04.  Upgrade to 24.04.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: benchmark_test is throwing `NullPointerException`\nDescription: When benchmark_test is run using docker, following error is thrown in both native and JVM modes. ChatGPT suggested to use a JDK image with latest patch release({{{}17.0.12+7{}}} or {{21.0.4+7)}} and it worked. Maybe we might want to upgrade our JDK image too\r\n{code:java}\r\n[INFO:2025-09-03 09:41:49,007]: RunnerClient: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput.acks=-1.topic=topic-replication-factor-three.metadata_quorum=ISOLATED_KRAFT: FAIL: RemoteCommandError({'ssh_config': {'host': 'ducker02', 'hostname': 'ducker02', 'user': 'ducker', 'port': 22, 'password': '', 'identityfile': '/home/ducker/.ssh/id_rsa', 'connecttimeout': None}, 'hostname': 'ducker02', 'ssh_hostname': 'ducker02', 'user': 'ducker', 'externally_routable_ip': 'ducker02', '_logger': , 'os': 'linux', '_ssh_client': , '_sftp_client': , '_custom_ssh_exception_checks': None}, '/opt/kafka-dev/bin/kafka-storage.sh format --ignore-formatted --config /mnt/kafka/kafka.properties --cluster-id I2eXt9rvSnyhct8BYmW6-w --feature transaction.version=0', 1, b'java.lang.NullPointerException: Cannot invoke \"jdk.internal.platform.CgroupInfo.getMountPoint()\" because \"anyController\" is null\\n\\tat java.base/jdk.internal.platform.cgroupv3.CgroupV2Subsystem.getInstance(CgroupV2Subsystem.java:81)\\n\\tat java.base/jdk.internal.platform.CgroupSubsystemFactory.create(CgroupSubsystemFactory.java:113)\\n\\tat java.base/jdk.internal.platform.CgroupMetrics.getInstance(CgroupMetrics.java:167)\\n\\tat java.base/jdk.i", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Encryption tests fail for public buckets \nDescription: When running with SSE-C (and probably other encryption methods?),  reading from a public bucket such as `noaa-cors`, tests will fail as objects in the public bucket are no encrypted. Any test reading pre-existing objects should be skipped when setting encryption to anything other than SSE-S3.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Call kill executor\r\n\r\n!image-2025-10-30-17-22-25-127.png|width=1255,height=241!\r\n\r\nContainer 18 didn't kill it\r\n \r\n!image-2025-10-30-17-24-03-632.png|width=1087,height=183!\r\n\r\n \r\n\r\nPending container causing  task can't be scheduled .", "output": "When TaskSchedulerImpl searches for an executor to schedule an unschedulable task, it should exclude executors that are pending to remove"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Log warning message on every set/get of a deprecated configuration property\nDescription: Currently, the warning message is logged once (or at most twice after HADOOP-8865) when we first use (set/get) a deprecated configuration key and most of the time this happens early on during system startup. \r\n\r\nUsers tend to set/get properties on their job scripts/applications  that keep running on a hourly/daily/etc basis. When a problem comes up the user will package the latest logs and send them to us (developers/support) for further analysis and troubleshooting. However, it's very likely that these logs will not contain information about the deprecated usages which might be crucial for advancing the investigation.\r\n\r\nOn the other end, a warning that appears just once is not that worrisome so even if the users/customers/developers see it, they can easily ignore it, and move on, thinking that there is no action needed on their end.\r\n\r\nThe above scenarios are based on applications such as the Hivemetastore, HiveServer2, which use the Hadoop Configuration, and usually run for weeks/months without a restart.\r\n\r\nI propose to change the existing logic to always log a message when a deprecated configuration key is in use. This will minimize the risk of losing important deprecation logs and will also simplify the implementation.\r\n\r\nMoreover, since there is a dedicated logger (HADOOP-9487) for these warning messages applications/users can suppress/limit the log content by changing their logging configuration. It may not be as precise as logging each deprecation once but it can sti", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FNSOverBlob] Add Distinct String In User Agent to Get Telemetry for FNS-Blob\nDescription: Add a unique identifier in FNS-Blob user agent to get their usage through telemetry\nQ: manika137 opened a new pull request, #7713:\nURL: https://github.com/apache/hadoop/pull/7713\n\n   ## Description of PR\r\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19575\r\n   Adding a unique identifier in the user agent for FNS-Blob case for telemetry purposes.\r\n   \r\n   ## How was this patch tested?\r\n   Test suite was run- the results of which are added in the comments below", "output": "anmolanmol1234 commented on code in PR #7713:\nURL: https://github.com/apache/hadoop/pull/7713#discussion_r2107198302\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsClient.java:\n##########\n@@ -355,6 +356,25 @@ public void verifyUserAgentClusterType() throws Exception {\n       .contains(DEFAULT_VALUE_UNKNOWN);\n   }\n \n+  @Test\n+  // Test to verify that the user agent string for FNS-Blob accounts\n+  public void verifyUserAgentForFNSBlob() throws Exception {\n\nReview Comment:\n   add similar test for dfs and validate that no extra string is added."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Spark 4.0.1 still throws \"String length (20054016) exceeds the maximum length (20000000)\" and \"from_json\" fails on a very large JSON with a jackson_core parse error\nDescription: Based on JIRA *SPARK-49872* and the implementation in [{{JsonProtocol.scala}}|https://github.com/apache/spark/blob/29434ea766b0fc3c3bf6eaadb43a8f931133649e/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L71], the relevant limit was removed in {*}4.0.1{*}. However, in our environment we can still reliably reproduce:\r\n # When generating/processing a very large string:\r\n\r\n{code:java}\r\nCaused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String value length (20040525) exceeds the maximum allowed (20000000, from `StreamReadConstraints.getMaxStringLength()`){code}\r\n # When using {{from_json}} on a _valid_ very large single-line JSON (no missing comma), Jackson throws at around column {*}20,271,838{*}:\r\n\r\n{code:java}\r\nCaused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('1' (code 49)): was expecting comma to separate Object entries\r\n at [Source: UNKNOWN; line: 1, column: 20271838]{code}\r\nI'm sure this is not a formatting issue. If I truncate the JSON to below column {*}20,271,838{*}, it parses successfully.\r\nHere is my parsing code:\r\n\r\n{code:java}\r\nraw_df.withColumn(\"parsed_item\", f.from_json(f.col(\"item\"), my_schema){code}", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This Jira is to track the migration of Javax to Jakarta in Hadoop and identify all the transitive dependencies which need to be migrated as well. Currently, I see the following major upgrades required:\r\n # Jersey: 2.46 to 3.x\r\n # Jetty: 9.x to 12.x", "output": "Migrate from Java to Jakarta - Upgrade to Jersey 3.x, Jetty 12.x"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Clients | Fix order of arguments to assertEquals()\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add a config entry to make IPC.Client checkAsyncCall off by default\nDescription: Add a config entry to make IPC.Client checkAsyncCall off by default", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In https://github.com/apache/kafka/commit/e1b7699975e787a78f5e9bca540b4aa19f0d419d we bumped the versions of many Github Actions but we did not bump the following:\r\n- gradle/actions/setup-gradle\r\n- aquasecurity/trivy-action\r\n- docker/setup-qemu-action\r\n- docker/setup-buildx-action\r\n- docker/login-action", "output": "Consider bumping 3rd party Github Actions"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add utility functions to detect JVM GCs\nDescription: Both G1GC and ZGC needs extra object header (usually 16 bytes in 64bit system) when allocating region or ZPage for java long array, so the really bytes allocated is pageSize+16.\r\n\r\nSo we need consider the object header when allocating spark pages.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use Java `(Map|Set).of` instead of `Collections.(empty|singleton)(Set|Map)`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: volcano tos: disable shading when -DskipShade is set on a build\nDescription: hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M artifact, with unknown content inside\r\n{code}\r\n 92K    share/hadoop/common/lib/hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n912K    share/hadoop/common/lib/hadoop-aws-3.5.0-20250916.124028-686.jar\r\n808K    share/hadoop/common/lib/hadoop-azure-3.5.0-20250916.124028-685.jar\r\n 36K    share/hadoop/common/lib/hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n 72K    share/hadoop/common/lib/hadoop-cos-3.5.0-20250916.124028-683.jar\r\n136K    share/hadoop/common/lib/hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n140K    share/hadoop/common/lib/hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n3.8M    share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar\r\n{code}\r\n\r\nOne thing it includes is yet-another mozilla/public-suffix-list.txt. These are a recurrent PITA and I don't want\r\nto find them surfacing again.\r\n{code}\r\n\r\n15. Required Resources\r\n======================\r\n\r\nresource: mozilla/public-suffix-list.txt\r\n       jar:file:/Users/stevel/Projects/Releases/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar!/mozilla/public-suffix-list.txt\r\n{code}\r\n\r\nPlan\r\n* Move the shade stage behind a profile; off for ASF releases. Exclude mozilla/public-suffix-list.txt  \r\n* Explicitly declare and manage httpclient5 dependency\r\n* hadoop-cloud-storage pom to include hadoop-tos but not dependencies in build, unless asked.\r\n* LICENSE-binary to declare optional redist of ve-tos-java-sdk-hadoop and its lice", "output": "In Progress"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK 17] Implementation of JAXB-API has not been found on module path or classpath\nDescription: When i try to run the *org.apache.hadoop.yarn.webapp.TestWebApp* tests over JDK17 i am facing the following errors:\r\n\r\n{noformat}\r\nCaused by: A MultiException has 2 exceptions.  They are:\r\n1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver\r\n{noformat}\r\n\r\nRepro steps:\r\n- run: ./start-build-env.sh ubuntu_24\r\n- in docker run: mvn clean install -Pjdk17+ -T 32 -DskipTests \r\n- in hadoop-yarn-common modul run: mvn test -Dtest=org.apache.hadoop.yarn.webapp.TestWebApp\r\n\r\nI found a similar error here:\r\nhttps://issues.apache.org/jira/browse/HDDS-5068\r\n\r\nBased on my understanding the problem is the JDK11+ environments does not have the JAXB runtime, so we have to explicit provide them. Maybe this is just a temporal solution, till the whole Jakarta upgrade can be done.\r\n\r\n{panel:title=Full error log}\r\n{code}\r\n[ER\nQ: Thank you. I was able to repro the problem\r\n\r\nDo you plan to provide a patch [~bkosztolnik] ?", "output": "K0K0V0K opened a new pull request, #7928:\nURL: https://github.com/apache/hadoop/pull/7928\n\n   https://issues.apache.org/jira/browse/HADOOP-19674\r\n   \r\n   ### Description of PR\r\n   \r\n   When we try to create an instance of `JettisonJaxbContext` a `JAXBContext.newInstance` call will happen. With my understanding JAXB is removed since JDK11 ([source](https://docs.oracle.com/en/java/javase/24/migrate/removed-tools-and-components.html#GUID-11F78105-D735-430D-92DD-6C37958FCBC3)) and if we would like to access JAXB, then we should explicit include them to the jars. So a new **jaxb-runtime** dependency is included in the code base. I used test scope where ever it was possible but for example in the `MRClientService` the `JAXBContextResolver` is in use, so i left the test scope there.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Unit tests\r\n   - I created a JDK8 build and run the `org.apache.hadoop.yarn.webapp.TestWebApp` test\r\n   - I created a JDK17 build and run the` org.apache.hadoop.yarn.webapp.TestWebApp` test\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip tests in Hadoop common that depend on SecurityManager if the JVM does not support it\nDescription: TestExternalCall fails when SecurityManager cannot be set, we need to skip it on thos JVMs.\r\n\r\nTestGridmixSubmission has already been rewritten to use ExitUtil, we just need to remove the leftover SecurityManager calls.\nQ: hadoop-yetus commented on PR #7567:\nURL: https://github.com/apache/hadoop/pull/7567#issuecomment-2768959741\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 27s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  22m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 52s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 47s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 21s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 50s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 53s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  25m 22s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  14m 28s |  |  hadoop-gridmix in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 120m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7567/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7567 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 79aac4ca5778 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 94c543b40111d65e1c531cb1b7f4e8e81aaecfb4 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7567/1/testReport/ |\r\n   | Max. process+thread count | 1043 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-distcp hadoop-tools/hadoop-gridmix U: hadoop-tools |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7567/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "Hexiaoqiao commented on code in PR #7567:\nURL: https://github.com/apache/hadoop/pull/7567#discussion_r2031242280\n\n\n##########\nhadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestExternalCall.java:\n##########\n@@ -66,7 +67,11 @@ private static Configuration getConf() {\n   public void setup() {\n \n     securityManager = System.getSecurityManager();\n\nReview Comment:\n   `System.getSecurityManager()` will also be one Deprecate APIs based on JEP-411, Would you mind to process it together. Thanks."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Native profile fails to build on SLES 15\nDescription: Hadoop build fails to find pthreads on SLES 15 builds while linking rpc. It looks like it checks for SunRPC library via rpc/rpc.h, but instead finds tirpc and sets it up incorrectly.\r\n\r\n{code}\r\nPerforming C SOURCE FILE Test CMAKE_HAVE_LIBC_PTHREAD failed with the following output:\r\nChange Dir: /grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/gmake cmTC_4ddc0/fast && /usr/bin/gmake  -f CMakeFiles/cmTC_4ddc0.dir/build.make CMakeFiles/cmTC_4ddc0.dir/build\r\ngmake[1]: Entering directory '/grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeTmp'\r\nBuilding C object CMakeFiles/cmTC_4ddc0.dir/src.c.o\r\n/usr/bin/gcc-8 -DCMAKE_HAVE_LIBC_PTHREAD   -o CMakeFiles/cmTC_4ddc0.dir/src.c.o -c /grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeT\nQ: hadoop-yetus commented on PR #7789:\nURL: https://github.com/apache/hadoop/pull/7789#issuecomment-3049613739\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  34m 51s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | -1 :x: |  shadedclient  |  37m 50s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |   1m 24s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 23s |  |  hadoop-pipes in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 31s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  43m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7789 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets |\r\n   | uname | Linux f4434e8fb98a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bdd0879bbd4e216137aaa8afda7b182766588173 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_412-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/1/testReport/ |\r\n   | Max. process+thread count | 98 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-pipes U: hadoop-tools/hadoop-pipes |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/1/console |\r\n   | versions | git=2.9.5 maven=3.9.10 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7789:\nURL: https://github.com/apache/hadoop/pull/7789#issuecomment-3049733649\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  34m 11s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/2/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | -1 :x: |  shadedclient  |  37m  9s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |   1m 24s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 23s |  |  hadoop-pipes in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 31s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  42m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7789 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets |\r\n   | uname | Linux 95e5ae1d0505 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / aa20624f0b5df333575ee2ed3cd14e52da3dc7f1 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_412-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/2/testReport/ |\r\n   | Max. process+thread count | 65 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-pipes U: hadoop-tools/hadoop-pipes |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/2/console |\r\n   | versions | git=2.9.5 maven=3.9.10 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix ByteBuf leaks in TestShuffleChannelHandler\nDescription: A lot of ByteBuf leaks are reported when running with JDK23.\r\n\r\nI have not idea why those are not detected with JDK8, 11 or 17. Maybe something about the logging setup ?\nQ: stoty opened a new pull request, #7500:\nURL: https://github.com/apache/hadoop/pull/7500\n\n   ### Description of PR\r\n   \r\n   Fixes ByteBuf leaks in TestShuffleChannelHandler. For some reason the ByteBuf leaks are not detected on older (8,11,17) JVMs, though in retrospect it's quite obvious that the tests don't release ByteBufs as they should.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   The patch was tested on my JDK23 branch. \r\n   The test probably cannot be run with JDK23 on trunk.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7500:\nURL: https://github.com/apache/hadoop/pull/7500#issuecomment-2719316865\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  8s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-shuffle.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7500/1/artifact/out/results-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-shuffle.txt) |  hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 59s |  |  hadoop-mapreduce-client-shuffle in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 114m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7500/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7500 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2b91e841e013 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e252de3bcf40124251e60f4dc777752311fab9b0 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7500/1/testReport/ |\r\n   | Max. process+thread count | 544 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7500/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Thread.wait(0) unintentionally called under rare conditions in ExecuteGrpcResponseSender\nDescription: A bug in ExecuteGrpcResponseSender causes RPC streams to hang indefinitely when the configured deadline passes. The bug was introduced in [[PR|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253]|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253] during migration from System.currentTimeMillis() to System.nanoTime(), where an integer division error converts sub-millisecond timeout values to 0, triggering Java's wait(0) behavior (infinite wait).\r\nh2. Root Cause\r\nexecutionObserver.responseLock.wait(timeoutNs / NANOS_PER_MILLIS)  // ← BUG\r\n{*}The Problem{*}: When deadlineTimeNs < System.nanoTime() (deadline has passed):\r\n # Math.max(1, negative_value) clamps to 1 nanosecond\r\n\r\n # Math.min(progressInterval_ns, 1) remains 1 nanosecond\r\n\r\n # Integer division: 1 / 1,000,000 = 0 milliseconds\r\n\r\n # wait(0) in Java means *wait indefinitely until\nQ: Issue resolved by pull request 52609\n[https://github.com/apache/spark/pull/52609]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Updating Documentations of Hadoop Drivers for Azure\nDescription: Fixing some typos, details and adding links for better readability in the documentation files for ABFS driver.\nQ: manika137 opened a new pull request, #7540:\nURL: https://github.com/apache/hadoop/pull/7540\n\n   ### Description of PR\r\n   Fixing some typos, details and adding links for better readability in the .md files for ABFS driver.\r\n   \r\n   ### How was this patch tested?\r\n   No production code change, no testing needed.", "output": "hadoop-yetus commented on PR #7540:\nURL: https://github.com/apache/hadoop/pull/7540#issuecomment-2753466292\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  73m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7540/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 116m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7540/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7540 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint |\r\n   | uname | Linux 19618c012176 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b0bf7a91e6078fbc1a9b2a5c3d2d5d61286d260b |\r\n   | Max. process+thread count | 553 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7540/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade to hadoop-thirdparty 1.4.0\nDescription: HADOOP-19483 releases thirdparty 1.4.0; upgrade hadoop to it", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration\nDescription: Establish the foundational build infrastructure for RISC-V CRC32 hardware acceleration support while maintaining full backward compatibility with existing software implementations.\nQ: PeterPtroc opened a new pull request, #7903:\nURL: https://github.com/apache/hadoop/pull/7903\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Add a RISC-V-specific compilation unit: org/apache/hadoop/util/bulk_crc32_riscv.c.\r\n   \r\n   - Contains a no-op constructor reserved for future HW capability detection and dispatch.\r\n   - Keeps runtime behavior unchanged (falls back to the generic software path in bulk_crc32.c).\r\n   - Wire CMake to select bulk_crc32_riscv.c on riscv32/riscv64, mirroring other platforms.\r\n   \r\n   This PR establishes the foundational build infrastructure for future RISC-V Zbc (CLMUL) CRC32/CRC32C acceleration without changing current behavior. Follow-ups (HADOOP-19655) will introduce HW-accelerated implementations and runtime dispatch.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - Ensured native build for hadoop-common compiles cleanly with RISC-V selection.\r\n   - Verified by test_bulk_crc32.\r\n   - No new tests added, as this patch is scaffolding-only without any behavior change.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3225752648\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  14m 21s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 56s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  94m 44s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 54s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 54s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 198m 14s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7903 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux af19edaaec5b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1b159b6f8fadc7e4229a3aa0eeaa184b6d2f25cc |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/testReport/ |\r\n   | Max. process+thread count | 1279 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Deflake StateStoreSuite `SPARK-40492: maintenance before unload`\nDescription: `SPARK-40492: maintenance before unload` in StateStoreSuite is flaky due to timing issues. Fix this by adding hooks to pause maintenance.\nQ: Issue resolved by pull request 52783\n[https://github.com/apache/spark/pull/52783]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "On blob endpoint, there are a couple of handling that is needed to be done on client side.\r\nThis involves:\r\n # Parsing of xml response and converting them to VersionedFileStatus list\r\n # Removing duplicate entries for non-empty explicit directories coming due to presence of the marker files\r\n # Trigerring Rename recovery on the previously failed rename indicated by the presence of pending json file.\r\n\r\nCurrently all three are done in a separate iteration over whole list. This is to pbring all those things to a common place so that single iteration over list reposne can handle all three.", "output": "ABFS: [FnsOverBlob] Listing Optimizations to avoid multiple iteration over list response."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add `Example` section in `operators.md`\nDescription: \nQ: Issue resolved by pull request 380\n[https://github.com/apache/spark-kubernetes-operator/pull/380]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [Java 25] few tests are failling (class: clients/org.apache.kafka.common.network.SslTransportLayerTest)\nDescription: {panel:title=Notes:|borderStyle=dashed|borderColor=#cccccc|titleBGColor=#f7d6c1|bgColor=#ffffce}\r\n * (!) this ticket blocks KAFKA-19664 *_Support building with Java 25 (LTS release)_*\r\n * (on) looks similar to: KAFKA-15117 *_SslTransportLayerTest.testValidEndpointIdentificationCN fails with Java 20 & 21_*{panel}\r\n*Reproducer:*\r\n * use this branch: [https://github.com/dejan2609/kafka/tree/KAFKA-19664] (i.e. this PR: [https://github.com/apache/kafka/pull/20561])\r\n * execute: *./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -i* and change Java versions\r\n * test results:\r\n ** {*}Java 17{*}: (/)\r\n ** J{*}ava 24{*}: (/)\r\n ** {*}Java 25{*}: 159 tests completed, *8 failed* (x)\r\n\r\n*Test results on Github CI:*\r\n!Screenshot from 2025-10-07 19-59-32.png!\r\n\r\n*Test results locally:*\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ git log -3 --oneline \r\na37616a1eb (HEAD -> KAFKA-19664, origin/KAFKA-19664) KAFKA-19664 Support building with Java 25 (LTS release)\r\nc6bbbbe24d KAFKA-19174 Gradle version upgrade 8 -->> 9 (#19513)\r\nf5a87b3703 KAFKA-19748: Add a note in docs about memory leak in Kafka Streams 4.1.0  (#20639)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version \r\nopenjdk version \"17.0.16\" 2025-07-15\r\nOpenJDK Runtime Environment Temurin-17.0.16+8 (build 17.0.16+8)\r\nOpenJDK 64-Bit Server VM Temurin-17.0.16+8 (build 17.0.16+8, mixed mode, sharing)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.comm", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: SecretManager configuration at runtime\nDescription: In case of TEZ *DAGAppMaster* the Hadoop *SecretManager* code can not read yarn config xml file, therefore the SELECTED_ALGORITHM and SELECTED_LENGTH variables in SecretManager can not be set at runtime.\r\nThis can results with the following exception in FIPS environment:\r\n\r\n{code:java}\r\njava.security.InvalidParameterException: Key size for HMAC must be at least 112 bits in approved mode: SHA-1/HMAC\r\n\tat com.safelogic.cryptocomply.fips.core/com.safelogic.cryptocomply.jcajce.provider.BaseKeyGenerator.engineInit(Unknown Source)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:540)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:517)\r\n\tat org.apache.hadoop.security.token.SecretManager.(SecretManager.java:157)\r\n\tat org.apache.hadoop.yarn.security.client.BaseClientToAMTokenSecretManager.(BaseClientToAMTokenSecretManager.java:38)\r\n\tat org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager.(ClientToAMTokenSecretManager.java:46)\r\n\tat org.apache.tez.co\nQ: let me add the motivation from Tez side as a context, so as [~bkosztolnik] mentioned we faced a problem after YARN-11738,\r\nbecause Tez is supposed to work by runtime payloads (containing all the hadoop config), but in the hadoop layer, the options are initialized in the static initializer, so tez cannot help with that exception, because new Configuration() depends on the core-site.xml config which is *maybe* on the classpath, otherwise, everything turns to default\r\nfirst I tried to make the TezClientToAMTokenSecretManager extend a custom Tez implementation of SecretManager, but it led to compilation issues, as Hadoop layers expect a Hadoop SecretManager (see RPC), so eventually we cannot pass a Tez SecretManager that doesn’t inherit Hadoop’s SecretManager, but as long as we extend the Hadoop one, the static initializer and then the field initializer of keyGen keyGen.init(SELECTED_LENGTH) will kick in immediately\r\n\r\nso this cannot be fixed from Tez, we have 2 options:\r\n1. rework YARN-11738 (as this problem was also implied in this comment upstream)\r\n2. make the core-site.xml localized to tez containers to have it picked up <- this is against tez design, so I would prefer 1)\r\n\r\noptimal way would be completely eliminate static fields from SecretManager, but I'm afraid they are there for a reason, so basically, anything could work for us which makes Tez able to intercept, and configure the SecretManager from a Configuration object, which is different than the default one (which is instantiated by new Configuration())\r\n\r\nso this cannot be fixed from Tez, we have 2 options:\r\n1. rework YARN-11738 (as this problem was also implied in this comment upstream)\r\n2. make the core-site.xml localized to tez containers to have it picked up\r\n\r\nI’m a bit against 2), because it’s also a design decision to make file config resources available to tez containers instead of payload, so I would definitely be in favor of 1), here is where I need the opinion of Hadoop folks", "output": "abstractdog commented on code in PR #7827:\nURL: https://github.com/apache/hadoop/pull/7827#discussion_r2228219825\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManagerConfig.java:\n##########\n@@ -0,0 +1,123 @@\n+\n+package org.apache.hadoop.security.token;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.Mac;\n+import javax.crypto.SecretKey;\n+import java.security.NoSuchAlgorithmException;\n+\n+/**\n+ * Provides configuration and utility methods for managing cryptographic key generation\n+ * and message authentication code (MAC) generation using specified algorithms and key lengths.\n+ * \n+ * This class supports static access to the selected cryptographic algorithm and key length,\n+ * and provides methods to create configured {@link javax.crypto.KeyGenerator} and {@link javax.crypto.Mac} instances.\n+ * The configuration is initialized statically from a provided {@link Configuration} object.\n+ * \n+ * The {@link SecretManager} has some static method, so static configuration is required\n+ */\n+public class SecretManagerConfig {\n+    private static final Logger LOG = LoggerFactory.getLogger(SecretManagerConfig.class);\n+    private static String SELECTED_ALGORITHM;\n+    private static int SELECTED_LENGTH;\n+\n+    static {\n+        update(new Configuration());\n+    }\n+\n+    /**\n+     * Updates the selected cryptographic algorithm and key length using the provided\n+     * Hadoop {@link Configuration}. This method reads the values for\n+     * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY} and\n+     * {@code HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY}, or uses default values if not set.\n+     *\n+     * @param conf the configuration object containing cryptographic settings\n+     */\n+    public static void update(Configuration conf) {\n+        SELECTED_ALGORITHM = conf.get(\n+                CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_KEY,\n+                CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_GENERATOR_ALGORITHM_DEFAULT);\n+        LOG.debug(\"Selected hash algorithm: {}\", SELECTED_ALGORITHM);\n+        SELECTED_LENGTH = conf.getInt(\n+                CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_KEY,\n+                CommonConfigurationKeysPublic.HADOOP_SECURITY_SECRET_MANAGER_KEY_LENGTH_DEFAULT);\n+        LOG.debug(\"Selected hash key length: {}\", SELECTED_LENGTH);\n+    }\n+\n+    /**\n+     * Returns the currently selected cryptographic algorithm.\n+     *\n+     * @return the name of the selected algorithm\n+     */\n+    public static String getSelectedAlgorithm() {\n+        return SELECTED_ALGORITHM;\n+    }\n+\n+    /**\n+     * Returns the currently selected key length in bits.\n+     *\n+     * @return the selected key length\n+     */\n+    public static int getSelectedLength() {\n+        return SELECTED_LENGTH;\n+    }\n+\n+    /**\n+     * Sets the cryptographic algorithm to use.\n+     *\n+     * @param algorithm the algorithm name (e.g., \"HmacSHA256\", \"AES\")\n+     */\n+    public static void setSelectedAlgorithm(String algorithm) {\n+        SELECTED_ALGORITHM = algorithm;\n+        LOG.debug(\"Selected hash algorithm set to {}\", algorithm);\n+    }\n+\n+    /**\n+     * Sets the cryptographic key length to use (in bits).\n+     *\n+     * @param length the key length\n+     */\n+    public static void setSelectedLength(int length) {\n+        SELECTED_LENGTH = length;\n+        LOG.debug(\"Selected hash key length set to{}\", length);\n\nReview Comment:\n   nit: 1 space before the length"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Since upgrading to Kafka 4.1.0, we again see in our metrics collection:\r\n{code:java}\r\nException thrown from GraphiteReporter#report. Exception was suppressed.\r\njava.util.NoSuchElementException: null\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:1863)\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:393)\r\n\tat org.apache.kafka.streams.internals.metrics.OpenIterators.lambda$add$0(OpenIterators.java:57)\r\n\tat org.apache.kafka.common.metrics.KafkaMetric.metricValue(KafkaMetric.java:81) {code}\r\nI think unfortunately the dynamic registration introduced in KAFKA-19398 ([https://github.com/apache/kafka/pull/20022]) might have reverted the fix from [https://github.com/apache/kafka/pull/18771]\r\n\r\nThere is still a race between removing the metric gauge, and the last iterator being removed from the tracking set.", "output": "NoSuchElementException (again) in Kafka Streams iterator metrics"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: S3ABlockOutputStream to never log/reject hflush(): calls\nDescription: Parquet's GH-3204 patch uses hflush() just before close()\r\n\r\nthis is needless and hurts write performance on hdfs.\r\nFor s3A it will trigger a warning long (Syncable is not supported) or an actual failure if\r\nfs.s3a.downgrade.syncable.exceptions is false\r\n\r\nproposed: hflush to log at debug -only log/reject on hsync, which is the real place where semantics cannot be met", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Custom Gradle tasks (`unitTest` and `integrationTest`) are not executing tests in Gradle 9\nDescription: Prologue: KAFKA-19174\r\n\r\n\r\n\r\n\r\nSee here for description: https://github.com/apache/kafka/pull/19513#issuecomment-3388760324", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [ABFS] Fix logging in FSDataInputStream buffersize as that is not used and confusing the customer\nDescription: Fix debug logs here. \r\n\r\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java#L350", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consistency of command-line arguments for consumer performance tests\nDescription: This implements KIP-1147 for kafka-consumer-perf-test.sh and kafka-share-consumer-perf-test.sh.\nQ: Hi Andrew Schofield, I saw you didin't assign anyone to this jira, does that mean I can help with this one? Thanks!", "output": "[~aheev] got there first :)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade to Junit 5.13.3\nDescription: JUnit has been upgraded to 5.13.3 so that test suites can\r\nbe parameterized at the class level (again).\r\nThis requires a matching upgrade to Surefire and some\r\ntuning of surefire options to restore existing behavior.\r\n\r\nDependency Changes:\r\n* junit.jupiter and junit.vintage => 5.13.3\r\n* junit.platform => 1.13.3\r\n* Surefire => 3.5.3.\r\n\r\nChanged Surefire Flags: \r\n* trimStackTrace => false\r\n* surefire.failIfNoSpecifiedTests => false\r\n\r\nh2. Build property trimStackTrace\r\n\r\ntrimStackTrace is set to false by default\r\n\r\nThis restores the behavior that any test failure\r\nincludes a deep stack trace rather than the first two elements.\r\nIn this version of surefire it also seems to print the full stack\r\nof JUnit test execution and the java runtime itself.\r\n\r\nThis is only an issue for when tests fail.\r\nIf the short trace is desired, enable it on the command line:\r\n{code}\r\n    mvn test -DtrimStackTrace\r\n{code}\r\n\r\nh2. Build property surefire.failIfNoSpecifiedTests\r\n\r\nsurefire.failIfNoSpecifiedTests i\nQ: hadoop-yetus commented on PR #7785:\nURL: https://github.com/apache/hadoop/pull/7785#issuecomment-3075916579\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  85m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 125m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |   3m 27s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 12s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 30s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 132m 55s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7785/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7785 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 200bb3fe4862 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ac5300cfbf822a579168bec04f9029875b1289f2 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7785/3/testReport/ |\r\n   | Max. process+thread count | 549 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7785/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "steveloughran commented on PR #7785:\nURL: https://github.com/apache/hadoop/pull/7785#issuecomment-3084561721\n\n   tested s3 london. lots of Itests failing, which will need fixing after. That test `TestStagingPartitionedJobCommit` is good"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix testRenameFileWithFullQualifiedPath on Windows\nDescription: The Apache Commons Net FTP library is used for FTPFileSystem. In *TestFTPFileSystem#testRenameFileWithFullQualifiedPath()*, the FS operations (such as touch and rename) are made using absolute paths. However, the library expects relative paths.\r\nThis caused *FTPFileSystem#getFileStatus()* to throw FileNotFoundException since the library was trying to look for the absolute path under which the FileSystem was mounted.\r\n\r\nThis worked fine on Linux as it just appended the absolute path under the FileSystem's mount path -\r\n\r\n !image-2025-04-27-23-05-05-212.png! \r\n\r\nHowever, this fails on Windows since suffixing the absolute path under the FileSystem's mount path doesn't yield a valid path due to the drive letter in the absolute path.\r\n\r\nConsider the following illustration -\r\n+On Linux+\r\n{text}\r\npath1 => /mnt/d/a/b\r\npath2 => /mnt/d/x/y\r\npath1 + path2 yields a valid path => /mnt/d/a/b/mnt/d/x/y\r\n{text}\r\n\r\n+On Windows+\r\n{text}\r\npath1 => C:\\a\\b\r\npath2 => C:\\x\\y\r\npath1 + path2 doesn't yield a valid path => C:\\a\\b\\C:\\x\\y\r\n{text}\r\n\r\nSo, to fix this, we need to treat the FS operations as purely relative.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively\nDescription: It has been observed that certain requests taking more time than expected to complete hinders the performance of whole workload. Such requests are known as tailing requests. They can be taking more time due to a number of reasons and the prominent among them is a bad network connection. In Abfs driver we cache network connections and keeping such bad connections in cache and reusing them can be bad for perf.\r\n\r\nIn this effort we try to identify such connections and close them so that new good connetions can be established and perf can be improved. There are two parts of this effort.\r\n # Identifying Tailing Requests: This involves profiling all the network calls and getting percentiles value optimally. By default we consider p99 as the tail latency and all the future requests taking more than tail latency will be considere as Tailing requests.\r\n\r\n # Proactively Killing Socket Connections: With Apache client, we can now kill the socket connection and fail the tailing request. Such failures will not be thrown back to user and retried immediately without any sleep but from another socket connection.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Make `KubernetesClientUtils` Java-friendly\nDescription: \nQ: Issue resolved by pull request 52542\n[https://github.com/apache/spark/pull/52542]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix ByteBuf leaks in TestShuffleChannelHandler\nDescription: A lot of ByteBuf leaks are reported when running with JDK23.\r\n\r\nI have not idea why those are not detected with JDK8, 11 or 17. Maybe something about the logging setup ?", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add JDK 24 to Ubuntu 20.04 docker development images\nDescription: The first step to supporting JDK23/24 is being able to test it.\r\n\r\nAdd JDK24 to the default docker images, so that both manual and automated testing is possible.\nQ: stoty opened a new pull request, #7495:\nURL: https://github.com/apache/hadoop/pull/7495\n\n   ### Description of PR\r\n   \r\n   Adds JDK23 to Ubuntu 20.04 development docker images.\r\n   Also sets the default JDK to 1.8 to be consistent with the existing JAVA_HOME setting\r\n   \r\n   This patch does not change the JDK used by default (apart from setting up java/javac alternatives to be consistent with JAVA_HOME), but adds the possibility to easily switch to JDK23 for testing.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran start-build-env.sh on x86_64 and ARM hosts, and checked that JDK23 is present and working.\r\n   Also checked that the java and javac command on the PATH and JAVA_HOME point to the same 1.8 JDK.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7495:\nURL: https://github.com/apache/hadoop/pull/7495#issuecomment-2714712545\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  49m 30s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 10s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  44m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 135m 21s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7495/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7495 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 9d93aa5da0b8 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4e547793e29c1894fd6a830a8a0a29300671f58b |\r\n   | Max. process+thread count | 534 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7495/1/console |\r\n   | versions | git=2.9.5 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Ticket for KIP-1226. The KIP proposes introducing the concept of lag for share partitions, which will be persisted in the __share_group_state topic and can be retrieved using admin.listShareGroupOffsets", "output": "Share Partition Lag Persistence and Retrieval"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Some tests in TestThrottledInputStream intermittently fail when the measured bandwidth is equal to the one set for throttling.", "output": "Fix TestThrottledInputStream when bandwidth is equal to throttle limit"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Resolve flaky test: org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFails\nDescription: EosIntegrationTest.shouldNotViolateEosIfOneTaskFails is currently flaky for tests run with the \"streams\" group protocol.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consumer NoOffsetForPartitionException for partitions being revoked\nDescription: Currently, the consumer attempts to update positions for all partitions it owns that don't have a position, even when the partition may be already being revoked. This could lead to NoOffsetForPartitionException for such partitions (if there is no reset strategy defined). There are 2 main issues around this:\r\n * is probably unneeded work to fetch offsets and update positions for partitions being revoked. At that point we're actually not allowing fetching from there anymore\r\n * throwing NoOffsetForPartitionException may lead applications to take action to set positions themselves, which will most probably fail (the partition will most probably not owned by the consumer anymore since it was already being revoked when the NoOffsetForPartitionException was generated). We've seen this on Kafka Streams, that handled NoOffsetForPartitionException by calling seek to set positions.   \r\n\r\nThis task is to review if there are no other implications I may be missing? Then fix to ensure we don't updat\nQ: This is what i understand , can can try reproducing it through test also\r\n\r\n.  1. Partitions marked for revocation (pendingRevocation = true) are excluded from fetching but NOT from position updates\r\n  2. shouldInitialize() only checks if state is INITIALIZING, ignoring pendingRevocation\r\n  3. hasValidPosition() only checks fetch state, ignoring pendingRevocation\r\n  4. This causes updateFetchPositions() to attempt fetching offsets for partitions being revoked", "output": "Hi [~goyarpit], I was already doing some work on this one. I will ping you when ready and would be great if you help with reviews (or follow-ups if I find). Will keep you in the loop. Thanks!"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Purgatory should clear up after share consumers are stopped\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have evidence that `IOStatisticsSupport.snapshotIOStatistics()` can hang, specifically on the statistics collected by an S3AInputStream, whose statistics are merged in to the FS stats in close();\r\n\r\n{code}\r\njdk.internal.misc.Unsafe.park(Native Method)\r\njava.util.concurrent.locks.LockSupport.park(LockSupport.java:341)\r\njava.util.concurrent.ForkJoinTask.awaitDone(ForkJoinTask.java:468)\r\njava.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:687)\r\njava.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:927)\r\njava.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\njava.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)\r\norg.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap.entrySet(EvaluatingStatisticsMap.java:166)\r\njava.util.Collections$UnmodifiableMap.entrySet(Collections.java:1529)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.copyMap(IOStatisticsBinding.java:172)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:216)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:199)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSnapshot.snapshot(IOStatisticsSnapshot.java:165)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSnapshot.(IOStatisticsSnapshot.java:125)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSupport.snapshotIOStatistics(IOStatisticsSupport.java:49)\r\n{code}\r\n\r\nthe code in question is calling `parallelStream()`, which uses a fixed pool of threads shared by all uses of the API\r\n{code}\r\n    Set> r = evalEntries.parallelStream().map((e) ->\r\n        new EntryImpl<>(e.getKey(), e.getValue().apply(e.getKey())))\r\n        .collect(Collectors.toSet());\r\n{code}\r\n\r\nProposed: \r\n* move off parallelStream() to stream()\r\n* review code to if there is any other way this iteration can lead to a deadlock, e.g. the apply() calls.\r\n* could we do the merge more efficiently?", "output": "S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Bump zstd-jni 1.5.7-6\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1\nDescription: The project is currently using {{restrict-imports-enforcer-rule}} version {*}2.0.0{*}, which was released in *2021* and is now outdated. Since then, several new versions have been released with important bug fixes, performance improvements, and enhanced compatibility. To maintain a modern and stable build environment, it is necessary to upgrade to version {*}2.6.1{*}.\nQ: hadoop-yetus commented on PR #8012:\nURL: https://github.com/apache/hadoop/pull/8012#issuecomment-3379348657\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 43s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 25s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 25s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  mvnsite  |   0m 25s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  shadedclient  |   3m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 24s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 24s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 22s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   0m 25s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  shadedclient  |   4m 41s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 28s |  |  ASF License check generated no output?  |\r\n   |  |   |  13m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8012 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux bcae851086b8 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f59839a3b410ba91d777948cc1b8683d10006e31 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/testReport/ |\r\n   | Max. process+thread count | 55 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #8012:\nURL: https://github.com/apache/hadoop/pull/8012#issuecomment-3379885563\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   3m 52s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   1m 29s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   3m 18s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  mvnsite  |   1m 41s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  shadedclient  |  13m 13s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   1m 44s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 39s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 39s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 24s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   0m 24s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  shadedclient  |   4m  6s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 24s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  23m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8012 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 679e127e6ac4 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 67cae1f9eee8d0b5bb049e7b261a4e70c00d46a3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/testReport/ |\r\n   | Max. process+thread count | 106 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Balance GHA CI jobs\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add `SparkOperatorConfManager.getAll` method\nDescription: \nQ: Issue resolved by pull request 402\n[https://github.com/apache/spark-kubernetes-operator/pull/402]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "this is the follow-up of KAFKA-18884", "output": "Move TransactionLogTest to transaction-coordinator module"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support setMaxRows for SparkConnectStatement\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump zstd-jni 1.5.7-6\nDescription: \nQ: Issue resolved by pull request 52684\n[https://github.com/apache/spark/pull/52684]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update SpotBugs version and enable Spotbugs Gradle tasks on Java 25\nDescription: *Prologue:* KAFKA-19664 \r\n\r\n*In more details:* Apache commons bcel Java 25 compatible version will be created soon (and SpotBugs version will follow immediately): https://issues.apache.org/jira/browse/BCEL-377?focusedCommentId=18028106&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18028106\r\n\r\n*Action points:*\r\n - upgrade SpotBugs version (use Java 25 compatible version)\r\n - enable SpotBugs checks for Java 25 Github actions build\r\n\r\n*Related links: *\r\n- [https://issues.apache.org/jira/projects/BCEL/versions/12354966] \r\n- [https://github.com/spotbugs/spotbugs/issues/3564]\nQ: *Apache* *{{commons-bcel}}* new version has been released; Spotbugs version should be released soon:\r\n - [https://github.com/spotbugs/spotbugs/pull/3763]\r\n - [https://github.com/spotbugs/spotbugs/milestone/31?closed=1] \r\n - [https://github.com/spotbugs/spotbugs/discussions/3771]", "output": "GitHub PR is created here: https://github.com/apache/kafka/pull/20704"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I notice that the code in Kafka for handling of the scopes claim does not comply with the RFC.\r\n\r\n[https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim |https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim]says:\r\n{quote}The value of the {{scope}} claim is a JSON string containing a space-separated list of scopes associated with the token, in the format described in [Section 3.3|https://www.rfc-editor.org/rfc/rfc6749#section-3.3] of [[RFC6749|https://datatracker.ietf.org/doc/html/rfc6749]]\r\n{quote}\r\n \r\n\r\nHowever the code in Kafka that parses the JWT payload does not permit a space separated list.  It would treat a value like \"email phone address\" as a single scope \"email phone address\" rather than a three separate scopes of \"email\", \"phone\", \"address\".\r\n\r\nThe affected code is here:\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/BrokerJwtValidator.java#L166]\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredJws.java#L343]\r\n\r\nImpact:\r\n\r\nLooking at the production code in Apache Kafka itself, I think the defect currently harmless.  As far as I can tell, there's no production code that makes use of  org.apache.kafka.common.security.oauthbearer.internals.secured.BasicOAuthBearerToken#scope.\r\n\r\nI think there would be a potential for impact for a user writing their own OAuthBearerValidatorCallbackHandler that uses Kafka's BrokerJwtValidator and made use of the scope value.\r\n\r\nFailing unit test:\r\n\r\n[https://github.com/apache/kafka/compare/trunk...k-wall:kafka:KAFKA-19790]", "output": "Parsing of the scope claim does not comply with RFC-8693"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Percentile estimation functions\nDescription: Similar to https://issues.apache.org/jira/browse/SPARK-53885, it would be useful to have the following percentile estimation functions based on data sketch. These functions provide a lightweight, mergeable representation of percentile distributions, enabling efficient approximate percentile computation over large or streaming datasets without maintaining full data samples.\r\n * *APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm.\r\n\r\n * *APPROX_PERCENTILE_ESTIMATE(state [, k] ])* : Returns the approximate percentile value(s) from a previously accumulated sketch state.\r\n\r\n * *APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation.\nQ: [~Gengliang.Wang] I can take these on.", "output": "[~Gengliang.Wang] Are we interested in creating something like this [https://github.com/apache/datasketches-hive], but for spark? Basically introducing sketch dependencies through connector repo instead of adding them directly to the spark codebase."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix PluginUtils#pluginLocations warning log not being printed\nDescription: This fix addresses an issue where no warning log was displayed when the `plugin.path` configuration contained empty string elements.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade libopenssl to 3.1.1 for rsync on Windows\nDescription: We're currently using libopenssl 3.1.0 which is needed for rsync 3.2.7 on Windows for the Yetus build validation.\r\nHowever, libopenssl 3.1.0 is no longer available for download on the msys2 site -\r\n\r\n{code}\r\nPS D:\\projects\\github\\apache\\hadoop> Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.0-2-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.0-2-x86_64.pkg.tar.zst\r\nInvoke-WebRequest:\r\n404 Not Found\r\n\r\n404 Not Found\r\nnginx/1.26.3\r\n{code}\r\n\r\nThus, we need to upgrade libopenssl to the next available version - 3.1.1 to mitigate this issue.\nQ: GauthamBanasandra commented on PR #7487:\nURL: https://github.com/apache/hadoop/pull/7487#issuecomment-2708470616\n\n   Jenkins CI validation for Windows is in progress - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-win10-x86_64/839/.", "output": "hadoop-yetus commented on PR #7487:\nURL: https://github.com/apache/hadoop/pull/7487#issuecomment-2708471286\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7487/1/console in case of problems."}
{"instruction": "Answer the question based on the bug.", "input": "Title: kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0\nDescription: *  kafka server version is 2.5.1\r\n *  kafka-client version bigger than 3.1.1 \r\n\r\n \r\n{code:java}\r\nimport org.apache.kafka.clients.producer.KafkaProducer;\r\nimport org.apache.kafka.clients.producer.Producer;\r\nimport org.apache.kafka.clients.producer.ProducerRecord;\r\n\r\nimport java.util.Properties;\r\n\r\n\r\npublic class KafkaSendTest {\r\n\r\n  public static void main(String[] args) {\r\n    Properties props = new Properties();\r\n    props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\");\r\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"transaction.timeout.ms\", \"300000\");\r\n    props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully\r\n    props.put(\"compression.type\", \"lz4\");\r\n    props.put(\"security.protocol\", \"SASL_PLAINTEXT\");\r\n    props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\");\r\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka\nQ: Hi [~zhumingustc] I tried to replicate this with acks: -1. I am able to send the record, could you share your server setup?\r\n\r\n \r\n{code:java}\r\n14:28:31.273 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.\r\nSent record(key=null ){code}", "output": "Thanks. I find the reason of this  ClusterAuthorizationException. It  involves the config of idempotence.\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-13673   \r\n\r\nIn this kafka-client 3.1.1 issue  {color:#de350b}b. enable.idempotence unset && acks=all => enable idempotence .{color}\r\n\r\nSo Idempotence is set to true based on the producer properties  However, the Kafka server has not enabled the idempotence right for admin. Finally it throws such ClusterAuthorizationException.\r\n\r\nAfter following command executed in kafka server.  Producer can send record  successfully with acks: -1 \r\n\r\n \r\n{code:java}\r\nsh kafka-acls.sh --authorizer-properties  zookeeper.connect=xxxx:2191/xxx --add --allow-principal User:admin --allow-host \"*\" --operation IdempotentWrite --cluster {code}"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Insert Overwrite Jobs With MagicCommitter Fails On S3 Express Storage\nDescription: Query engines which uses Magic Committer to overwrite a directory would ideally upload the MPUs (not complete) and then delete the contents of the directory before committing the MPU.\r\n\r\n \r\n\r\nFor S3 express storage, The directory purge operation is enabled by default. Refer [here|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L688] for code pointers.\r\n\r\n \r\n\r\nDue to this, the pending MPU uploads are purged and query fails with \r\n\r\n{{NoSuchUpload: The specified multipart upload does not exist. The upload ID might be invalid, or the multipart upload might have been aborted or completed. }}", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: StreamsGroupDescribe result is missing topology when topology not configured\nDescription: StreamsGroupDescribe does not include a topology if the topology is not configured.\r\n\r\n \r\n\r\nThe problem is that it relies on `ConfiguredTopology` which may not be present or empty. Instead, we should use the unconfigured topology when the configured topology is not present", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h2. Environment details\r\nh3. OpenLineage version\r\n\r\n{quote}io.openlineage:openlineage-spark_2.13:1.39.0\r\nTechnology and package versions\r\n\r\nPython: 3.13.3\r\nScala: 2.13.16\r\nJava: OpenJDK 64-Bit Server VM, 17.0.16\r\n\r\npip freeze\r\npy4j==0.10.9.9\r\npyspark==4.0.1{quote}\r\n\r\n\r\nFor the openlineage set up, I used the default setting:\r\n\r\n\r\n{quote}$ git clone https://github.com/MarquezProject/marquez.git && cd marquez\r\n\r\n$ ./docker/up.sh{quote}\r\n\r\nh3. Spark Deployment details\r\n\r\nI used native spark on local machine. There is no managed services involved.\r\nProblem details\r\n\r\nh2. Issue details\r\nWhen using Spark structured streaming to write parquet file to file systems,\r\n\r\n*     File sink will only generate openlineage event with streaming processing type with output information as empty.\r\n*     Foreachbatch sink will generate openlineage event with both streaming processing type and batch processing type. The batch processing type will have valid output information.\r\n\r\nThe bug is that File sink in Spark structured streaming does not generate open lineage event with output details.\r\n\r\nMore details about the sample code and sample events are following.\r\nFile sink:\r\nSample code:\r\n{quote}\r\nquery = streaming_df.writeStream \\\r\n    .format('parquet') \\\r\n    .outputMode('append') \\\r\n    .partitionBy('year', 'month', 'day') \\\r\n    .option('checkpointLocation', checkpoint_path) \\\r\n    .option('path', output_path) \\\r\n    .queryName('filesink') \\\r\n    .start()\r\n{quote}\r\nSample event for \"processingType\":\"STREAMING\"\r\n{quote}\r\n25/10/29 00:49:02 DEBUG wire: http-outgoing-52 >> \"{\"eventTime\":\"2025-10-28T13:45:34.282Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b14-4e9d-7574-95a9-55182f07591d\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineag", "output": "Spark Structured Streaming Filesink can not generate open lineage with output details"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Define Proto for Sinks\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Prologue: KAFKA-19174\r\n\r\n\r\n\r\n\r\nSee here for description: https://github.com/apache/kafka/pull/19513#issuecomment-3388760324", "output": "Custom Gradle tasks (`unitTest` and `integrationTest`) are not executing tests in Gradle 9"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use Swift 6.2 as the minimum supported version\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Unexpected Fatal error InvalidTxnStateException on Kafka Streams (KIP-890)\nDescription: We are seeing cases where a Kafka Streams (KS) thread stalls for ~20 seconds, resulting in a visible gap in the logs. During this stall, the broker correctly aborts the open transaction (triggered by the 10-second transaction timeout). So far, this is expected behavior. However, when the KS thread resumes, instead of receiving the expected InvalidProducerEpochException (which we already handle gracefully as part of transaction abort), the client is instead hit with an InvalidTxnStateException. KS currently treats this as a fatal error, causing the application to fail.\nQ: 3.9: https://github.com/apache/kafka/commit/4dd35126656e2af2ff3cd67705d1dd495f68c0ec\r\ntrunk: https://github.com/apache/kafka/commit/0a483618b9cc169a0f923478812141630baf2a4c", "output": "4.1: https://github.com/apache/kafka/commit/a10c1f3ea1454dbd644ea65a885c965529e1d37d"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: commit-rate metric\nDescription: Measures the frequency of calls to {{StateStore#commit}}", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When opening a {{StateStore}} that {{{}managesOffsets{}}}, existing {{.checkpoint}} offsets should be \"migrated\" to the {{StateStore}} using {{{}StateStore#commit{}}}.", "output": "Auto migrate .checkpoint offsets"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [ABFS] Enable rename and create recovery from client transaction id over DFS endpoint\nDescription: We have implemented create and rename recovery using client transaction IDs over the DFS endpoint ([HADOOP-19450] [ABFS] Rename/Create path idempotency client-level resolution - ASF JIRA). Since the backend changes were not fully rolled out, we initially implemented the changes with the flag disabled. With this update, we aim to enable the flag, which will start sending client transaction IDs. In case of a failure, we will attempt to recover from the failed state if possible. Here are the detailed steps and considerations for this process:\r\n\r\n1. **Implementation Overview**: We introduced a mechanism for create and rename recovery via client transaction IDs to enhance reliability and data integrity over the DFS endpoint. This change was initially flagged as disabled due to incomplete backend rollouts.\r\n\r\n2. **Current Update**: With the backend changes now in place, we are ready to enable the flag. This will activate the sending of client transaction IDs, allowing us to track and manage \nQ: bhattmanish98 commented on PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#issuecomment-2726292282\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 796, Failures: 0, Errors: 0, Skipped: 152\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 799, Failures: 0, Errors: 0, Skipped: 106\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 638, Failures: 0, Errors: 0, Skipped: 214\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 796, Failures: 0, Errors: 0, Skipped: 163\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 2\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 641, Failures: 0, Errors: 0, Skipped: 144\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 635, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 638, Failures: 0, Errors: 0, Skipped: 145\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 636, Failures: 0, Errors: 0, Skipped: 163\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 670, Failures: 0, Errors: 0, Skipped: 159\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 635, Failures: 0, Errors: 0, Skipped: 213\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24", "output": "hadoop-yetus commented on PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#issuecomment-2726370077\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 33s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 135m 25s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7509 |\r\n   | JIRA Issue | HADOOP-19497 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 22cf15295129 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d3493c94846ed4ac94ba064f22c0350826dc7193 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/1/testReport/ |\r\n   | Max. process+thread count | 535 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Replace AssumptionViolatedException with TestAbortedException\nDescription: In JUnit 4, {{org.junit.internal.AssumptionViolatedException}} is used to indicate assumption failure and skip the test. However, {{AssumptionViolatedException}} is an implementation in JUnit 4, and in JUnit 5, we can use {{TestAbortedException}} to replace {{{}AssumptionViolatedException{}}}.\r\n\r\n{{TestAbortedException}} is used to indicate that a test has been aborted, and it can be used to replace {{{}AssumptionViolatedException{}}}. However, it is not directly related to assumption failure and is more commonly used in situations where the test needs to be aborted during execution.\nQ: slfan1989 opened a new pull request, #7800:\nURL: https://github.com/apache/hadoop/pull/7800\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19618. Replace AssumptionViolatedException with TestAbortedException.\r\n   \r\n   In JUnit 4, org.junit.internal.AssumptionViolatedException is used to indicate assumption failure and skip the test. However, AssumptionViolatedException is an implementation in JUnit 4, and in JUnit 5, we can use TestAbortedException to replace AssumptionViolatedException.\r\n   \r\n   TestAbortedException is used to indicate that a test has been aborted, and it can be used to replace AssumptionViolatedException. However, it is not directly related to assumption failure and is more commonly used in situations where the test needs to be aborted during execution.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Junit Test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7800:\nURL: https://github.com/apache/hadoop/pull/7800#issuecomment-3065373842\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 12 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 48s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m 10s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 28s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   6m 33s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   6m 15s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  11m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 23s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   4m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 24s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   6m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   6m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  12m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 53s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 104m  3s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 53s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m  7s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 45s |  |  hadoop-aliyun in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 45s |  |  hadoop-cos in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 46s |  |  hadoop-huaweicloud in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 10s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 419m  4s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7800/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7800 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ca7de1a90d3f 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 717ce8af91220c2dd9d28d3bdea477758df8ff5a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7800/1/testReport/ |\r\n   | Max. process+thread count | 1894 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-aliyun hadoop-cloud-storage-project/hadoop-cos hadoop-cloud-storage-project/hadoop-huaweicloud U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7800/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support restart counter in Spark app\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "It is much easier to maintain than the current direct dependencyManagement entries.", "output": "Use jackson-bom to set jackson versions"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. \nDescription: Removed build support for the following EOL (End-of-Life) operating system versions (e.g., CentOS 7, Debian 9, etc.).\r\n\r\nCleaned up redundant code and dependency configurations in the Dockerfile related to these operating systems.\r\n\r\nOptimized the build logic to ensure that currently supported OS versions build successfully.\nQ: hadoop-yetus commented on PR #7822:\nURL: https://github.com/apache/hadoop/pull/7822#issuecomment-3108826927\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/1/console in case of problems.", "output": "hadoop-yetus commented on PR #7822:\nURL: https://github.com/apache/hadoop/pull/7822#issuecomment-3109175364\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   2m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  1s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 42s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  hadolint  |   0m  2s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  40m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  95m 21s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7822 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 6fb52aab7b2a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d4650c0d79570cf96fd5c62db6e8a00157b647d5 |\r\n   | Max. process+thread count | 564 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 hadolint=1.11.1-0-g0e692dd shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update Boost to 1.86.0 in Windows build image\nDescription: HADOOP-19475 has updated Boost to 1.86.0, but it missed the Windows docker image, which doesn't use the same mechanism for dependencies as the Linux ones.\r\n\r\nUpdate Boost in the Windows build Docker image to the same version.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Native docker image fails to start when using SASL OAUTHBEARER mechanism\nDescription: Running the native image with `KAFKA_SASL_ENABLED_MECHANISMS=OAUTHBEARER` results in an exception that prevents it starting:\r\n\r\n\r\n \r\n{code:java}\r\n[2025-08-06 22:55:09,656] ERROR Exiting Kafka due to fatal exception during startup. (kafka.Kafka$) org.apache.kafka.common.KafkaException: org.apache.kafka.common.KafkaException: Could not find a public no-argument constructor for org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredLoginCallbackHandler at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:184) ~[?:?] at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:188) ~[?:?] at org.apache.kafka.common.network.ChannelBuilders.serverChannelBuilder(ChannelBuilders.java:105) ~[?:?] at kafka.network.Processor.(SocketServer.scala:883) ~[?:?] at kafka.network.Acceptor.newProcessor(SocketServer.scala:791) ~[kafka.Kafka:?] at kafka.network.Acceptor.$anonfun$addProcessors$1(SocketServer.scala:757) ~[kafka.Kafka:?] at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:190) ~[kafka.Kafka:?] at kafka.network.Acceptor.addProcessors(SocketServer.scala:756) ~[kafka.Kafka:?] at kafka.network.DataPlaneAcceptor.configure(SocketServer.scala:472) ~[?:?] at kafka.network.SocketServer.createDataPlaneAcceptorAndProcessors(SocketServer.scala:222) ~[?:?] at kafka.network.SocketServer.$anonfun$new$16(SocketServer.scala:149) ~[?:?] at kafka.network.SocketServer.$anonfun$new$16$adapted(SocketServer.scala", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "```\r\n\r\n./bin/kafka-metadata-shell.sh -s /tmp/kafka-meta/bootstrap.checkpoint\r\n\r\n17:31:43 Loading...\r\n\r\n[2025-09-08 17:32:19,271] ERROR Ignoring control record with type SNAPSHOT_HEADER at offset 0 (org.apache.kafka.metadata.util.SnapshotFileReader)\r\n\r\n[2025-09-08 17:32:19,274] ERROR Ignoring control record with type SNAPSHOT_FOOTER at offset 5 (org.apache.kafka.metadata.util.SnapshotFileReader) Starting...\r\n```", "output": "MetaDataShell log error message for SNAPSHOT_HEADER and SNAPSHOT_FOOTER"}
{"instruction": "Answer the question based on the bug.", "input": "Title: OOM when loading large uncompacted __consumer_offsets partitions with transactional workload\nDescription: When loading a large poorly compacted __consumer_offsets partition with a transactional workload (alternating transaction offset commits and commit markers), we create and discard lots of TimelineHashMaps. These accumulate in the SnapshotRegistry's rollback mechanism. Overall, memory usage is linear in the size of the partition.\r\n\r\nWe can commit the snapshot every N offsets in {{{}CoordinatorLoaderImpl{}}}, to allow the memory to be reclaimed.\nQ: Fix for 4.0.x and 4.1.x is tracked by https://issues.apache.org/jira/browse/KAFKA-19732.", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In [https://github.com/apache/kafka/pull/17952], if ELR is enabled, we (1) disallow min.insync.replicas at the broker level; (2) automatically add min.insync.replicas at the cluster level, if not present; (3) disallow removing min.insync.replicas at the cluster level.  The reason for this is that if brokers disagree about which partitions are under min ISR, it breaks the KIP-966 replication invariants.\r\n\r\nHowever, even if ELR is not enabled, it's bad to have different min.insync.replicas on different brokers since if a leader is moved to a different broker, it will behave differently on the min.insync.replicas semantic. So, it's probably better to always enforce the above regardless whether ELR is enabled or not. Similarly, we probably want to do the same for at least unclean.leader.election.enable.\r\n\r\nSince this is a public facing change, it requires a KIP.", "output": "always disallow min.insync.replicas at the broker level"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix time unit mismatch in method updateDeferredMetrics\nDescription: Fix time unit mismatch in method updateDeferredMetrics.\r\n\r\nThe time unit of passed param is nanos. But rpcmetrics's is mills.\nQ: hfutatzhanghb opened a new pull request, #7557:\nURL: https://github.com/apache/hadoop/pull/7557\n\n   ### Description of PR\r\n   Fix time unit mismatch in method updateDeferredMetrics.\r\n   \r\n   The time unit of passed param is nanos. But rpcmetrics's is mills.", "output": "hfutatzhanghb closed pull request #7557: HADOOP-19521. Fix time unit mismatch in method updateDeferredMetrics.\nURL: https://github.com/apache/hadoop/pull/7557"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade libopenssl to 3.5.2-1 needed for rsync\nDescription: The currently used libopenssl-3.1.4-1 is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n\r\nThe Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows.\r\n\r\n{code}\r\n00:25:33 SUCCESS: Specified value was saved.\r\n00:25:46 Removing intermediate container 5ce7355571a1\r\n00:25:46 ---> a13a4bc69545\r\n00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n00:25:46 ---> Running in d2dafad446f9\r\n00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n00:25:54 \u001b[0m\u001b[91mAt line:1 char:1\r\n00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ...\r\n00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:Htt\r\n{code}\r\n\r\nThus, we need to upgrade to the latest version to address this.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Delete the UID in the dev container that is the same as the host user\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "* we need to worry about noaa data going away if the US govt decides to do that\r\n* there's a big dataset of blockchain data *in parquet format* https://registry.opendata.aws/aws-public-blockchain/\r\n\r\nWe should look at this and see if it is good to play with as\r\n* large\r\n* complex", "output": "S3A: use blockchain datasets for parquet data"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Gradle build fails after Swagger patch version update\nDescription: *How to reproduce:* \r\n * checkout trunk (Swagger version: 2.2.25)\r\n * change `swaggerVersion` in `gradle.properties` to some more recent version (something in between of 2.2.27 and 2.2.39)\r\n * execute *./gradlew clean releaseTarGz*\r\n * build fails with an error _*Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String.*_\r\n * see details below\r\n * (!) note: build works for swagger version 2.2.26 (although reporter had some issues once or twice with that particular version, so please keep that on your mind)\r\n\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew clean releaseTarGz \r\n\r\n> Configure project :\r\nStarting build with version 4.2.0-SNAPSHOT (commit id 34581dff) using Gradle 9.1.0, Java 17 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\n\r\n[Incubating] Problems report is available at: file:///home/dej\nQ: Thanks for reporting this! I’ll help look into the issue.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Exclude `KerberosConfDriverFeatureStep` during benchmarking\nDescription: \nQ: Issue resolved by pull request 404\n[https://github.com/apache/spark-kubernetes-operator/pull/404]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove Google Analytics from Hadoop Website\nDescription: Hi Hadoop Team,\r\n\r\nThe ASF {_}*Privacy Policy*{_}[1][2] does not permit the use of _*Google Analytics*_ on any ASF websites.\r\n\r\nIt looks like _*Google Analytics*_ was removed from the Hadoop site on *13th Nov 2024* in [Commit 86c0957|https://github.com/apache/hadoop-site/commit/86c09579bff7bf26e86475939d106e900a7da94d] and re-introduced on *20th Feb 2025* in [Commit 27f835e|https://github.com/apache/hadoop-site/commit/27f835ef0369a6bc95a12131af75c696cafb4b4c].\r\n\r\nI'm not sure how this has happened, but I see the following:\r\n* [layouts/partials/footer.html|https://github.com/apache/hadoop-site/blob/asf-site/layouts/partials/footer.html] references *__internal/google_analytics.html_*\r\n* I can't find *__internal/google_analytics.html_*\r\n* It looks like the safest action is to remove the reference to *google_analytics.html* in the *footer.html*\r\n\r\nI have created [PR #67|https://github.com/apache/hadoop-site/pull/67] to remove the reference in the footer, although I have not understood full\nQ: [~stevel@apache.org], thanks for applying [PR #67|https://github.com/apache/hadoop-site/pull/67]. Could the Hadoop site be *re-generated* because the generated content still has references to *_Google Analytics_*?", "output": "done it; covered in https://cwiki.apache.org/confluence/display/HADOOP2/HowToRelease / publishing\r\n\r\nHope I've done it right ..."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension\nDescription: This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc\r\ninstruction sets, with full functional verification and performance testing completed.\r\n\r\nThe implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction.\r\n\r\nKey Features: \r\n1. Runtime Hardware Detection\r\nThe PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime.\r\n\r\n2. Performance Improvement\r\nHardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation.\nQ: leiwen2025 opened a new pull request, #7912:\nURL: https://github.com/apache/hadoop/pull/7912\n\n   This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc\r\n   instruction sets, with full functional verification and performance testing completed.\r\n   \r\n   The implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction.\r\n   \r\n   Key Features:\r\n   1. Runtime Hardware Detection\r\n   The PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime.\r\n   \r\n   2. Performance Improvement\r\n   Hardware-accelerated CRC32 achieves a performance boost of over **3X** compared to the software implementation.\r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7912:\nURL: https://github.com/apache/hadoop/pull/7912#issuecomment-3232576752\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  13m  5s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   7m 30s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 29s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  56m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |   7m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |   7m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   7m  3s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/blanks-eol.txt) |  The patch has 20 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-tabs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/blanks-tabs.txt) |  The patch 2 line(s) with tabs.  |\r\n   | -1 :x: |  mvnsite  |   1m 29s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  19m 44s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7912 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 3c797fab6900 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 67c832e1dc930b52b9a68c261f372b14e6cf1639 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/testReport/ |\r\n   | Max. process+thread count | 1262 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7912/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: AAL - Update to version 1.2.1\nDescription: Version 1.2.1 brings in better memory management, readVectored and SSE-C support.\nQ: ahmarsuhail commented on PR #7807:\nURL: https://github.com/apache/hadoop/pull/7807#issuecomment-3079244390\n\n   @steveloughran @mukund-thakur small PR to update AAL version. We've made a few improvements since 1.0.0, mainly in how memory is managed. So would be great to get 3.4.2 released with 1.2.1, instead of the current 1.0.0 version. \r\n   \r\n   This also brings in support for readVectored, IoStats and SSE-C, which is required by the other PR's we have up.", "output": "steveloughran commented on PR #7807:\nURL: https://github.com/apache/hadoop/pull/7807#issuecomment-3079690164\n\n   a provisional +1 pending that test result."}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade bouncycastle to 1.82 due to CVE-2025-8916\nDescription: https://github.com/advisories/GHSA-4cx2-fc23-5wg6\r\n\r\nThought it was tidier to upgrade to latest version even if the fix was a while ago.\nQ: hadoop-yetus commented on PR #8039:\nURL: https://github.com/apache/hadoop/pull/8039#issuecomment-3420366545\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m 51s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  28m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 12s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 28s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  mvnsite  |   8m 58s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 36s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 36s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  27m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 49s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 37s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   7m  1s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 42s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 36s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  45m 37s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 807m 40s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1064m 10s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList |\r\n   |   | hadoop.hdfs.tools.TestDFSAdmin |\r\n   |   | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |\r\n   |   | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes |\r\n   |   | hadoop.hdfs.TestRollingUpgrade |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   |   | hadoop.yarn.service.TestYarnNativeServices |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8039 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint compile javac javadoc mvninstall unit shadedclient xmllint shellcheck shelldocs |\r\n   | uname | Linux 66cf96c27f49 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 695a0a30232b143ec8837d6a6648344ffd4efec0 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/testReport/ |\r\n   | Max. process+thread count | 4498 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-cloud-storage-project/hadoop-cos . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8039/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 merged PR #8039:\nURL: https://github.com/apache/hadoop/pull/8039"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "According to [Develocity|https://develocity.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&tests.container=org.apache.kafka.tools.streams.DescribeStreamsGroupTest&tests.sortField=FLAKY], {{testDescribeStreamsGroupWithShortTimeout}} is very flaky.", "output": "Mark DescribeStreamsGroupTest testDescribeStreamsGroupWithShortTimeout() as flaky"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Upgrade to 2.29.52 -the last version compatible with third party stores until there are fixes in the AWS SDK or workarounds added in the S3A connector\r\n\r\nThis SDK update doesn't need to come with some changes to disable some new features (default integrity protections),\r\nand to apply critical changes related to the SDK\r\n\r\nDefault integrity protection came with 2.30, and is on unless disabled.\r\nhttps://github.com/aws/aws-sdk-java-v2/issues/5801\r\n\r\nAs well as being incompatible with third party stores, it has also affected S3 multiregion Access Points: https://github.com/aws/aws-sdk-java-v2/issues/5878\r\n\r\nThis has broken most interaction with third party stores, hence fixes in Iceberg https://github.com/apache/iceberg/pull/12264 and Trinio https://github.com/trinodb/trino/pull/24954\r\n\r\nThere's also [AWS v2.30 SDK InputStream behavior changes #5859](AWS v2.30 SDK InputStream behavior changes).\r\nIt looks like our code is safer from that, but it did require code review.\r\n\r\nSDK 2.30.19 seems good with this", "output": "S3A: Upgrade AWS V2 SDK to 2.29.52"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-federation-balance.\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp class to bind to an IPv6 address. WebApp uses the HttpServer2, and adding the IPv6 connector to this class makes the solution more elegant.\r\n\r\nTo enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host.\r\nWhen the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6.\r\nWhen java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors.\r\nTo disable IPv4, you need to configure the OS at the system level.", "output": "Add dual-stack/IPv6 Support to HttpServer2"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Protobuf 3.25.5 for docker images\nDescription: HADOOP-19289 upgraded protobuf-java 3.25.5, we should use same version for protobuf installed in docker images.\nQ: GauthamBanasandra commented on code in PR #7780:\nURL: https://github.com/apache/hadoop/pull/7780#discussion_r2196670293\n\n\n##########\ndev-support/docker/vcpkg/vcpkg.json:\n##########\n@@ -10,7 +10,7 @@\n   \"overrides\": [\n     {\n       \"name\": \"protobuf\",\n-      \"version\": \"3.21.12\"\n+      \"version\": \"3.25.5\"\n\nReview Comment:\n   Thanks for PR @pan3793. Please let me know once you're done with all the changes and I can verify it on Windows.", "output": "hadoop-yetus commented on PR #7780:\nURL: https://github.com/apache/hadoop/pull/7780#issuecomment-3248461577\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/console in case of problems."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Global stream thread ignores all exceptions\nDescription: {{globalStreamThread}} in {{KafkaStreams}} class ignores all exceptions and fails to apply the user-provided {{StreamsUncaughtExceptionHandler}} in:\r\n{code:java}\r\npublic void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler userStreamsUncaughtExceptionHandler)\r\n{code}\r\nThis can lead to streams being stuck in faulty state after an exception is thrown during their initialization phase (e.g. failure to load the RocksDB native library). The exception isn't logged, so debugging such problem is difficult.\r\n\r\nFrom my understanding of the {{b62d8b97}} commit message, the following code is unnecessary as the {{globalStreamThread}} can't be replaced and the issue description from KAFKA-12699 doesn't apply to it. Removing it should help.\r\n{code:java}\r\nif (globalStreamThread != null) {\r\n    globalStreamThread.setUncaughtExceptionHandler((t, e) -> { }\r\n    );\r\n}\r\n{code}\nQ: Thanks for reporting the issue. – Not logging some exception is for sure a problem. This should be easy to fix.\r\n\r\nWhile I agree that `globalStreamThread.setUncaughtExceptionHandler((t, e) -> \\{ });` is not useful any longer, I don't think removing this code solves the problem. – In older version of KS, we use the Java uncaught exception handler, but we did move off it, in favor of a custom implementation. However, this custom implementation which allows to restart dying thread, only makes sense for `StreamsThreads`, but not for the `GlobalThread` – of the `GlobalThread` dies, we can only close KafkaStreams client with an ERROR state (what we do). [In AK 4.0, we remove the old code, and it seems we did not do a proper cleanup; that's all.]\r\n\r\nBut yes, we should fix the logging issue for sure.", "output": "You're right, now I see that the first call to {{setUncaughtExceptionHandler}} on {{globalStreamThread}} is using the new implementation and sets it right. I might have incorrectly pinpointed my issue.\r\n\r\nI've verified that the problem I've encountered happens when {{initialize()}} call in {{GlobalStreamThread:276}} throws an {{ExceptionInInitializerError}} due to RocksDB failing to load the library in static initializer of {{{}DBOptions{}}}. This exception was only caught inside the {{java.lang.Thread}} exception handler of the {{{}globalStreamThread{}}}."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*  kafka server version is 2.5.1\r\n *  kafka-client version bigger than 3.1.1 \r\n\r\n \r\n{code:java}\r\nimport org.apache.kafka.clients.producer.KafkaProducer;\r\nimport org.apache.kafka.clients.producer.Producer;\r\nimport org.apache.kafka.clients.producer.ProducerRecord;\r\n\r\nimport java.util.Properties;\r\n\r\n\r\npublic class KafkaSendTest {\r\n\r\n  public static void main(String[] args) {\r\n    Properties props = new Properties();\r\n    props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\");\r\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"transaction.timeout.ms\", \"300000\");\r\n    props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully\r\n    props.put(\"compression.type\", \"lz4\");\r\n    props.put(\"security.protocol\", \"SASL_PLAINTEXT\");\r\n    props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\");\r\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"xxx\\\" password=\\\"xxxx\\\";\");\r\n\r\n    Producer producer = new KafkaProducer<>(props);\r\n\r\n    try {\r\n      String topic = \"topic1\";\r\n      byte[] value = new byte[]{1,2}; // example\r\n      ProducerRecord record = new ProducerRecord<>(topic, null, value);\r\n      producer.send(record, (metadata, exception) -> {\r\n        if (exception == null) {\r\n          System.out.printf(\"Sent record(key=%s value=%s) meta(partition=%d, offset=%d)\\n\",\r\n                  record.key(), new String(record.value()), metadata.partition(), metadata.offset());\r\n        } else {\r\n          exception.printStackTrace();\r\n        }\r\n      });\r\n      producer.close();\r\n    } catch (Exception e) {\r\n      e.printStackTrace();\r\n    }\r\n  }\r\n} {code}\r\npom.xml config\r\n{code:java}\r\n\r\n  org.apache.kafka\r\n  kafka-clients\r\n  3.4.0\r\n {code}\r\n         When kafka producer acks=-1, It will throw exception.\r\n\r\n \r\n{code:java}\r\norg.apache.kafka.common.KafkaException: Cannot exe", "output": "kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "JVM are compatible with  0.10.9.7+ and above versions have some correctness fixes", "output": "Relex Py4J requirement to 0.10.9.7+"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration\nDescription: Establish the foundational build infrastructure for RISC-V CRC32 hardware acceleration support while maintaining full backward compatibility with existing software implementations.\nQ: hadoop-yetus commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3225752648\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  14m 21s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 56s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  94m 44s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  13m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 54s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 54s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 198m 14s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7903 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux af19edaaec5b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1b159b6f8fadc7e4229a3aa0eeaa184b6d2f25cc |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/testReport/ |\r\n   | Max. process+thread count | 1279 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7903:\nURL: https://github.com/apache/hadoop/pull/7903#issuecomment-3226747056\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  14m  4s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 55s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  94m 56s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  13m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  13m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  13m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 57s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 39s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 53s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 176m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7903 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux c0759ac9ab3e 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3d607d41a2a311260c28800135009dfca09fb4f3 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/testReport/ |\r\n   | Max. process+thread count | 2145 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7903/2/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Create python3 symlink needed for mvnsite\nDescription: The mvnsite compilation step needs python3. Although we're installing python3 in the build environment, the python3 executable is missing.\r\nThus, we need to create a symbolic link python3 pointing to python.exe needed for mvnsite.\r\n\r\nFailure -\r\n{code}\r\n[INFO] -------------------------------------\r\n[INFO] Building Apache Hadoop Common 3.5.0-SNAPSHOT                    [11/115]\r\n[INFO]   from hadoop-common-project\\hadoop-common\\pom.xml\r\n[INFO] --------------------------------[ jar ]---------------------------------\r\n[INFO] \r\n[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ hadoop-common ---\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\target\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\src\\site\\markdown (includes = [UnixShellAPI.md], excludes = [])\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\src\\site\\resources (includes = [configuration.xsl, core-default.xml], excludes = [])\r\n[INFO] \r\n[INFO] --- exec-maven-plugin:1.3.1:exe\nQ: GauthamBanasandra opened a new pull request, #7452:\nURL: https://github.com/apache/hadoop/pull/7452\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   * The mvnsite compilation step needs python3.\r\n     Although we're installing python3 in the build\r\n     environment, the python3 executable is missing.\r\n   * Thus, we need to create a symbolic link python3\r\n     pointing to python.exe needed for mvnsite.\r\n   \r\n   Failure logs - [archive.zip](https://github.com/user-attachments/files/19041819/archive.zip)\r\n   \r\n   ```\r\n   [INFO] -------------------------------------\r\n   [INFO] Building Apache Hadoop Common 3.5.0-SNAPSHOT                    [11/115]\r\n   [INFO]   from hadoop-common-project\\hadoop-common\\pom.xml\r\n   [INFO] --------------------------------[ jar ]---------------------------------\r\n   [INFO] \r\n   [INFO] -", "output": "hadoop-yetus commented on PR #7452:\nURL: https://github.com/apache/hadoop/pull/7452#issuecomment-2694903468\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7452/1/console in case of problems."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Consistency of command-line arguments for console producer/consumer\nDescription: This implements KIP-1147 for kafka-console-producer, kafka-console-consumer and kafka-console-share-consumer.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Similar to https://issues.apache.org/jira/browse/SPARK-53885, it would be useful to have the following percentile estimation functions based on data sketch. These functions provide a lightweight, mergeable representation of percentile distributions, enabling efficient approximate percentile computation over large or streaming datasets without maintaining full data samples.\r\n * *APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm.\r\n\r\n * *APPROX_PERCENTILE_ESTIMATE(state [, k] ])* : Returns the approximate percentile value(s) from a previously accumulated sketch state.\r\n\r\n * *APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation.", "output": "Percentile estimation functions"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "EosIntegrationTest.shouldNotViolateEosIfOneTaskFails is currently flaky for tests run with the \"streams\" group protocol.", "output": "Resolve flaky test: org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFails"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Revisit `JUnit` assert usage in test cases\nDescription: \nQ: Issue resolved by pull request 384\n[https://github.com/apache/spark-kubernetes-operator/pull/384]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Complex sql with expand operator and code gen enabled, very slow\nDescription: Complex sql with expand operator and code gen enabled, very slow\r\n\r\nsql format like select keya,keyb,count(distinct case when),...,count(distinct case when),sum(a),sum(b) from x group by keya,keyb\r\n\r\nwhen disable whole stage code gen, run will speed up 20x times\r\n\r\nwhen add executor jvm parameter -XX:-TieredCompilation, run will speed up 20x times\r\n\r\nreduce select column count, such as 28 -> 27, can speed up 10x times\nQ: !https://wiki.in.zhihu.com/download/attachments/640447372/image2025-10-2_13-32-26.png?version=1&modificationDate=1759383146774&api=v2!\r\n\r\nfrom flame graph we can see, most time cost is setNullAt call when enable whole stage code gen and not add -XX:-TieredCompilation jvm parameter", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Consistency of command-line arguments for remaining CLI tools\nDescription: This implements KIP-1147 for kafka-cluster.sh, kafka-leader-election.sh and kafka-streams-application-reset.sh.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "S3 Express now supports atomic renames: [https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-s3-express-one-zone-atomic-renaming-objects-api/]\r\n\r\n \r\n\r\nRename API is available as of SDK version 2.31.66.", "output": "Support atomic rename() for S3 express"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/pull/18949/files#r2296809389", "output": "KRaft servers don't handle the cluser-level configs in starting"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Set Up CI Support JDK17 & JDK21\nDescription: Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to upgrade the project’s Java compatibility. This pipeline will serve as the foundation for testing and validating future JDK 17 support and ensuring smooth integration with existing CI/CD processes.\nQ: slfan1989 commented on PR #7831:\nURL: https://github.com/apache/hadoop/pull/7831#issuecomment-3121008019\n\n   @GauthamBanasandra I’m working on adding a new pipeline in Trunk to support JDK17. While reviewing the history, I noticed you've done in-depth work on this area, particularly around the upgrade to `jenkins.sh`\r\n   \r\n   As part of this change, I reviewed the implementation in HADOOP-16888(#2012) to better understand how JDK11 support was introduced. From what I can tell, it seems that enabling JDK17 unit test support only requires configuring the appropriate JDK17-related variables in `jenkins.sh`, similar to how JDK11 was handled. Could you please confirm if that's sufficient? Let me know if there are any additional compatibility steps or considerations I should be aware of.\r\n   \r\n   Thank you very much!", "output": "slfan1989 commented on PR #7831:\nURL: https://github.com/apache/hadoop/pull/7831#issuecomment-3121011939\n\n   @ayushtkn @aajisaka Could you kindly provide some help and guidance? I would really appreciate it."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add a separate docker file for Python 3.14 daily build\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Create python3 symlink needed for mvnsite\nDescription: The mvnsite compilation step needs python3. Although we're installing python3 in the build environment, the python3 executable is missing.\r\nThus, we need to create a symbolic link python3 pointing to python.exe needed for mvnsite.\r\n\r\nFailure -\r\n{code}\r\n[INFO] -------------------------------------\r\n[INFO] Building Apache Hadoop Common 3.5.0-SNAPSHOT                    [11/115]\r\n[INFO]   from hadoop-common-project\\hadoop-common\\pom.xml\r\n[INFO] --------------------------------[ jar ]---------------------------------\r\n[INFO] \r\n[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ hadoop-common ---\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\target\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\src\\site\\markdown (includes = [UnixShellAPI.md], excludes = [])\r\n[INFO] Deleting C:\\hadoop\\hadoop-common-project\\hadoop-common\\src\\site\\resources (includes = [configuration.xsl, core-default.xml], excludes = [])\r\n[INFO] \r\n[INFO] --- exec-maven-plugin:1.3.1:exe\nQ: hadoop-yetus commented on PR #7452:\nURL: https://github.com/apache/hadoop/pull/7452#issuecomment-2694903468\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7452/1/console in case of problems.", "output": "hadoop-yetus commented on PR #7452:\nURL: https://github.com/apache/hadoop/pull/7452#issuecomment-2695132487\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  13m 15s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 52s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  44m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  93m 35s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7452/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7452 |\r\n   | Optional Tests | dupname asflicense |\r\n   | uname | Linux 9ae494fbaf64 5.15.0-126-generic #136-Ubuntu SMP Wed Nov 6 10:38:22 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9ade311abd815cb28c10653ea6820aad02c17778 |\r\n   | Max. process+thread count | 558 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7452/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Marker creation is attempted during certain operations such as {*}create{*}, {*}getPathStatus{*}, {*}setPathProperties{*}, and {*}rename{*}, in order to add a 0-byte file that signifies the existence of a folder at that path. However, if marker creation fails, the failure should not be propagated to the user.", "output": "ABFS: [FNS Over Blob] Marker creation fail exception should not be propagated"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix flaky RemoteLogManagerTest#testCopyQuota\nDescription: see https://github.com/apache/kafka/actions/runs/16651849478/job/47166854957?pr=20269", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title:  Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c\nDescription: ## Description\r\n \r\nThere is a duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c` at lines 29-30.\r\n\r\n## Current Code\r\n```c\r\n#include \"gcc_optimizations.h\"\r\n#include \"gcc_optimizations.h\"\r\n```", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Description:\r\nWhen running the following command in a Kafka cluster with a large number of consumer groups (over 380) and topics (over 500), the kafka-consumer-groups.sh --describe --all-groups operation consistently times out and fails to return results.\r\n\r\nCommand used:\r\n\r\n./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups\r\nObserved behavior:\r\nThe command fails with a TimeoutException, and no consumer group information is returned. The following stack trace is observed:\r\n\r\njava.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeConsumerGroups, deadlineMs=1753170317381, tries=1, nextAllowedTryMs=1753170317482) timed out at 1753170317382 after 1 attempt(s)\r\n    at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\r\n    at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\r\n    ...\r\nCaused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeConsumerGroups, deadlineMs=..., tries=1, ...) timed out\r\nCaused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: describeConsumerGroups\r\nExpected behavior:\r\nThe command should be able to return the description of all consumer groups, or at least fail more gracefully. Ideally, there should be:\r\n\r\nA way to paginate or batch the describe operation;\r\n\r\nOr configuration options to increase internal timeout thresholds;\r\n\r\nOr better recommendations for dealing with large clusters.\r\n\r\nAdditional context:\r\n\r\nManually describing individual consumer groups via --group performs as expected and returns data quickly.\r\n\r\nThe issue appears to scale linearly with the number of consumer groups.", "output": "kafka-consumer-groups.sh --describe --all-groups command times out on large clusters with many consumer groups"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update script `free_disk_space`\nDescription: \nQ: Issue resolved by pull request 52541\n[https://github.com/apache/spark/pull/52541]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix SpotBugs warnings introduced after SpotBugs version upgrade.\nDescription: Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged.\r\nWe plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules.\nQ: I agree with your point. We’ll work on submitting a common PR that includes a SpotBugs rule to temporarily suppress the new static analysis warnings and restore the state back to the 4.2.0 level.", "output": "Sounds awesome.\r\nThanks for all the efforts."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix typo in state-change log filename after rotate\nDescription: The log4j2 config file was incorrectly rotating the state-change.log to {{stage-change.log.[date]}} (changing the filename from state to stage). The below PR corrects the file name for rotated logs.\r\n\r\nAfter this change is applied, the log4j2 config will not take any actions with previously-created {{stage-change.log.[date]}} files. These may need to be manually removed by users.\r\n\r\n[https://github.com/apache/kafka/pull/20269]", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix race condition issue related to ObservedMetrics\nDescription: In Spark Connect environment, QueryExecution#observedMetrics can be called by two threads concurrently.\r\n\r\n \r\n\r\nThread1(ObservationManager)\r\n{code:java}\r\nprivate def tryComplete(qe: QueryExecution): Unit = {\r\n  val allMetrics = qe.observedMetrics\r\n  qe.logical.foreach {\r\n    case c: CollectMetrics =>\r\n      allMetrics.get(c.name).foreach { metrics =>\r\n        val observation = observations.remove((c.name, c.dataframeId))\r\n        if (observation != null) {\r\n          observation.setMetricsAndNotify(metrics)\r\n        }\r\n      }\r\n    case _ =>\r\n  }\r\n}\r\n{code}\r\nThread2(SparkConnectPlanExecution)\r\n{code:java}\r\nprivate def createObservedMetricsResponse(\r\n    sessionId: String,\r\n    observationAndPlanIds: Map[String, Long],\r\n    dataframe: DataFrame): Option[ExecutePlanResponse] = {\r\n  val observedMetrics = dataframe.queryExecution.observedMetrics.collect {\r\n    case (name, row) if !executeHolder.observations.contains(name) =>\r\n      val values = SparkConnectPlanExecution.toObservedMetricsValues(row)\r\n      name -> values\r\n  } {code}\r\n\r\nThis can cause race condition issues. We can see CI failure caused by this issue.\r\nhttps://github.com/apache/spark/actions/runs/18422173471/job/52497913985\r\n\r\n{code}\r\n======================================================================\r\nERROR [0.181s]: test_observe_with_map_type (pyspark.sql.tests.connect.test_parity_observation.DataFrameObservationParityTests.test_observe_with_map_type)\r\n-----------------------------------------------------------", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0\nDescription: *  kafka server version is 2.5.1\r\n *  kafka-client version bigger than 3.1.1 \r\n\r\n \r\n{code:java}\r\nimport org.apache.kafka.clients.producer.KafkaProducer;\r\nimport org.apache.kafka.clients.producer.Producer;\r\nimport org.apache.kafka.clients.producer.ProducerRecord;\r\n\r\nimport java.util.Properties;\r\n\r\n\r\npublic class KafkaSendTest {\r\n\r\n  public static void main(String[] args) {\r\n    Properties props = new Properties();\r\n    props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\");\r\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"transaction.timeout.ms\", \"300000\");\r\n    props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully\r\n    props.put(\"compression.type\", \"lz4\");\r\n    props.put(\"security.protocol\", \"SASL_PLAINTEXT\");\r\n    props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\");\r\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka\nQ: Thanks. I find the reason of this  ClusterAuthorizationException. It  involves the config of idempotence.\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-13673   \r\n\r\nIn this kafka-client 3.1.1 issue  {color:#de350b}b. enable.idempotence unset && acks=all => enable idempotence .{color}\r\n\r\nSo Idempotence is set to true based on the producer properties  However, the Kafka server has not enabled the idempotence right for admin. Finally it throws such ClusterAuthorizationException.\r\n\r\nAfter following command executed in kafka server.  Producer can send record  successfully with acks: -1 \r\n\r\n \r\n{code:java}\r\nsh kafka-acls.sh --authorizer-properties  zookeeper.connect=xxxx:2191/xxx --add --allow-principal User:admin --allow-host \"*\" --operation IdempotentWrite --cluster {code}", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Consistency of command-line arguments for verifiable producer/consumer\nDescription: This implements KIP-1147 for kafka-verifiable-producer.sh and kafka-verifiable-consumer.sh.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1\nDescription: \nQ: Issue resolved by pull request 252\n[https://github.com/apache/spark-connect-swift/pull/252]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/pull/14873#discussion_r2442794887, \r\n\r\nIn below code snippets\r\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L116\r\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/StreamsGroupHeartbeatRequestManager.java#L333\r\n\r\nwe use max.poll.interval.ms (default 300000) as the jitter value. Although RequestState.remainingBackoffMs guards with Math.max(0, …), which won’t make the value become negative, using a jitter that isn’t in (0–1) is unexpected.\r\n\r\nIn addition, we should validate that ExponentialBackoff receives a jitter strictly within (0, 1) to prevent this scenario in the future.", "output": "Incorrect jitter value in StreamsGroupHeartbeatRequestManager and AbstractHeartbeatRequestManager"}
{"instruction": "Answer the question based on the bug.", "input": "Title: HADOOP-19455. S3A: Enable logging of SDK client metrics\nDescription: HADOOP-19455. S3A: Enable logging of SDK client metrics\r\n\r\nTo log the output of the AWS SDK metrics, set the log\r\n`org.apache.hadoop.fs.s3a.DefaultS3ClientFactory` to `TRACE`.\nQ: the upgrade SDK work will include this as part of debugging some stuff", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-rumen.\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support spark.sql(\"SELECT ...\") inside Pipelines query functions\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "SparkSQL uses the partition location or table location as the commit path (except in *_dynamic partition overwrite_* mode and *_custom partition path_* mode). This has at least the following issues:\r\n\r\n* As described in SPARK-37210, conflicts can occur when multiple partitions job of the same table are run concurrently. Using a staging directory can avoid this issue.\r\n* As described in SPARK-53937, using a staging directory allows for near-atomic operations.\r\n\r\n_*Dynamic partition overwrite*_ mode and *_custom partition path_* mode already use the staging directory. And *_dynamic partition overwrite_* mode and _*custom partition path*_ are implemented differently, which can be further simplified into a unified process. And in https://github.com/apache/spark/pull/29000, reset the staging directory as the output directory of FileOutputCommitter. This way is more safer. It should be modified to this way.", "output": "Use the staging directory as the output path then move to final path"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When I use the SMT *org.apache.kafka.connect.transforms.InsertHeader* and the header value is a number, part of the value is dropped. Example: \r\n\r\n{code:yaml}\r\n# Insert KafkaHeaders for avro serialization\r\ntransforms: insertSpecHeader\r\n\r\ntransforms.insertSpecHeader.type: org.apache.kafka.connect.transforms.InsertHeader\r\ntransforms.insertSpecHeader.header: \"specversion\"\r\ntransforms.insertSpecHeader.value.literal: \"2.0\"\r\n{code}\r\n\r\nThen, the record is produced with the header *\"specversion\": \"2\"*\r\n\r\nIs KafkaConnect doing a sort of casting and treating the value as float even though I am using literal?", "output": "InsertHeader drops part of the value when header value is a number"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add JDK 21 to Ubuntu 20.04 docker development images\nDescription: We want to support JDK21, we better have it available in the development image for testing.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: the `record-size` and `throughput`arguments don't work in TestRaftServer\nDescription: see https://github.com/apache/kafka/blob/656242775c321c263a3a01411b560098351e8ec4/core/src/main/scala/kafka/tools/TestRaftServer.scala#L118\r\n\r\nwe always hard code the `recordsPerSec` and `recordSize`\nQ: I'm working on this, thanks :)", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Compute share partition lag in GroupCoordinatorService\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, the warning message is logged once (or at most twice after HADOOP-8865) when we first use (set/get) a deprecated configuration key and most of the time this happens early on during system startup. \r\n\r\nUsers tend to set/get properties on their job scripts/applications  that keep running on a hourly/daily/etc basis. When a problem comes up the user will package the latest logs and send them to us (developers/support) for further analysis and troubleshooting. However, it's very likely that these logs will not contain information about the deprecated usages which might be crucial for advancing the investigation.\r\n\r\nOn the other end, a warning that appears just once is not that worrisome so even if the users/customers/developers see it, they can easily ignore it, and move on, thinking that there is no action needed on their end.\r\n\r\nThe above scenarios are based on applications such as the Hivemetastore, HiveServer2, which use the Hadoop Configuration, and usually run for weeks/months without a restart.\r\n\r\nI propose to change the existing logic to always log a message when a deprecated configuration key is in use. This will minimize the risk of losing important deprecation logs and will also simplify the implementation.\r\n\r\nMoreover, since there is a dedicated logger (HADOOP-9487) for these warning messages applications/users can suppress/limit the log content by changing their logging configuration. It may not be as precise as logging each deprecation once but it can still address the verbosity concern that was raised in the past.\r\n\r\n+History+\r\nIn HADOOP-6105, where the  logging was first introduced the initial intention was to log on every usage as proposed here. Just before merging there were some concerns that this may clutter the logs and a more conservative approach was adopted:\r\n\r\nbq. The warning message will be printed for every set and also for first get of deprecated key after every reload of configuration.\r\n\r\nThen HADOOP-8197 came in, claiming that it", "output": "Log warning message on every set/get of a deprecated configuration property"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The commons-text version used by Hadoop is incompatible with commons-lang3 3.12.\r\n\r\nThis is actually with an older Hadoop, but with the same {_}commons-lang3{_}, _commons-configuration2_ and _commons-text_ versions as trunk:\r\n{noformat}\r\njava.lang.NoSuchMethodError: 'org.apache.commons.lang3.Range org.apache.commons.lang3.Range.of(java.lang.Comparable, java.lang.Comparable)'\r\n    at org.apache.commons.text.translate.NumericEntityEscaper.(NumericEntityEscaper.java:97)\r\n    at org.apache.commons.text.translate.NumericEntityEscaper.between(NumericEntityEscaper.java:59)\r\n    at org.apache.commons.text.StringEscapeUtils.(StringEscapeUtils.java:271)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.unescapePropertyName(PropertiesConfiguration.java:690)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.initPropertyName(PropertiesConfiguration.java:583)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.parseProperty(PropertiesConfiguration.java:640)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.nextProperty(PropertiesConfiguration.java:626)\r\n    at org.apache.commons.configuration2.PropertiesConfigurationLayout.load(PropertiesConfigurationLayout.java:443)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration.read(PropertiesConfiguration.java:1500)\r\n    at org.apache.commons.configuration2.io.FileHandler.loadFromReader(FileHandler.java:712)\r\n    at org.apache.commons.configuration2.io.FileHandler.loadFromTransformedStream(FileHandler.java:782)\r\n    at org.apache.commons.configuration2.io.FileHandler.loadFromStream(FileHandler.java:738)\r\n    at org.apache.commons.configuration2.io.FileHandler.load(FileHandler.java:693)\r\n    at org.apache.commons.configuration2.io.FileHandler.load(FileHandler.java:596)\r\n    at org.apache.commons.configuration2.io.FileHandler.load(FileHandler.java:569)\r\n    at org.apache.hadoop.metrics2.impl.Metrics", "output": "Update commons-lang3 to 3.17.0"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analytics-Accelerator: Upgrade AAL to 1.0.0 \nDescription: Latest version of AAL is now available, changes since last time:\r\n * Moves all logging to debug\r\n * Adds in timeout and retries to requests to mitigate (very rare) hanging seen when reading from AsyncClient, related github issue: https://github.com/aws/aws-sdk-java-v2/issues/5755\nQ: ahmarsuhail opened a new pull request, #7469:\nURL: https://github.com/apache/hadoop/pull/7469\n\n   ### Description of PR\r\n   \r\n    Latest version of AAL is now available, changes since 0.0.4:\r\n   \r\n   * Moves all logging to debug\r\n   * Adds in timeout and retries to requests to mitigate (very rare) hanging seen when reading from AsyncClient, related github issue: https://github.com/aws/aws-sdk-java-v2/issues/5755\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran ITestS3AAnalyticsAcceleratorStreamReading, will run the test suite once it's available in Maven!", "output": "ahmarsuhail commented on PR #7469:\nURL: https://github.com/apache/hadoop/pull/7469#issuecomment-2700895757\n\n   @mukund-thakur @steveloughran - PR to use AAL 1.0.0. No major changes since 0.0.4, moves logging to debug and adds timeouts and retries for a hanging issue we're seeing (happens really rarely)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Metrics from FetchMetricsManager containing a topic tag are duplicated\nDescription: Hello,\r\n\r\nSince Kafka 4.1.0, we're experiencing an issue with Kafka Streams metrics for topics containing dots in their names. (I think the problem is also for simple usage of consumers not only kstream)\r\n\r\nIn FetchMetricsManager, methods like {{recordRecordsFetched()}} now create duplicate sensors for the same topic if they contain dot in their name: \r\n{code:java}\r\nvoid recordRecordsFetched(String topic, int records) { \r\n\r\n\r\n String name = topicRecordsFetchedMetricName(topic); \r\n\r\n\r\n maybeRecordDeprecatedRecordsFetched(name, topic, records);  Map.of(\"topic\", topic)) \r\n\r\n\r\n .withAvg(metricsRegistry.topicRecordsPerRequestAvg) \r\n\r\n\r\n .withMeter(metricsRegistry.topicRecordsConsumedRate, metricsRegistry.topicRecordsConsumedTotal) \r\n\r\n\r\n .build(); \r\n\r\n\r\n recordsFetched.record(records); \r\n\r\n\r\n }  {code}\r\nIt currently record two sensors, one with my original topic name, one time with a topic name with dots replaced by underscore. \r\n\r\n-While we can work around this by reversing the transformat\nQ: Hey you are right, it is not clear, but in our case if we put the same name and tags, under the hood they are considered duplicates when registered and one of them will be discarded. (like we want)\r\n\r\n \r\n\r\nEDIT: after doing tests, what's above does not work, the metrics are merged instead of being deduplicated and therefore each increment of one of them is incrementing the same counter", "output": "Excuse me. I don’t get the point of the issue. Kafka now create two sensor and then why the deprecated one makes trouble on your application? The deprecated one is kept for the compatibility, and so it means it exists for a while already."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Address the compileClasspath resolution warnings for the `releaseTarGz` task\nDescription: {code:java}\r\n[warn]  Resolution\r\n of the configuration :tools:tools-api:runtimeClasspath was attempted \r\nfrom a context different than the project context. Have a look at the \r\ndocumentation to understand why this is a problem and how it can be \r\nresolved. This behavior has been deprecated. (1)    - [warn]  Resolution\r\n of the configuration :tools:tools-api:runtimeClasspath was attempted \r\nfrom a context different than the project context. Have a look at the \r\ndocumentation to understand why this is a problem and how it can be \r\nresolved. This behavior has been deprecated.This will fail with an error in Gradle 9.0.        - Locations- ``- `:core:releaseTarGz` {code}\r\nThe issue was introduced by [https://github.com/apache/kafka/pull/13454]\r\n\r\n \r\n\r\n`tools-api` is already in core module runtime path, so adding it to `releaseTarGz` causes the resolution conflicts, which will be a fatal error in gradle 9", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When running with SSE-C (and probably other encryption methods?),  reading from a public bucket such as `noaa-cors`, tests will fail as objects in the public bucket are no encrypted. Any test reading pre-existing objects should be skipped when setting encryption to anything other than SSE-S3.", "output": "Encryption tests fail for public buckets "}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Bump commons-lang3 to 3.18.0 due to CVE-2025-48924\nDescription: https://www.cve.org/CVERecord?id=CVE-2025-48924\r\n\r\nWill update commons-text to 1.14.0 which was released with commons-lang3 3.18.0. Due to https://issues.apache.org/jira/browse/HADOOP-19532 - seems best to upgrade them together.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Limit Arrow batch sizes in SQL_GROUPED_AGG_ARROW_UDF\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The goal is to add a new *default* method, getDefaultMessageTemplate, to the public SparkThrowable interface. This gives clients a consistent, machine-readable *default* template for error rendering, while leaving them free to localize or otherwise transform the message.", "output": "Enable messageTemplate propagation to SparkThrowable "}
{"instruction": "Answer the question based on the bug.", "input": "Title: Backport HADOOP-15984 (Update jersey from 1.19 to 2.x) on branch-3.4.0 \nDescription: In Hadoop 3.5.0 we uses GlassFish Jersey (modern Jersey).\r\n\r\nIn Hadoop 3.4.0 we use Sun Jersey (legacy Jersey).\r\n\r\nJetty submodule is not published under _com.sun.jersey.jersey-test-framework_ so upgrading jersey will be a better idea to remove grizzly-http-* dependencies (which is already done in 3.5.0 through  YARN-11793).\nQ: [~susheel_7] Thank you for raising this JIRA. Regarding your suggestion, my current plan is not to upgrade Branch-3.4. The purpose of upgrading Jersey is to provide full support for JDK 17; however, Branch-3.4 is intended to continue supporting JDK 8, so we do not plan to apply this upgrade to that branch.\r\n\r\n \r\n\r\ncc: [~hexiaoqiao] [~stevel@apache.org]", "output": "I agree that backporting this to 3.4 is problematic. I think this is much too large a change for a patch release.\r\nI think the priority should be on pushing for a 3.5.0 release."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/pull/14359#issuecomment-3372367774", "output": "remove the unnecessary copy from AbstractFetch#fetchablePartitions "}
{"instruction": "Answer the question based on the bug.", "input": "Title: Refactor RelationResolution to enable code reuse\nDescription: Refactor RelationResolution to enable code reuse\nQ: Issue resolved by pull request 52781\n[https://github.com/apache/spark/pull/52781]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Replace Thread with SubjectPreservingThread\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Revoked partitions are included in currentOffsets passed to preCommit on task stop\nDescription: When a task stops, [{{WorkerSinkTask#closeAllPartitions()}}|https://github.com/apache/kafka/blob/84caaa6e9da06435411510a81fa321d4f99c351f/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L664] runs and the [task’s {{preCommit}} is invoked|https://github.com/apache/kafka/blob/3c7f99ad31397a6a7a4975d058891f236f37d02d/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L444]. At that time, the {{currentOffsets}} argument may include {*}revoked partitions{*}.\r\n\r\nThis appears to be caused by:\r\n * {{WorkerSinkTask}} does not remove revoked partitions from {{currentOffsets}}\r\n * {{WorkerSinkTask#closeAllPartitions()}} passes its {{currentOffsets}} to {{SinkTask#preCommit(...)}} _as-is_ (i.e., without filtering).\r\n\r\nDuring normal iterations, {{SinkTask#preCommit(...)}} receives {{{}KafkaConsumer#assignment(){}}}, so revoked partitions are *not* included.\r\n\r\nHaving revoked partitions included *only* at stop is confusing behavior. If this behavior is specified, Could we add a brief note to the {{SinkTask#preCommit(...)}} Javadoc to clarify this behavior?", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: ITestS3ACommitterMRJob failing on Junit5\nDescription: NPE in test200 of ITestS3ACommitterMRJob.\r\n\r\nCause is\r\n* test dir setup and propagation calls Path.getRoot().toUri() which returns the /home dir\r\n* somehow the locatedFileStatus of that path being 1 so a codepath in FileInputFormat NPEs.\r\n\r\nFix is in test code, though FileInputFormat has its NPE messages improved to help understand what is going wrong.\nQ: steveloughran opened a new pull request, #7968:\nURL: https://github.com/apache/hadoop/pull/7968\n\n   \r\n   Adds extra logging as to what is happening, passes in actual test dir set at class level.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   s3 london\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "steveloughran commented on PR #7968:\nURL: https://github.com/apache/hadoop/pull/7968#issuecomment-3293282933\n\n   ```\r\n   [INFO] Running org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob\r\n   [INFO] Running org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob\r\n   [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.97 s"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade spotbug to 4.9.4\nDescription: see discussion https://github.com/apache/kafka/pull/20295#issuecomment-3146551515\nQ: hello,I will fix it", "output": "I've raised https://github.com/apache/kafka/pull/20333 to fix this.\r\n\r\n[~gongxuanzhang] Sorry, I didn't notice your comment until now, I didn't mean to steal this issue, it just kind of happened since I did the previous spotbugs-related work too. Feel free to let me know if you have a better fix than the one I linked, and I can close my PR in favor of yours."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Read Buffer Manager V2 should not be allowed untill implemented\nDescription: Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used.\r\nThis is to stop any user explicitly enabling the config to enable RBMV2.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix TestFsShellList.testList on Windows\nDescription: * The test *org.apache.hadoop.fs.TestFsShellList#testList* creates files with special characters and tries to list them using *ls*.\r\n* Filenames with special characters aren't allowed in Windows.\r\n* Thus, we need to modify the test to only test for non-special characters on Windows and include the filenames with special characters on non-Windows environments.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: testVectoredReadAfterNormalRead() failing with 412 response from S3\nDescription: This is surfacing on a bucket using versionid for change detection: block reads are failing in the test {{ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead()}}\r\n\r\n{code}\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 412, Request ID: 0AN2EB8QXC75HH0T, Extended Request ID: U5l/UnIF4n3NO1mrZVzS2vv72F3LgUoVJxR4XodUSaTWCerfjmmpH45CbFGKkTkfgfnykwzseGo=)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:104)\r\n\r\n{code}\r\n\r\n* this is the normal readFully() call, before the vectored one\r\n* it worked last week\r\n* also found on branch-3.4 before the SDK update, so not an issue caused by the SDK unless my maven repo is badly contaminated\r\n* seems unrelated to versioning -still there when disabled.\r\n* applies on unversioned s3 express store too.\r\n\r\nAbout the main way I could see this surface is if the test file is less than the actual length of file created, so the GET is rejected for reading off the end (the openfile passes in the length to save the HEAD)", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump Maven 3.9.10\nDescription: \nQ: pan3793 commented on PR #7760:\nURL: https://github.com/apache/hadoop/pull/7760#issuecomment-3050892025\n\n   #7782 gets merged, rebase this PR", "output": "hadoop-yetus commented on PR #7760:\nURL: https://github.com/apache/hadoop/pull/7760#issuecomment-3050918256\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7760/14/console in case of problems."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Experienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests.\r\n\r\nHypothesis: a transient failure meant the server receiving the POST calls to complete the uploads was mistakenly reporting no upload IDs.\r\n\r\nOutcome: all active write operations failed, without any retry attempts. This can lose data and fail jobs, even though the store may recover.\r\n\r\nProposed. The multipart uploads, especially block output stream, retry on this error; treat it as a connectivity issue.", "output": "S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\""}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I am using *Apache Kafka 4.0* with *MirrorMaker 2* to link the primary cluster ({*}clusterA{*}) to the secondary cluster ({*}clusterB{*}).\r\nThe secondary cluster will not have any producers or consumers until a disaster recovery event occurs, at which point all producers and consumers will switch to it.\r\n\r\n*Setup:*\r\n * Dedicated standalone MirrorMaker 2 node\r\n * {{IdentityReplicationPolicy}} (no topic renaming)\r\n * No clients connected to secondary cluster under normal operation\r\n\r\n*MirrorMaker 2 config:*\r\n {{# Cluster aliases\r\nclusters = clusterA, clusterB\r\n\r\n# Bootstrap servers\r\nclusterA.bootstrap.servers = serverA-kafka-1:9092\r\nclusterB.bootstrap.servers = serverB-kafka-1:9092\r\n\r\n# Replication policy\r\nreplication.policy.class=org.apache.kafka.connect.mirror.IdentityReplicationPolicy\r\n\r\n# Offset/Checkpoint sync\r\nemit.checkpoints.enabled=true\r\nemit.checkpoints.interval.seconds=5\r\nsync.group.offsets.enabled=true\r\nsync.group.offsets.interval.seconds=5\r\noffset.lag.max=10\r\nrefresh.topics.interval.seconds=5}}\r\n----\r\nh3. Test results:\r\n # *Produce 300 messages when MirrorMaker is running*\r\n*Expected:* Topic offset synced within a minute\r\n*Result:* ✅ Passed\r\n\r\n # *Consume 100 messages when MirrorMaker is running, then terminate the consumer*\r\n*Expected:* Consumer offset synced\r\n*Result:* ❌ Failed — offset is not synced to clusterB\r\n\r\n # *Restart MirrorMaker after test #2*\r\n*Expected:* Consumer offset synced\r\n*Result:* ✅ Passed\r\n\r\n # *Repeat test #2 — consume 100 messages when MirrorMaker is running, then terminate the consumer*\r\n*Expected:* Consumer offset synced\r\n*Result:* ❌ Failed — offset is not synced to clusterB\r\n\r\n # *Restart MirrorMaker after test #4*\r\n*Expected:* Consumer offset synced\r\n*Result:* ❌ Failed — offset is not synced to clusterB\r\n\r\n # *Consume messages but keep consumer running*\r\n*Expected:* Offset synced\r\n*Result:* ✅ Passed\r\n\r\n----\r\nh3. Problem:\r\n\r\nConsumer offsets appear to only sync in these cases:\r\n # When MirrorMaker is restarted and the consumer o", "output": "MirrorMaker2 Offset Replication Issue"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: always disallow min.insync.replicas at the broker level\nDescription: In [https://github.com/apache/kafka/pull/17952], if ELR is enabled, we (1) disallow min.insync.replicas at the broker level; (2) automatically add min.insync.replicas at the cluster level, if not present; (3) disallow removing min.insync.replicas at the cluster level.  The reason for this is that if brokers disagree about which partitions are under min ISR, it breaks the KIP-966 replication invariants.\r\n\r\nHowever, even if ELR is not enabled, it's bad to have different min.insync.replicas on different brokers since if a leader is moved to a different broker, it will behave differently on the min.insync.replicas semantic. So, it's probably better to always enforce the above regardless whether ELR is enabled or not. Similarly, we probably want to do the same for at least unclean.leader.election.enable.\r\n\r\nSince this is a public facing change, it requires a KIP.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add --enable-native-access=ALL-UNNAMED JVM option\nDescription: We get warnings like \r\n{noformat}\r\nWARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module\r\nWARNING: Restricted methods will be blocked in a future release unless native access is enabled\r\n{noformat}\r\non JDK21+.\r\n\r\nWhile this works now, it's better to add this early, and avoid breakage later.\r\n\r\nThis also cleans up the the console output.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move DynamicTopicClusterQuotaPublisher to metadata module\nDescription: ", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: fixing a doc of from_protobuf function pyspark.\nDescription: fixing a doc of from_protobuf function pyspark.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Follow up for KIP-1188\nDescription: [KIP-1188|https://cwiki.apache.org/confluence/x/2IkvFg] deprecated the Principal connector client override policy. This policy should be removed in 5.0.0. Also the default policy should switch to Allowlist in 5.0.0.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Cleanup suppressions.xml\nDescription: Currently the rules in suppressions.xml are a bit messy and need to be cleaned up.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consumer throughput drops by 10 times with Kafka v3.9.0 in ZK mode\nDescription: Kafka consumer throughput in best-case drops by ~10 times after upgrading to kafka v3.9.0 from v3.5.1. Note that this is in ZK mode and KRAFT migration is not done yet.\nQ: Hi,\r\n\r\nUpdating here on behalf of [~gargsha] -\r\n\r\n \r\n\r\n*Topic configs :*\r\n\r\n{{cleanup.policy=compact,delete; retention.ms=1200000; min.insync.replicas=2}}\r\n\r\n*Consumer configs :*\r\n\r\n{{fetch.min.bytes=131072,fetch.max.wait.ms=500}}\r\n\r\n( acks=1 for the Producer )\r\n\r\n \r\n\r\nI have generally observed this case for when the topic has a high throughput producer(s) running on it with upwards of *590 MB/sec* ( *600 K msgs/sec* with *1000 b* message size each ). Can also simulate this case with running multiple producers on the topic to achieve close to 600 MB/sec.\r\n\r\n \r\n\r\nIn the above test scenario, the consumer throughput drops down to {*}< 90MB/sec{*}. This is for a 1 Producer + 1 Consumer test, which should be the ideal test scenario and was getting me upto *250+ MB/sec* consumer throughput in the old {*}v3.5.1 Kafka cluster{*}.\r\n\r\n \r\n\r\nMy best guess is some form of loss in *insync replicas* ( with the high throughput producers as *acks=1* ), as only the leader replica would stay in-sync in case of a high throughput on the topic. And if this can have a performance impact on the consumer in some way ?? \r\n\r\nTo reason on this I had conducted the below test ( *min.insync.replicas=2* vs *min.insync.replicas=1* ) -\r\n\r\n \r\n \r\n|*test*|*Test Name*|*Observed message rate (K msgs/sec*|*Test description*|\r\n|1| | | |\r\n|Consumer|Throughput test run ( Java client ) - Min ISR = 2 - 1 producer & 1 consumer|*{color:#de350b}89.21{color}*|Test Configs = acks:1, partitions:1, rf:3, min-isr:2, producers:1, consumers:1, batch_size:256KB|\r\n|Producer|Throughput test run ( Java client ) - Min ISR = 2 - 1 producer & 1 consumer|617.20|Test Configs = acks:1, partitions:1, rf:3, min-isr:2, producers:1, consumers:1, batch_size:256KB|\r\n|2| | | |\r\n|Consumer|Throughput test run ( Java client ) - Min ISR = 1 - 1 producer & 1 consumer|*{color:#00875a}298.99{color}*|Test Configs = acks:1, partitions:1, rf:3, min-isr:1, producers:1, consumers:1, batch_size:256KB|\r\n|Producer|Throughput test run ( Java client ) - Min ISR = 1 - 1 producer & 1 consumer|597.20|Test Configs = acks:1, partitions:1, rf:3, min-isr:1, producers:1, consumers:1, batch_size:256KB|\r\n\r\n \r\n\r\nJust to note I have also observed this heavy drop in consumer throughput in a *KRaft mode cluster ( v3.9.0 ).*\r\n\r\nLet me know if to share more details regarding the tests I conducted or any other configs, for getting to debug this.", "output": "Hi [~gargsha] , [~mjsax] , I tried reproducing this,  in 3.9.0 the consumer throughput drops a lot (~250MB/s → ~90MB/s) when {{{}min.insync.replicas=2{}}}.\r\n\r\nI noticed {{findPreferredReadReplica}} only picks from ISR. When followers drop out, consumers end up reading only from the leader, which seems to cause the slowdown.\r\n\r\nWould it be safe to let consumers read from out-of-ISR followers if they have the data (with HW checks), or is that too risky?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We create—and wait on—{{{}PollEvent{}}} in {{Consumer.poll()}} to ensure we wait for reconciliation and/or auto-commit. However, reconciliation is relatively rare, and auto-commit only happens every _N_ seconds, so the remainder of the time, we should try to avoid sending poll events.", "output": "Reduce waiting for event completion in AsyncKafkaConsumer.poll()"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Although the message key is not really {_}metadata{_}, it may be useful to include it in the _org.apache.kafka.clients.producer.RecordMetadata_ class so that metdata can be tied back to a specific message.\r\nWhen using a standard Kafka producer it is easy to tie back the metadata to a specific message by using the callback mechanism.\r\nHowever, when using Kafka streams, the only way to access the metadata and log the details is to register a stream-level _org.apache.kafka.clients.producer.ProducerInterceptor_ instance.\r\nThis mechanism has a drawback in that it is impossible to tie the RecordMetadata instance back to a particular message. \r\nIncluding the message key in the metadata would solve this problem.", "output": "Add message key to org.apache.kafka.clients.producer.RecordMetadata"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We run Kafka Streams 4.1.0 with custom processors that heavily use state store range iterators.\r\n\r\nWhile investigating disappointing performance, we found a surprising source of lock contention.\r\n\r\nOver the course of about a 1 minute profiler sample, the {{org.apache.kafka.common.metrics.Metrics}} lock is taken approximately 40,000 times and blocks threads for about 1 minute.\r\n\r\nThis appears to be because our state stores generally have no iterators open, except when their processor is processing a record, in which case it opens an iterator (taking the lock through {{OpenIterators.add}} into {{{}Metrics.registerMetric{}}}), does a tiny bit of work, and then closes the iterator (again taking the lock through {{OpenIterators.remove}} into {{{}Metrics.removeMetric{}}}).\r\n\r\nSo, stream processing threads takes a globally shared lock twice per record, for this subset of our data. I've attached a profiler thread state visualization with our findings - the red bar indicates the thread was blocked during the sample on this lock. As you can see, this lock seems to be severely hampering our performance.\r\n\r\n \r\n\r\n!image-2025-09-05-12-13-24-910.png!", "output": "Streams open iterator tracking has high contention on metrics lock"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use `4.1.0-preview3-java21-scala` image for preview examples \nDescription: \nQ: Issue resolved by pull request 413\n[https://github.com/apache/spark-kubernetes-operator/pull/413]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Develocity Gradle plugin version can't be upgraded (CI build fails with error: \"build scan failed to be published\")\nDescription: *Prologue:* this issue was discovered while working on Gradle upgrade (from 8 to 9); related JIRA tickets:\r\n * KAFKA-19174\r\n * KAFKA-19654\r\n\r\n*Environment:*\r\n * Develocity instance (version 2025.2.4): [https://develocity.apache.org/scans?search.rootProjectNames=kafka]\r\n * Gradle version 8.14.3, GitHub Actions for Gradle builds: 4.3.0\r\n * Gradle version 9.0.0, GitHub Actions for Gradle builds: 4.4.3\r\n * note: Github Actions for Gradle is using wrapper Gradle version (definition is here: .github/actions/setup-gradle/action.yml)\r\n\r\n*Scenario:*\r\n * upgrade {{com.gradle.develocity}} Gradle plugin version from a current version (3.19) to any recent version (3.19.1 and/or more recent)\r\n * GitHub Action build breaks (with this error: \"build scan failed to be published\") for both Gradle 8.14.3 and Gradle 9.0.0\r\n * (!) note for commit that contains Gradle 9.0.0: Github Actions build shows wrong Gradle version (8.14.3)\r\n\r\n*Test cases:*\r\n * test case [1]\r\n ** baseline: Gradle 8.14.3 /com.gradle.de\nQ: Solving as *_Not a Bug_* (see explanation here: [https://github.com/gradle/actions/issues/731#issuecomment-3293524417|https://github.com/gradle/actions/issues/731#issuecomment-3293524417)] by [~daz]).", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The log4j2 config file was incorrectly rotating the state-change.log to {{stage-change.log.[date]}} (changing the filename from state to stage). The below PR corrects the file name for rotated logs.\r\n\r\nAfter this change is applied, the log4j2 config will not take any actions with previously-created {{stage-change.log.[date]}} files. These may need to be manually removed by users.\r\n\r\n[https://github.com/apache/kafka/pull/20269]", "output": "Fix typo in state-change log filename after rotate"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Consistency of command-line arguments for consumer performance tests\nDescription: This implements KIP-1147 for kafka-consumer-perf-test.sh and kafka-share-consumer-perf-test.sh.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, version strings for some dependencies are hard coded in DependencyOverrides in SparkBuild, so developers need to keep consistent with version strings specified in pom.xml manually.", "output": "Retrieve dependency version from pom.xml for DependencyOverrides in SparkBuild.scala"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update to ApacheDS 2.0.0.AM27 and ldap-api 2.1.7\nDescription: ApacheDS 2.0.0AM26 (or at least its test integration) is incompatible with JDK21+.\r\n\r\nUpdate to 2.0.0.AM27 and the corresponding ldap-api version.\r\n\r\nThis also requires migrating some dependent tests from Junit 4 to Junit 5.\r\n\r\n{noformat}\r\n[ERROR] org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler -- Time elapsed: 1.426 s <<< ERROR!\r\njava.lang.NoSuchMethodError: 'void sun.security.x509.X509CertInfo.set(java.lang.String, java.lang.Object)'\r\n        at org.apache.directory.server.core.security.CertificateUtil.setInfo(CertificateUtil.java:96)\r\n        at org.apache.directory.server.core.security.CertificateUtil.generateSelfSignedCertificate(CertificateUtil.java:194)\r\n        at org.apache.directory.server.core.security.CertificateUtil.createTempKeyStore(CertificateUtil.java:337)\r\n        at org.apache.directory.server.factory.ServerAnnotationProcessor.instantiateLdapServer(ServerAnnotationProcessor.java:158)\r\n        at org.apache.directory.server.factory.ServerAnnotationProcessor.createLdapServer(ServerAnnotationProcessor.java:318)\r\n        at org.apache.directory.server.factory.ServerAnnotationProcessor.createLdapServer(ServerAnnotationProcessor.java:351)\r\n        at org.apache.directory.server.core.integ.FrameworkRunner.run(FrameworkRunner.java:139)\r\n{noformat}", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "https://github.com/advisories/GHSA-4g8c-wm8x-jfhw", "output": "upgrade to netty 4.1.118 due to CVE-2025-24970"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Theta sketch aggregate currently supports only quick select.\r\n\r\nConsumers like Iceberg{^}[1][2]{^} might benefit will benefit from the sketch aggregate if has the ability to specify `ALPHA family`\r\n\r\n[1] [Iceberg specification to use ALPHA sketches|https://iceberg.apache.org/puffin-spec/#apache-datasketches-theta-v1-blob-type]\r\n\r\n[2] [Custom implementation of theta sketch aggregates in Iceberg|https://github.com/apache/iceberg/blob/2f6e7e6371902bcb72f21deeaea8889d4768004e/spark/v3.5/spark/src/main/scala/org/apache/spark/sql/stats/ThetaSketchAgg.scala#L67] that can be replaced with Spark Theta aggregates", "output": "Support for Sketch family in ThetaSketch Aggregates"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Close stale PRs updated over 100 days ago.\nDescription: Close stale PRs on GitHub which updated over 100 days ago, base on the voting thread: https://lists.apache.org/thread/7x6c029fp9k62bxt2995dz6q6j6sg0bh", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix preservedEntries count in CopyCommitter when preserving directory attributes\nDescription: The preservedEntries count always be 0\nQ: hadoop-yetus commented on PR #7555:\nURL: https://github.com/apache/hadoop/pull/7555#issuecomment-2764577171\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  27m  3s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 154m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7555/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7555 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d27f44be1b70 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 14978fffaadabdb1b168c2e9047aacc4561c565e |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7555/1/testReport/ |\r\n   | Max. process+thread count | 700 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7555/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hemanthboyina commented on PR #7555:\nURL: https://github.com/apache/hadoop/pull/7555#issuecomment-2773385719\n\n   /build"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update Daemon to restore pre JDK22 Subject behaviour in Threads\nDescription: \nQ: Merged into HADOOP-196668", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add dependency checks for Python-related tests in the connect module\nDescription: \nQ: Issue resolved by pull request 52588\n[https://github.com/apache/spark/pull/52588]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "If there is a KS application that uses a regex source, and we create a new topic that matches that regex, but produce no messages, the application will get into an {{ERROR}} state.\r\n\r\n \r\n\r\nif we take {}[RegexSourceIntegrationTest#testRegexRecordsAreProcessedAfterNewTopicCreatedWithMultipleSubtopologies|#L206{}}}], but without producing any messages to {{TEST-TOPIC-2}} the problem will reproduce:\r\n{quote}{{org.apache.kafka.streams.errors.StreamsException: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TEST-TOPIC-2-0}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:981)}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:898)}}\r\n{{Caused by: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TEST-TOPIC-2-0}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamTask.findOffsetAndMetadata(StreamTask.java:480)}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamTask.committableOffsetsAndMetadata(StreamTask.java:511)}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:454)}}\r\n{{    at org.apache.kafka.streams.processor.internals.TaskExecutor.commitTasksAndMaybeUpdateCommittableOffsets(TaskExecutor.java:145)}}\r\n{{    at org.apache.kafka.streams.processor.internals.TaskManager.commitTasksAndMaybeUpdateCommittableOffsets(TaskManager.java:2025)}}\r\n{{    at org.apache.kafka.streams.processor.internals.TaskManager.commit(TaskManager.java:1992)}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:1836)}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:1288)}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:938)}}\r\n{{    ... 1 more}}\r\n{quote}", "output": "Error if an empty topic is created when there is a regex source KS"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Optimizing the null check logic of Lists#addAll method\nDescription: Optimizing the null check logic of Lists#addAll method\nQ: hfutatzhanghb opened a new pull request, #7361:\nURL: https://github.com/apache/hadoop/pull/7361\n\n   ### Description of PR\r\n    Optimizing the null check logic of Lists#addAll method", "output": "hadoop-yetus commented on PR #7361:\nURL: https://github.com/apache/hadoop/pull/7361#issuecomment-2639686269\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  19m 30s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  17m 25s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 15s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 30s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  18m 47s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  18m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 15s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  javac  |  17m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 58s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   5m 21s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  2s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 219m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7361/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7361 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5b41c598efe9 5.15.0-125-generic #135-Ubuntu SMP Fri Sep 27 13:53:58 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 19fa2b5bc78eff24363161d437d7ffc1ae44f9ec |\r\n   | Default Java | Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7361/1/testReport/ |\r\n   | Max. process+thread count | 586 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7361/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The current version of libxxhash - 0.8.1 isn't available on the msys repo. This is causing the Hadoop Jenkins CI for Windows to fail -\r\n\r\n{code}\r\n00:34:06  Step 25/75 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libxxhash-0.8.1-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libxxhash-0.8.1-1-x86_64.pkg.tar.zst\r\n00:34:06   ---> Running in e9a8dd91a514\r\n00:34:15  \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n{code}\r\n\r\nThus, we need to upgrade to 0.8.3.", "output": "Upgrade libxxhash to 0.8.3 in Windows 10"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: RunJar throws UnsupportedOperationException on Windows\nDescription: On Windows, run {{hadoop jar}} (with any jar). One immediately gets the following exception:\r\n\r\n{code}\r\nException in thread \"main\" java.lang.UnsupportedOperationException: 'posix:permissions' not supported as initial attribute\r\n    at java.base/sun.nio.fs.WindowsSecurityDescriptor.fromAttribute(WindowsSecurityDescriptor.java:358)\r\n    at java.base/sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:497)\r\n    at java.base/java.nio.file.Files.createDirectory(Files.java:690)\r\n    at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:135)\r\n    at java.base/java.nio.file.TempFileHelper.createTempDirectory(TempFileHelper.java:172)\r\n    at java.base/java.nio.file.Files.createTempDirectory(Files.java:966)\r\n    at org.apache.hadoop.util.RunJar.run(RunJar.java:296)\r\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:245)\r\n{code}\r\n\r\nI'm running Windows 11 with OpenJDK 11.0.26.\r\n\r\nThis bug does not exist in 3.3.6.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This just adds the RPC schema and public API changes to let the implementation progress with multiple teams.", "output": "Protocol schema and public API changes"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce Geography and Geometry physical types\nDescription: \nQ: Issue resolved by pull request 52629\n[https://github.com/apache/spark/pull/52629]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A : Add option fs.s3a.s3-acceleration.enabled to enable S3 Transfer Acceleration\nDescription: [S3 Acceleration|https://aws.amazon.com/s3/transfer-acceleration/] can be used to speed up transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects.\r\n\r\nFor more details on using S3 Acceleration, please refer to [Configuring fast, secure file transfers using Amazon S3 Transfer Acceleration|https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html].", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: remove the unnecessary copy from AbstractFetch#fetchablePartitions \nDescription: see https://github.com/apache/kafka/pull/14359#issuecomment-3372367774", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Revert KIP-939 API's and Client Code for 4.2\nDescription: KIP-939 has been paused but some of the client code and new API's have already been merged to trunk. We need to revert the changes in the 4.2 release branch.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add JDK 24 to Ubuntu 20.04 docker development images\nDescription: The first step to supporting JDK23/24 is being able to test it.\r\n\r\nAdd JDK24 to the default docker images, so that both manual and automated testing is possible.\nQ: hadoop-yetus commented on PR #7495:\nURL: https://github.com/apache/hadoop/pull/7495#issuecomment-2714712545\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  49m 30s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 10s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  44m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 34s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 135m 21s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7495/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7495 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 9d93aa5da0b8 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4e547793e29c1894fd6a830a8a0a29300671f58b |\r\n   | Max. process+thread count | 534 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7495/1/console |\r\n   | versions | git=2.9.5 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7495:\nURL: https://github.com/apache/hadoop/pull/7495#issuecomment-2715086169\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  26m 40s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 42s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 41s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 47s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 106m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7495/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7495 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 0354c9145372 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4e547793e29c1894fd6a830a8a0a29300671f58b |\r\n   | Max. process+thread count | 528 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7495/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)\nDescription: The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone.\r\n\r\nAfter the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will take time, and during this period, other jobs are still catching up with the new realm configuration. However, the namenode down disrupts all of them.\r\n\r\n \r\n{code:java}\r\nERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception\r\njava.lang.IllegalArgumentException: Illegal principal name hive/xxxxx@yyyy.COM\r\norg.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM         at org.apache.hadoop.security.User.(User.java:51)\r\n        at org.apache.hadoop.\nQ: looks a duplicate of HDFS-17138.\r\n\r\n[~kpalanisamy] please set the hadoop version you saw it with. If it is a version without HDFS-17138 -please upgrade.\r\n\r\nclosing as a duplicate. If it surfaces on branches with HDFS-17138, re-open", "output": "You’re right [~stevel@apache.org]. My user version is 3.1.1, so it is missing this HDFS-17138 fix, which clearly addresses my user scenarios. Thanks for checking so quickly. I should have checked it, but unfortunately I taken your time :)"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: The Content-Security-Policy header must not be overridden\nDescription: [https://github.com/apache/kafka-site/blob/905299a0b7e2e3892d9493c7dcaaf78dce035c00/.htaccess#L13]\r\n\r\nThe Content-Security-Policy header must not be overridden.\r\n\r\nThere is now a standard way to add local exceptions to the CSP:\r\n\r\n[https://infra.apache.org/tools/csp.html]\r\n\r\nPlease update the .htaccess file accordingly.\r\n\r\nPlease note that the policy says that any exceptions must have been approved, and a reference to the approval must be commented in the .htaccess file", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce client-side Geography and Geometry classes\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52804.", "output": "Issue resolved by pull request 52804\n[https://github.com/apache/spark/pull/52804]"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: AAL - Add support for stream leak detection\nDescription: AAL currently does not support leak detection. \r\n\r\nIt may not require this, as individual streams do not hold only to any connections/resources, the factory does. We should verify if it's required, and if yes, add support.\nQ: ...although we have stream leak detection, do you mean extra stuff for the actual factory?\r\n\r\nthe key leak problems we have hit so far are\r\n* libraries/apps forgetting to close() streams.\r\n* code which builds a large list of streams and only calls close() on them at the end of a larger piece of work. (iceberg did this at some point).\r\n\r\nthe stream factory has its lifecycle tied to that of the store, so provided the fs closes that, it will release all its resources. and as the s3 clients are released at the same time, their pools close", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0\nDescription: *  kafka server version is 2.5.1\r\n *  kafka-client version bigger than 3.1.1 \r\n\r\n \r\n{code:java}\r\nimport org.apache.kafka.clients.producer.KafkaProducer;\r\nimport org.apache.kafka.clients.producer.Producer;\r\nimport org.apache.kafka.clients.producer.ProducerRecord;\r\n\r\nimport java.util.Properties;\r\n\r\n\r\npublic class KafkaSendTest {\r\n\r\n  public static void main(String[] args) {\r\n    Properties props = new Properties();\r\n    props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\");\r\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"transaction.timeout.ms\", \"300000\");\r\n    props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully\r\n    props.put(\"compression.type\", \"lz4\");\r\n    props.put(\"security.protocol\", \"SASL_PLAINTEXT\");\r\n    props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\");\r\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"xxx\\\" password=\\\"xxxx\\\";\");\r\n\r\n    Producer producer = new KafkaProducer<>(props);\r\n\r\n    try {\r\n      String topic = \"topic1\";\r\n      byte[] value = new byte[]{1,2}; \r\n      ProducerRecord record = new ProducerRecord<>(topic, null, value);\r\n      producer.send(record, (metadata, exception) -> {\r\n        if (exception == null) {\r\n          System.out.printf(\"Sent record(key=%s value=%s) meta(partition=%d, offset=%d)\\n\",\r\n               ", "output": "Closed"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h1. Context\r\n\r\nWe are using Kafka v3.7.1 with Zookeeper, our brokers are configured with multiple disks in a JBOD setup, routine intra-broker data rebalancing is performed using Cruise Control to manage disk utilization. During these rebalance operations, a race condition between a log segment flush operation and the file deletion that is part of the replica's directory move. This race leads to a `NoSuchFileException` when the flush operation targets a file path that has just been deleted by the rebalance process. This exception incorrectly forces the broker to take the entire log directory offline.\r\nh1. Logs / Stack trace\r\n{code:java}\r\n2025-07-23 19:03:30,114 WARN Failed to flush file /var/lib/kafka-08/topic_01-12/00000000024420850595.snapshot (org.apache.kafka.\r\ncommon.utils.Utils)\r\njava.nio.file.NoSuchFileException: /var/lib/kafka-08/topic_01-12/00000000024420850595.snapshot\r\n        at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\r\n        at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\r\n        at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\r\n        at java.base/sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:182)\r\n        at java.base/java.nio.channels.FileChannel.open(FileChannel.java:292)\r\n        at java.base/java.nio.channels.FileChannel.open(FileChannel.java:345)\r\n        at org.apache.kafka.common.utils.Utils.flushFileIfExists(Utils.java:1029)\r\n        at kafka.log.UnifiedLog.$anonfun$flushProducerStateSnapshot$2(UnifiedLog.scala:1766)\r\n        at kafka.log.UnifiedLog.flushProducerStateSnapshot(UnifiedLog.scala:1915)\r\n        at kafka.log.UnifiedLog.$anonfun$roll$2(UnifiedLog.scala:1679)\r\n        at java.base/java.util.Optional.ifPresent(Optional.java:183)\r\n        at kafka.log.UnifiedLog.$anonfun$roll$1(UnifiedLog.scala:1679)\r\n        at org.apache.kafka.server.util.KafkaScheduler.lambda$schedule$1(KafkaScheduler.java:15", "output": "Race condition between log segment flush and file deletion causing log dir to go offline"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Reorder Gradle tasks (in order to bump Shadow plugin version)\nDescription: *Prologue:*\r\n * JIRA ticket: KAFKA-19174\r\n * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027]\r\n\r\n*Scenario:*\r\n * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+\r\n * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x)\r\n\r\n*Action points (what needs to be done):*\r\n * use `com.gradleup.shadow` recent version (9+)\r\n * reorder Gradle tasks so that Gradle command mentioned above can work\r\n\r\n*Definition of done (at the minimum):*\r\n * Gradle command mentioned above works as expected\r\n * also: *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc): [https://kafka.apache.org/quickstart]\nQ: Hi Dejan, Can I pick this up. I am new to open source. I think this could be a good one to start with.", "output": "Hi [~nish4_nth] \r\n\r\nAlthough I don't actually decide who can work on this (because I'm not a maintainer but only a contributor) I guess you are free to start if you want to contribute :)"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fully Support Java 23, 24 and 25\nDescription: Hadoop trunk today mostly supports JDK17, but doesn't work at all on JDK23. (and conversely on JDK24 to be released in less than two weeks)\r\n\r\nWhile there are many smaller issues, the major breaking change is the SecurityManager removal (JEP411/486), and its many consequences.\r\n\r\nThe obvious change is that Subjec.doAs() and Subject.current() no longer work by default, and the replacement APIs must be used.\r\n\r\nThe more insidius change is that when SecurityManager is disabled then JDK22+ does not propapage the Subject to new Threads, which is something that Hadoop absolutely relies on.\r\n\r\n\r\nNote that Hadoop is always built with with JDK 17  (if the JDK is 17 or newer), unless the target version is specifically overriden.\r\nThis is not a problem, JDK17 class files running on a JDK 24 JVM is the expected use case for binary distributions.\r\n\r\nWe may want to run some tests where Hadoop is also compiled for the lastest JVM later. (taget Java 24 + JVM 24)", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add a missing test for ContinuousMemorySink\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Allow to configure custom `ReplicaPlacer` implementation\nDescription: Replica assignment is a complex issue, as it depends on how a kafka cluster is run, maintained, and used. KAFKA-19507 aims to enhance the default assignment policy, and in my opinion, the best approach is to make the system flexible enough to allow users to customize the policy according to their specific needs\nQ: We could make it pluggable by exposing the internal config at the very least, if there are concerns or negative experiences with exposing such internal functionality", "output": "That might be a bit workaround, but it is a way to allow users to customize without imposing strict compatibility constraints"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Automatic Snapshot Repair for State store\nDescription: Today, the engine currently treats both the changelog and snapshot files with the same importance, so when a state store needs to be loaded it reads the latest snapshot and applies the subsequent changes on it. If the snapshot is bad or corrupted, then the query will fail and be completely down and blocked, needing manual intervention. This leads to user clearing their query checkpoint and having to do full recomputation.\r\n\r\nThis shouldn’t be the case. The changelog should be treated as the “source of truth” and the snapshot is just a disposable materialization of the log.\r\n\r\nIntroducing Automatic snapshot repair, which will automatically repair the checkpoint by skipping bad snapshots and rebuilding the current state from the last good snapshot (works even if there’s none) and applying the changelogs on it. This eliminates the need for manual intervention and unblocks the pipeline to keep it running.\r\n\r\nAlso emit metrics about number of state stores that were auto repaired in a given batch, so that you can build alert and dashboard for it.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip doctest `pyspark.sql.pandas.functions` without pyarrow/pandas\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob] Empty Page Issue on Subsequent ListBlob call\nDescription: We came across a new behavior from server where ListBlob call can return empty list even after returning a next marker(continuation token) from previous list call.\r\n\r\nThis is to handle that case and do not infer listing to be incomplete.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Refactor ParameterizedQuery arguments validation\nDescription: * In this issue I propose to refactor ParameterizedQuery arguments validation to ParameterizedQueryArgumentsValidator so it can be reused between single-pass and fixed-point analyzer implementations. We also remove one redundant case from the ParameterizedQueryArgumentsValidator.isNotAllowed (Alias shouldn't call isNotAllowed again as it introduces unnecessary overhead to the method) to improve performance.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Support for S3express Access Points\nDescription: * Currently, the endpoint for using S3 accesspoint is resolved for S3 and S3 on outposts: https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/ArnResource.java#L29-L30. However, for s3express, the endpoint is \"s3express-..amazonaws.com\". (https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-az-networking.html). This ticket adds support for s3express endpoint.\r\n\r\n* Additionally, for access objects using s3express in the `S3AFileSystem`, we need to parse the access point ARN for an S3express access point into the access point name, since ARNs aren't supported on S3express other than in IAM policies. (https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-directory-buckets-restrictions-limitations-naming-rules.html). This ticket also addresses this change.\r\n\r\n* This ticket also updates the SDK version used by the changes to `2.31.12`, which is the version (at minimum) needed to use s3express access points. It does not do the SDK upgrade, but only the version of the dependency.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: The default value of io.compression.codec.lzo.buffersize should not be in the core-site-default.xml file.\nDescription: The default value of io.compression.codec.lzo.buffersize should not be in the core-site-default.xml file.\r\n\r\nThe io.compression.codec.lzo.buffersize configuration file is the core configuration file of LZO. The default value is 245 MB. The default value should be controlled by ZLO and should not be contained in the core-site file.\r\n\r\n \r\n\r\n!image-2025-05-12-16-21-43-114.png!", "output": "Patch Available"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Task:* upgrade checkstyle version from *10.20.2* to *12.0.1*\r\n\r\n*Related link:* [https://checkstyle.org/releasenotes.html#Release_12.0.1]\r\n\r\n *Note:* difference between versions is quite big:\r\n * version *10.20.2* (published in Novemeber 2024)\r\n * version *12.0.1* (published in Octoober 2025)\r\n * git diff between versions: [https://github.com/checkstyle/checkstyle/compare/checkstyle-10.20.2...checkstyle-12.0.1]", "output": "CheckStyle version upgrade: 10 -->> 12"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Kafka Source RTM support\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve perfomance Authorizer.authorizeByResourceType by using Prefix Trie\nDescription: Class Authorizer.authorizeByResourceType under the hood uses HashMap for \r\ncheck if the caller is authorized to perform the given ACL operation on at least one resource of the given type.\r\n\r\nIt check each character in allowPatterns and pass to deny patterns\r\n\r\n{code:java}\r\n// For any literal allowed, if there's no dominant literal and prefix denied, return allow.\r\n        // For any prefix allowed, if there's no dominant prefix denied, return allow.\r\n        for (Map.Entry> entry : allowPatterns.entrySet()) {\r\n            for (String allowStr : entry.getValue()) {\r\n                if (entry.getKey() == PatternType.LITERAL\r\n                        && denyPatterns.get(PatternType.LITERAL).contains(allowStr))\r\n                    continue;\r\n                StringBuilder sb = new StringBuilder();\r\n                boolean hasDominatedDeny = false;\r\n                for (char ch : allowStr.toCharArray()) {\r\n                    sb.append(ch);\r\n                    if (denyPatterns.get(PatternType.PREFIXED).contains(sb.toString())) {\r\n                        hasDominatedDeny = true;\r\n                        break;\r\n                    }\r\n                }\r\n                if (!hasDominatedDeny)\r\n                    return AuthorizationResult.ALLOWED;\r\n            }\r\n        }\r\n{code}\r\n\r\nTo improve performance better use Prefix Tree", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: MergeScalarSubqueries code cleanup\nDescription: \nQ: Issue resolved by pull request 52763\n[https://github.com/apache/spark/pull/52763]", "output": "I collected this as a subtask of SPARK-51166 in order to improve the visibility of this improvement."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Abfs: Fix Case Sensitivity Issue for hdi_isfolder metadata\nDescription: In the blob endpoint, we determine whether the path is a file, or a directory based on the metadata attribute hdi_isfolder. When creating a directory, we set hdi_isfolder to true. Currently, our method for checking if the path is a directory involves a case-sensitive equality check. Consequently, if someone configures a directory with Hdi_isfolder, the driver will not recognize that path as a directory. We need to address this issue because, in the backend, hdi_isfolder and Hdi_isfolder are considered the same metadata attribute. Therefore, the solution involves modifying our equality check to be case-insensitive, ensuring that the driver correctly identifies directories regardless of case variations in the metadata attribute.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: The `version-mapping` of kafka-features.sh should work without requiring the bootstrap\nDescription: It shows the following error.\r\n{code:java}\r\nchia7712@fedora:~/project/kafka$ ./bin/kafka-features.sh version-mapping\r\nusage: kafka-features [-h] [--command-config COMMAND_CONFIG] (--bootstrap-server BOOTSTRAP_SERVER | --bootstrap-controller BOOTSTRAP_CONTROLLER) {describe,upgrade,downgrade,disable,version-mapping,feature-dependencies} ...\r\nkafka-features: error: one of the arguments --bootstrap-server --bootstrap-controller is required\r\n{code}\r\n\r\nBy contrast, storage tool works well\r\n\r\n{code:java}\r\nchia7712@fedora:~/project/kafka$ ./bin/kafka-storage.sh version-mapping\r\nmetadata.version=27 (4.1-IV1)\r\nkraft.version=1\r\ntransaction.version=2\r\ngroup.version=1\r\neligible.leader.replicas.version=1\r\nshare.version=0\r\nstreams.version=0\r\n\r\n{code}", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Bump netty to 4.1.127 due to CVE-2025-58057\nDescription: https://www.cve.org/CVERecord?id=CVE-2025-58057\r\n\r\nfixed in 4.1.125 but no harm upgrading to latest", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Structured JSON output for Spark SQL metadata commands\nDescription: {{`DESCRIBE AS JSON`}} is a Spark SQL syntax added in Spark 4.0.0 that returns table metadata as structured JSON.\r\nSee SPARK-50541 for more information.\r\n\r\nThis umbrella JIRA tracks ongoing work to provide structured JSON output for additional Spark SQL metadata inspection commands.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Netty to 4.2.7.Final\nDescription: \nQ: Issue resolved by pull request 52695\n[https://github.com/apache/spark/pull/52695]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: stream write/close fails badly once FS is closed\nDescription: when closing a process during a large upload, and NPE is triggered in the abort call. This is because the S3 client has already been released.\r\n\r\n{code}\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$abortMultipartUpload$41(S3AFileSystem.java:5337)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.abortMultipartUpload(S3AFileSystem.java:5336)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$abortMultipartUpload$4(WriteOperationHelper.java:392)\r\n{code}\r\n\r\n* close() in small writes also fails, just with a different exception\r\n* and on some large writes, the output stream hangs as it awaits the end of the queued writes. This is a problem inside the semaphore executor", "output": "In Progress"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix TestFsShellList.testList on Windows\nDescription: * The test *org.apache.hadoop.fs.TestFsShellList#testList* creates files with special characters and tries to list them using *ls*.\r\n* Filenames with special characters aren't allowed in Windows.\r\n* Thus, we need to modify the test to only test for non-special characters on Windows and include the filenames with special characters on non-Windows environments.\nQ: GauthamBanasandra merged PR #7771:\nURL: https://github.com/apache/hadoop/pull/7771", "output": "Merged PR to trunk - https://github.com/apache/hadoop/pull/7771."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The consumer provides metrics for execution of the consumer rebalance listener:\r\n * Consumer PartitionsLost Latency\r\n\r\n * Consumer PartitionsAssigned Latency\r\n\r\n * Consumer PartitionsRevoked Latency\r\n\r\nIt would be useful for the StreamsRebalanceListener to replicate semilar metrics for TasksLost, TasksAssigned and TasksRevoked.", "output": "Add metrics corresponding to consumer rebalance listener metrics"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Wrapping StateStore that implements legacy {{.checkpoint}} behaviour, moving its logic out of the {{ProcessorStateManager.}}", "output": "Add LegacyCheckpointingStateStore"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove waiting for event completion in AsyncKafkaConsumer.pollForRecords()\nDescription: We create—and wait on—{{{}CreateFetchRequestEvent{}}}s in {{{}AsyncKafkaConsumer.pollForRecords(){}}}. The inter-thread communication involved in sending the event, notifying the event future, and blocking on the event future results in high CPU usage. Investigate if it is possible to reduce/eliminate the need for the event future.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: The `version-mapping` of kafka-features.sh should work without requiring the bootstrap\nDescription: It shows the following error.\r\n{code:java}\r\nchia7712@fedora:~/project/kafka$ ./bin/kafka-features.sh version-mapping\r\nusage: kafka-features [-h] [--command-config COMMAND_CONFIG] (--bootstrap-server BOOTSTRAP_SERVER | --bootstrap-controller BOOTSTRAP_CONTROLLER) {describe,upgrade,downgrade,disable,version-mapping,feature-dependencies} ...\r\nkafka-features: error: one of the arguments --bootstrap-server --bootstrap-controller is required\r\n{code}\r\n\r\nBy contrast, storage tool works well\r\n\r\n{code:java}\r\nchia7712@fedora:~/project/kafka$ ./bin/kafka-storage.sh version-mapping\r\nmetadata.version=27 (4.1-IV1)\r\nkraft.version=1\r\ntransaction.version=2\r\ngroup.version=1\r\neligible.leader.replicas.version=1\r\nshare.version=0\r\nstreams.version=0\r\n\r\n{code}\nQ: related discussion: https://github.com/apache/kafka/pull/20248#discussion_r2395108942", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The kafka-share-groups.sh tool can be used to reset the start offset for consumption if the share group is empty. One use for this is to initialise the start offset before starting to use the share group. Currently, the tool only lets the user reset the start offset for a topic which already has offsets in the share group. Instead, it should permit setting the SPSO for any topic, subject to auth checking.\r\n\r\nThis brings the tool's behaviour in line with kafka-consumer-groups.sh.", "output": "Support resetting offsets in kafka-share-groups.sh for topics which are not currently subscribed"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add SubjectInheritingThread to restore pre JDK22 Subject behaviour in Threads\nDescription: This is the first part of HADOOP-19574 which adds the compatibility code, and fixes Daemon, but does not replace native Thread instances.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Extract common Generate resolution logic\nDescription: I'm planning to add Generate node resolution to the single-pass analyzer. Before that, I need to do minor refactoring to extract common logic from ResolveGenerate rule.\nQ: Issue resolved by pull request 52571\n[https://github.com/apache/spark/pull/52571]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip tests that depend on SecurityManager if the JVM does not support it\nDescription: Due to JEP411, depending on the Java version, SecurityManager either has to be explicitly enabled, or is completely disabled.\nQ: stoty opened a new pull request, #7507:\nURL: https://github.com/apache/hadoop/pull/7507\n\n   ### Description of PR\r\n   \r\n   Due to JEP411 / JEP486 SecurityManager is either completely removed or has to be explicitly re-enabled in newer Java versions.\r\n   \r\n   This patch adds assume() methods to those tests, so that they are skipped instead of erroring out when the\r\n   JVM does not let them set the SecurityManager.\r\n   \r\n   A few tests have already been converted not to require SecurityManager, but the SecurityManager setting code\r\n   has not been removed. In those cases I have simply removed the SecurityManager-related calls.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   This was tested in my JDK23 branch with JDK23, and the standard CI checks for older Javas.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7507:\nURL: https://github.com/apache/hadoop/pull/7507#issuecomment-2726031007\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 30s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  7s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  31m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 38s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 41s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   6m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 36s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 57s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   9m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   3m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 52s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 43s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m  8s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/2/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 2 new + 160 unchanged - 2 fixed = 162 total (was 162)  |\r\n   | +1 :green_heart: |  mvnsite  |   6m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   6m  4s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |  11m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 31s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   1m 53s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 111m 34s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n   | -1 :x: |  unit  |  27m 35s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt) |  hadoop-yarn-server-nodemanager in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 129m 22s |  |  hadoop-mapreduce-client-jobclient in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  26m  3s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  17m  3s |  |  hadoop-gridmix in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 14s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 555m 43s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7507 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a16c748b3d8d 5.15.0-134-generic #145-Ubuntu SMP Wed Feb 12 20:08:39 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ef27d7d0ffcc1186408ffd5ecb652f330022dea2 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/2/testReport/ |\r\n   | Max. process+thread count | 1248 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-gridmix U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The commit on trunk {{60c7d4fea010}} and on branch 3.4 {{f3ec55b}} fixes the logging, but does not\r\naddress the underlying issue.\r\n\r\nh2. problem\r\n\r\nDuring PUT calls, even of 0 byte objects, our UploadContentProver is reporting a recreation of \r\nof the input stream of an UploadContentProvider, as seen by our logging at info of this happening\r\n\r\n{code}\r\n bin/hadoop fs -touchz $v3/4\r\n2025-03-26 13:38:53,377 [main] INFO  impl.UploadContentProviders (UploadContentProviders.java:newStream(289)) - Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-stevel/s3a/s3ablock-0001-659277820991634509.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-03-26T13:38:53.355, streamCreationCount=2, currentStream=null}\r\n{code}\r\n\r\nThis code was added in HADOOP-19221, S3A: Unable to recover from failure of multipart block upload attempt \"Status Code: 400; Error Code: RequestTimeout\"; it logs at INFO because it is considered both rare and serious enough that we should log it, based on our hypothesis that it was triggered by a transient failure of the S3 service front and and the inability of the SDK to recover from it\r\n\r\nIt turns out that uploading even a zero byte file to S3 triggers the dual creation of the stream, apparently from a dual signing.\r\n\r\nThis *does not* happen on multipart uploads.", "output": "S3A: SDK reads content twice during PUT to S3 Express store."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add `build_python_3.11_macos26.yml` GitHub Action job\nDescription: \nQ: Issue resolved by pull request 52740\n[https://github.com/apache/spark/pull/52740]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This implements KIP-1147 for kafka-cluster.sh, kafka-leader-election.sh and kafka-streams-application-reset.sh.", "output": "Consistency of command-line arguments for remaining CLI tools"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Set charsetEncoder in HadoopArchiveLogs\nDescription: The test for it fails in CI on Centos 7, likely because it has different default charsets than the other CI environments.\r\n{noformat}\r\n[ERROR] org.apache.hadoop.tools.TestHadoopArchiveLogs.testGenerateScript  Time elapsed: 0.036 s  <<< ERROR!\r\njava.lang.IllegalStateException: Mismatched Charset(UTF-8) and CharsetEncoder(US-ASCII)\r\n\tat org.apache.commons.io.output.FileWriterWithEncoding$Builder.get(FileWriterWithEncoding.java:113)\r\n\tat org.apache.hadoop.tools.HadoopArchiveLogs.generateScript(HadoopArchiveLogs.java:512)\r\n\tat org.apache.hadoop.tools.TestHadoopArchiveLogs._testGenerateScript(TestHadoopArchiveLogs.java:280)\r\n\tat org.apache.hadoop.tools.TestHadoopArchiveLogs.testGenerateScript(TestHadoopArchiveLogs.java:248)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)\r\n\tat org.junit.inter", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title:  Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c\nDescription: ## Description\r\n \r\nThere is a duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c` at lines 29-30.\r\n\r\n## Current Code\r\n```c\r\n#include \"gcc_optimizations.h\"\r\n#include \"gcc_optimizations.h\"\r\n```\nQ: PeterPtroc opened a new pull request, #7899:\nURL: https://github.com/apache/hadoop/pull/7899\n\n   …_crc32_x86.c\r\n   \r\n   Removes duplicate #include \"gcc_optimizations.h\" statement.\r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Removes duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c`.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - [x] Code compiles without issues\r\n   - [x] No functional changes expected\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7899:\nURL: https://github.com/apache/hadoop/pull/7899#issuecomment-3224065363\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  28m 36s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 57s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 50s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 107m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 54s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 43s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 14s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 223m 13s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7899 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux e8837fccd16c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c1d9fb6a820785db99554892fe312f5135201bf3 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/testReport/ |\r\n   | Max. process+thread count | 1253 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: testIfMatchOverwriteWithOutdatedEtag() fails when not using SSE-KMS\nDescription: ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteWithOutdatedEtag() fails when no encryption method is set. \r\n \r\nThis is because it does  \r\ncreateFileWithFlags(fs, path, SMALL_FILE_BYTES, true, null);\r\n \r\nand then to overwrite the file, also does\r\n \r\ncreateFileWithFlags(fs, path, SMALL_FILE_BYTES, true, null);\r\n \r\nWhen no encryption is used, the eTAG is the md5 of the object, and so will always be the same, and won't result in the 412 conditional write failure. \r\n \r\nTest passes when using SSE-KMS, as when using encryption, eTag is no longer the md5 of the object content, and changes on every write. \r\n \r\n \r\nFix is simple enough, change the object content on the second write.\nQ: I'll get a PR out for this today", "output": "ahmarsuhail opened a new pull request, #7816:\nURL: https://github.com/apache/hadoop/pull/7816\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   We were previously creating second file with the same object content, so the etag will always be the same, leading to test failures, except when testing with an encryption method set, as in that case eTag is no longer the md5. \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Can't test on trunk as the all tests in this class fail due to the Junit5 migration. Will test on 3.4."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h3. Problem Summary\r\n\r\nMy Kafka Streams application cannot move its {{state_store}} from {{STARTING}} to {{{}RUNNING{}}}.\r\n\r\nI'm using a *Strimzi Kafka cluster* with:\r\n * 3 *controller nodes*\r\n\r\n * 4 *broker nodes*\r\n\r\nh3. Observations\r\nh4. Partition {{__consumer_offsets-35}} is {*}stuck{*}.\r\n\r\nFrom AKHQ, partition details:\r\n * *Broker 10* is the *leader* of {{__consumer_offsets-35}}\r\n\r\n * There are *no interesting logs* on broker 10\r\n\r\n * However, logs are *spamming every 10ms* from broker 11 (a {*}replica{*}):\r\n\r\n2025-08-11 04:05:50 INFO  [TxnMarkerSenderThread-11] TransactionMarkerRequestCompletionHandler:66 \r\n[Transaction Marker Request Completion Handler 10]: Sending irm_r_sbm2_web-backend-web-1-6cad35c7-2be9-4ed2-9849-9c059cc8c409-4's transaction marker for partition __consumer_offsets-35 has failed with error org.apache.kafka.common.errors.NotLeaderOrFollowerException, retrying with current coordinator epoch 38\r\nh4. Brokers 20 and 21 — neither leaders nor replicas — also spamming the same error:\r\n\r\n*Broker 20:*\r\n2025-08-11 04:39:45 INFO  [TxnMarkerSenderThread-20] TransactionMarkerRequestCompletionHandler:66 \r\nSending irm_r_sbm2_web-backend-web-1-6cad35c7-2be9-4ed2-9849-9c059cc8c409-3's transaction marker for partition __consumer_offsets-35 has failed with error org.apache.kafka.common.errors.NotLeaderOrFollowerException, retrying with current coordinator epoch 54\r\n \r\n*Broker 21:*\r\n2025-08-11 04:39:58 INFO  [TxnMarkerSenderThread-21] TransactionMarkerRequestCompletionHandler:66 \r\nSending irm_r_sbm2_web-backend-web-1-6cad35c7-2be9-4ed2-9849-9c059cc8c409-2's transaction marker for partition __consumer_offsets-35 has failed with error org.apache.kafka.common.errors.NotLeaderOrFollowerException, retrying with current coordinator epoch 28\r\n \r\n----\r\nh3. Kafka Streams App Behavior\r\n\r\nLogs from the Kafka Streams app (at debug level) repeat continuously. The {{state_store}} *never transitions* from {{STARTING}} to {{{}RUNNING{}}}.\r\n\r\nKey repeated logs (debug log level)", "output": "Stuck __consumer_offsets partition (kafka streams app)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Implement Geography and Geometry accessors across Catalyst\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52723.", "output": "Issue resolved by pull request 52723\n[https://github.com/apache/spark/pull/52723]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Set `io.netty.noUnsafe` to `true` to avoid JEP-498 warnings\nDescription: *BEFORE*\r\n{code}\r\nStarting Operator...\r\nWARNING: A terminally deprecated method in sun.misc.Unsafe has been called\r\nWARNING: sun.misc.Unsafe::allocateMemory has been called by io.netty.util.internal.PlatformDependent0$2 (file:/opt/spark-operator/operator/spark-kubernetes-operator.jar)\r\nWARNING: Please consider reporting this to the maintainers of class io.netty.util.internal.PlatformDependent0$2\r\nWARNING: sun.misc.Unsafe::allocateMemory will be removed in a future release\r\n25/10/09 03:35:46 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:35:46 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.\r\n{code}\r\n\r\n*AFTER*\r\n{code}\r\nStarting Operator...\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.                                                                    │\r\n{code}", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Log warning message on every set/get of a deprecated configuration property\nDescription: Currently, the warning message is logged once (or at most twice after HADOOP-8865) when we first use (set/get) a deprecated configuration key and most of the time this happens early on during system startup. \r\n\r\nUsers tend to set/get properties on their job scripts/applications  that keep running on a hourly/daily/etc basis. When a problem comes up the user will package the latest logs and send them to us (developers/support) for further analysis and troubleshooting. However, it's very likely that these logs will not contain information about the deprecated usages which might be crucial for advancing the investigation.\r\n\r\nOn the other end, a warning that appears just once is not that worrisome so even if the users/customers/developers see it, they can easily ignore it, and move on, thinking that there is no action needed on their end.\r\n\r\nThe above scenarios are based on applications such as the Hivemetastore, HiveServer2, which use the Hadoop Configuration, and usually run for weeks/mo\nQ: Please let me know if you agree with the proposal and I can create a PR.", "output": "zabetak commented on PR #7766:\nURL: https://github.com/apache/hadoop/pull/7766#issuecomment-3022702343\n\n   Thanks for the review @slfan1989 ! I fixed the SpotBug warning in https://github.com/apache/hadoop/pull/7766/commits/7412e89112970f53a1682452d6a771703fcd022c."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions\nDescription: There is a bug in the broker logic of splitting bytes in {{PartitionMaxBytesStrategy.java}} due to which we are setting {{partitionMaxBytes}} as 0 in case requestMaxBytes is lesser than \r\nacquiredPartitionsSize", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Metrics from FetchMetricsManager containing a topic tag are duplicated\nDescription: Hello,\r\n\r\nSince Kafka 4.1.0, we're experiencing an issue with Kafka Streams metrics for topics containing dots in their names. (I think the problem is also for simple usage of consumers not only kstream)\r\n\r\nIn FetchMetricsManager, methods like {{recordRecordsFetched()}} now create duplicate sensors for the same topic if they contain dot in their name: \r\n{code:java}\r\nvoid recordRecordsFetched(String topic, int records) { \r\n\r\n\r\n String name = topicRecordsFetchedMetricName(topic); \r\n\r\n\r\n maybeRecordDeprecatedRecordsFetched(name, topic, records);  Map.of(\"topic\", topic)) \r\n\r\n\r\n .withAvg(metricsRegistry.topicRecordsPerRequestAvg) \r\n\r\n\r\n .withMeter(metricsRegistry.topicRecordsConsumedRate, metricsRegistry.topicRecordsConsumedTotal) \r\n\r\n\r\n .build(); \r\n\r\n\r\n recordsFetched.record(records); \r\n\r\n\r\n }  {code}\r\nIt currently record two sensors, one with my original topic name, one time with a topic name with dots replaced by underscore. \r\n\r\n-While we can work around this by reversing the transformat\nQ: [~chosante] Thanks for the report. Out of curiosity, why are the underscores being replaced with dots? I would have assumed you could simply focus on the new sensor and ignore the deprecated one", "output": "Hey you are right, it is not clear, but in our case if we put the same name and tags, under the hood they are considered duplicates when registered and one of them will be discarded. (like we want)\r\n\r\n \r\n\r\nEDIT: after doing tests, what's above does not work, the metrics are merged instead of being deduplicated and therefore each increment of one of them is incrementing the same counter"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update the command usage of NNThroughputBenchmark by adding the \"-blockSize\" option.\nDescription: In HDFS-15652, make block size from NNThroughputBenchmark configurable. Benchmarking.md should also be updated.\nQ: fuchaohong opened a new pull request, #7711:\nURL: https://github.com/apache/hadoop/pull/7711\n\n   In [HDFS-15652](https://issues.apache.org/jira/browse/HDFS-15652), make block size from NNThroughputBenchmark configurable. Benchmarking.md should also be updated.", "output": "hadoop-yetus commented on PR #7711:\nURL: https://github.com/apache/hadoop/pull/7711#issuecomment-2909065569\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  73m 14s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7711/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7711 |\r\n   | JIRA Issue | HADOOP-19579 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint |\r\n   | uname | Linux 4bf23d90b0e0 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 309232f8b4f786afb79a08f6620e5a09fb38cc3f |\r\n   | Max. process+thread count | 676 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7711/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix DSv2 in PushVariantIntoScan by adding SupportsPushDownVariants\nDescription: This goes to add DSv2 support to the optimization rule PushVariantIntoScan. The PushVariantIntoScan rule only supports DSv1 Parquet (ParquetFileFormat) source. It limits the effectiveness of variant type usage on DSv2.\nQ: Issue resolved by pull request 52578\n[https://github.com/apache/spark/pull/52578]", "output": "[~dongjoon] Thank you for updating this ticket."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade libopenssl to 3.1.4 for rsync on Windows\nDescription: * We're currently using libopenssl 3.1.2 which\r\n  is needed for rsync 3.2.7 on Windows for the\r\n  Yetus build validation.\r\n* However, libopenssl 3.1.2 is no longer\r\n  available for download on the msys2 site.\r\n* This PR upgrades libopenssl to the next\r\n  available version - 3.1.4 to mitigate this\r\n  issue.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `Gradle` to 9.2.0\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "During testing, an artifact of the new rebalance protocol showed up. In some cases, the first joining member gets all active tasks assigned, and is slow to revoke the tasks after more member has joined the group. This affects in particular cases where the first member is slow (possibly overloaded in the case of cloudlimits benchmarks) and there are a lot of tasks to be assigned.\r\n\r\nTo help with this situation, we want to introduce a new group-specific configuration to delay the initial rebalance.", "output": "Implement group-level initial rebalance delay"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce Geometry and Geography in-memory wrapper formats\nDescription: \nQ: Issue is resolved by: [https://github.com/apache/spark/pull/52761.] cc [~cloud_fan]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Mechanism to cordon brokers and log directories\nDescription: Jira for KIP-1066: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1066%3A+Mechanism+to+cordon+brokers+and+log+directories", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A Analytics-Accelerator: Move AAL to use Java sync client\nDescription: Java sync client is giving the best performance for our use case, especially for readVectored() where a large number of requests can be made concurrently.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update non-thirdparty Guava version to 32.0.1\nDescription: Guava has been already updated to 32.0.1 in hadoop-thirdparty.\r\nHowever, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners.\r\n\r\nSync the non-thirdparty Guava version to the thirdparty one.\nQ: hadoop-yetus commented on PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940#issuecomment-3264930118\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 57s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  87m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 17s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 134m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7940 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux fe8aa1f6455e 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8794a7e5b6cf955551d93f3a15effbd83d5174b7 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/testReport/ |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "pan3793 commented on PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940#issuecomment-3280459780\n\n   cc @cnauroth"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Avoid function conflicts in test_pandas_grouped_map\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Fix time unit mismatch in method updateDeferredMetrics.\r\n\r\nThe time unit of passed param is nanos. But rpcmetrics's is mills.", "output": "Fix time unit mismatch in method updateDeferredMetrics"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `Selenium` to 4.32.0\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "On Windows, run {{hadoop jar}} (with any jar). One immediately gets the following exception:\r\n\r\n{code}\r\nException in thread \"main\" java.lang.UnsupportedOperationException: 'posix:permissions' not supported as initial attribute\r\n    at java.base/sun.nio.fs.WindowsSecurityDescriptor.fromAttribute(WindowsSecurityDescriptor.java:358)\r\n    at java.base/sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:497)\r\n    at java.base/java.nio.file.Files.createDirectory(Files.java:690)\r\n    at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:135)\r\n    at java.base/java.nio.file.TempFileHelper.createTempDirectory(TempFileHelper.java:172)\r\n    at java.base/java.nio.file.Files.createTempDirectory(Files.java:966)\r\n    at org.apache.hadoop.util.RunJar.run(RunJar.java:296)\r\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:245)\r\n{code}\r\n\r\nI'm running Windows 11 with OpenJDK 11.0.26.\r\n\r\nThis bug does not exist in 3.3.6.", "output": "RunJar throws UnsupportedOperationException on Windows"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support Spatial Reference System mapping in PySpark\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52799.", "output": "Issue resolved by pull request 52799\n[https://github.com/apache/spark/pull/52799]"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Simplify Java Home finding for SBT unidoc\nDescription: \nQ: Issue resolved by pull request 52676\n[https://github.com/apache/spark/pull/52676]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add automatic commit offset caching in subscribe mode\nDescription: Suppose that no new messages have been written to the target topic for a long time, for example, the producer has not generated new data for nearly an hour. On the consumer side, when the user sets enable.auto.commit=true, the  OFFSET_COMMIT request will still be repeatedly submitted every 5 seconds. This not only wastes network resources but also increases the burden on __consumer_offset.\r\n\r\n \r\nTherefore, maybe we can add a cache for committed offsets to check if the currently committed offset is consistent with the most recently successfully committed offset. If they are the same, this offset commit can be ignored. Of course, all the above applies to the subscribe mode.\r\n\r\n \r\nAdvantages:\r\n * Reduce unnecessary network requests\r\n * Alleviate the processing pressure on the broker side\r\n * Relieve the pressure of log cleaning for __consumer_offset\r\n\r\n \r\nDisadvantage:\r\n * There may be hidden bugs that need to be discussed and identified", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use `Consumer.poll(Duration)` instead of `Consumer.poll(long)`\nDescription: ", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Percentile estimation functions\nDescription: Similar to https://issues.apache.org/jira/browse/SPARK-53885, it would be useful to have the following percentile estimation functions based on data sketch. These functions provide a lightweight, mergeable representation of percentile distributions, enabling efficient approximate percentile computation over large or streaming datasets without maintaining full data samples.\r\n * *APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm.\r\n\r\n * *APPROX_PERCENTILE_ESTIMATE(state [, k] ])* : Returns the approximate percentile value(s) from a previously accumulated sketch state.\r\n\r\n * *APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Ingress Related operations on blob Endpoint (Create + Append + Flush filesystem calls)\r\n\r\nThis Jira tracks implementing these tests.", "output": "ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Delete Operation"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Two Kafka brokers were not active in 3 node cluster setup\nDescription: Hi Team,\r\n\r\nWe were facing kafka issue where two of the kafka brokers were fenced and Kafka was not able to process messages. We are using Kafka 4.0.0 version. Below are the errors.\r\n\r\n \r\n\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: [2025-09-22 07:41:42,419] ERROR Encountered fatal fault: Unexpected error in raft IO thread (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler)\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: java.lang.IllegalStateException: Received request or response with leader OptionalInt[3] and epoch 55 *which is inconsistent with current leader* OptionalInt.empty and epoch 55\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.maybeTransition(KafkaRaftClient.java:2528) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.maybeHandleCommonResponse(KafkaRaftClient.java:2484) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleFetchResponse(KafkaRaftClient.java:1707) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleResponse(KafkaRaftClient.java:2568) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleInboundMessage(KafkaRaftClient.java:2724) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.poll(Kafka", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Disallow casting geospatial types to/from other data types\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52806.", "output": "Issue resolved by pull request 52806\n[https://github.com/apache/spark/pull/52806]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve `SparkSubmit` to invoke `exitFn` with the root cause instead of `SparkUserAppException`\nDescription: I hit this when I ran a pipeline that had no flows:\r\n```\r\norg.apache.spark.SparkUserAppException: User application exited with 1\r\n\tat org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:127)\r\n\tat org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1028)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:226)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:95)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1166)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1175)\r\n\tat org.apache.spark.deploy.SparkPipelines$.main(SparkPipelines.scala:42)\r\n\tat org.apache.spark.deploy.SparkPipelines.main(SparkPipelines.scala)\r\n```\r\n\r\nThis is not information that's relevant to the user.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update assertGeneratedCRDMatchesHelmChart to include diff\nDescription: \nQ: Issue resolved by pull request 414\n[https://github.com/apache/spark-kubernetes-operator/pull/414]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix TestTextCommand on Windows\nDescription: The following 2 tests are failing in TestTextCommand -\r\n1. testDisplayForNonWritableSequenceFile\r\n2. testDisplayForSequenceFileSmallMultiByteReads\r\n\r\nThese tests create a file and flush a string ending with \"\\n\". However, for verification, the test expects \"\\r\\n\" as the line ending on Windows. Thus, the test fails.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Kafka repo to use JUnit6\nDescription: As JUnit6 was released [https://docs.junit.org/6.0.0/release-notes], it would be great to upgrade repo with such version. Currently {*}5.13.1{*}.\nQ: Unless JUnit 6 brings major enhancements, I'd rather stick to JUnit 5 across the full code base instead of using different versions.", "output": "Are there any plans to drop Java 11 support for those specific modules, or is that still on the distant horizon?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Document skip.platformToolsetDetection option in BUILDING.txt\nDescription: Readme.md has a complete command line example for building Hadoop on Windows.\r\nHowever, that one doesn't work in the docker image, because that doesn't have full Visual Studio install, and misses the expected programs.\r\n\r\nAdd to README.md that the *-Dskip.platformToolsetDetection* maven option is needed when building from the docker image.\nQ: stoty commented on PR #7600:\nURL: https://github.com/apache/hadoop/pull/7600#issuecomment-2796073693\n\n   Found this while testing the Windows image @GauthamBanasandra .", "output": "hadoop-yetus commented on PR #7600:\nURL: https://github.com/apache/hadoop/pull/7600#issuecomment-2796171327\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  24m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  46m 54s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7600/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7600 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets |\r\n   | uname | Linux 89b57ef208bc 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fe8fbe8b296b2e5a9ab7fef0e771d4a4dbe00e10 |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7600/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add message key to org.apache.kafka.clients.producer.RecordMetadata\nDescription: Although the message key is not really {_}metadata{_}, it may be useful to include it in the _org.apache.kafka.clients.producer.RecordMetadata_ class so that metdata can be tied back to a specific message.\r\nWhen using a standard Kafka producer it is easy to tie back the metadata to a specific message by using the callback mechanism.\r\nHowever, when using Kafka streams, the only way to access the metadata and log the details is to register a stream-level _org.apache.kafka.clients.producer.ProducerInterceptor_ instance.\r\nThis mechanism has a drawback in that it is impossible to tie the RecordMetadata instance back to a particular message. \r\nIncluding the message key in the metadata would solve this problem.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Intermittent test failures when using chained emit strategy on window close\nDescription: Hi,\r\n\r\nI have a test case that contains a topology with 2 time windows and chained emitStrategy calls. The standalone reproduction use case is attached to this issue\r\nThe problem is that about 25% of the time the test fails. About 75% of the time the test succeeds. I followed the conclusions of \r\n!https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype|width=16,height=16! KAFKA-19810\r\nto make sure that I am sending events at the correct times (at least I think my calculations are correct). I really need to get to the bottom of why the test fails intermittently, as the test somewhat reflects a real life topology of a project I am working on, and we cannot have an intermittently failing test in our build pipeline. \r\nGreg", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Kinesis tests are broken\nDescription: Running Kinesis test with {{ENABLE_KINESIS_TEST=1}} fails with {{java.lang.NoClassDefFoundError}}:\r\n\r\n{code:java}\r\nENABLE_KINESIS_TESTS=1 ./build/sbt -Pkinesis-asl\r\n...\r\nUsing endpoint URL https://kinesis.us-west-2.amazonaws.com for creating Kinesis streams for tests.\r\n[info] WithoutAggregationKinesisBackedBlockRDDSuite:\r\n[info] org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite *** ABORTED *** (1 second, 131 milliseconds)\r\n[info]   java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/PropertyNamingStrategy$PascalCaseStrategy\r\n[info]   at com.amazonaws.services.kinesis.AmazonKinesisClient.(AmazonKinesisClient.java:86)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient$lzycompute(KinesisTestUtils.scala:59)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient(KinesisTestUtils.scala:58)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:169)\r\n[in\nQ: Issue resolved in https://github.com/apache/spark/pull/52630", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove the `gradlew` workaround for running the old `gradle-wrapper.jar`\nDescription: see https://github.com/apache/kafka/pull/20658", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Thread is not marked interrupted even when AsyncKafkaConsumer.close() throws InterruptException\nDescription: There is a unit tests which perform these steps:\r\n # Create consumer\r\n # Interrupt thread\r\n # Call Consumer.close()\r\n # Check that InterruptException was thrown\r\n # Call Thread.interrupted()\r\n\r\nIn those cases, the return value from Thread.interrupted() is actually false. We're catching the interrupted in the close() method but not restoring it before we throw. This is inconsistent with the ClassicKafkaConsumer.", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: ITestS3ACommitterMRJob failing on Junit5\nDescription: NPE in test200 of ITestS3ACommitterMRJob.\r\n\r\nCause is\r\n* test dir setup and propagation calls Path.getRoot().toUri() which returns the /home dir\r\n* somehow the locatedFileStatus of that path being 1 so a codepath in FileInputFormat NPEs.\r\n\r\nFix is in test code, though FileInputFormat has its NPE messages improved to help understand what is going wrong.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FNS Over Blob] Marker creation fail exception should not be propagated\nDescription: Marker creation is attempted during certain operations such as {*}create{*}, {*}getPathStatus{*}, {*}setPathProperties{*}, and {*}rename{*}, in order to add a 0-byte file that signifies the existence of a folder at that path. However, if marker creation fails, the failure should not be propagated to the user.\nQ: hadoop-yetus commented on PR #7825:\nURL: https://github.com/apache/hadoop/pull/7825#issuecomment-3110072912\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 59s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 32s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 129m 46s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7825 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ecc3a3683189 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e7f2bc3c1192045d0e6d24f754f7bbee8f9bea9f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/1/testReport/ |\r\n   | Max. process+thread count | 632 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "anmolanmol1234 commented on PR #7825:\nURL: https://github.com/apache/hadoop/pull/7825#issuecomment-3114250873\n\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 209\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 863, Failures: 0, Errors: 0, Skipped: 161\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 10\r\n    \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 702, Failures: 0, Errors: 0, Skipped: 242\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n    \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 220\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 12\r\n   [WARNING] Tests run: 707, Failures: 0, Errors: 0, Skipped: 159\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 244\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 12\r\n   [WARNING] Tests run: 704, Failures: 0, Errors: 0, Skipped: 171\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 12\r\n   [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 212\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 734, Failures: 0, Errors: 0, Skipped: 216\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 699, Failures: 0, Errors: 0, Skipped: 241\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Store the last used assignment configuration in the group metadata\nDescription: We should store the last used assignment configuration in the group metadata and bump the group epoch if the assignment configuration is changed.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Let approx_top_k handle NULLs\nDescription: Spark uses FrequentItemsSketch of Apache DataSketches in the approx_top_k function, which does not consider NULL values by itself ([https://github.com/apache/datasketches-java/blob/main/src/main/java/org/apache/datasketches/frequencies/FrequentItemsSketch.java#L587).] However, NULL value could be meaningful in some use cases and users might want to include NULL in the approx_top_k output. Therefore, this ticket aims to add a nullCounter associated with the FrequentItemsSketch to count for NULL in the approx_top_k aggregation.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce Geography and Geometry data types to Java API\nDescription: \nQ: Issue resolved by pull request 52623\n[https://github.com/apache/spark/pull/52623]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Delete Operation\nDescription: We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Ingress Related operations on blob Endpoint (Create + Append + Flush filesystem calls)\r\n\r\nThis Jira tracks implementing these tests.\nQ: hadoop-yetus commented on PR #7376:\nURL: https://github.com/apache/hadoop/pull/7376#issuecomment-2648641086\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 49s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 118m 23s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7376/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7376 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 9fb3e2ec763a 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6262ad8b1f8e25aeffd762fed8491b5ecc581768 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7376/1/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7376/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "manika137 commented on PR #7376:\nURL: https://github.com/apache/hadoop/pull/7376#issuecomment-2670413767\n\n   ## Test Results\r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 767, Failures: 0, Errors: 0, Skipped: 139\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 32\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 770, Failures: 0, Errors: 0, Skipped: 94\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 19\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 609, Failures: 0, Errors: 0, Skipped: 192\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 20\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 767, Failures: 0, Errors: 0, Skipped: 144\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 2\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 32\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 612, Failures: 0, Errors: 0, Skipped: 134\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 20\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 606, Failures: 0, Errors: 0, Skipped: 193\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 33\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 609, Failures: 0, Errors: 0, Skipped: 135\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 33\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 607, Failures: 0, Errors: 0, Skipped: 152\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 33\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 641, Failures: 1, Errors: 0, Skipped: 142\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 32\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 606, Failures: 0, Errors: 0, Skipped: 191\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 33"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: RecordTooLargeExceptions in group coordinator when offsets.topic.compression.codec is used\nDescription: The coordinator runtime can be overly optimistic about compression ratios when filling up batches and can often overflow `max.message.bytes` of the topic. When this happens, all the group coordinator requests that wrote to the batch fail with an UNKNOWN_SERVER_ERROR. This error is non-retriable and disruptive to client applications.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add option, \"fs.option.openfile.footer.length\" to declare length of the footer in parquet/orc files.\r\n\r\nThis will tell prefetch/cache code how much to prefetch and keep.", "output": "add openFile() option to pass down footer length"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add automatic commit offset caching in subscribe mode\nDescription: Suppose that no new messages have been written to the target topic for a long time, for example, the producer has not generated new data for nearly an hour. On the consumer side, when the user sets enable.auto.commit=true, the  OFFSET_COMMIT request will still be repeatedly submitted every 5 seconds. This not only wastes network resources but also increases the burden on __consumer_offset.\r\n\r\n \r\nTherefore, maybe we can add a cache for committed offsets to check if the currently committed offset is consistent with the most recently successfully committed offset. If they are the same, this offset commit can be ignored. Of course, all the above applies to the subscribe mode.\r\n\r\n \r\nAdvantages:\r\n * Reduce unnecessary network requests\r\n * Alleviate the processing pressure on the broker side\r\n * Relieve the pressure of log cleaning for __consumer_offset\r\n\r\n \r\nDisadvantage:\r\n * There may be hidden bugs that need to be discussed and identified\nQ: [~lianetm]   Sorry for the late reply. I think in assign mode, apart from the current consumer being able to modify the offset of the corresponding TopicPartition, the Admin can also make modifications. Let’s assume such a scenario: the offset cached by the consumer is 10, and all subsequent requests to commit offset 10 will return successfully quickly without being sent to the broker. At this point, if the Admin is used to set the offset to 11, the consumer will not be aware of this change. As a result, the consumer caches an invalid offset, which is inconsistent with expectations.   WDYT ?", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Reduce the frequency of ReplicaNotAvailableException thrown to clients when RLMM is not ready\nDescription: During broker restarts, the topic-based RemoteLogMetadataManager constructs the state by reading the internal {{__remote_log_metadata}} topic. When the partition is not ready to perform remote storage operations, then [ReplicaNotAvailableException|https://sourcegraph.com/github.com/apache/kafka/-/blob/storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemotePartitionMetadataStore.java?L127] thrown back to the consumer. The clients retries the request immediately. \r\n\r\nThis result in lot of FetchConsumer requests on the broker and utilizes the request handler threads. Using CountdownLatch the frequency of ReplicaNotAvailableException thrown back to the clients can be reduced. This will improve the request handler thread usage on the broker.\r\n\r\nReproducer: \r\n# Standalone one node cluster with LocalTieredStorage setup. \r\n# Create a topic with remote storage enabled. RF = 1 and partitionCount = 2\r\n# Produce few message and ensure that the segments are uploaded to remote storage. \r\n# Use console-consumer to read the produced messages from the beginning of the topic.\r\n# Update [RemoteLogMetadataPartitionStore|https://sourcegraph.com/github.com/apache/kafka/-/blob/storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemotePartitionMetadataStore.java?L166] to micmic that the partition is not ready.\r\n# Replace the kafka-storage module jar and restart the broker. \r\n# Start the console-consumer to read from the beginning of the topic.  \r\n\r\n~9K ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider\nDescription: Externally Reported Enhancement:\r\n\r\n*Current Limitation*\r\nThe current WorkloadIdentityTokenProvider implementation works well for file-based token scenarios, but it's tightly coupled to file system operations and cannot be easily extended for alternative token sources\r\n\r\n{*}Use Case{*}: *Kubernetes TokenRequest API* \r\nIn modern Kubernetes environments, the recommended approach is to use the TokenRequest API to generate short-lived, on-demand service account tokens rather than relying on projected volume mounts.\r\n\r\n*Proposed Enhancement* \r\nI propose modifying WorkloadIdentityTokenProvider to accept a Supplier for token retrieval instead of being hardcoded to file operations:\nQ: [~anujmodi] can you assign this to me?", "output": "Draft PR without test to see if the structure looks okay - [https://github.com/apache/hadoop/pull/7901]\r\n\r\ncc: [~anujmodi]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: hadoop binary distribution to move cloud connectors to hadoop common/lib\nDescription: Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed.\r\n\r\n* filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. \r\n* Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. \r\n\r\nThis adds a lot more stuff into the distribution, so I'm doing the following design\r\n* all hadoop-* modules in common/lib\r\n* minimal dependencies for hadoop-azure and hadoop-gcs (once we get those right!)\r\n* hadoop-aws: everything except bundle.jar\r\n* other connectors: only included with explicit profiles.\r\n\r\nASF releases will support azure out the box, the others once you add the dependencies. And anyone can build their own release with everything\r\n\r\nOne concern here, we make hadoop-cloud-storage artifact incomplete at pulling in things when depended on. We may need a separate module for the distro setup.\r\n\r\nNoticed during this that the hadoop-tos component is shaded and includes stuff (httpclient5) that we need under control. Filed HADOOP-19708 and incorporating here.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Weird behavior on Kafka Connect 4.1 class loading\nDescription: I have the [DebeziumOpenLineageEmitter|https://github.com/debezium/debezium/blob/main/debezium-openlineage/debezium-openlineage-api/src/main/java/io/debezium/openlineage/DebeziumOpenLineageEmitter.java] class in the *debezium-openlineage-api* that internally has a static map to maintain the registered emitter, the key of this map is \"connectoLogicalName-taskid\"\r\nThen there is the [OpenLineage SMT|https://github.com/debezium/debezium/blob/main/debezium-core/src/main/java/io/debezium/transforms/openlineage/OpenLineage.java], which is part of the *debezium-core.* In this SMT, I simply pass the same context to instantiate the same emitter via the connector.\r\n\r\nNow I'm running the following image\r\n{code:java}\r\nFROM quay.io/debezium/connect:3.3.0.Final\r\nENV MAVEN_REPO=\"https://repo1.maven.org/maven2\"\r\nENV GROUP_ID=\"io/debezium\"\r\nENV DEBEZIUM_VERSION=\"3.3.0.Final\"\r\nENV ARTIFACT_ID=\"debezium-openlineage-core\"\r\nENV CLASSIFIER=\"-libs\"\r\nCOPY log4j.properties /kafka/config/log4j.properties\r\n\r\nAdd \nQ: After some investigations, it looks like KIP-891 broke plugin isolation. For example if we have the same transformation under multiple directories, when Connect tries to execute it, it doesn't call apply with the classloader from the right connector.\r\n\r\nThe issue seems to be in DelegatingClassLoader.findPluginLoader() where it always loops through all directories to find the transformation (or predicate) and keeps the last instance. It then uses the classloader of that instance which may not be the same one as the connector.\r\n\r\nAdding custom tracing I see:\r\n\r\n{noformat}\r\nConnectorConfig.getTransformationOrPredicate connector=io.debezium.connector.postgresql.PostgresConnector connectorRange=null range=[3.4.0-SNAPSHOT,3.4.0-SNAPSHOT]\r\nConnectorConfig.getTransformationOrPredicate classloader=PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-postgres/}\r\nOpenLineage instantiated io.debezium.transforms.openlineage.OpenLineage@2675b74a from /tmp/plugins/debezium-connector-sqlserver/debezium-core-3.4.0-SNAPSHOT.jar\r\nclassloader null PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-sqlserver/}\r\n{noformat}\r\n\r\nThe connector is running from PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-postgres/} and trying to run its transformation using PluginClassLoader{pluginLocation=file:/tmp/plugins/debezium-connector-sqlserver/}. The OpenLineage transformation exists in both directories but the DelegatingClassLoader.findPluginLoader() fails to find the appropriate one.\r\n\r\nWe should clarify the expected behavior when a plugins exists under multiple directories. Having multiple times the same connector with the same version should probably be rejected as from the connector configuration, where you have the name and version, you can't decide which one to run. On the other hands, the other connector plugins can exists in multiple copies as long as they are in different directories, thus isolated, as they are always associated with a connector so we can identify a preferred copy.\r\n\r\nAnother inconsistency found while looking at this is that we have default versions for transformations and predicates but the other plugins default to null. For example, the connector configuration submitted via the REST API does not include any version but the computed configuration injects 3.4.0-SNAPSHOT for the transformation.\r\n\r\n{noformat}\r\n        config.action.reload = restart\r\n\tconnector.class = io.debezium.connector.postgresql.PostgresConnector\r\n\tconnector.plugin.version = null\r\n\terrors.log.enable = false\r\n\terrors.log.include.messages = false\r\n\terrors.retry.delay.max.ms = 60000\r\n\terrors.retry.timeout = 0\r\n\terrors.tolerance = none\r\n\theader.converter = null\r\n\theader.converter.plugin.version = null\r\n\tkey.converter = null\r\n\tkey.converter.plugin.version = null\r\n\tname = inventory-connector-postgres\r\n\tpredicates = []\r\n\ttasks.max = 1\r\n\ttasks.max.enforce = true\r\n\ttransforms = [openlineage]\r\n\ttransforms.openlineage.negate = false\r\n\ttransforms.openlineage.plugin.version = 3.4.0-SNAPSHOT\r\n\ttransforms.openlineage.predicate = null\r\n\ttransforms.openlineage.type = class io.debezium.transforms.openlineage.OpenLineage\r\n\tvalue.converter = null\r\n\tvalue.converter.plugin.version = null\r\n{noformat}", "output": "[~gharris] [~snehashisp] WDYT?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When using the {{find}} command, it currently lists all directories and files. However, if it cannot find a child node, it throws an exception and prevents the subsequent directories from being printed. I think we should change this behavior to print an error message instead of throwing an exception, which would be more user-friendly.\r\n\r\nIn addition, since {{zkMigrationState}} is not used in Kafka 4.X, we should remove this child node.", "output": "Optimize MetadataShell tool find command print warning message intead of throwing exception"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade libopenssl to 3.1.4 for rsync on Windows\nDescription: * We're currently using libopenssl 3.1.2 which\r\n  is needed for rsync 3.2.7 on Windows for the\r\n  Yetus build validation.\r\n* However, libopenssl 3.1.2 is no longer\r\n  available for download on the msys2 site.\r\n* This PR upgrades libopenssl to the next\r\n  available version - 3.1.4 to mitigate this\r\n  issue.\nQ: slfan1989 commented on PR #7770:\nURL: https://github.com/apache/hadoop/pull/7770#issuecomment-3060807079\n\n   LGTM.", "output": "GauthamBanasandra merged PR #7770:\nURL: https://github.com/apache/hadoop/pull/7770"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix `gradlew` content: add missing parts (old Groovy template version is being referenced)\nDescription: {panel:bgColor=#ffffce}\r\n*_Prologue: Kafka developers can't commit jar's into Gir repo (that includes Gradle wrapper jar_*\r\nRelated links:\r\n * https://issues.apache.org/jira/browse/KAFKA-2098?focusedCommentId=14481979&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-14481979\r\n * https://issues.apache.org/jira/browse/KAFKA-1490\r\n * [https://stackoverflow.com/questions/39856714/gradle-wrapper-without-the-jar]\r\n * [https://discuss.gradle.org/t/how-hand-compile-gradle-wrapper-jar-when-gradle-cannot-be-installed-and-cannot-check-jar-files-into-git/4813]{panel}\r\n \r\n\r\n*(i) Intro:* Groovy file _*unixStartScript.txt*_ (that servers as a template for a {_}*gradlew*{_}) path/module was changed in Gradle version 8.8.0\r\n\r\n _*(x) Problem description:*_\r\n # at the moment Kafka trunk branch is using Gradle version [8.14.1|https://github.com/apache/kafka/blob/8deb6c6911616f887ebb2678f3f12ee1da09a618/gradle/wrapper/gradle-wrapper.properties] but thing is that _*gradlew*_ i\nQ: New patch (with code for KAFKA-19174) is submitted here:https://github.com/apache/kafka/pull/19513 \r\n\r\n-Progress is kind of blocked by these issues: KAFKA-19636 -\r\n\r\nSee this comment for a latest update:\r\nhttps://github.com/apache/kafka/pull/19513#issuecomment-3213821218", "output": "(Will be) implemented via KAFKA-19174 (and hence resolving)."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: upgrade nimbus-jose-jwt due to CVE-2025-53864\nDescription: https://www.cve.org/CVERecord?id=CVE-2025-53864", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches`\nDescription: Similar to `Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` resolved in SPARK-54058, `Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite` also seems flaky.\r\n\r\nhttps://github.com/micheal-o/spark/actions/runs/18895266750/job/53931238163\r\n\r\n{code}\r\n[info] - Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) report available offset which is less than end offset during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-42, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scala\nQ: Issue resolved by pull request 52777\n[https://github.com/apache/spark/pull/52777]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The Infra team has issued a requirement to comply with CSP standards, prohibiting the use of external resource files on the site. Currently, we have found that the Hadoop website has lost its styling. The goal of this PR is to comply with the CSP standards by changing the references to external CSS resources to local ones.", "output": "Fix site CSP issue which to match Infra team requirement."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Spark attempts to save views in a Hive-compatible format and only sets the schema to empty if the save operation fails.\r\n\r\nHowever, due to certain Hive compatibility issues, the save operation may succeed while subsequent read operations fail. This issue arises after the change introduced in SPARK-46934, which removed the {{verifyColumnDataType}} check during the {{ALTER TABLE}} operation.", "output": "Use empty schema when altering a view which is not Hive compatible"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix uncaching table by name without cascading\nDescription: {{cacheManager.uncacheTableOrView(spark, Seq(\"testcat\", \"tbl\"), cascade = false)}} does not find the table reference because it is wrapped into a subquery alias.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The legacy WASB driver has been deprecated and is no longer recommended for use. To support customer onboard for migration from WASB to ABFS driver, we've introduced a script to help with the configuration changes required for the same.\r\n\r\nThe script requires the configuration file (in XML format) used for WASB and would generate configuration file required for ABFS driver respectively.", "output": "ABFS: [FnsOverBlob] WASB to ABFS Migration Config Support Script"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update GitHub Actions workflow to use the latest versions of actions\nDescription: The current GitHub Actions workflows in the Hadoop repository are using outdated versions of GitHub Actions, such as {{{}actions/checkout@v3{}}}. To ensure better security, performance, and compatibility, we should update them to the latest stable versions.", "output": "Reopened"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add RPC-level integration tests for StreamsGroupHeartbeat\nDescription: Add integration test similar to `ShareGroupHeartbeatRequestTest` and `ConsumerGroupHeartbeatRequestTest` for `StreamsGroupHeartbeat`", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "My First Kafka code improvement.", "output": "Refactore DescribeTopicPartitionsRequestHandler: Improve readability and add code documentation"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "docs/streams/quickstart.html and docsk/streams/developer-guide/datatypes.html both need to be updated to change the flags for kafka-console-consumer.sh to rename --property to --formatter-property.", "output": "Update streams documentation with KIP-1147 changes"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update Junit 5 version to 5.12.1\nDescription: Hadoop uses a pretty old Junit 5 version, it's a good idea to update to the latest.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: KIP-1224: Implement adaptive append.linger.ms for the group coordinator and share coordinator\nDescription: Add a new allowed value for group.coordinator.append.linger.ms and share.coordinator.append.linger.ms of -1.\r\nWhen append.linger.ms is set to -1, use the flush strategy outlined in the KIP.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Let approx_top_k_accumulate/combine/estimate handle NULLs\nDescription: As a follow-up of https://issues.apache.org/jira/browse/SPARK-53947, let approx_top_k_accumulate/combine/estimate handle NULLs.\nQ: Issue resolved by pull request 52673\n[https://github.com/apache/spark/pull/52673]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In several tests, for example PlaintextAdminIntegrationTest, we see this exception silently triggered when the consumer is closed.\r\n\r\n \r\n{code:java}\r\norg.apache.kafka.common.errors.TimeoutException: StreamsOnTasksAssignedCallbackNeededEvent could not be completed before the consumer closed[2025-09-05 14:17:40,972] ERROR [Consumer clientId=consumer-stream_group_id_1-1, groupId=stream_group_id_1] Reconciliation failed: callback invocation failed for tasks LocalAssignment{localEpoch=0, activeTasks={subtopology-0=[0, 1, 2]}, standbyTasks={}, warmupTasks={}} (org.apache.kafka.clients.consumer.internals.StreamsMembershipManager:1077)java.util.concurrent.CompletionException: org.apache.kafka.common.errors.TimeoutException: StreamsOnTasksAssignedCallbackNeededEvent could not be completed before the consumer closed\tat java.base/java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:368)\tat java.base/java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:377)\tat java.base/java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:1097)\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\tat java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)\tat org.apache.kafka.clients.consumer.internals.events.CompletableEventReaper.completeEventsExceptionallyOnClose(CompletableEventReaper.java:202)\tat org.apache.kafka.clients.consumer.internals.events.CompletableEventReaper.reap(CompletableEventReaper.java:149)\tat org.apache.kafka.clients.consumer.internals.AsyncKafkaConsumer.close(AsyncKafkaConsumer.java:1493) {code}", "output": " StreamsOnTasksAssignedCallbackNeededEvent could not be completed "}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Address memory leak in SparkConnectAddArtifactsHandler\nDescription: [stagedArtifacts|https://github.com/apache/spark/blob/master/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala#L48-L49] buffer is never cleared when artifacts are flushed/on error", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Enhance performance of ABFS driver for write-heavy workloads\nDescription: The goal of this work item is to enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently we disable it only for [the main consumer|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1793], but not for the [restore|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1832] or [global|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1865] consumers.", "output": "Disable topic autocreation for streams consumers."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [RISC-V]  Add rv bulk CRC32 (non-CRC32C) optimized path\nDescription: \nQ: hadoop-yetus commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3398778305\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  14m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  | 101m 41s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  13m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 25s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 206m 55s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8031 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux cf2b3cead534 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0e62b049f710f680823489b1a77892ed49252fc4 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_462-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/testReport/ |\r\n   | Max. process+thread count | 1376 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8031/1/console |\r\n   | versions | git=2.43.7 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "PeterPtroc commented on PR #8031:\nURL: https://github.com/apache/hadoop/pull/8031#issuecomment-3401469367\n\n   @steveloughran could you please review this PR if you have time? thanks!"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve K8s support in Spark 4.1.0\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add off-heap mode support for LongHashedRelation\nDescription: LongHashedRelation now only supports on-heap mode. The task is for adding off-heap mode support for the relation.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce PersisterBatch in SharePartition\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Managed Identity Token Provider is not implemented\nDescription: Managed Identity Token Provider is not implemented in the hadoop-azure jar. Now, if one wants to use either User Assigned Managed Identity or System Assigned Managed Identity in Azure, this will throw an error because it's not implemented yet.\nQ: I have a fix for this myself, will PR it soon", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: MergeScalarSubqueries code cleanup\nDescription: \nQ: I collected this as a subtask of SPARK-51166 in order to improve the visibility of this improvement.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Close Consumer in ConsumerPerformance only after metrics displayed\nDescription: In {{ConsumerPerformance}} (used by {{kafka-consumer-perf-test.sh}}), the metrics are shown, but only after the {{Consumer}} has been closed. Because metrics are removed from the {{Metrics}} object on {{Consumer.close()}}, this means that the complete set of metrics is not displayed when the performance tool outputs the metrics.\nQ: Hi [~kirktrue], I think I could help with this issue if you haven't start to handle it.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Document newly added K8s configurations\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update byte-buddy to 1.15.11\nDescription: This is required for being able to handle recent Java bytecodes.\r\n\r\nByte-buddy is used by BOTH mockito and maven-shade-plugin.\r\n\r\n1.15.11 is the last byte-buddy version that maven-shade-plugin 3.6.0 works with.\nQ: slfan1989 commented on code in PR #7687:\nURL: https://github.com/apache/hadoop/pull/7687#discussion_r2106550167\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -146,6 +146,7 @@\n     4.1.118.Final\n     1.1.10.4\n     1.7.1\n+    1.15.11\n\nReview Comment:\n   Thank you for your contribution. Where will we be using this package?", "output": "stoty commented on code in PR #7687:\nURL: https://github.com/apache/hadoop/pull/7687#discussion_r2108177956\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -146,6 +146,7 @@\n     4.1.118.Final\n     1.1.10.4\n     1.7.1\n+    1.15.11\n\nReview Comment:\n   Both maven-shade-plugin and mockito already uses it, but the current version doesn't handle Java 24 class files.\r\n   \r\n   We could probably update mockito to a newer version (which supports Java 24), but the latest maven-shade-plugin 3.6.0 version still uses an old byte-buddy version.\r\n   \r\n   Dependency managing byte-buddy to this version fixes both maven-shade-plugin and mockito to work with Java 24."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add utility functions to detect JVM GCs\nDescription: Both G1GC and ZGC needs extra object header (usually 16 bytes in 64bit system) when allocating region or ZPage for java long array, so the really bytes allocated is pageSize+16.\r\n\r\nSo we need consider the object header when allocating spark pages.\nQ: Issue resolved by pull request 52678\n[https://github.com/apache/spark/pull/52678]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler.", "output": "[JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions\nDescription: This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4's {{org.junit.Assume}} API to JUnit5's {{org.junit.jupiter.api.Assumptions}} API in order to unify the testing framework and improve maintainability.\r\nh4. Scope of changes:\r\n * Replace usages of {{{}Assume.assumeTrue(...){}}}, {{{}Assume.assumeFalse(...){}}}, etc., with the corresponding methods from JUnit5, such as {{{}Assumptions.assumeTrue(...){}}};\r\n\r\n * Ensure the assertion logic remains consistent with the original behavior;\r\n\r\n * Update any outdated import statements referencing JUnit4's {{{}Assume{}}};\r\n\r\n * Verify that all affected unit tests pass correctly under JUnit5.\nQ: hadoop-yetus commented on PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858#issuecomment-3157797389\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 23s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7858 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5abd35351f22 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6ca49d5da145efa997723827e84e0d9b047683a3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858#issuecomment-3158154572\n\n   @steveloughran @anujmodi2021 Could you please review this PR? Thank you very much!"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When sending records to a topic created like this:\r\n{quote}            admin.createTopics(List.of(new NewTopic(\"foo\", 24, (short) 1)\r\n                    .configs(Map.of(TopicConfig.COMPRESSION_TYPE_CONFIG,\"zstd\"))));\r\n{quote}\r\nWith a producer configured with:\r\n{quote}producerProps.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"zstd\");\r\n{quote}\r\nThe server fails with:\r\n{quote}[TIMESTAMP] ERROR [ReplicaManager broker=1] Error processing append operation on partition Z6OCnSwIS9KVFCN2gLaxnQ:foo-1 (kafka.server.ReplicaManager)\r\njava.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?]\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:423) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:405) ~[?:?]\r\n...\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?]\r\nSuppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?]\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?]\r\n...\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?]\r\nSuppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.", "output": "Native-image (dockerimage) does not work with compression.type=zstd"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Correct hadoop-thirdparty license and site for protobuf 3.25.5\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I have the [DebeziumOpenLineageEmitter|https://github.com/debezium/debezium/blob/main/debezium-openlineage/debezium-openlineage-api/src/main/java/io/debezium/openlineage/DebeziumOpenLineageEmitter.java] class in the *debezium-openlineage-api* that internally has a static map to maintain the registered emitter, the key of this map is \"connectoLogicalName-taskid\"\r\nThen there is the [OpenLineage SMT|https://github.com/debezium/debezium/blob/main/debezium-core/src/main/java/io/debezium/transforms/openlineage/OpenLineage.java], which is part of the *debezium-core.* In this SMT, I simply pass the same context to instantiate the same emitter via the connector.\r\n\r\nNow I'm running the following image\r\n{code:java}\r\nFROM quay.io/debezium/connect:3.3.0.Final\r\nENV MAVEN_REPO=\"https://repo1.maven.org/maven2\"\r\nENV GROUP_ID=\"io/debezium\"\r\nENV DEBEZIUM_VERSION=\"3.3.0.Final\"\r\nENV ARTIFACT_ID=\"debezium-openlineage-core\"\r\nENV CLASSIFIER=\"-libs\"\r\nCOPY log4j.properties /kafka/config/log4j.properties\r\n\r\nAdd OpenLineage\r\nRUN mkdir -p /tmp/openlineage-libs && \\\r\n    curl \"$MAVEN_REPO/$GROUP_ID/$ARTIFACT_ID/$DEBEZIUM_VERSION/$ARTIFACT_ID-${DEBEZIUM_VERSION}${CLASSIFIER}.tar.gz\" -o /tmp/debezium-openlineage-core-libs.tar.gz && \\\r\n    tar -xzvf /tmp/debezium-openlineage-core-libs.tar.gz -C /tmp/openlineage-libs --strip-components=1\r\n\r\nRUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-postgres/\r\nRUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-mongodb/\r\nADD openlineage.yml /kafka/ {code}\r\nSo is practically debezium connect image with just openlineage jars copied into postgres and mongodb connector folders.\r\n\r\nWhen I register the PostgreSQL connector\r\n{code:java}\r\n{\r\n  \"name\": \"inventory-connector-postgres\",\r\n  \"config\": {\r\n    \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\r\n    \"tasks.max\": \"1\",\r\n    \"database.hostname\": \"postgres\",\r\n    \"database.port\": \"5432\",\r\n    \"database.user\": \"postgres\",\r\n    \"database.password\": \"postgres\",", "output": "Weird behavior on Kafka Connect 4.1 class loading"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Remove powermock dependency\nDescription: The powermock dependency is specified in\r\n- hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml\r\n- hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml\r\n\r\nbut not used anywhere.  We should remove it.\nQ: Reopen for removing unused powermock dependencies.", "output": "szetszwo opened a new pull request, #7939:\nURL: https://github.com/apache/hadoop/pull/7939\n\n   ### Description of PR\r\n   \r\n   HADOOP-19678\r\n   \r\n   The powermock dependency is specified in\r\n   \r\n   - hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml\r\n   - hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml\r\n   \r\n   but not used anywhere. We should remove it.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   By existing tests.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [NA] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [NA ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [NA ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: The assignment generated by ReassignPartitionsCommand should include directories\nDescription: Since the `calculateAssignment` method does not provide directory inforamtion, the output assignment also omits it.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `Ammonite` to 3.0.3\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Disallow create too many partitions also in createPartitions\nDescription: Similarly to \r\nhttps://issues.apache.org/jira/browse/KAFKA-17870\r\n\r\nExtending existing topics with a request that asks to create too many partitions can lead to OOM error and the client receiving an UnlknownServerException.\r\n\r\n \r\n\r\nWe should limit the number of new partitions similarly as when creating new topics.", "output": "Patch Available"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update Boost to 1.86.0\nDescription: The currentlly used Boost 1.72.0 does not compile with recent C++ compilers, which prevents compiling on recent systems (at least without replacing the C++ compiler)\r\n\r\nhdfs-native does not compile with 1.87.0, but does with 1.86.0.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "some test regressions when testing with third party stores and the 3.4.2 branch\r\n\r\n* ITestS3ACopyFromLocalFile \" software.amazon.awssdk.services.s3.model.S3Exception: The Content-SHA256 you specified did not match what we received (Service: S3, Status Code: 400,\"\r\n\r\n* conditional overwrite tests fail because it's not there. Even when disabled in the config, some tests still try to use it (create cost), and find assertions about overwrites not met.\r\n\r\n* ITestS3ABucketExistence for bucket existence probes failures; assumption is that bucket probes succeed for all bucket names, it's when you actually try to work with one you get a failure.\r\n\r\nThe copy from local failure an unwelcome change, but maybe could be addressed with doc changes.\r\n\r\nThe conditional overwrite thing is something to flag in release notes as possible regression.", "output": "S3A: tests against third party stores failing after latest update & conditional creation "}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analytics-Accelerator: Add support SSE-C\nDescription: Pass down SSE-C key to AAL so it can be attached while making the GET request.\nQ: mukund-thakur commented on code in PR #7738:\nURL: https://github.com/apache/hadoop/pull/7738#discussion_r2205717828\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsStream.java:\n##########\n@@ -205,6 +209,12 @@ private OpenStreamInformation buildOpenStreamInformation(ObjectReadParameters pa\n           .etag(parameters.getObjectAttributes().getETag()).build());\n     }\n \n+    if(parameters.getEncryptionSecrets().getEncryptionMethod() == S3AEncryptionMethods.SSE_C) {\n\nReview Comment:\n   Why don't I see the condition for other encryption methods like SSE_KMS etc?", "output": "ahmarsuhail commented on code in PR #7738:\nURL: https://github.com/apache/hadoop/pull/7738#discussion_r2242908029\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsStream.java:\n##########\n@@ -205,6 +209,12 @@ private OpenStreamInformation buildOpenStreamInformation(ObjectReadParameters pa\n           .etag(parameters.getObjectAttributes().getETag()).build());\n     }\n \n+    if(parameters.getEncryptionSecrets().getEncryptionMethod() == S3AEncryptionMethods.SSE_C) {\n\nReview Comment:\n   On the GET, you only need it for SSE_C i think, this is what the current S3A implementation does as well here: https://github.com/apache/hadoop/blob/636d822682715dbc05af9483e91a9f0ee72f83b8/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RequestFactoryImpl.java#L639\n   \n   On writing you need to set it for KMS as well, so the write operation knows what key to use. Think on the GET, the right key just gets picked up automatically from KMS."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Backport HADOOP-15984 (Update jersey from 1.19 to 2.x) on branch-3.4.0 \nDescription: In Hadoop 3.5.0 we uses GlassFish Jersey (modern Jersey).\r\n\r\nIn Hadoop 3.4.0 we use Sun Jersey (legacy Jersey).\r\n\r\nJetty submodule is not published under _com.sun.jersey.jersey-test-framework_ so upgrading jersey will be a better idea to remove grizzly-http-* dependencies (which is already done in 3.5.0 through  YARN-11793).", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade kafka to 3.9.0 to fix CVE-2024-31141\nDescription: Upgrade kafka-clients to 3.9.0 to fix [https://nvd.nist.gov/vuln/detail/CVE-2024-31141]\nQ: Sure [~stevel@apache.org], working on it.", "output": "[https://nvd.nist.gov/vuln/detail/CVE-2024-31141] indicates that you need to upgrade the jar but also set a system property to enable the secure setting."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "https://github.com/advisories/GHSA-66rc-vg9f-48m7\r\n\r\nNo fix release is available yet.\r\n\r\nIt looks like we would need a fix release for the older branch (1.0). jsonschema2pojo 1.1 and above needs a version of javax.validation that is incompatible with the version of javax.validation needed by Jersey 2.", "output": "upgrade jsonschema2pojo due to CVE-2025-3588"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix typo in state-change log filename after rotate\nDescription: The log4j2 config file was incorrectly rotating the state-change.log to {{stage-change.log.[date]}} (changing the filename from state to stage). The below PR corrects the file name for rotated logs.\r\n\r\nAfter this change is applied, the log4j2 config will not take any actions with previously-created {{stage-change.log.[date]}} files. These may need to be manually removed by users.\r\n\r\n[https://github.com/apache/kafka/pull/20269]\nQ: trunk: https://github.com/apache/kafka/commit/66b3c07954c94ed2f3c7dec95128d3bc5c1b7049\r\n\r\n4.1: https://github.com/apache/kafka/commit/fc030b411c3881e752856c443c7b14f393d4c46e\r\n\r\n4.0: https://github.com/chia7712/kafka/commit/0f9b3127030f596c8c187e22b1badd88e4bf9662", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade to jackson 2.18\nDescription: follow up to HADOOP-19259\nQ: hadoop-yetus commented on PR #7623:\nURL: https://github.com/apache/hadoop/pull/7623#issuecomment-2811310081\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 37s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 49s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  19m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 31s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |  10m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 30s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  3s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  30m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 29s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   2m 22s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |   8m 44s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 46s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  11m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 25s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  2s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |  15m 14s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 544m 27s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  0s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 676m 39s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServices |\r\n   |   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerMultiNodes |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   |   | hadoop.yarn.server.timeline.webapp.TestTimelineWebServices |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints |\r\n   |   | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMProxy |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClient |\r\n   |   | hadoop.yarn.client.api.impl.TestYarnClient |\r\n   |   | hadoop.yarn.client.api.impl.TestNMClient |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7623 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux 1106262bb69e 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 41ab79fb7df5086d2cd8dddcfa19f501a036a28e |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/1/testReport/ |\r\n   | Max. process+thread count | 3279 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7623:\nURL: https://github.com/apache/hadoop/pull/7623#issuecomment-2812048459\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  1s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   7m  6s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  31m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 48s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 50s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |  21m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 31s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 41s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  50m 53s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 38s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  31m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  4s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  4s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 43s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 27s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 43s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  51m 58s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 141m 44s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/2/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 30s |  |  ASF License check generated no output?  |\r\n   |  |   | 419m  2s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServices |\r\n   |   | hadoop.yarn.server.timeline.webapp.TestTimelineWebServices |\r\n   |   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerMultiNodes |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7623 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux 0a701d1dc28d 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 08e8bf5ecb086fac4554679c012fb3dd033f81e5 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/2/testReport/ |\r\n   | Max. process+thread count | 3150 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Release a minor update to hadoop", "output": "Release Hadoop 3.4.2"}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade jsonschema2pojo due to CVE-2025-3588\nDescription: https://github.com/advisories/GHSA-66rc-vg9f-48m7\r\n\r\nNo fix release is available yet.\r\n\r\nIt looks like we would need a fix release for the older branch (1.0). jsonschema2pojo 1.1 and above needs a version of javax.validation that is incompatible with the version of javax.validation needed by Jersey 2.\nQ: pjfanning opened a new pull request, #7645:\nURL: https://github.com/apache/hadoop/pull/7645\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   CVE-2025-3588\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7645:\nURL: https://github.com/apache/hadoop/pull/7645#issuecomment-2826220805\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 35s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 20s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m  5s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 21s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 47s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  | 148m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 49s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   5m  7s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7645/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |  15m 39s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  0s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  15m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m  1s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m  5s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |  50m  0s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 599m 42s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7645/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 839m 22s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndWeight |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModePercentageAndWeight |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsCustomResourceTypes |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.fairscheduler.TestRMWebServicesFairScheduler |\r\n   |   | hadoop.yarn.server.resourcemanager.nodelabels.TestNodeLabelFileReplication |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndWeightVector |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDefaultLabel |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentageAndWeightVector |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps |\r\n   |   | hadoop.yarn.server.resourcemanager.TestClientRMService |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesReservation |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppAttempts |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesSchedulerActivities |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedLegacyQueueCreation |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.fairscheduler.TestRMWebServicesFairSchedulerCustomResourceTypes |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModePercentageAndWeightVector |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentageVector |\r\n   |   | hadoop.yarn.webapp.TestRMWithXFSFilter |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfigWeightModeDQC |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfig |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentage |\r\n   |   | hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestFSSchedulerConfigurationStore |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedMode |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedLegacyQueueCreationAbsoluteMode |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppCustomResourceTypes |\r\n   |   | hadoop.yarn.webapp.TestRMWithCSRFFilter |\r\n   |   | hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher |\r\n   |   | hadoop.yarn.server.resourcemanager.TestRMAdminService |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesConfigurationMutation |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerConfigMutation |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesContainers |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesForCSWithPartitions |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentageAndWeight |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServiceAppsNodelabel |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodeLabels |\r\n   |   | hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisher |\r\n   |   | hadoop.yarn.server.resourcemanager.TestRMHA |\r\n   |   | hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesSchedulerActivitiesWithMultiNodesEnabled |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfigAbsoluteMode |\r\n   |   | hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfigWeightMode |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7645/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7645 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 38da54e35de4 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 63799fc7f155daaedbc47f8baf19bf1717b3e2a7 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7645/1/testReport/ |\r\n   | Max. process+thread count | 4010 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7645/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update commons-lang3 to 3.17.0\nDescription: The commons-text version used by Hadoop is incompatible with commons-lang3 3.12.\r\n\r\nThis is actually with an older Hadoop, but with the same {_}commons-lang3{_}, _commons-configuration2_ and _commons-text_ versions as trunk:\r\n{noformat}\r\njava.lang.NoSuchMethodError: 'org.apache.commons.lang3.Range org.apache.commons.lang3.Range.of(java.lang.Comparable, java.lang.Comparable)'\r\n    at org.apache.commons.text.translate.NumericEntityEscaper.(NumericEntityEscaper.java:97)\r\n    at org.apache.commons.text.translate.NumericEntityEscaper.between(NumericEntityEscaper.java:59)\r\n    at org.apache.commons.text.StringEscapeUtils.(StringEscapeUtils.java:271)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.unescapePropertyName(PropertiesConfiguration.java:690)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.initPropertyName(PropertiesConfiguration.java:583)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.parseProperty(PropertiesConfiguration.java:640)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.nextProperty(PropertiesConfiguration.java:626)\r\n    at org.apache.commons.configuration2.PropertiesConfigurationLayout.load(PropertiesConfigurationLayout.java:443)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration.read(PropertiesConfiguration.java:1500)\r\n    at org.apache.commons.configuration2.io.FileHandler.loadFromReader(FileHandler.java:712", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Publish Apache Spark 4.1.0-preview3 to docker registry\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix RBAC to allow `Spark` driver to create `StatefulSet`\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add examples for function unwrap_udt\nDescription: \nQ: Issue resolved by pull request 52647\n[https://github.com/apache/spark/pull/52647]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-resourceestimator.\nDescription: \nQ: hadoop-yetus commented on PR #7548:\nURL: https://github.com/apache/hadoop/pull/7548#issuecomment-2760047235\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 59s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/1/artifact/out/blanks-eol.txt) |  The patch has 4 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  9s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 33s | [/patch-unit-hadoop-tools_hadoop-resourceestimator.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/1/artifact/out/patch-unit-hadoop-tools_hadoop-resourceestimator.txt) |  hadoop-resourceestimator in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  75m 39s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.resourceestimator.service.TestResourceEstimatorService |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7548 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c32e5cb0de61 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 294d118c73af4343bc03498e359b42e6bdd0cadc |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/1/testReport/ |\r\n   | Max. process+thread count | 553 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-resourceestimator U: hadoop-tools/hadoop-resourceestimator |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7548:\nURL: https://github.com/apache/hadoop/pull/7548#issuecomment-2761171592\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 12 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m  2s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/2/artifact/out/blanks-eol.txt) |  The patch has 5 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 35s |  |  hadoop-resourceestimator in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  68m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7548 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 0582679e9c72 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / cc7fa27df09bba5bb11a0fea788ae57d81f07394 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/2/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-resourceestimator U: hadoop-tools/hadoop-resourceestimator |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Install `pyarrow/torch/torchvision` packages to `Python 3.14` Dockefile\nDescription: \nQ: Issue resolved by pull request 52751\n[https://github.com/apache/spark/pull/52751]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Disable topic autocreation for streams consumers.\nDescription: Currently we disable it only for [the main consumer|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1793], but not for the [restore|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1832] or [global|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1865] consumers.\nQ: Hi [~nikita-shupletsov]  can i pick this up ?", "output": "Hi, [~goyarpit]. \r\nyes, please"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hi Hadoop Team,\r\n\r\nThe ASF {_}*Privacy Policy*{_}[1][2] does not permit the use of _*Google Analytics*_ on any ASF websites.\r\n\r\nIt looks like _*Google Analytics*_ was removed from the Hadoop site on *13th Nov 2024* in [Commit 86c0957|https://github.com/apache/hadoop-site/commit/86c09579bff7bf26e86475939d106e900a7da94d] and re-introduced on *20th Feb 2025* in [Commit 27f835e|https://github.com/apache/hadoop-site/commit/27f835ef0369a6bc95a12131af75c696cafb4b4c].\r\n\r\nI'm not sure how this has happened, but I see the following:\r\n* [layouts/partials/footer.html|https://github.com/apache/hadoop-site/blob/asf-site/layouts/partials/footer.html] references *__internal/google_analytics.html_*\r\n* I can't find *__internal/google_analytics.html_*\r\n* It looks like the safest action is to remove the reference to *google_analytics.html* in the *footer.html*\r\n\r\nI have created [PR #67|https://github.com/apache/hadoop-site/pull/67] to remove the reference in the footer, although I have not understood fully how the content for your website is generated and would appreciate if you could check for the correct resolution to this issue.\r\n\r\nThanks\r\n\r\nNiall\r\n\r\n[1] [https://privacy.apache.org/policies/website-policy.html]\r\n[2] [https://privacy.apache.org/faq/committers.html#can-i-use-google-analytics]\r\n\r\nh1. Matomo\r\n\r\nThe ASF hosts its own _*Matomo*_ instance to provide projects with analytics and you can request a tracking id for your project by sending a mail to *privacy AT apache.org.*\r\n * [https://privacy.apache.org/faq/committers.html#can-i-use-web-analytics-matomo]\r\n\r\nAnalytics for ASF projects using _*Matomo*_ can be viewed at the following location:\r\n * https://analytics.apache.org/", "output": "Remove Google Analytics from Hadoop Website"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move ClientOAuthIntegrationTest to clients-integration-tests\nDescription: This is continue with the effort to move the clients test to {{clients-integration-tests}} a la KAFKA-19042.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "JMX metrics RequestHandlerAvgIdlePercent reports a value close to 2 in combined kraft mode but it's expected to be b/w 0 and 1.\r\n\r\nThis is an issue with combined mode specifically because both controller + broker are using the same Meter object in combined mode, defined in {{{}RequestThreadIdleMeter#requestThreadIdleMeter{}}}, but the controller and broker are using separate {{KafkaRequestHandlerPool}} objects, where each object's {{{}threadPoolSize == KafkaConfig.numIoThreads{}}}. This means when calculating idle time, each pool divides by its own {{numIoThreads}} value before reporting to the shared meter and  {{RequestHandlerAvgIdlePercent}} calculates the final result by accumulating all the values reported by all threads. However, since there are actually 2 × numIoThreads total threads contributing to the metric, the denominator should be doubled to get the correct average.", "output": "Anomaly of JMX metrics RequestHandlerAvgIdlePercent in kraft combined mode"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When a table is cached using one table implementation (e.g., V2) and written through the other (e.g., V1), Spark may not automatically trigger recaching. As a result, the cached data can become stale even though the underlying table content has changed.\r\n\r\n \r\n\r\nThis issue arises because the current recaching mechanism does not consistently handle cross-implementation writes. Given that the community is actively working on Data Source V2 (DSV2), many data sources are expected to have both V1 and V2 implementations for a period of time, making this issue more likely to occur in practice.\r\n\r\n \r\n\r\n*Proposed Fix:*\r\n\r\nEnhance the cache invalidation logic to detect writes that occur through a different table implementation (V1 ↔ V2) and trigger recaching accordingly.\r\n\r\n \r\n\r\n{*}Expected Outcome:{*}{*}{*}\r\n * Cached data remains up to date when a table is written through either V1 or V2 paths.\r\n\r\n * Both logical-plan-based and file-path-based recaching continue to work as expected for V1&V2 connectors", "output": "Support recaching when a table is written via a different table implementation (V1 or V2)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Initial implement Connect JDBC driver\nDescription: \nQ: Issue resolved by pull request 52705\n[https://github.com/apache/spark/pull/52705]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve heartbeat request manager initial HB interval \nDescription: With KIP-848, consumer HB interval config moved to the broker, so currently, the consumer HB request manager starts with a 0ms interval (mainly to send a first HB right away after the consumer subscribe + poll). Once a response is received, the consumer takes the interval from the response and starts using it. \r\n\r\nThat 0ms initial interval makes that the HB mgr poll continuously executes logic on a tight loop that may not really be needed. It mostly has to wait for a response (or a failure).\r\n\r\nProbably worse than this, is the impact on the app thread, given that pollTimeout takes into account the maxTimeToWait from the network thread, that is directly impacted by the timeToNextHeartbeat\r\n * [https://github.com/apache/kafka/blob/388739f5d847d7a16e389d9891f806547f023476/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1764-L1766]\r\n * [https://github.com/apache/kafka/blob/781bc7a54b8c4f7c86f0d6bb9ef8399d86d0735e/clients/src/main/java/org/apache/k\nQ: Sure, feel free to take it and I can help with reviews. Thanks for your help!", "output": "[~brandboat]  It seems you are already working on other issues .Can i take this up ? .It would be my first hand on the new consumer group protocol."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob] Listing Optimizations to avoid multiple iteration over list response.\nDescription: On blob endpoint, there are a couple of handling that is needed to be done on client side.\r\nThis involves:\r\n # Parsing of xml response and converting them to VersionedFileStatus list\r\n # Removing duplicate entries for non-empty explicit directories coming due to presence of the marker files\r\n # Trigerring Rename recovery on the previously failed rename indicated by the presence of pending json file.\r\n\r\nCurrently all three are done in a separate iteration over whole list. This is to pbring all those things to a common place so that single iteration over list reposne can handle all three.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK17] Remove mockito-all 1.10.19 and powermock\nDescription: - The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11.\r\n\r\n - The powermock dependency is specified in the pom below. We should replace it with mockito since it is known to be incompatible with JDK17.\r\n -* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix TestFsShellList.testList on Windows\nDescription: * The test *org.apache.hadoop.fs.TestFsShellList#testList* creates files with special characters and tries to list them using *ls*.\r\n* Filenames with special characters aren't allowed in Windows.\r\n* Thus, we need to modify the test to only test for non-special characters on Windows and include the filenames with special characters on non-Windows environments.\nQ: GauthamBanasandra commented on PR #7771:\nURL: https://github.com/apache/hadoop/pull/7771#issuecomment-3058616524\n\n   Attaching the logs of the unit tests run locally on my Windows PC.\r\n   [unit-test-results.log](https://github.com/user-attachments/files/21169533/unit-test-results.log)", "output": "GauthamBanasandra merged PR #7771:\nURL: https://github.com/apache/hadoop/pull/7771"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h2. 1. Background & Motivation\r\n\r\nCurrently, Kafka uses a JSON template-based pre-generation approach to create protocol classes under {{{}org.apache.kafka.common.message{}}}. This solution is mature, stable, and provides excellent multi-language support.\r\n\r\nHowever, I've been considering whether we could provide an alternative *compile-time code generation* approach for the Java ecosystem, similar to the AST modification technology used by Lombok. This approach might offer better development experience in certain scenarios.\r\nh2. 2. Proposal Overview\r\n\r\n{*}Core Idea{*}: Use annotation processors to directly modify the AST during compilation, dynamically generating complete implementations of protocol classes instead of pre-generating Java source files.\r\n\r\n{*}Example Implementation Concept{*}:\r\n{code:java}\r\n@KafkaMessage(apiKey = 1, version = \"0.1.0\")\r\npublic class FetchRequestSpec {\r\n    @ProtocolField(type = \"string\", order = 1)\r\n    private String groupId;\r\n    \r\n    @ProtocolField(type = \"int16\", order = 2) \r\n    private short acks;\r\n    \r\n    // More fields...\r\n}{code}\r\n \r\nh2. 3. Comparative Analysis\r\n||Aspect||Current Template Approach||Compile-time AST Approach||Hybrid Possibility||\r\n|*Development Experience*|Requires viewing generated source files|Cleaner source code, Lombok-like|AST during development, pre-gen for release|\r\n|*Build Process*|Explicit pre-generation step|Integrated into standard compilation|Configurable generation strategy|\r\n|*Debugging Support*|✅ Full source-level debugging|⚠️ Requires IDE plugin support|Source output on demand|\r\n|*Multi-language Support*|✅ Single definition, multiple outputs|❌ Java-only|Preserve existing multi-language capabilities|\r\n|*Performance Impact*|One-time generation at build time|Regeneration on each compilation|Smart caching mechanism|\r\nh2. 4. Potential Value\r\n * {*}Faster Development Iteration{*}: Immediate compilation after protocol definition changes, no pre-generation step required\r\n\r\n * {*}Source Code Cleanlin", "output": "[DISCUSS] Exploring Compile-time AST Generation as an Alternative Approach for Kafka Protocol Classes"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Streams open iterator tracking has high contention on metrics lock\nDescription: We run Kafka Streams 4.1.0 with custom processors that heavily use state store range iterators.\r\n\r\nWhile investigating disappointing performance, we found a surprising source of lock contention.\r\n\r\nOver the course of about a 1 minute profiler sample, the {{org.apache.kafka.common.metrics.Metrics}} lock is taken approximately 40,000 times and blocks threads for about 1 minute.\r\n\r\nThis appears to be because our state stores generally have no iterators open, except when their processor is processing a record, in which case it opens an iterator (taking the lock through {{OpenIterators.add}} into {{{}Metrics.registerMetric{}}}), does a tiny bit of work, and then closes the iterator (again taking the lock through {{OpenIterators.remove}} into {{{}Metrics.removeMetric{}}}).\r\n\r\nSo, stream processing threads takes a globally shared lock twice per record, for this subset of our data. I've attached a profiler thread state visualization with our findings - the red bar indicates the thread was blocked during the sample on this lock. As you can see, this lock seems to be severely hampering our performance.\r\n\r\n \r\n\r\n!image-2025-09-05-12-13-24-910.png!", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FNS Over Blob] Marker creation fail exception should not be propagated\nDescription: Marker creation is attempted during certain operations such as {*}create{*}, {*}getPathStatus{*}, {*}setPathProperties{*}, and {*}rename{*}, in order to add a 0-byte file that signifies the existence of a folder at that path. However, if marker creation fails, the failure should not be propagated to the user.\nQ: anmolanmol1234 opened a new pull request, #7825:\nURL: https://github.com/apache/hadoop/pull/7825\n\n   Marker creation is a best-effort operation performed during folder-related actions such as create, getPathStatus, setPathProperties, and rename. It involves writing a 0-byte file to indicate the presence of a folder. However, marker creation is not critical to the success of the primary operation. This change ensures that failures encountered during marker creation (e.g., due to transient issues or permission errors) are not propagated back to the user, preserving expected behavior and preventing unnecessary operation failures.", "output": "hadoop-yetus commented on PR #7825:\nURL: https://github.com/apache/hadoop/pull/7825#issuecomment-3110072912\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 59s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 32s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 129m 46s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7825 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ecc3a3683189 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e7f2bc3c1192045d0e6d24f754f7bbee8f9bea9f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/1/testReport/ |\r\n   | Max. process+thread count | 632 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7825/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support multiple files in `pyFiles`\nDescription: \nQ: Issue resolved by pull request 382\n[https://github.com/apache/spark-kubernetes-operator/pull/382]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade Volcano to 1.13.0\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Documentation reference: [https://datasketches.apache.org/docs/KLL/KLLSketch.html].\r\n\r\nDataSketches code API reference: [https://apache.github.io/datasketches-java/6.1.0/org/apache/datasketches/kll/KllLongsSketch.html] \r\n\r\nReference PR for recently adding Theta sketches: [https://github.com/apache/spark/pull/51298]", "output": "Add support for KLL quantiles functions based on DataSketches"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support row position for SparkConnectResultSet\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [DISCUSS] Exploring Compile-time AST Generation as an Alternative Approach for Kafka Protocol Classes\nDescription: h2. 1. Background & Motivation\r\n\r\nCurrently, Kafka uses a JSON template-based pre-generation approach to create protocol classes under {{{}org.apache.kafka.common.message{}}}. This solution is mature, stable, and provides excellent multi-language support.\r\n\r\nHowever, I've been considering whether we could provide an alternative *compile-time code generation* approach for the Java ecosystem, similar to the AST modification technology used by Lombok. This approach might offer better development experience in certain scenarios.\r\nh2. 2. Proposal Overview\r\n\r\n{*}Core Idea{*}: Use annotation processors to directly modify the AST during compilation, dynamically generating complete implementations of protocol classes instead of pre-generating Java source files.\r\n\r\n{*}Example Implementation Concept{*}:\r\n{code:java}\r\n@KafkaMessage(apiKey = 1, version = \"0.1.0\")\r\npublic class FetchRequestSpec {\r\n    @ProtocolField(type = \"string\", order = 1)\r\n    private String groupId;\r\n    \r\n    @ProtocolField(type = \"int16\", order = 2) \r\n    private short acks;\r\n    \r\n    // More fields...\r\n}{code}\r\n \r\nh2. 3. Comparative Analysis\r\n||Aspect||Current Template Approach||Compile-time AST Approach||Hybrid Possibility||\r\n|*Development Experience*|Requires viewing generated source files|Cleaner source code, Lombok-like|AST during development, pre-gen for release|\r\n|*Build Process*|Explicit pre-generation step|Integrated into standard compilation|Configurable generation strategy|\r\n|*Debugging Support*|✅ Full ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce APPROX_PERCENTILE_ACCUMULATE\nDescription: *APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: JDK8: RawLocalFileSystem calls ByteBuffer.flip\nDescription: running 3.4.2 rc1 on older JDK 8 runtimes can fail\r\n{code}\r\n\r\n\r\n[INFO] -------------------------------------------------------\r\n[INFO] Running org.apache.parquet.hadoop.TestParquetReader\r\nException in thread \"Thread-8\" java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;\r\n        at org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler.completed(RawLocalFileSystem.java:428)\r\n        at org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler.completed(RawLocalFileSystem.java:362)\r\n        at sun.nio.ch.Invoker.invokeUnchecked(Invoker.java:126)\r\n        at sun.nio.ch.SimpleAsynchronousFileChannelImpl$2.run(SimpleAsynchronousFileChannelImpl.java:335)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:750)\r\n{code}\r\n\r\n\r\nthis can be fixed trivially, casting to java.io.Buffer first: ((Byffer)buff).flip())", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove decay-scheduler.service-users in the fair call guide\nDescription: As per guide: 'Service users will always be scheduled into the highest-priority queue and won’t be included in the priority computation of normal user calls.'\r\n\r\nHowever, this functionality was never implemented due to some redundant issue during initial development. So this decay-scheduler.service-users needs to be removed from document.\r\n\r\n \r\n\r\n[https://hadoop.apache.org/docs/r3.4.1/hadoop-project-dist/hadoop-common/FairCallQueue.html]\r\n\r\nRef: https://issues.apache.org/jira/browse/HADOOP-10281", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip tests that require JavaScript engine when it's not available\nDescription: Some features require a JavaScript engine, which has been removed from JDK.\r\n\r\nSkip those tests if JavaScript is not available.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob] Streaming List Path Result Should Happen Inside Retry Loop\nDescription: Listing APIs on both DFS and Blob Endpoints return response as part of response body and has to be read from an Socket Input Stream.\r\nAny network error occuring while reading the stream should be retried.\r\n\r\nToday, this parsing happens in client and such errors are not retried.\r\nThis change fixes this behavior", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: NPE in dependency-check of hadoop-thirdparty\nDescription: the dependency checker of hadoop-thirdparty PRs fails with an NPE in the plugin.\r\n{code}\r\nError:  Failed to execute goal org.owasp:dependency-check-maven:6.1.5:aggregate (default-cli) on project hadoop-thirdparty: Fatal exception(s) analyzing Apache Hadoop Third-party Libs: One or more exceptions occurred during analysis:\r\nError:  \tUpdateException: java.util.concurrent.ExecutionException: java.lang.NullPointerException\r\nError:  \t\tcaused by ExecutionException: java.lang.NullPointerException\r\nError:  \t\tcaused by NullPointerException: null\r\n{code}\r\n\r\n\r\nThere is a much newer version of the library, but it is a java11 release. we will need to find the most recent release without this issue that is java8 only.\r\n\r\nAgain, this highlights why trying to continue build on java8 is futile\nQ: It's nice to automate checks like this but in the end of the day, it seems ok to turn this off if it is stopping our builds. GitHub Dependabot will do this check for us anyway if we enable it.", "output": "the checker now runs on java17, even if we release on java8."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Extract dropping table from catalog from DataFrameWriter in case of overwriting table during creation\nDescription: Currently, if we call DataFrameWriter.saveAsTable(tableName) with the SaveMode equal to Overwrite and the DataSource is V1, the table is dropped from catalog directly in the DataFrameWriter and only then the CreateTable command is created. \r\n\r\nIn case if we want to somehow change the default behavior using Spark extensions, for instance, postpone the table dropping to decrease the time interval, when the old table is dropped and the new one is not created, such hardcoded behavior in DataFrameWriter makes it almost impossible if we want to use the original saveAsTable method. \r\n\r\nFor instance, if we call df.write.mode(\"overwrite\").insertInto(table_name) then it's possible to control the table removal logic if we provide our own implementation of the spark.sql.sources.commitProtocolClass. In case of saveAsTable it makes no sense, because a table is unconditionally dropped before handling the CreateTable command.\r\n\r\nTherefore, is it possible to extract the table dropping logic from DataFrameWriter to the corresponding LogicalPlan commands (as it was done for V2 Datasources) in order to give more flexibility to Spark extensions?", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update Surefire plugin to 3.5.3\nDescription: Hadoop currently uses the old 3.0.0-M4  surefire version, which swallows all exceptions thrown by @BeforeAll methods.\r\n\r\nThis leads to some tests not being run and not being reported as failing/erroring.\r\n\r\nUpdate to the latest version, which fixes this issue.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`\nDescription: Recently, `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` frequently fails.\r\n\r\n[https://github.com/apache/spark/actions/runs/18872699886/job/53854858890]\r\n\r\n \r\n{code:java}\r\n[info] - Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) have been lost during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-41, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info] \tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add dual-stack/IPv6 Support to HttpServer2\nDescription: To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp class to bind to an IPv6 address. WebApp uses the HttpServer2, and adding the IPv6 connector to this class makes the solution more elegant.\r\n\r\nTo enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host.\r\nWhen the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6.\r\nWhen java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors.\r\nTo disable IPv4, you need to configure the OS at the system level.\nQ: hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3303626071\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 55s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  1s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 40s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 14s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 1 new + 68 unchanged - 0 fixed = 69 total (was 68)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 44s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  42m 42s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 33s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  5s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 251m 12s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 69abc2152550 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 05144e7770fd55d4ed400b5ee8d71ea02d4d37b6 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/testReport/ |\r\n   | Max. process+thread count | 3098 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3304428637\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 48s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  0s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 57s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 43s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  42m 52s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 37s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  5s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 251m 40s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2f758f82d727 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4e384cfeaf21c2674e992b6ec288d9403a8d1adb |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/2/testReport/ |\r\n   | Max. process+thread count | 2679 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: cos use token credential will lost token field\nDescription: Hi,\r\n\r\nI've discovered a bug when accessing COSN using temporary credentials (access key, secret key, and session token).\r\n\r\nIn the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n\r\nFurthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n!image-2025-08-11-10-37-12-451.png|width=1048,height=540! \r\n!image-2025-08-11-10-42-36-375.png!\nQ: leosanqing opened a new pull request, #7866:\nURL: https://github.com/apache/hadoop/pull/7866\n\n   \r\n   ### Description of PR\r\n   In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n   Furthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [✔] Does the title or this PR starts with the corresponding JIRA issue id", "output": "leosanqing closed pull request #7866: HADOOP-19648. [hotfix] Cos use token credential will lose token field\nURL: https://github.com/apache/hadoop/pull/7866"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{code}\r\n======================================================================\r\nERROR [0.007s]: test_in_memory_data_source (pyspark.sql.tests.test_python_datasource.PythonDataSourceTests.test_in_memory_data_source)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps\r\n    return cloudpickle.dumps(obj, pickle_protocol)\r\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1537, in dumps\r\n    cp.dump(obj)\r\n    ~~~~~~~^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1303, in dump\r\n    return super().dump(obj)\r\n           ~~~~~~~~~~~~^^^^^\r\nTypeError: cannot pickle '_abc._abc_data' object\r\nwhen serializing dict item '_abc_impl'\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item '__annotate_func__'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item 'reader'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_python_datasource.py\", line 283, in test_in_memory_data_source\r\n    self.spark.dataSource.register(InMemoryDataSour", "output": "Fix `test_in_memory_data_source` in Python 3.14"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Depreacte broker-level config group.coordinator.rebalance.protocols\nDescription: https://github.com/apache/kafka/pull/20466#discussion_r2379695582", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Prevent spark.sql session state mutation within Python pipeline files\nDescription: We should ban all uses of spark.sql except for select statements.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK25] Bump ByteBuddy 1.17.6 and ASM 9.8 to support Java 25 bytecode\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Parsing of the scope claim does not comply with RFC-8693\nDescription: I notice that the code in Kafka for handling of the scopes claim does not comply with the RFC.\r\n\r\n[https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim |https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim]says:\r\n{quote}The value of the {{scope}} claim is a JSON string containing a space-separated list of scopes associated with the token, in the format described in [Section 3.3|https://www.rfc-editor.org/rfc/rfc6749#section-3.3] of [[RFC6749|https://datatracker.ietf.org/doc/html/rfc6749]]\r\n{quote}\r\n \r\n\r\nHowever the code in Kafka that parses the JWT payload does not permit a space separated list.  It would treat a value like \"email phone address\" as a single scope \"email phone address\" rather than a three separate scopes of \"email\", \"phone\", \"address\".\r\n\r\nThe affected code is here:\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/BrokerJwtValidator.java#L166]\r\n\r\n[https://github.com/apache/\nQ: This issue is not blocking me (I noticed it in passing), but I'm happy to put up a PR if there is interest in fixing it.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`\nDescription: \nQ: Issue resolved by pull request 251\n[https://github.com/apache/spark-connect-swift/pull/251]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add support for KLL quantiles functions based on DataSketches\nDescription: Documentation reference: [https://datasketches.apache.org/docs/KLL/KLLSketch.html].\r\n\r\nDataSketches code API reference: [https://apache.github.io/datasketches-java/6.1.0/org/apache/datasketches/kll/KllLongsSketch.html] \r\n\r\nReference PR for recently adding Theta sketches: [https://github.com/apache/spark/pull/51298]\nQ: We can use the following function names. For each category, we can support long integer, single-precision floating-point, and double-precision floating-point variants (it is necessary to specify which one we are using since the representation of the sketch buffers is different for different input value data types).\r\n\r\n \r\n\r\nAggregate functions to consume input values and return a sketch buffer:\r\n\r\n{{kll_sketch_agg_bigint(col)}}\r\n\r\n{{kll_sketch_agg_float(col)}}\r\n\r\n{{kll_sketch_agg_double(col)}}\r\n\r\n \r\n\r\nScalar functions to merge two sketch buffers together into another sketch buffer:\r\n\r\n{{kll_sketch_merge_bigint(sketch1, sketch2)}}\r\n\r\n{{kll_sketch_merge_float(sketch1, sketch2)}}\r\n\r\n{{kll_sketch_merge_double(sketch1, sketch2)}}\r\n\r\n \r\n\r\nScalar functions to extract a single value from the quantiles sketch representing the desired quantile given the input rank (for example, \"float median = sketch.getQuantile(0.5)\"). We can also implement the functions in this category to also support accepting an array of input ranks and return an array of result quantiles.\r\n\r\n{{kll_sketch_get_quantile_bigint(sketch, rank)}}\r\n\r\n{{kll_sketch_get_quantile_float(sketch, rank)}}\r\n\r\n{{kll_sketch_get_quantile_double(sketch, rank)}}\r\n\r\n \r\n\r\nScalar functions to extract a single value from the quantiles sketch representing the desired rank given the input quantile (for example, \"double rankOf1000 = sketch.getRank(1000)\"). We can also implement the functions in this category to also support accepting an array of input quantiles and return an array of result ranks.\r\n\r\n{{kll_sketch_get_rank_bigint(sketch, quantile)}}\r\n\r\n{{kll_sketch_get_rank_float(sketch, quantile)}}\r\n\r\n{{kll_sketch_get_rank_double(sketch, quantile)}}\r\n\r\n \r\n\r\nOptional, scalar functions to return a string representation of a sketch buffer:\r\n\r\n{{kll_sketch_to_string_bigint(sketch)}}\r\n\r\n{{kll_sketch_to_string_float(sketch)}}\r\n\r\n{{kll_sketch_to_string_double(sketch)}}", "output": "Here is a pull request to implement the functionality: [https://github.com/apache/spark/pull/52800|https://github.com/apache/spark/pull/52800]"}
{"instruction": "Answer the question based on the bug.", "input": "Title: hadoop-client-api exclude webapps/static front-end resources\nDescription: \nQ: cxzl25 opened a new pull request, #7804:\nURL: https://github.com/apache/hadoop/pull/7804\n\n   ### Description of PR\r\n   \r\n   `hadoop-client-api` contains some front-end resources in the webapps/static path.\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7804:\nURL: https://github.com/apache/hadoop/pull/7804#issuecomment-3076022688\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  72m 32s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 115m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   8m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 51s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 19s |  |  hadoop-client-api in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 190m 18s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7804 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 8a0fed5ae6d7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0f2cc88fd55ceb31d30a1157bebd34a35e39289f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-client-modules/hadoop-client-api U: hadoop-client-modules/hadoop-client-api |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7804/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve the testing experience for TransformWithState\nDescription: Currently it's hard to write unit tests for user's implementation of StatefulProcessor used in TransformWithState. User either needs to test it by running actual streaming query, or they need to refactor business logic into a separate function (that will be called from handleInputRows), and write unit tests for that function.\r\n \r\nI propose to add a simple testing framework for TransformWithState that will allow users easily test their business logic inside StatefulProcessor.\r\n \r\nOn high-level, it's a class TwsTester that takes StatefulProcessor and allows to feed in input rows and immediately returns what rows would be produced by TWS. It also allows to set and inspect state. This can be used in unit tests without having to run streaming query and it won't need RocksDB (it will use in-memory state store). I will start with implementing this for Scala users, potentially making it available for Python users later.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: KRaft voter auto join will add a removed voter immediately\nDescription: In v4.2.0, we are able to auto join a controller with the configuration `controller.quorum.auto.join.enable=true` set ([KIP-853|https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes#KIP853:KRaftControllerMembershipChanges-Controllerautojoining](KAFKA-19078)). This is a good improvement for controller addition, but it has a UX issue, which is that when a controller is removed via removeVoterRequest, it will be added immediately due to `controller.quorum.auto.join.enable=true`. In the KIP, we also mention you have to stop the controller before removing the controller:\r\n\r\n \r\n{noformat}\r\ncontroller.quorum.auto.join.enable:\r\n\r\nControls whether a KRaft controller should automatically join the cluster \r\nmetadata partition for its cluster id. If the configuration is set to \r\ntrue the controller must be stopped before removing the controller with kafka-metadata-quorum remove-controller.{noformat}\r\n \r\n\r\nThis is not a user friendly behavior in my opinion. And it will cause many confusion to users and thought there is something wrong in the controller removal. Furthermore, in the kubernetes environment which is controlled by the operator, it is not the cloud native way to shutdown a node, do some operation, then start it up. \r\n\r\n \r\n\r\nSo, I propose we can improve it by \"the removed controller will not be auto joined before this controller restarted\". That is:\r\n1. Once the controller is removed from voters set, it won't be auto joined even if `con", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The Surefire upgrade to 3.3+ added enableOutErrElements which defaults to true and includes system-out and system-err in the xml artifacts. This significantly increases their size: ~45KB to ~1MB in many cases. In my test environment, that leads to\r\n{code}\r\nRecording test results\r\nERROR: Step ‘Publish JUnit test result report’ aborted due to exception: \r\njava.lang.OutOfMemoryError: Java heap space\r\n{code}\r\n\r\nCapturing stdout seems useful when doing ad-hoc testing or smaller test runs. I'd propose adding a profile to set enableOutErrElements=false for CI environments where that extra output can cause problems.", "output": "Surefire upgrade leads to increased report output, can cause Jenkins OOM"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consistency of command-line arguments for consumer performance tests\nDescription: This implements KIP-1147 for kafka-consumer-perf-test.sh and kafka-share-consumer-perf-test.sh.\nQ: [~aheev] got there first :)", "output": "I made an error in Jira number while creating the PR. Updated it, but for some reason it doesn't link automatically now\r\n\r\nPR: https://github.com/apache/kafka/pull/20385"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Hook to enable / disable multi-partition remote fetch feature\nDescription: ", "output": "Patch Available"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SS | `Add TaskContext information to KafkaDataConsumer release() logs for better debugging`\nDescription: Currently, log messages in the {{release()}} method of {{KafkaDataConsumer}} do not include Spark task context information (e.g., TaskID).\r\n\r\nThis change adds task context details to the {{release()}} logs to improve traceability and debugging of Kafka consumer lifecycle events.\r\n\r\nWe previously added similar context in {{{}KafkaBatchReaderFactory.createReader{}}}, which has proven useful for debugging. Including this information in {{release()}} helps correlate consumer release metrics with the specific Spark tasks that borrowed the consumer.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use `MacOS 26` in `build_maven_java21_macos15.yml`\nDescription: \nQ: Issue resolved by pull request 52625\n[https://github.com/apache/spark/pull/52625]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A : Add option fs.s3a.dual-stack.enabled to enable Amazon S3 dual-stack endpoints\nDescription: [S3 Dual-stack|https://docs.aws.amazon.com/AmazonS3/latest/userguide/dual-stack-endpoints.html] allows a client to access an S3 bucket through a dual-stack endpoint. When clients request a dual-stack endpoint, the bucket URL resolves to an IPv6 address if possible, otherwise fallback to IPv4.\r\n\r\nFor more details on using S3 Dual-stack, please refer [Using dual-stack endpoints from the AWS CLI and the AWS SDKs|https://docs.aws.amazon.com/AmazonS3/latest/userguide/dual-stack-endpoints.html#dual-stack-endpoints-cli]", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix TestThrottledInputStream when bandwidth is equal to throttle limit\nDescription: Some tests in TestThrottledInputStream intermittently fail when the measured bandwidth is equal to the one set for throttling.\nQ: hadoop-yetus commented on PR #7517:\nURL: https://github.com/apache/hadoop/pull/7517#issuecomment-2729164087\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  25m 33s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 101m 28s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7517/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7517 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux beeb10e8f485 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1ee1fbe3af26fd7dac706815ae1c42647acab073 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7517/1/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7517/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 merged PR #7517:\nURL: https://github.com/apache/hadoop/pull/7517"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [ABFS]: Throw HTTPException when AAD token fetch fails \nDescription: Reported by [~enigma25] :\r\nIn [this code snippet](https://github.com/Azure/azure-data-lake-store-java/blob/26eca936da60286aa70b0dd7823ff5e627987bc4/src/main/java/com/microsoft/azure/datalake/store/oauth2/AzureADAuthenticator.java#L252-L293) from AzureADAuthenticator, the `conn` is being returned as null due to a (possibly) corrupted connection between Azure Data Lake Store Client and AAD while fetching AAD token. The code snippet linked, runs into an NPE. I wanted to know from the maintainers and community if this bug/symptom has been seen by them earlier and what might be the best way to handle this? Secondly, I wanted to know the right place for the fix too - whether it should be the application code or the SDK code itself should handle such NPEs and fail more gracefully?\r\nOpen to thoughts and comments.\r\nCheers,\r\nNikhil\r\n \r\n \r\n```\r\njava.util.concurrent.ExecutionException: java.lang.NullPointerException: Cannot invoke \"java.io.InputStream.read(byte[], int, int)\" because \"inStream\" is null\r\nat org.apache.kafka.connect.util.ConvertingFutureCallback.result(ConvertingFutureCallback.java:135)\r\nat org.apache.kafka.connect.util.ConvertingFutureCallback.get(ConvertingFutureCallback.java:122)\r\nat org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource.validateConfigs(ConnectorPluginsResource.java:129)\r\nat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:580)", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Surfaced during HADOOP-19696 and work with all the cloud connectors on the classpath.\r\n\r\nThere's a missing dependency causing the gcs connector to fail to register *when the first filesystem is instantiated*, because the service registration process loads the gcs connector class, instantiates one and asks for its schema.\r\n\r\nAs well as a sign of a problem, it's better to just add an entry in core-default.xml as this saves all classloading overhead. It's what the other asf bundled ones do.", "output": "google gs connector registration failing"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Restore Compatibility with EMRFS FileSystem\nDescription: After HADOOP-19278 , The S3N folder marker *_$folder$* is not skipped during listing of S3 directories leading to S3A filesystem not able to read data written by legacy Hadoop S3N filesystem and AWS EMR's EMRFS (S3 filesystem) leading to compatibility issues and possible migration risks to S3A filesystem.\nQ: shameersss1 opened a new pull request, #7410:\nURL: https://github.com/apache/hadoop/pull/7410\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   After [HADOOP-19278](https://issues.apache.org/jira/browse/HADOOP-19278) , The S3N folder marker _$folder$ is not skipped during listing of S3 directories leading to S3A filesystem not able to read data written by legacy Hadoop S3N filesystem and AWS EMR's EMRFS (S3 filesystem) leading to compatibility issues and possible migration risks to S3A filesystem.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Added integration test `ITestEMRFSCompatibility` and ran other UT/IT in `us-east-1 `region\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "steveloughran commented on code in PR #7410:\nURL: https://github.com/apache/hadoop/pull/7410#discussion_r1963335389\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestEMRFSCompatibility.java:\n##########\n@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a;\n+\n+import org.assertj.core.api.Assertions;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.touch;\n+import static org.apache.hadoop.fs.s3a.Constants.S3N_FOLDER_SUFFIX;\n+\n+/**\n+ * This test verifies that the EMRFS or legacy S3N filesystem compatibility with\n+ * S3A works as expected.\n+ */\n+public class ITestEMRFSCompatibility extends AbstractS3ATestBase {\n\nReview Comment:\n   are there other tests here? I think I'd like\r\n   \r\n   * list parent path reports an empty dir\r\n   * delete parent dir results in a getFileStatus(parent) => 404, and same for marker.\r\n   \r\n   \r\n   What does dir rename do? I know for normal / markers we only create a dir marker if there's nothing underneath, but that's just an optimisation. Here we'd want:\r\n   \r\n   ```\r\n   touch parent/src/subdir/$folder$\r\n   mv parent/src parent/dest\r\n   isDir(parent/dest/subdir)\r\n   isNotFound(parent/src)\r\n   ```"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Handle situations where broker responses appear logically incorrect\nDescription: We have seen a situation in which the ShareFetch responses from the broker appeared logically incorrect. The consumer should be more defensive, and at least log the problem.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: some tests failing on third-party stores\nDescription: Some tests against third-party stores fail\r\n- Includes fix for the assumeStoreAwsHosted() logic.\r\n- Documents how to turn off multipart uploads with third-party stores.\nQ: doing as a sequence with the SDK update. Many tests always fail and will need turning off, but we can at least have more tests for aws only features know to skip their tests when not running on AWS infra.", "output": "ITestS3AEndpointRegion.testCentralEndpointAndNullRegionWithCRUD with non aws bucket/credentials fails. \r\n\r\nFix: don't do that.\r\n\r\n{code}\r\n[ERROR] testCentralEndpointAndNullRegionWithCRUD(org.apache.hadoop.fs.s3a.ITestS3AEndpointRegion)  Time elapsed: 1.88 s  <<< ERROR!\r\njava.nio.file.AccessDeniedException: s3a://dellecs719/test/testCentralEndpointAndNullRegionWithCRUD/srcdir: getFileStatus on s3a://dellecs719/test/testCentralEndpointAndNullRegionWithCRUD/srcdir: software.amazon.awssdk.services.s3.model.S3Exception: The AWS Access Key Id you provided does not exist in our records. (Service: S3, Status Code: 403, Request ID: 2CZF3PJDSKRYABWS, Extended Request ID: h8T/uz2tPH4zwA6W/dTqqSZfm5lAT9JetkO8moPu9R9L5QkqTP0VwfWrAZWo3J/WMy/nollnxT8=):InvalidAccessKeyId\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:271)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:158)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4088)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3954)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem$MkdirOperationCallbacksImpl.probePathStatus(S3AFileSystem.java:3811)\r\n\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.probePathStatusOrNull(MkdirOperation.java:218)\r\n\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.getPathStatusExpectingDir(MkdirOperation.java:239)\r\n\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:134)\r\n\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:59)\r\n\tat org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation.apply(ExecutingStoreOperation.java:76)\r\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\r\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\r\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2844)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2863)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:3782)\r\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2496)\r\n\tat org.apache.hadoop.fs.s3a.ITestS3AEndpointRegion.assertOpsUsingNewFs(ITestS3AEndpointRegion.java:541)\r\n\tat org.apache.hadoop.fs.s3a.ITestS3AEndpointRegion.testCentralEndpointAndNullRegionWithCRUD(ITestS3AEndpointRegion.java:495)\r\n{code}"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Suppose that no new messages have been written to the target topic for a long time, for example, the producer has not generated new data for nearly an hour. On the consumer side, when the user sets enable.auto.commit=true, the  OFFSET_COMMIT request will still be repeatedly submitted every 5 seconds. This not only wastes network resources but also increases the burden on __consumer_offset.\r\n\r\n \r\nTherefore, maybe we can add a cache for committed offsets to check if the currently committed offset is consistent with the most recently successfully committed offset. If they are the same, this offset commit can be ignored. Of course, all the above applies to the subscribe mode.\r\n\r\n \r\nAdvantages:\r\n * Reduce unnecessary network requests\r\n * Alleviate the processing pressure on the broker side\r\n * Relieve the pressure of log cleaning for __consumer_offset\r\n\r\n \r\nDisadvantage:\r\n * There may be hidden bugs that need to be discussed and identified", "output": "Add automatic commit offset caching in subscribe mode"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove Redundant Validation in StripedReplicaPlacer\nDescription: Currently, the following validation checks are performed twice for each partition placement:\r\n # {{throwInvalidReplicationFactorIfNonPositive()}}\r\n\r\n # {{throwInvalidReplicationFactorIfTooFewBrokers()}}\r\n\r\n # {{throwInvalidReplicationFactorIfZero()}}\r\n\r\nThese checks are already performed in the public {{place()}} method before the partition placement loop begins. Since the cluster state and replication factor don't change during the placement operation, these checks only need to be performed once.\r\n\r\n \r\n\r\nThis redundant validation could be removed from the inner loop to improve performance, especially when placing many partitions.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When we want to run docker_sanity_test.py for a local image, we need to pass a local archive image via --kafka-archive to docker_build_test.py.\r\nBut currently it fails to run the test(docker_sanity_test.py) when we pass --kafka-archive.\r\nCurrently the tests run only when we pass a URL via --kafka-url.\r\n\r\n \r\n{code:java}\r\npython docker_build_test.py kafka/test --image-tag=4.2.0-SNAPSHOT --image-type=jvm --kafka-archive=/Users/shivsundarr/dev/opensource/kafka/core/build/distributions/kafka_2.13-4.2.0-SNAPSHOT.tgz{code}\r\nAfter building the image, got the following output when it attempted to run the tests.\r\n{code:java}\r\nTraceback (most recent call last):\r\n  File \"/Users/shivsundarr/dev/opensource/kafka/docker/docker_build_test.py\", line 50, in run_docker_tests\r\n    execute([\"wget\", \"-nv\", \"-O\", f\"{temp_dir_path}/kafka.tgz\", kafka_url])\r\n  File \"/Users/shivsundarr/dev/opensource/kafka/docker/common.py\", line 24, in execute\r\n    if subprocess.run(command).returncode != 0:\r\n       ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/shivsundarr/.pyenv/versions/3.12.7/lib/python3.12/subprocess.py\", line 548, in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/shivsundarr/.pyenv/versions/3.12.7/lib/python3.12/subprocess.py\", line 1026, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"/Users/shivsundarr/.pyenv/versions/3.12.7/lib/python3.12/subprocess.py\", line 1885, in _execute_child\r\n    self.pid = _fork_exec(\r\n               ^^^^^^^^^^^\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/Users/shivsundarr/dev/opensource/kafka/docker/docker_build_test.py\", line 85, in \r\n    run_docker_tests(args.image, args.tag, args.kafka_url, args.image_type)\r\n  File \"/Users/shivsundarr/dev/opensource/kafka/docker/docker_build_test.py\", line 55, in run_docker_tests\r\n    raise S", "output": "docker_build_test.py does not run tests when using --kafka-archive."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Kafka Broker Fails to Start After Upgrade to 4.1.0 Due to Invalid segment.bytes Configuration\nDescription: After upgrading our Kafka brokers to version {*}4.1.0{*}, the broker fails to start due to a misconfigured topic-level {{segment.bytes}} setting. The new version enforces a *minimum value of 1MB (1048576 bytes)* for this configuration, and any value below this threshold causes the broker to terminate during startup.\r\n*Error Details:*\r\n**\r\n \r\n{code:java}\r\n[2025-09-12 14:39:51,285] ERROR Encountered fatal fault: Error starting LogManager (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler) org.apache.kafka.common.config.ConfigException: Invalid value 75000 for configuration segment.bytes: Value must be at least 1048576 at org.apache.kafka.common.config.ConfigDef$Range.ensureValid(ConfigDef.java:989) ~[kafka-clients-4.1.0.jar:?]\r\n \r\n{code}\r\nIn our setup, some topics were previously configured with a lower segment.bytes value (e.g., 75000), which was allowed in earlier Kafka versions but is now invalid.\r\n\r\nAs a result Kafka broker cannot start, leading to downtime and unavailabil\nQ: Hi [~brandboat] , thank you for the suggestion.\r\n\r\nUnfortunately, reverting to a previous Kafka version to update the {{segment.bytes}} configuration is not a viable option for us. Kafka is embedded in our product, which follows a bi-weekly release cycle. Even if we were to fix the configuration in one release and upgrade Kafka in a subsequent one, there's no guarantee that all customers will upgrade sequentially. As a result, we require a mechanism to bypass this validation during the Kafka upgrade itself, ensuring compatibility regardless of the upgrade path.\r\n\r\nAs an additional note, we had previously tested an upgrade to Kafka 4.0.0 and did not encounter this {{segment.bytes}} issue - brokers started successfully. However, we had to roll back due to KAFKA-19427, which caused out-of-memory errors. This prompted us to move directly to 4.1.0, where the stricter enforcement of the {{segment.bytes}} minimum led to this startup failure.\r\n\r\nWe’re looking for guidance on how to handle this scenario cleanly, ideally without requiring a rollback or risking data loss.", "output": "{quote}Even if we were to fix the configuration in one release and upgrade Kafka in a subsequent one, there's no guarantee that all customers will upgrade sequentially.\r\n{quote}\r\nThis sounds like an issue with your release process. I think you should solve this on your end and not in upstream dependencies."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Metrics & semantic modeling in Spark\nDescription: SPIP: https://docs.google.com/document/d/1xVTLijvDTJ90lZ_ujwzf9HvBJgWg0mY6cYM44Fcghl0/edit?tab=t.0#heading=h.4iogryr5qznc", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Resolve build error caused by missing Checker Framework (NonNull not recognized)\nDescription: In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed.\r\n{code:java}\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[216,50] package org.checkerframework.checker.nullness.qual does not exist\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[2509,30] cannot find symbol\r\n[ERROR]   symbol:   class NonNull\r\n[ERROR]   location: class org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.AsyncThreadFactory\r\n {code}\r\nI checked the usage in the related modules, and we should use *org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.\nQ: pan3793 commented on code in PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#discussion_r2409341008\n\n\n##########\nhadoop-cloud-storage-project/hadoop-tos/src/main/java/org/apache/hadoop/fs/tosfs/util/Iterables.java:\n##########\n@@ -19,7 +19,7 @@\n package org.apache.hadoop.fs.tosfs.util;\n \n import org.apache.hadoop.util.Preconditions;\n-import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.Nullable;\n\nReview Comment:\n   I suspect this makes the `Nullable` useless, I don't think the static analyzer tools can recognize such a relocated annotation.", "output": "hadoop-yetus commented on PR #8015:\nURL: https://github.com/apache/hadoop/pull/8015#issuecomment-3375421893\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 35s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  22m 41s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   5m 25s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   4m 39s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   0m 24s | [/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-mvnsite-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) |  hadoop-hdfs-rbf in trunk failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 18s | [/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-mvnsite-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-javadoc-hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-hdfs-rbf in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tos in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 25s | [/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-spotbugs-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) |  hadoop-hdfs-rbf in trunk failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 16s | [/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/branch-spotbugs-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  24m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 23s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 37s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |   5m 41s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   5m 41s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   5m 11s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   5m 10s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  hadoop-hdfs-project_hadoop-hdfs-rbf-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 0 new + 0 unchanged - 1 fixed = 0 total (was 1)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  hadoop-hdfs-project_hadoop-hdfs-rbf-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2)  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  hadoop-cloud-storage-project_hadoop-tos-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 0 new + 0 unchanged - 2 fixed = 0 total (was 2)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  38m 54s |  |  hadoop-hdfs-rbf in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 54s |  |  hadoop-tos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 154m  4s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8015 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 1ad3180c6b2e 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f0c771fd1f48e1cc45617d4e2eb0afb552e5ba1f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/testReport/ |\r\n   | Max. process+thread count | 4610 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-rbf hadoop-cloud-storage-project/hadoop-tos U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8015/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: error stack traces printed on analytics stream factory close\nDescription: When you close an s3a filesystem there is a lot of ERROR level stack traces about a CancellationException -despite that being exactly what is wanted.\r\n\r\nCore of it comes from netty.\r\n{code}\r\n        Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 1 failure: Unable to execute HTTP request: The connection was closed during the request. The request will usually succeed on a retry, but if it does not: consider disabling any proxies you have configured, enabling debug logging, or performing a TCP dump to identify the root cause. If this is a streaming operation, validate that data is being read or written in a timely manner. Channel Information: ChannelDiagnostics(channel=[id: 0x801baead, L:0.0.0.0/0.0.0.0:59534 ! R:bucket.vpce-0117f6033eaf7aee5-2hxd9fg4.s3.us-west-2.vpce.amazonaws.com/10.80.134.179:443], channelAge=PT0.676S, requestCount=1, responseCount=0, lastIdleDuration=PT0.006284125S)\r\nCaused by: java.lang.IllegalStateException: executor not accep\nQ: steveloughran opened a new pull request, #7701:\nURL: https://github.com/apache/hadoop/pull/7701\n\n   \r\n   HADOOP-19567.\r\n   \r\n   catch all exception raised in stream close; log at debug\r\n   \r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Surfaced during a cloudstore bandwidth test; manual testing can check.\r\n   \r\n   Hard to write an ITest as it probably depends on the stream state at the time...will have to explore.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7701:\nURL: https://github.com/apache/hadoop/pull/7701#issuecomment-2897716585\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 17s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m  2s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 40s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  81m 22s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7701/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7701 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 730169d3cabd 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d847a1e7fd0f6950b4d06d8481c936455c97ea4e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7701/1/testReport/ |\r\n   | Max. process+thread count | 553 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7701/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Spark Executor launch task failed should return task killed message\nDescription: When not enable speculation ,executor launch task failed caused by OOM or other issue, DAGScheduler won;t know task failed and won't scheduler again, causing application stuck\r\n\r\n!image-2025-10-30-16-52-22-524.png|width=589,height=233!", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-distcp.\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FNS Over Blob] Marker creation fail exception should not be propagated\nDescription: Marker creation is attempted during certain operations such as {*}create{*}, {*}getPathStatus{*}, {*}setPathProperties{*}, and {*}rename{*}, in order to add a 0-byte file that signifies the existence of a folder at that path. However, if marker creation fails, the failure should not be propagated to the user.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have been having a huge restriction on the number of shuffle partitions in streaming - once the streaming query runs, there is no way but discard the checkpoint to change the number of shuffle partitions. There has been consistent requests for unblocking this.\r\n\r\nThe main reason of the limitation is due to the fact the stateful operator has fixed partitions and we should make sure it is unchanged. While the invariant is not changed, there is no technical reason to also disallow changing of the number of shuffle partitions in stateless shuffle e.g. stream-static join, MERGE INTO, etcetc.", "output": "Support changing stateless shuffle partitions upon restart of streaming query"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: region resolution within AWS infra always goes to us-east-2\nDescription: I think this is new to the V2 SDK, or at least our region logic there.\r\nWhen you try to connect to a bucket without specifying the region, then even if you're running in the same region of the store a HEAD request is still made to US Central. This adds latency, makes us-central a SPoF and if the VM/container has network rules which blocks such access, we actually timeout and eventually fail.\r\n\r\nWhile Machine configurations should ideally have the fs.s3a.endpoint.region setting configured, that information is actually provided as IAM metadata. Therefore it would be possible \r\n\r\nThis is actually included in the default region chain according to the SDK docs \"If running in EC2, check the EC2 metadata service for the region\", so maybe this isn't being picked up because\r\n\r\n# cross region access is being checked for first.\r\n# the region chain we are setting off doesn't check the EC2 metadata service.\r\n\r\nThe SDK region chain does do the right thing within AWS infra. How do we restore that whi\nQ: Steve, this is from our region logic: [https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/DefaultS3ClientFactory.java#L355]\r\n\r\nif no region is configured, and none could be determined from the endpoint, US_EAST_2 is used as default.  We actually don't end up using the EC2 metadata service, unless the region is configured as an empty string \"\" I believe. If region is never configured, we will never use the metadata service.\r\n\r\nAgree on using the default chain with EC2, but will need to update that region logic again...", "output": "This has surfaced in the wild again.\r\n\r\nI propose some new regions\r\n* \"sdk\": hand off to the sdk\r\n* \"auto\": do whatever we think is clever. May change from version to version\r\n* \"ec2\" code is running in EC2,"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump commons-lang3 to 3.18.0 due to CVE-2025-48924\nDescription: https://www.cve.org/CVERecord?id=CVE-2025-48924\r\n\r\nWill update commons-text to 1.14.0 which was released with commons-lang3 3.18.0. Due to https://issues.apache.org/jira/browse/HADOOP-19532 - seems best to upgrade them together.\nQ: pjfanning opened a new pull request, #7970:\nURL: https://github.com/apache/hadoop/pull/7970\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   CVE-2025-48924\r\n   \r\n   See https://issues.apache.org/jira/browse/HADOOP-19690 for reason to upgrade commons-text too.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7970:\nURL: https://github.com/apache/hadoop/pull/7970#issuecomment-3305413646\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 19s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  24m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  14m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 58s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  31m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 23s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  22m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 11s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  11m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  6s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  32m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 963m 33s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1144m 26s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7970 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux e242a07f343d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 82dae1282632826d8977c56821441f5b32ee1ec8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/testReport/ |\r\n   | Max. process+thread count | 4334 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7970/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Drop temporary functions in Arrow UDF tests\nDescription: \nQ: Issue resolved by pull request 52682\n[https://github.com/apache/spark/pull/52682]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Restore metrics are calculated by measuring the time of the init method: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java#L135]\r\n\r\nBut the restoration process has been moved to the poll loop: [https://github.com/apache/kafka/commit/b78d7ba5d58fa77b831244444eef005a62883f9b]", "output": "Restore metrics are calculated incorrectly"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Make open telemetry object instantiation configurable\nDescription: After the 3.7 release of Kafka, the java client will emit many warning messages and more objects deployed due to an open telemetry object instantiation (possibly due to KIP-714). It appears the open telemetry feature was enabled by default and there's no way for a user to disable it. \r\n\r\nUsers receive the INFO message defined [here |https://github.com/apache/kafka/blob/3.7.2/clients/src/main/java/org/apache/kafka/common/telemetry/internals/ClientTelemetryReporter.java#L479] after activating INFO log level for \"org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter\" class. This log line appears lots of times.\r\n\r\nIt appears that the occurrence of this info messages shows ClientTelemetry objects are being created if the feature is not explicitly disabled. Users would like this feature to be opt-in and not enabled by default as it is.\nQ: [~lacey] Thanks for raising it but it's not a bug rather the feature is per design in KIP-714, i.e. feature default behaviour as `opt-in`.", "output": "The user has the ability to disable this feature - enable.metrics.push=false"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "document the behavior of  \"-1\" (HIGH_WATERMARK)", "output": "improve the documentation of `RecordsToDelete`"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix time unit mismatch in method updateDeferredMetrics\nDescription: Fix time unit mismatch in method updateDeferredMetrics.\r\n\r\nThe time unit of passed param is nanos. But rpcmetrics's is mills.\nQ: hfutatzhanghb closed pull request #7557: HADOOP-19521. Fix time unit mismatch in method updateDeferredMetrics.\nURL: https://github.com/apache/hadoop/pull/7557", "output": "hfutatzhanghb opened a new pull request, #7557:\nURL: https://github.com/apache/hadoop/pull/7557\n\n   ### Description of PR\r\n   Fix time unit mismatch in method updateDeferredMetrics.\r\n   \r\n   The time unit of passed param is nanos. But rpcmetrics's is mills."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: make container build work on macOS Tahoe\nDescription: macOS Tahoe includes a native container daemon and runtime and it is supposed to be near feature-compatible with docker. In principle, it should be possible to run the container build using the container command line on macOS Tahoe.\r\n\r\nIt would be great if we can add this functionality to the build script so folks can build on macOS natively without creating another VM.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Problem*\r\n\r\nWhile building Hadoop native code on the riscv64 architecture, the build fails due to missing support for bswap operations. The standard implementation relies on compiler intrinsics or platform-specific assembly which are not defined for RISC-V targets.\r\n\r\nThis results in compilation errors such as:\r\n\r\n* Error: unrecognized opcode `bswap a4'\r\n* Error: unrecognized opcode `bswap a3'\r\n\r\n*Resolution*\r\n\r\nAdd bswap support for RISC-V.", "output": "Add bswap support for RISC-V"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use `4.1.0-preview3` instead of `RC1`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade commons-validator to 1.10.0\nDescription: In KAFKA-19359, the commons-beanutils transitive dependency was force bumped in the project to avoid related CVEs. The commons-validator already has a new release, which solves this problem:\r\n\r\n[https://github.com/apache/commons-validator/tags]\r\n\r\nThe workaround could be deleted as part of the version bump.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implement Geography and Geometry accessors across Catalyst\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Streams groups sometimes describe as NOT_READY when STABLE. That is, the group is configured and all topics exist, but when you use LIST_GROUP and STREAMS_GROUP_DESCRIBE, the group will show up as not ready.\r\n\r\nThe root cause seems to be that [https://github.com/apache/kafka/pull/19802] moved the creation of the soft state configured topology from the replay path to the heartbeat. This way, LIST_GROUP and STREAMS_GROUP_DESCRIBE, which show the snapshot of the last committed offset, may not show the configured topology, and therefore assume that the streams group is NOT_READY instead of STABLE.", "output": "Streams groups sometimes describe as NOT_READY when STABLE"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce client-side Geography and Geometry classes\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-dynamometer-workload.\nDescription: \nQ: slfan1989 opened a new pull request, #7587:\nURL: https://github.com/apache/hadoop/pull/7587\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19432. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-dynamometer-workload.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7587:\nURL: https://github.com/apache/hadoop/pull/7587#issuecomment-2788155635\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m  7s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 52s |  |  hadoop-dynamometer-workload in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  77m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7587/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7587 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f3c6312400bb 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8e75e22d749c761d646c0dc2863b82bf1f0bd8f2 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7587/1/testReport/ |\r\n   | Max. process+thread count | 678 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload U: hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7587/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update assertGeneratedCRDMatchesHelmChart to include diff\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add -o option in hdfs \"count\" command to show the owner's summarization.\nDescription: Add -o option in hdfs \"count\" command to show the owner's summarization.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Upgrade kafka-clients to 3.9.0 to fix [https://nvd.nist.gov/vuln/detail/CVE-2024-31141]", "output": "Upgrade kafka to 3.9.0 to fix CVE-2024-31141"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Make DSv2 table resolution aware of cached tables. If the user executes CACHE TABLE t, the subsequent resolution calls must use the cached table metadata, instead of potentially allowing the connectors to refresh and break the cache lookup.", "output": "Make DSv2 table resolution aware of cached tables"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Background:*\r\n\r\n \r\n\r\nCurrently, the two implementations of org.apache.hadoop.security.SecurityUtil.HostResolver, *StandardHostResolver and QualifiedHostResolver* in Hadoop performs hostname resolution each time it is called. *Each heartbeat between the AM and RM causes the RM to invoke the* HostResolver#getByName {*}method once{*}. In large-scale clusters running numerous applications, this results in *a high frequency of redundant hostname resolutions.*\r\n\r\n \r\n\r\n*Proposal:*\r\n\r\n \r\n\r\nIntroduce a caching mechanism in HostResolver to store resolved hostnames for a configurable duration. This would:\r\n\r\n•Reduce redundant DNS queries.\r\n\r\n•Improve performance for frequently used hostnames.\r\n\r\n•Allow configuration options for cache size and TTL (Time-to-Live).\r\n\r\n \r\n\r\n*Suggested Implementation:*\r\n\r\n1.{*}Leverage Existing CachedResolver{*}:\r\n\r\nThe NodesListManager.CachedResolver class in Hadoop already implements a caching mechanism for hostname resolution. Instead of introducing an entirely new solution, we propose *extracting the caching logic from* NodesListManager.CachedResolver {*}into a separate reusable utility class{*}.\r\n\r\n2.{*}Create a Shared Caching Utility{*}:\r\n\r\n•Extract the caching logic from NodesListManager.CachedResolver.\r\n\r\n•Implement a new class, e.g., HostnameCache, and place it in the Hadoop Common module to ensure it can be used across different components.\r\n\r\n3.{*}Integrate{*} HostnameCache with *HostResolver &QualifiedHostResolver*:\r\n\r\n•Modify HostResolver to use HostnameCache for hostname lookups.\r\n\r\n•Update NodesListManager.CachedResolver to use HostnameCache instead of its own internal cache.", "output": "Add Caching Mechanism to HostResolver to Avoid Redundant Hostname Resolutions"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Remove Duplicates from Blob Endpoint Listing Across Iterations\nDescription: On FNS-Blob, List Blobs API is known to return duplicate entries for the non-empty explicit directories. One entry corresponds to the directory itself and another entry corresponding to the marker blob that driver internally creates and maintains to mark that path as a directory. We already know about this behaviour and it was handled to remove such duplicate entries from the set of entries that were returned as part current list iterations.\r\n\r\nDue to possible partition split if such duplicate entries happen to be returned in separate iteration, there is no handling on this and caller might get back the result with duplicate entries as happening in this case. The logic to remove duplicate was designed before the realization of partition split came.\r\n\r\nThis PR fixes this bug\nQ: anmolanmol1234 commented on code in PR #7614:\nURL: https://github.com/apache/hadoop/pull/7614#discussion_r2044245375\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemListStatus.java:\n##########\n@@ -532,6 +532,28 @@ public void testEmptyContinuationToken() throws Exception {\n         .describedAs(\"Listing Size Not as expected\").hasSize(1);\n   }\n \n+  @Test\n+  public void testDuplicateEntriesAcrossListBlobIterations() throws Exception {\n\nReview Comment:\n   You can add one more test where there are more than one files in the directory and max list result is 1 and multiple list status calls and verify the overall file statuses don't have any duplicate entries", "output": "hadoop-yetus commented on PR #7614:\nURL: https://github.com/apache/hadoop/pull/7614#issuecomment-2804684790\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m  8s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7614/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 5 new + 1 unchanged - 0 fixed = 6 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m  1s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 51s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 128m 23s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7614/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7614 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 1f91c61b08e7 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bca367cd74c0d86e37b3b14ec1a9e70ddefebfbb |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7614/1/testReport/ |\r\n   | Max. process+thread count | 707 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7614/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve repartition optimization\nDescription: Kafka Streams supports \"merge.repartition.topic\" optimization. This optimization tries to push repartition topics further upstream, to be able to merge multiple repartition topics from different downstream branches into a single repartition topic.\r\n\r\nIn the current implementation, the optimization stops to push repartition topics upstream, if the parent node is a value-changing operation. We cannot push-up further, because the data type might change, and thus we won't have the correct serde at hand.\r\n\r\nHowever, we could actually also inherit the correct serde from further upstream. Thus, we could actually push a repartition-step upstream (and at the same time, switch the serdes), if we can find the right serde before the value-changing operations further upstream.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Document List Offset changes in upgrade.html\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add SparkThrowable wrapper to workaround Py4J limitation\nDescription: We could have a wrapper class to work around the problem in https://issues.apache.org/jira/browse/SPARK-52022", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Streaming Query RecentProgress performance regression in Classic Pyspark\nDescription: We have identified a significant performance regression in Apache Spark's streaming recentProgress method in python notebook starting from version 4.0.0. The time required to fetch recentProgress increases substantially as the number of progress records grows, creating a linear or worse scaling issue. We only observe this behavior in classic pyspark.\r\n\r\nWith the following code, it output charts for time it takes to get recentProgress before and after changes in [this commit|https://github.com/apache/spark/commit/22eb6c4b0a82b9fcf84fc9952b1f6c41dde9bd8d#diff-4d4ed29d139877b160de444add7ee63cfa7a7577d849ab2686f1aa2d5b4aae64]\r\n\r\n```\r\n%python\r\nfrom datetime import datetime\r\nimport time\r\n\r\ndf = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\r\nq = df.writeStream.format(\"noop\").start()\r\nprint(\"begin waiting for progress\")\r\n\r\nprogress_list = []\r\ntime_diff_list = []\r\n\r\nnumProgress = len(q.recentProgress)\r\nwhile numProgress < 70 and q.exception() is None:\r\ntime.sleep(1)\r\nbefore\nQ: Issue resolved by pull request 52688\n[https://github.com/apache/spark/pull/52688]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: asyncCheckpoint.enabled incompatible with transformWithStateInPandas (StatefulProcessor)\nDescription: In structured streaming job, we use transformWithStateInPandas stateful processing and we have com.databricks.sql.streaming.state.RocksDBStateStoreProvider enabled. Everything works as expected.\r\n\r\nBut when we enable:\r\nspark.conf.set(\"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\", \"true\")\r\n\r\nWe get an error: org.apache.spark.SparkUnsupportedOperationException: Synchronous commit is not supported. Use asynchronous commit.\r\n \r\nWe have successfully replicated that in notebook, and also tried the same thing with applyInPandasWithState.\r\n * transformWithStateInPandas - throws Synchronous commit error\r\n * applyInPandasWithState - works as expected\r\n\r\nWe followed this guide: [https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/async-checkpointing] and also used chatGPT for references.\r\n\r\nLooking through the stacktrace, It seems like the issue is where {{TransformWithStateInPandasExec}} uses the sync {{commit()}} instead of the async {{{}commitAsync(){}}}.\r\n\r\n \r\n\r\nPlease check the attached notebook below", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Support of CSE-KMS in version 3.4.1 seems to be broken\nDescription: Seem like on Commit c54bf19 (moving to aws sdk v2) the support of CSE-KMS was dropped or the way to configure it was changed without updating documentation. \r\n\r\n[https://github.com/apache/hadoop/commit/81d90fd65b3c0c7ac66ebae78e22dcb240a0547b#diff-a8dabc9bdb3ac3b04f92eadd1e3d9a7076d8983ca4fb7d1d146a1ac725caa309]\r\n\r\nin file: DefaultS3ClientFactory.java\r\n\r\nfor example code that was removed:\r\n\r\n!image-2025-07-21-18-13-16-765.png!\r\n\r\n \r\nFYI: [~stevel@apache.org]\nQ: marking as resolved by HADOOP-18708.\r\n\r\n[~igreenfi] if You could checkout and test the branch-3.4.2  release and make sure that CSE works for you, we can be confident that the final release works too. Please do try this -or at least test the next RC. thanks", "output": "[~stevel@apache.org] Where I can find how to configure it?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [Connect] Provide a configuration to disable SNI required and SNI host checking for the REST API\nDescription: h3. Summary\r\n\r\nThe upgrade to Jetty 12 in Kafka 4.0 enables a strict SNI host check by default for the Connect REST API. This change breaks HTTPS request forwarding between Connect workers when they connect via IP address, causing requests to fail with a 400: Invalid SNI error.\r\nh3. The Problem\r\n\r\nPrior to Kafka 4.0, the Jetty server used for the Connect REST API did not enforce a strict match between the TLS SNI hostname and the HTTP Host header.\r\n\r\nWith the upgrade to Jetty 12, this check is now enabled by default at the HTTP level. This causes legitimate HTTPS requests to fail in environments where the client connects using an IP address or a hostname that is not listed in the server's TLS certificate.\r\n\r\nThis results in the following error:\r\n{code:java}\r\norg.eclipse.jetty.http.BadMessageException: 400: Invalid SNI\r\n{code}\r\nh3. Impacted Use Case: Inter-Node Request Forwarding\r\n\r\nThis change specifically breaks the request forwarding mechanism between Connect workers in a common deployment scenario:\r\n # A follower Connect instance needs to forward a REST request to the leader.\r\n # The follower connects directly to the leader's IP address over HTTPS.\r\n # Security is handled by mTLS certificates, often managed by a custom certificate provider.\r\n\r\nThis setup worked flawlessly before Kafka 4.0. Now, because the follower connects via IP, the SNI check fails, and the forwarding mechanism is broken.\r\nh3. Proposed Solution\r\n\r\nThis behavior cannot be disabled through any existing Ka", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Surefire upgrade leads to increased report output, can cause Jenkins OOM\nDescription: The Surefire upgrade to 3.3+ added enableOutErrElements which defaults to true and includes system-out and system-err in the xml artifacts. This significantly increases their size: ~45KB to ~1MB in many cases. In my test environment, that leads to\r\n{code}\r\nRecording test results\r\nERROR: Step ‘Publish JUnit test result report’ aborted due to exception: \r\njava.lang.OutOfMemoryError: Java heap space\r\n{code}\r\n\r\nCapturing stdout seems useful when doing ad-hoc testing or smaller test runs. I'd propose adding a profile to set enableOutErrElements=false for CI environments where that extra output can cause problems.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In Hadoop 3.5.0 we uses GlassFish Jersey (modern Jersey).\r\n\r\nIn Hadoop 3.4.0 we use Sun Jersey (legacy Jersey).\r\n\r\nJetty submodule is not published under _com.sun.jersey.jersey-test-framework_ so upgrading jersey will be a better idea to remove grizzly-http-* dependencies (which is already done in 3.5.0 through  YARN-11793).", "output": "Backport HADOOP-15984 (Update jersey from 1.19 to 2.x) on branch-3.4.0 "}
{"instruction": "Answer the question based on the bug.", "input": "Title:  StreamsOnTasksAssignedCallbackNeededEvent could not be completed \nDescription: In several tests, for example PlaintextAdminIntegrationTest, we see this exception silently triggered when the consumer is closed.\r\n\r\n \r\n{code:java}\r\norg.apache.kafka.common.errors.TimeoutException: StreamsOnTasksAssignedCallbackNeededEvent could not be completed before the consumer closed[2025-09-05 14:17:40,972] ERROR [Consumer clientId=consumer-stream_group_id_1-1, groupId=stream_group_id_1] Reconciliation failed: callback invocation failed for tasks LocalAssignment{localEpoch=0, activeTasks={subtopology-0=[0, 1, 2]}, standbyTasks={}, warmupTasks={}} (org.apache.kafka.clients.consumer.internals.StreamsMembershipManager:1077)java.util.concurrent.CompletionException: org.apache.kafka.common.errors.TimeoutException: StreamsOnTasksAssignedCallbackNeededEvent could not be completed before the consumer closed\tat java.base/java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:368)\tat java.base/java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:3\nQ: I'm closing this, since it's noise and as disucssed wih Lianet, the noise is hard avoid", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "All subclasses of EpochState do not throw an IOException when closing, so catching it is unnecessary. We could override close to remove the IOException declaration and streamline the code\r\n\r\n{code:java}\r\n        if (state != null) {\r\n            try {\r\n                state.close();\r\n            } catch (IOException e) {\r\n                throw new UncheckedIOException(\r\n                    \"Failed to transition from \" + state.name() + \" to \" + newState.name(), e);\r\n            }\r\n        }\r\n{code}", "output": "EpochState should override close to avoid throwing IOException"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix `initialize` to add `CREATE` option additionally in `DriverRunner`\nDescription: When submitting jobs to standalone cluster using restful api, we get errors like:\r\n\r\n```\r\n25/10/30 16:02:59 INFO DriverRunner: Killing driver process!\r\n25/10/30 16:02:59 INFO CommandUtils: Redirection to /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stdout closed: Stream closed\r\n25/10/30 16:02:59 WARN Worker: Driver driver-20251030160259-0020 failed with unrecoverable exception: java.nio.file.NoSuchFileException: /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stderr\r\n\r\n```\r\n\r\nWe need to fix the usage of `Files.writeString` in DriverRunner\nQ: Issue resolved by pull request 52789\n[https://github.com/apache/spark/pull/52789]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: setting --no-initial-controllers flag should not validate kraft version against metadata version\nDescription: Just because a controller node sets `--no-initial-controllers` flag does not mean it is necessarily running kraft.version=1. The more precise meaning is that the controller node does not know what kraft version the cluster should be in.\r\n\r\nIt is a valid configuration to run a static quorum defined by `controller.quorum.voters` but have all the controllers format with `--no-initial-controllers`.\nQ: 4.1: https://github.com/apache/kafka/commit/012e4ca6d8fcd9a76dcb60480d9ba9cb7827816e", "output": "4.0: https://github.com/apache/kafka/commit/099e91f5fc7e0a44ffec05d60cba650ceea4109a"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Ingress Operations\nDescription: We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Ingress Related operations on blob Endpoint (Create + Append + Flush filesystem calls)\r\n\r\nThis Jira tracks implementing these tests.\nQ: hadoop-yetus commented on PR #7424:\nURL: https://github.com/apache/hadoop/pull/7424#issuecomment-2674706460\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  8s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m  0s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 3 new + 11 unchanged - 0 fixed = 14 total (was 11)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 58s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 134m 26s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7424 |\r\n   | JIRA Issue | HADOOP-19444 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 865901031f1d 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6039e0e78db5f7428899066b00da992623adf9e3 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/1/testReport/ |\r\n   | Max. process+thread count | 524 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7424:\nURL: https://github.com/apache/hadoop/pull/7424#issuecomment-2678447819\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   7m  3s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 26s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  78m 19s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7424 |\r\n   | JIRA Issue | HADOOP-19444 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a259f9a08739 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fd992497f587b1061cfb1604b6ff626d4d6b657e |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/2/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Make ResolvedCollation evaluable\nDescription: In this PR I propose to make ResolvedCollation evaluable. By making ResolvedCollation evaluable, it can now pass the canEvaluateWithinJoin check, allowing the optimizer to use efficient hash joins for queries with inline COLLATE in join conditions (before this change we would fallback to BroadcastNestedLoopJoin).\nQ: Hi, [~mihailoale-db] .\r\n\r\nApache Spark community has a policy which manages `Fix Version` and `Target Version` like the following. So, please don't set it when you file a JIRA issue.\r\n\r\n- https://spark.apache.org/contributing.html\r\n\r\n{quote}Do not set the following fields:\r\n- Fix Version. This is assigned by committers only when resolved.\r\n- Target Version. This is assigned by committers to indicate a PR has been accepted for possible fix by the target version.\r\n{quote}", "output": "Issue resolved by pull request 52779\n[https://github.com/apache/spark/pull/52779]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Enhance the documentation for producer headers\nDescription: 1. `Header#key` never returns null\r\n2. `Header#value` may return null\r\n3. the order of  `Iterable headers` passed to a `ProducerRecord` is preserved and will match the order of eaders in the corresponding `ConsumerRecord`", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add a separate docker file for Python 3.14 daily build\nDescription: \nQ: Issue resolved by pull request 52544\n[https://github.com/apache/spark/pull/52544]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Inter-cluster communication is normal without packet loss, and the cluster is properly configured.\r\nThe Kafka server continuously prints the following logs:\r\n{code:java}\r\n[2025-08-25 19:08:55,581] INFO [RaftManager id=1] Become candidate due to fetch timeout (org.apache.kafka.raft.KafkaRaftClient)\r\n[2025-08-25 19:08:55,686] INFO [RaftManager id=1] Disconnecting from node 2 due to request timeout. (org.apache.kafka.clients.NetworkClient)\r\n[2025-08-25 19:08:55,686] INFO [RaftManager id=1] Cancelled in-flight FETCH request with correlation id 128927 due to node 2 being disconnected (elapsed time since creation: 5147ms, elapsed time since send: 5146ms, throttle time: 0ms, request timeout: 5000ms) (org.apache.kafka.clients.NetworkClient)\r\n[2025-08-25 19:09:33,274] INFO [NodeToControllerChannelManager id=1 name=heartbeat] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)\r\n[2025-08-25 19:09:33,274] INFO [NodeToControllerChannelManager id=1 name=heartbeat] Cancelled in-flight BROKER_HEARTBEAT request with correlation id 871 due to node 3 being disconnected (elapsed time since creation: 4004ms, elapsed time since send: 4004ms, throttle time: 0ms, request timeout: 4000ms) (org.apache.kafka.clients.NetworkClient)\r\n[2025-08-25 19:09:33,807] INFO [RaftManager id=1] Disconnecting from node 3 due to request timeout. (org.apache.kafka.clients.NetworkClient)\r\n[2025-08-25 19:09:33,807] INFO [RaftManager id=1] Cancelled in-flight FETCH request with correlation id 128995 due to node 3 being disconnected (elapsed time since creation: 5720ms, elapsed time since send: 5720ms, throttle time: 0ms, request timeout: 5000ms) (org.apache.kafka.clients.NetworkClient) {code}\r\nAdjust Kafka parameters as follows:\r\n{code:java}\r\n# default 2000\r\nbroker.heartbeat.interval.ms=4000\r\n# default 9000\r\nbroker.session.timeout.ms=10000\r\n# default 2000\r\ncontroller.quorum.request.timeout.ms=5000\r\n# default 1000\r\ncontroller.quorum.election.timeout.ms=5000\r\n# default 1000\r\n", "output": "Controller keeps switching and occasionally goes offline."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hadoop currently uses the old 3.0.0-M4  surefire version, which swallows all exceptions thrown by @BeforeAll methods.\r\n\r\nThis leads to some tests not being run and not being reported as failing/erroring.\r\n\r\nUpdate to the latest version, which fixes this issue.", "output": "Update Surefire plugin to 3.5.3"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Implement transform in column API in PySpark\nDescription: related to https://issues.apache.org/jira/browse/SPARK-53779\nQ: I will work on this.", "output": "Issue resolved by pull request 52593\n[https://github.com/apache/spark/pull/52593]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support changing stateless shuffle partitions upon restart of streaming query\nDescription: We have been having a huge restriction on the number of shuffle partitions in streaming - once the streaming query runs, there is no way but discard the checkpoint to change the number of shuffle partitions. There has been consistent requests for unblocking this.\r\n\r\nThe main reason of the limitation is due to the fact the stateful operator has fixed partitions and we should make sure it is unchanged. While the invariant is not changed, there is no technical reason to also disallow changing of the number of shuffle partitions in stateless shuffle e.g. stream-static join, MERGE INTO, etcetc.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use Swift 6.2 for all Linux CIs\nDescription: \nQ: Issue resolved by pull request 255\n[https://github.com/apache/spark-connect-swift/pull/255]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade spotbug to 4.9.4\nDescription: see discussion https://github.com/apache/kafka/pull/20295#issuecomment-3146551515", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip `test_to_feather` in Python 3.14\nDescription: \nQ: Issue resolved by pull request 52771\n[https://github.com/apache/spark/pull/52771]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The current SpotBugs version used in the project is 4.2.2 (SpotBugs) and 4.2.0 (SpotBugs Maven Plugin), which is not fully compatible with JDK 17 compilation. To ensure proper functionality of the code quality check tool in a JDK 17 environment, SpotBugs needs to be upgraded to the latest version that supports JDK 17.", "output": "Upgrade SpotBugs Version to Support JDK 17 Compilation"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hi Team,\r\n\r\nWe were facing kafka issue where two of the kafka brokers were fenced and Kafka was not able to process messages. We are using Kafka 4.0.0 version. Below are the errors.\r\n\r\n \r\n\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: [2025-09-22 07:41:42,419] ERROR Encountered fatal fault: Unexpected error in raft IO thread (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler)\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: java.lang.IllegalStateException: Received request or response with leader OptionalInt[3] and epoch 55 *which is inconsistent with current leader* OptionalInt.empty and epoch 55\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.maybeTransition(KafkaRaftClient.java:2528) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.maybeHandleCommonResponse(KafkaRaftClient.java:2484) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleFetchResponse(KafkaRaftClient.java:1707) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleResponse(KafkaRaftClient.java:2568) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleInboundMessage(KafkaRaftClient.java:2724) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.poll(KafkaRaftClient.java:3460) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClientDriver.doWork(KafkaRaftClientDriver.java:64) [kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:136) [kafka-server-common-4.0.0.jar:?]\r\n\r\nBelow metrics shows Fenceborker count as 2.0\r\n\r\nkafka_controller_KafkaController_Value\\{name=\"ActiveBrokerCount\",} 1.0\r\nkafka_con", "output": "Two Kafka brokers were not active in 3 node cluster setup"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update non-thirdparty Guava version to 32.0.1\nDescription: Guava has been already updated to 32.0.1 in hadoop-thirdparty.\r\nHowever, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners.\r\n\r\nSync the non-thirdparty Guava version to the thirdparty one.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix the busy loop occurring in the broker observer\nDescription: see this comment https://github.com/apache/kafka/pull/19589#discussion_r2274143020", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade nimbusds to 10.0.2\nDescription: Includes fix for CVE-2025-53864", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use jackson-bom to set jackson versions\nDescription: It is much easier to maintain than the current direct dependencyManagement entries.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob] Updating Documentations of Hadoop Drivers for Azure\nDescription: Fixing some typos, details and adding links for better readability in the documentation files for ABFS driver.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Fix WASB ABFS compatibility issues\nDescription: Fix WASB ABFS compatibility issues. Fix issues such as:-\r\n # BlockId computation to be consistent across clients for PutBlock and PutBlockList\r\n # Restrict url encoding of certain json metadata during setXAttr calls.\r\n # Maintain the md5 hash of whole block to validate data integrity during flush.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][Tests] Update Test Scripts to Run Tests with Blob Endpoint\nDescription: Following Cobination of test Suites will run as a part of CI after blob endpoint support has been added.\r\n\r\nHNS-OAuth-DFS\r\nHNS-SharedKey-DFS\r\nNonHNS-SharedKey-DFS\r\nAppendBlob-HNS-OAuth-DFS\r\nNonHNS-SharedKey-Blob\r\nNonHNS-OAuth-DFS\r\nNonHNS-OAuth-Blob\r\nAppendBlob-NonHNS-OAuth-Blob\r\nHNS-Oauth-DFS-IngressBlob\r\nNonHNS-Oauth-DFS-IngressBlob\nQ: anmolanmol1234 commented on code in PR #7344:\nURL: https://github.com/apache/hadoop/pull/7344#discussion_r1938879124\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -524,10 +524,10 @@ public boolean isDfsToBlobFallbackEnabled() {\n   public void validateConfiguredServiceType(boolean isHNSEnabled)\n       throws InvalidConfigurationValueException {\n     // TODO: [FnsOverBlob][HADOOP-19179] Remove this check when FNS over Blob is ready.\n-    if (getFsConfiguredServiceType() == AbfsServiceType.BLOB) {\n-      throw new InvalidConfigurationValueException(FS_DEFAULT_NAME_KEY,\n-          \"Blob Endpoint Support not yet available\");\n-    }\n+//    if (getFsConfiguredServiceType() == AbfsServiceType.BLOB) {\n\nReview Comment:\n   remove todo and comments as well", "output": "anmolanmol1234 commented on code in PR #7344:\nURL: https://github.com/apache/hadoop/pull/7344#discussion_r1938879600\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -524,10 +524,10 @@ public boolean isDfsToBlobFallbackEnabled() {\n   public void validateConfiguredServiceType(boolean isHNSEnabled)\n       throws InvalidConfigurationValueException {\n     // TODO: [FnsOverBlob][HADOOP-19179] Remove this check when FNS over Blob is ready.\n-    if (getFsConfiguredServiceType() == AbfsServiceType.BLOB) {\n-      throw new InvalidConfigurationValueException(FS_DEFAULT_NAME_KEY,\n-          \"Blob Endpoint Support not yet available\");\n-    }\n+//    if (getFsConfiguredServiceType() == AbfsServiceType.BLOB) {\n\nReview Comment:\n   or this change has been removed as part of documentation PR"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Disallow create too many partitions in ZK mode\nDescription: Also when running in Zookeeper mode Kafka can go OOM if an attempt is made to create a topic with too many partitions, or to extend a topic with too many partitions.\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-17870\r\nhttps://issues.apache.org/jira/browse/KAFKA-19673\r\n\r\nI think it is worth to port also the constrain introduded in KRaft mode to the 3.9 version of Kafka.", "output": "Patch Available"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix the \"6.6 Java Version\" for branch 3.9\nDescription: see https://lists.apache.org/thread/xv04txcy4wwtpkn1zfqxqf95098ls61h\nQ: The web site needs to be updated :)", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In this PR I propose that we add tree node pattern bits for supported expressions in ParameterizedQuery argument list to prepare implementation of parameters in single-pass framework.", "output": "Add tree node pattern bits for supported expressions in ParameterizedQuery argument list"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce the framework for adding ST functions in Scala\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Support for new auth type: User-bound SAS\nDescription: Adding support for new authentication type: user bound SAS", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "1. `Header#key` never returns null\r\n2. `Header#value` may return null\r\n3. the order of  `Iterable headers` passed to a `ProducerRecord` is preserved and will match the order of eaders in the corresponding `ConsumerRecord`", "output": "Enhance the documentation for producer headers"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I hit this when I ran a pipeline that had no flows:\r\n```\r\norg.apache.spark.SparkUserAppException: User application exited with 1\r\n\tat org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:127)\r\n\tat org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1028)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:226)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:95)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1166)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1175)\r\n\tat org.apache.spark.deploy.SparkPipelines$.main(SparkPipelines.scala:42)\r\n\tat org.apache.spark.deploy.SparkPipelines.main(SparkPipelines.scala)\r\n```\r\n\r\nThis is not information that's relevant to the user.", "output": "Improve `SparkSubmit` to invoke `exitFn` with the root cause instead of `SparkUserAppException`"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: stream write/close fails badly once FS is closed\nDescription: when closing a process during a large upload, and NPE is triggered in the abort call. This is because the S3 client has already been released.\r\n\r\n{code}\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$abortMultipartUpload$41(S3AFileSystem.java:5337)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.abortMultipartUpload(S3AFileSystem.java:5336)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$abortMultipartUpload$4(WriteOperationHelper.java:392)\r\n{code}\r\n\r\n* close() in small writes also fails, just with a different exception\r\n* and on some large writes, the output str\nQ: Can also seem to hang if close() is called after the fs is shut as the submission failure isn't picked up on\r\n{code}\r\n2025-05-20 20:56:53,212 [JUnit-testOutputClosed] DEBUG s3a.Invoker (DurationInfo.java:(80)) - Starting: upload part request\r\n2025-05-20 20:56:53,212 [JUnit-testOutputClosed] DEBUG impl.RequestFactoryImpl (RequestFactoryImpl.java:newUploadPartRequestBuilder(662)) - Creating part upload request for dNf9u4SxpkPi9UN5XHGVK2BiAzR7JqRFsLwowvY2TtIS0vAo1IE9wgBprGTYYfeGOVMe5G_nbHHtjk8l1K7hcwBelCeWr2fNR3mhSe7._fmKNA2kDThpzX_.aAMqSXPS #1 size 1\r\n2025-05-20 20:56:53,216 [JUnit-testOutputClosed] DEBUG s3a.Invoker (DurationInfo.java:close(101)) - upload part request: duration 0:00.004s\r\n2025-05-20 20:56:53,225 [JUnit-testOutputClosed] ERROR util.BlockingThreadPoolExecutorService (BlockingThreadPoolExecutorService.java:rejectedExecution(141)) - Could not submit task to executor java.util.concurrent.ThreadPoolExecutor@2c12bb5e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\r\n2025-05-20 20:56:53,225 [JUnit-testOutputClosed] DEBUG s3a.S3ABlockOutputStream (S3ABlockOutputStream.java:clearActiveBlock(340)) - Clearing active block\r\n2025-05-20 20:56:53,225 [JUnit-testOutputClosed] DEBUG s3a.S3ABlockOutputStream (S3ABlockOutputStream.java:waitForAllPartUploads(1164)) - Waiting for 1 uploads to complete\r\n{code}", "output": "Complex. Need to add running checks in the store, state checks in the write operation callback invocations. As WriteOperationHelper still calls s3aFS direct, that's trickier. Should really remove that access and pull all into the callbacks"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support building with Java 25 (LTS release)\nDescription: *Depends upon/blocked by:*\r\n * Kafka ticket: KAFKA-19174 Gradle version upgrade 8 -->> 9 (review is under way)\r\n * Spotbugs next version:\r\n ** [https://github.com/spotbugs/spotbugs/issues/3564]\r\n ** [https://github.com/spotbugs/spotbugs/issues/3569]\r\n ** https://issues.apache.org/jira/browse/BCEL-377 \r\n ** [https://github.com/spotbugs/spotbugs/pull/3712]\r\n ** [https://github.com/spotbugs/spotbugs/discussions/3380] \r\n\r\n*Related links:*\r\n * JDK 25 release date: September 16th 2025:\r\n ** [https://mail.openjdk.org/pipermail/announce/2025-September/000360.html]\r\n ** [https://openjdk.org/projects/jdk/25]\r\n * Gradle 9.1 will support Java 25: [https://docs.gradle.org/9.1.0/release-notes.html#support-for-java-25]\r\n * Scala 2.13.17 is also announced:\r\n ** [https://docs.scala-lang.org/overviews/jdk-compatibility/overview.html#jdk-25-compatibility-notes]\r\n ** [https://contributors.scala-lang.org/t/scala-2-13-17-release-planning/6994/10]\r\n ** [https://github.com/scala/scala/milestone/109]\r\n * other\nQ: Update: *Mockito* version needs to be upgraded to resolve build issues (see the output of *_./gradlew test_* below).\r\n\r\n*Related links:*\r\n * https://issues.apache.org/jira/browse/SOLR-17718\r\n * [https://github.com/mockito/mockito/issues/3647]\r\n * [https://github.com/mockito/mockito/releases/tag/v5.20.0]\r\n\r\n{code:java}\r\nGroupCoordinatorShardTest > testReplayStreamsGroupTargetAssignmentMember() FAILED\r\n    org.mockito.exceptions.base.MockitoException: \r\n    Mockito cannot mock this class: class org.apache.kafka.coordinator.group.GroupMetadataManager.\r\n\r\n    If you're not sure why you're getting this error, please open an issue on GitHub.\r\n\r\n\r\n    Java               : 25\r\n    JVM vendor name    : Eclipse Adoptium\r\n    JVM vendor version : 25+36-LTS\r\n    JVM name           : OpenJDK 64-Bit Server VM\r\n    JVM version        : 25+36-LTS\r\n    JVM info           : mixed mode, sharing\r\n    OS name            : Linux\r\n    OS version         : 6.8.0-84-generic\r\n\r\n\r\n    You are seeing this disclaimer because Mockito is configured to create inlined mocks.\r\n    You can learn about inline mocks and their limitations under item #39 of the Mockito class javadoc.\r\n\r\n    Underlying exception : org.mockito.exceptions.base.MockitoException: Could not modify all classes [class java.lang.Object, class org.apache.kafka.coordinator.group.GroupMetadataManager]\r\n        at app//org.apache.kafka.coordinator.group.GroupCoordinatorShardTest.testReplayStreamsGroupTargetAssignmentMember(GroupCoordinatorShardTest.java:1117)\r\n\r\n        Caused by:\r\n        org.mockito.exceptions.base.MockitoException: Could not modify all classes [class java.lang.Object, class org.apache.kafka.coordinator.group.GroupMetadataManager]\r\n            at app//net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:168)\r\n            at app//net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:399)\r\n            at app//net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:190)\r\n            at app//net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:410)\r\n            ... 1 more\r\n\r\n            Caused by:\r\n            java.lang.IllegalStateException: \r\n            Byte Buddy could not instrument all classes within the mock's type hierarchy\r\n\r\n            This problem should never occur for javac-compiled classes. This problem has been observed for classes that are:\r\n             - Compiled by older versions of scalac\r\n             - Classes that are part of the Android distribution\r\n                at org.mockito.internal.creation.bytebuddy.InlineBytecodeGenerator.triggerRetransformation(InlineBytecodeGenerator.java:285)\r\n                at org.mockito.internal.creation.bytebuddy.InlineBytecodeGenerator.mockClass(InlineBytecodeGenerator.java:218)\r\n                at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.lambda$mockClass$0(TypeCachingBytecodeGenerator.java:78)\r\n                at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:168)\r\n                at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:399)\r\n                at net.bytebuddy.TypeCache.findOrInsert(TypeCache.java:190)\r\n                at net.bytebuddy.TypeCache$WithInlineExpunction.findOrInsert(TypeCache.java:410)\r\n                at org.mockito.internal.creation.bytebuddy.TypeCachingBytecodeGenerator.mockClass(TypeCachingBytecodeGenerator.java:75)\r\n                at org.mockito.internal.creation.bytebuddy.InlineDelegateByteBuddyMockMaker.createMockType(InlineDelegateByteBuddyMockMaker.java:408)\r\n                at org.mockito.internal.creation.bytebuddy.InlineDelegateByteBuddyMockMaker.doCreateMock(InlineDelegateByteBuddyMockMaker.java:367)\r\n                at org.mockito.internal.creation.bytebuddy.InlineDelegateByteBuddyMockMaker.createMock(InlineDelegateByteBuddyMockMaker.java:346)\r\n                at org.mockito.internal.creation.bytebuddy.InlineByteBuddyMockMaker.createMock(InlineByteBuddyMockMaker.java:56)\r\n                at org.mockito.internal.util.MockUtil.createMock(MockUtil.java:99)\r\n                at org.mockito.internal.MockitoCore.mock(MockitoCore.java:84)\r\n                at org.mockito.Mockito.mock(Mockito.java:2162)\r\n                at org.mockito.Mockito.mock(Mockito.java:2077)\r\n                ... 1 more\r\n\r\n                Caused by:\r\n                java.lang.IllegalArgumentException: Java 25 (69) is not supported by the current version of Byte Buddy which officially supports Java 24 (68) - update Byte Buddy or set net.bytebuddy.experimental as a VM property\r\n                    at net.bytebuddy.utility.OpenedClassReader.of(OpenedClassReader.java:120)\r\n                    at net.bytebuddy.utility.OpenedClassReader.of(OpenedClassReader.java:95)\r\n                    at net.bytebuddy.utility.AsmClassReader$Factory$Default.make(AsmClassReader.java:82)\r\n                    at net.bytebuddy.dynamic.scaffold.TypeWriter$Default$ForInlining.create(TypeWriter.java:4036)\r\n                    at net.bytebuddy.dynamic.scaffold.TypeWriter$Default.make(TypeWriter.java:2246)\r\n                    at net.bytebuddy.dynamic.DynamicType$Builder$AbstractBase$UsingTypeWriter.make(DynamicType.java:4057)\r\n                    at net.bytebuddy.dynamic.DynamicType$Builder$AbstractBase.make(DynamicType.java:3741)\r\n                    at org.mockito.internal.creation.bytebuddy.InlineBytecodeGenerator.transform(InlineBytecodeGenerator.java:402)\r\n                    at java.instrument/java.lang.instrument.ClassFileTransformer.transform(ClassFileTransformer.java:257)\r\n                    at java.instrument/sun.instrument.TransformerManager.transform(TransformerManager.java:188)\r\n                    at java.instrument/sun.instrument.InstrumentationImpl.transform(InstrumentationImpl.java:594)\r\n                    at java.instrument/sun.instrument.InstrumentationImpl.retransformClasses0(Native Method)\r\n                    at java.instrument/sun.instrument.InstrumentationImpl.retransformClasses(InstrumentationImpl.java:221)\r\n                    at org.mockito.internal.creation.bytebuddy.InlineBytecodeGenerator.triggerRetransformation(InlineBytecodeGenerator.java:281)\r\n                    ... 16 more\r\norg.apache.kafka.coordinator.group.GroupCoordinatorServiceTest.testFetchOffsetsWithWrappedError(boolean, Errors, Errors)[2] failed, log available in /home/dejan/kafka/group-coordinator/build/reports/testOutput/org.apache.kafka.coordinator.group.GroupCoordinatorServiceTest.testFetchOffsetsWithWrappedError(boolean, Errors, Errors)[2].test.stdout\r\n{code}", "output": "*Small update:*\r\n * Gradle version upgrade (8.14.3 -->> 9.1.0) will hopefully end up in trunk in the next few days\r\n * It seems that Apache *{{commons-bcel}}* will release Java 25 compatible version soon enough - this is important for us because SpotBugs Java 25 compatible versions will follow immediately"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: processValues() must be declared as value-changing operation\nDescription: When adding `KStreams#processValues()` we missed to declare the operation as \"value changing\". This can lead to an \"incorrectly\" built topology.\r\n\r\nThe main problem is, that `processValues()` is the replacement of `transformValues()` which we removed with AK 4.0.0 release. Thus, if users rewrite existing programs from `transformValues()` to the new `processValues()` (what will be required when upgrading to 4.x release), they might observe this change as a regression.\r\n\r\nThe impact of the changed topology is, that local state is effectively lost, and must be restored from the changelog topic, resulting in downtime after an upgrade.\r\n\r\nNote: the bug does only surface, if topology optimization is used, in particular the \"merge repartition topics\" rewrite.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-gridmix.\nDescription: \nQ: hadoop-yetus commented on PR #7578:\nURL: https://github.com/apache/hadoop/pull/7578#issuecomment-2777558635\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 19 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/artifact/out/blanks-eol.txt) |  The patch has 11 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 18s | [/results-checkstyle-hadoop-tools_hadoop-gridmix.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-gridmix.txt) |  hadoop-tools/hadoop-gridmix: The patch generated 65 new + 148 unchanged - 2 fixed = 213 total (was 150)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 44s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  16m 50s | [/patch-unit-hadoop-tools_hadoop-gridmix.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/artifact/out/patch-unit-hadoop-tools_hadoop-gridmix.txt) |  hadoop-gridmix in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 133m  6s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapred.gridmix.TestGridmixSummary |\r\n   |   | hadoop.mapred.gridmix.TestGridmixMemoryEmulation |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7578 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 915eeb034a41 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / adb77fea3c43c5a5e67a7661e1a8764a882b781a |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/testReport/ |\r\n   | Max. process+thread count | 1027 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-gridmix U: hadoop-tools/hadoop-gridmix |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7578:\nURL: https://github.com/apache/hadoop/pull/7578#issuecomment-2782922303\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 46s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 19 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/2/artifact/out/blanks-eol.txt) |  The patch has 6 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 18s | [/results-checkstyle-hadoop-tools_hadoop-gridmix.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-gridmix.txt) |  hadoop-tools/hadoop-gridmix: The patch generated 31 new + 148 unchanged - 2 fixed = 179 total (was 150)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  17m  3s | [/patch-unit-hadoop-tools_hadoop-gridmix.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/2/artifact/out/patch-unit-hadoop-tools_hadoop-gridmix.txt) |  hadoop-gridmix in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 150m 53s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapred.gridmix.TestGridmixSummary |\r\n   |   | hadoop.mapred.gridmix.TestGridmixMemoryEmulation |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7578 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 56e023788be9 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b2cbc6f54f5b4648f8b278cf17f064e3c3ffdb04 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/2/testReport/ |\r\n   | Max. process+thread count | 1041 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-gridmix U: hadoop-tools/hadoop-gridmix |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7578/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Custom Gradle tasks (`unitTest` and `integrationTest`) are not executing tests in Gradle 9\nDescription: Prologue: KAFKA-19174\r\n\r\n\r\n\r\n\r\nSee here for description: https://github.com/apache/kafka/pull/19513#issuecomment-3388760324\nQ: Hi [~isding_l] ! Yes, it seems that your PR resolves this issue. \r\n\r\nI will leave a review on your PR (maybe I can save you few lines).", "output": "Duplicate, will be solved here: [https://github.com/apache/kafka/pull/20684]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: add compression type and level to log_compaction_test.py\nDescription: as title", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support `spark.io.mode.default`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Document newly added `core` module configurations\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Client#call decrease asyncCallCounter incorrectlly when exceptions occur", "output": "Client#call decrease asyncCallCounter incorrectlly when exceptions occur"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "## Description\r\n \r\nThere is a duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c` at lines 29-30.\r\n\r\n## Current Code\r\n```c\r\n#include \"gcc_optimizations.h\"\r\n#include \"gcc_optimizations.h\"\r\n```", "output": " Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [ABFS] AzureADAuthenticator should be able to retry on UnknownHostException\nDescription: When Hadoop is requested to perform operations against ADLS Gen2 storage, *AbfsRestOperation* attempts to obtain an access token from Microsoft. Underneath the hood, it uses a simple *java.net.HttpURLConnection* HTTP client.\r\n\r\nOccasionally, environments may run into network intermittent issues, including DNS-related {*}UnknownHostException{*}. Technically, the HTTP client throws *IOException* whose cause is {*}UnknownHostException{*}. *AzureADAuthenticator* in its turn catches {*}IOException{*}, sets *httperror = -1* and then checks whether the error is recoverable and can be retried. However, it's neither an instance of {*}MalformedURLException{*}, nor an instance of {*}FileNotFoundException{*}, nor a recoverable status code ({*}= 500 && != 501 && != 505{*}), hence a retry never occurs which is sensitive for our project causing problems with state recovery.\r\n\r\nThe final exception stack trace on the client side looks as follows (Apache Spark application, tenant ID is redacted):\r\n{code\nQ: services.AbfsClient (AbfsRestOperation.java:signRequest(565)) - Authenticating request with OAuth2 access token\r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenUsingClientCreds(112)) - AADToken: starting to fetch token using client creds for client ID \r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenCall(367)) - Retrying getTokenSingleCall. RetryCount = 1\r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenCall(367)) - Retrying getTokenSingleCall. RetryCount = 2\r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenCall(367)) - Retrying getTokenSingleCall. RetryCount = 3\r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenCall(367)) - Retrying getTokenSingleCall. RetryCount = 4\r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenCall(367)) - Retrying getTokenSingleCall. RetryCount = 5\r\n\r\nThe exception which you shared above is thrown outside retry loop as follows\r\n\r\nAuth failure: HTTP Error -1; url='' AzureADAuthenticator.getTokenCall threw java.net.UnknownHostException : login.microsoftonline.com\r\norg.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator$HttpException: HTTP Error -1; url='' AzureADAuthenticator.getTokenCall threw java.net.UnknownHostException : login.microsoftonline.com\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:410)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:323)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:289)\r\nat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:494)\r\nat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:465)", "output": "[~anujmodi] Looks like you're right. Our retry just got exhausted obviously. The delay between retries is too small. I think we need to override *fs.azure.oauth.token.fetch.retry.delta.backoff* or f{*}s.azure.oauth.token.fetch.retry.min.backoff.interval{*} to a bigger value to increase intervals. I'm aslo seeing that higher level classes are also attempting to retry. Those *Retrying getTokenSingleCall* logs were printed 8 x 5 times which is a little inconvenient to configure. It's like retry over retry. Are there any suggestions how to properly set this up?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: DESCRIBE TABLE AS JSON: v2 table\nDescription: Currently Desc As Json is supported only for v1 table (see [error|https://github.com/apache/spark/blob/3d292dc7b1c5b5ff977c178a88f8ee73deaee88f/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DescribeRelationJsonCommand.scala#L99] thrown for v2 tables). \r\n\r\nAdd support v2 table path (ex: DescribeRelation v2 table [path)|https://github.com/apache/spark/blob/3d292dc7b1c5b5ff977c178a88f8ee73deaee88f/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala#L385]", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: add integration tests for Consumer#currentLag\nDescription: The following scenarios should be included.\r\n\r\n1. non-existed partitions\r\n2. undetermined offset\r\n3. normal case", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "There is a unit tests which perform these steps:\r\n # Create consumer\r\n # Interrupt thread\r\n # Call Consumer.close()\r\n # Check that InterruptException was thrown\r\n # Call Thread.interrupted()\r\n\r\nIn those cases, the return value from Thread.interrupted() is actually false. We're catching the interrupted in the close() method but not restoring it before we throw. This is inconsistent with the ClassicKafkaConsumer.", "output": "Thread is not marked interrupted even when AsyncKafkaConsumer.close() throws InterruptException"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade libopenssl to 3.1.1 for rsync on Windows\nDescription: We're currently using libopenssl 3.1.0 which is needed for rsync 3.2.7 on Windows for the Yetus build validation.\r\nHowever, libopenssl 3.1.0 is no longer available for download on the msys2 site -\r\n\r\n{code}\r\nPS D:\\projects\\github\\apache\\hadoop> Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.0-2-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.0-2-x86_64.pkg.tar.zst\r\nInvoke-WebRequest:\r\n404 Not Found\r\n\r\n404 Not Found\r\nnginx/1.26.3\r\n{code}\r\n\r\nThus, we need to upgrade libopenssl to the next available version - 3.1.1 to mitigate this issue.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "JUnit has been upgraded to 5.13.3 so that test suites can\r\nbe parameterized at the class level (again).\r\nThis requires a matching upgrade to Surefire and some\r\ntuning of surefire options to restore existing behavior.\r\n\r\nDependency Changes:\r\n* junit.jupiter and junit.vintage => 5.13.3\r\n* junit.platform => 1.13.3\r\n* Surefire => 3.5.3.\r\n\r\nChanged Surefire Flags: \r\n* trimStackTrace => false\r\n* surefire.failIfNoSpecifiedTests => false\r\n\r\nh2. Build property trimStackTrace\r\n\r\ntrimStackTrace is set to false by default\r\n\r\nThis restores the behavior that any test failure\r\nincludes a deep stack trace rather than the first two elements.\r\nIn this version of surefire it also seems to print the full stack\r\nof JUnit test execution and the java runtime itself.\r\n\r\nThis is only an issue for when tests fail.\r\nIf the short trace is desired, enable it on the command line:\r\n{code}\r\n    mvn test -DtrimStackTrace\r\n{code}\r\n\r\nh2. Build property surefire.failIfNoSpecifiedTests\r\n\r\nsurefire.failIfNoSpecifiedTests is set to false by default    \r\n\r\nThis is required to retain the behavior where the invocation\r\n{code}\r\n    mvn verify -Dtest=unknown\r\n{code}\r\nwill skip the unit tests but still run the integration tests, and a\r\nspecific property \"it.test\" can name the specific test to run:\r\n{code}\r\n    mvn verify -Dtest=unknown -Dit.test=ITestConnection\r\n{code}", "output": "Upgrade to Junit 5.13.3"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Also when running in Zookeeper mode Kafka can go OOM if an attempt is made to create a topic with too many partitions, or to extend a topic with too many partitions.\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-17870\r\nhttps://issues.apache.org/jira/browse/KAFKA-19673\r\n\r\nI think it is worth to port also the constrain introduded in KRaft mode to the 3.9 version of Kafka.", "output": "Disallow create too many partitions in ZK mode"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "At least in the Windows Docker image, vspkg currently builds 171 packages the vast majority of which are varios boost modules and their dependencies.\r\n\r\nHadoop only uses a handful of those modules.\r\n\r\nOptimize the Windows image by only installing the boost modules that Hadoop actually needs. This would save time, disk space, bandwidth.\r\n\r\nThis is less relevant in the Linux images, where the Boost build simply skips the parts with missing dependencies.", "output": "Optimize Boost dependency in build images"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade Debian 10 to 11 in build env Dockerfile\nDescription: Debian 10 EOL, and the apt repo is unavailable\r\n{code:bash}\r\ndocker run --rm -it debian:10 bash\r\nroot@bc2a4c509cb3:/# apt update\r\nIgn:1 http://deb.debian.org/debian buster InRelease\r\nIgn:2 http://deb.debian.org/debian-security buster/updates InRelease\r\nIgn:3 http://deb.debian.org/debian buster-updates InRelease\r\nErr:4 http://deb.debian.org/debian buster Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:5 http://deb.debian.org/debian-security buster/updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:6 http://deb.debian.org/debian buster-updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nReading package lists... Done\r\nE: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian-security buster/updates Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian buster-updates Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: ITestS3AConfiguration.testDirectoryAllocatorDefval() failing\nDescription: while working on HADOOP-19554 I added a per-bucket setting for fs.s3a.buffer.dir\r\n\r\nafter this, ITestS3AConfiguration.testDirectoryAllocatorDefval() would fail in a test run of the entire class, but not if run alone.\r\n\r\nCauses\r\n* dir allocator map of config key to allocator is static; previous uses tainted outcome\r\n* per-bucket settings were't being overridden. This is complicated by the fact that \"unset\" isn't a setting, therefore can't be forced in. Instead some whitespace needs to be set.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove BrokerNotFoundException\nDescription: h1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.*\r\n\r\nBrokerNotFoundException was deprecated for removal, as it's not used any longer.\r\n\r\nCf https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=373886192", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Some tests against third-party stores fail\r\n- Includes fix for the assumeStoreAwsHosted() logic.\r\n- Documents how to turn off multipart uploads with third-party stores.", "output": "S3A: some tests failing on third-party stores"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support TIME in the make_timestamp and try_make_timestamp functions in Scala\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52631.", "output": "Issue resolved by pull request 52631\n[https://github.com/apache/spark/pull/52631]"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Enhance performance of ABFS driver for write-heavy workloads\nDescription: The goal of this work item is to enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes.\nQ: anmolanmol1234 commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-2848521492\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 798, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 801, Failures: 0, Errors: 0, Skipped: 117\r\n   [ERROR] Tests run: 146, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 640, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 798, Failures: 0, Errors: 0, Skipped: 171\r\n   [WARNING] Tests run: 132, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 643, Failures: 0, Errors: 0, Skipped: 144\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 637, Failures: 0, Errors: 0, Skipped: 217\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [ERROR] Tests run: 640, Failures: 0, Errors: 0, Skipped: 146\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [ERROR] Tests run: 638, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 132, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [ERROR] Tests run: 672, Failures: 0, Errors: 0, Skipped: 167\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 637, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24", "output": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-2848547561\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 31s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 14 new + 2 unchanged - 0 fixed = 16 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 19s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 23s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  77m 36s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.WriteThreadPoolSizeManager.adjustThreadPoolSizeBasedOnCPU(double) does not release lock on all exception paths  At WriteThreadPoolSizeManager.java:on all exception paths  At WriteThreadPoolSizeManager.java:[line 268] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a1000f66baec 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1ba12f6247567e3f5e9087c00fb52e741b1eb98c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Share Partition Lag Persistence and Retrieval\nDescription: Ticket for KIP-1226. The KIP proposes introducing the concept of lag for share partitions, which will be persisted in the __share_group_state topic and can be retrieved using admin.listShareGroupOffsets\nQ: Hi [~chiragwadhwa55]  i am।happy to help in any of the subtasks Let me know if i can be of any help", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Factor out streaming tests from `spark-sql` and `spark-connect`\nDescription: \nQ: Issue resolved by pull request 52564\n[https://github.com/apache/spark/pull/52564]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use the staging directory as the output path then move to final path\nDescription: SparkSQL uses the partition location or table location as the commit path (except in *_dynamic partition overwrite_* mode and *_custom partition path_* mode). This has at least the following issues:\r\n\r\n* As described in SPARK-37210, conflicts can occur when multiple partitions job of the same table are run concurrently. Using a staging directory can avoid this issue.\r\n* As described in SPARK-53937, using a staging directory allows for near-atomic operations.\r\n\r\n_*Dynamic partition overwrite*_ mode and *_custom partition path_* mode already use the staging directory. And *_dynamic partition overwrite_* mode and _*custom partition path*_ are implemented differently, which can be further simplified into a unified process. And in https://github.com/apache/spark/pull/29000, reset the staging directory as the output directory of FileOutputCommitter. This way is more safer. It should be modified to this way.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Prefer to use native Netty transports by default\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Deadlock in Kafka Streams when processing Interactive Queries and state store updates concurrently\nDescription: We are using a Kafka Streams topology that continuously writes large volumes of data into a RocksDB state store with stable throughput. In parallel, another thread executes Interactive Query (IQ) requests against the same local state store.\r\n\r\nWhen the number of IQ requests in the queue grows (≈50+), the application enters a {*}deadlock state{*}.\r\n\r\n*Investigation:*\r\nUsing a thread dump, we discovered a lock inversion between RocksDB operations:\r\n * {{RocksDBStore.put}}\r\n\r\n ** blocked on {{org.apache.kafka.streams.query.Position@4ba00b6c}}\r\n\r\n ** holding {{org.apache.kafka.streams.state.internals.RocksDBStore@414cff0e}}\r\n\r\n * {{RocksDBStore.range}}\r\n\r\n ** blocked on {{org.apache.kafka.streams.state.internals.RocksDBStore@414cff0e}}\r\n\r\n ** holding {{org.apache.kafka.streams.query.Position@4ba00b6c}}\r\n\r\nThis indicates that {*}{{put}} and {{range}} acquire the same locks but in different order{*}, which leads to deadlock under concurrent load.\r\n\r\n*Expected Behavior:*\r\nKafka Streams API should guarantee deadlock-free operation. Store writes ({{{}put{}}}) and IQ reads ({{{}range{}}}) should not block each other in a way that leads to lock inversion.\r\n\r\n*Steps to Reproduce:*\r\n # Create a Kafka Streams topology with a RocksDB state store receiving continuous writes.\r\n\r\n # In a parallel thread, issue a high number of Interactive Query {{range}} requests (≈50+ queued).\r\n\r\n # Observe that the system eventually enters deadlock.\r\n\r\n *  \r\n\r\n*Impact:*\r\n * Application stops processing data.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: NPE when close() called on uninitialized filesystem\nDescription: code\r\n{code}\r\n  public void testABFSConstructor() throws Throwable {\r\n    new AzureBlobFileSystem().close();\r\n  }\r\n{code}\r\n\r\nstack\r\n{code}\r\n[ERROR] org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor -- Time elapsed: 0.003 s <<< ERROR!\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getClient()\" because \"this.abfsStore\" is null\r\n        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.close(AzureBlobFileSystem.java:800)\r\n        at org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor(TestRuntimeValid.java:49)\r\n{code}\nQ: hadoop-yetus commented on PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#issuecomment-3197386039\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 11s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 141m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7880 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 160d07de834c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fd85bee514d271d663f801bf3498b3145ec08fec |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/testReport/ |\r\n   | Max. process+thread count | 530 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7880/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "steveloughran commented on code in PR #7880:\nURL: https://github.com/apache/hadoop/pull/7880#discussion_r2282920640\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -148,7 +150,7 @@ public class AzureBlobFileSystem extends FileSystem\n   private URI uri;\n   private Path workingDir;\n   private AzureBlobFileSystemStore abfsStore;\n-  private boolean isClosed;\n+  private boolean isClosed = true;\n\nReview Comment:\n   so this really means inited and closed. Mention that in javadocs."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-sls.\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add support for using podman within system tests.", "output": "Add support for podman when running system tests"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Anomaly of JMX metrics RequestHandlerAvgIdlePercent in kraft combined mode\nDescription: JMX metrics RequestHandlerAvgIdlePercent reports a value close to 2 in combined kraft mode but it's expected to be b/w 0 and 1.\r\n\r\nThis is an issue with combined mode specifically because both controller + broker are using the same Meter object in combined mode, defined in {{{}RequestThreadIdleMeter#requestThreadIdleMeter{}}}, but the controller and broker are using separate {{KafkaRequestHandlerPool}} objects, where each object's {{{}threadPoolSize == KafkaConfig.numIoThreads{}}}. This means when calculating idle time, each pool divides by its own {{numIoThreads}} value before reporting to the shared meter and  {{RequestHandlerAvgIdlePercent}} calculates the final result by accumulating all the values reported by all threads. However, since there are actually 2 × numIoThreads total threads contributing to the metric, the denominator should be doubled to get the correct average.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently it's hard to write unit tests for user's implementation of StatefulProcessor used in TransformWithState. User either needs to test it by running actual streaming query, or they need to refactor business logic into a separate function (that will be called from handleInputRows), and write unit tests for that function.\r\n \r\nI propose to add a simple testing framework for TransformWithState that will allow users easily test their business logic inside StatefulProcessor.\r\n \r\nOn high-level, it's a class TwsTester that takes StatefulProcessor and allows to feed in input rows and immediately returns what rows would be produced by TWS. It also allows to set and inspect state. This can be used in unit tests without having to run streaming query and it won't need RocksDB (it will use in-memory state store). I will start with implementing this for Scala users, potentially making it available for Python users later.", "output": "Improve the testing experience for TransformWithState"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Address the compileClasspath resolution warnings for the `releaseTarGz` task\nDescription: {code:java}\r\n[warn]  Resolution\r\n of the configuration :tools:tools-api:runtimeClasspath was attempted \r\nfrom a context different than the project context. Have a look at the \r\ndocumentation to understand why this is a problem and how it can be \r\nresolved. This behavior has been deprecated. (1)    - [warn]  Resolution\r\n of the configuration :tools:tools-api:runtimeClasspath was attempted \r\nfrom a context different than the project context. Have a look at the \r\ndocumentation to understand why this is a problem and how it can be \r\nresolved. This behavior has been deprecated.This will fail with an error in Gradle 9.0.        - Locations- ``- `:core:releaseTarGz` {code}\r\nThe issue was introduced by [https://github.com/apache/kafka/pull/13454]\r\n\r\n \r\n\r\n`tools-api` is already in core module runtime path, so adding it to `releaseTarGz` causes the resolution conflicts, which will be a fatal error in gradle 9\nQ: [~chia7712], can I take over on this issue?", "output": "go ahead"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: upgrade to netty 4.1.118 due to CVE-2025-24970\nDescription: https://github.com/advisories/GHSA-4g8c-wm8x-jfhw", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Unbound Error Thrown if some variables are not set for SASL/SSL configuration\nDescription: I was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file — it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set -u}}{*}.\r\n\r\nThis issue is {*}just an enhancement to properly log the error details{*}.\nQ: Hi [~crw31] , I guess I don't have access to assign the issues.", "output": "Hi [~scienmanas] , thanks for letting me know. Since I don’t have assign permissions, I’ll start working on this. If someone with commit rights wants to assign the ticket later, that would be great. Thanks!"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `Ammonite` to 3.0.3\nDescription: \nQ: Issue resolved by pull request 52577\n[https://github.com/apache/spark/pull/52577]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: InsertHeader drops part of the value when header value is a number\nDescription: When I use the SMT *org.apache.kafka.connect.transforms.InsertHeader* and the header value is a number, part of the value is dropped. Example: \r\n\r\n{code:yaml}\r\n# Insert KafkaHeaders for avro serialization\r\ntransforms: insertSpecHeader\r\n\r\ntransforms.insertSpecHeader.type: org.apache.kafka.connect.transforms.InsertHeader\r\ntransforms.insertSpecHeader.header: \"specversion\"\r\ntransforms.insertSpecHeader.value.literal: \"2.0\"\r\n{code}\r\n\r\nThen, the record is produced with the header *\"specversion\": \"2\"*\r\n\r\nIs KafkaConnect doing a sort of casting and treating the value as float even though I am using literal?", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "* The test *org.apache.hadoop.fs.TestFsShellList#testList* creates files with special characters and tries to list them using *ls*.\r\n* Filenames with special characters aren't allowed in Windows.\r\n* Thus, we need to modify the test to only test for non-special characters on Windows and include the filenames with special characters on non-Windows environments.", "output": "Fix TestFsShellList.testList on Windows"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When adding `KStreams#processValues()` we missed to declare the operation as \"value changing\". This can lead to an \"incorrectly\" built topology.\r\n\r\nThe main problem is, that `processValues()` is the replacement of `transformValues()` which we removed with AK 4.0.0 release. Thus, if users rewrite existing programs from `transformValues()` to the new `processValues()` (what will be required when upgrading to 4.x release), they might observe this change as a regression.\r\n\r\nThe impact of the changed topology is, that local state is effectively lost, and must be restored from the changelog topic, resulting in downtime after an upgrade.\r\n\r\nNote: the bug does only surface, if topology optimization is used, in particular the \"merge repartition topics\" rewrite.", "output": "processValues() must be declared as value-changing operation"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: AWS SDK 2.30+ incompatible with third party stores\nDescription: Changes in the SDK related to a new AWS SDK Feature, Default integritY Protection {{https://docs.aws.amazon.com/sdkref/latest/guide/feature-dataintegrity.html}} break all intereraction with third party S3 Stores.\r\n\r\nSee {{https://github.com/aws/aws-sdk-java-v2/issues/5801}}\r\n\r\nThere are documented mechanisms to turn this off\r\n* an environment variable\r\n* a change in ~/..aws/config\r\n* a system property\r\n* looks like a new builder option, though the docs don't cover it.\r\n\r\nThat checksum builder option looks like the only viable strategy, but it will need testing. maybe even make it a property which can be enabled/disabled, with tests and docs.\r\n\r\nWhoever wants a 2.30.x feature either gets to do the fixing and testing. For 3.4.2 it'll be an older release.\nQ: steveloughran opened a new pull request, #7494:\nURL: https://github.com/apache/hadoop/pull/7494\n\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Dell ECS S3 store. tests fail. \r\n   \r\n   This PR is just up as a \"here is where I gave up\" PR\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "Pretty much all thirdparty s3 compatible stores are broken. We're fixing it in Ozone here: HDDS-12488"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `gRPC Swift NIO Transport` to 2.2.0\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade Jackson to 2.16.0\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add JDK 24 to Ubuntu 20.04 docker development images\nDescription: The first step to supporting JDK23/24 is being able to test it.\r\n\r\nAdd JDK24 to the default docker images, so that both manual and automated testing is possible.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Decouple ConsumerConfig and ShareConsumerConfig\nDescription: ShareConsumerConfig and ConsumerConfig are inherent at the moment.\r\nThe drawback is the config logic is mixed, for example: \r\nShareAcknowledgementMode.\r\nWe can decouple to prevent the logic is complicated in the future.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "HADOOP-19475 has updated Boost to 1.86.0, but it missed the Windows docker image, which doesn't use the same mechanism for dependencies as the Linux ones.\r\n\r\nUpdate Boost in the Windows build Docker image to the same version.", "output": "Update Boost to 1.86.0 in Windows build image"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: StreamThread blocks on StateUpdater during onAssignment()\nDescription: We've observed that the `StreamThread` blocks waiting for a `Future` from the `StateUpdater` in the `StreamsPartitionAssigner#onAssignment()` method when we are moving a task out of the `StateUpdater` and onto the `StreamThread`.\r\n\r\n \r\n\r\nThis can cause problems because, during restoration or with warmup replicas, the `StateUpdater#runOnce()` method can take a long time (upwards of 20 seconds) when RocksDB stalls writes to allow compaction to keep up. In EOS this blockage may cause the transaction to time out, which is a big mess. This is because the `StreamThread` may have an open transaction before the `StreamsPartitionAssignor#onAssignment()` method is called.\r\n\r\n \r\n\r\nSome screenshots from the JFR below (credit to [~eduwerc]).", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: LocalDirAllocator still doesn't always recover from directory tree deletion\nDescription: In HADOOP-18636, LocalDirAllocator was modified to recreate missing dirs. But it appears that there are still codepaths which don't do that.\r\n\r\nwhen charrypicking this -please follow up with  HADOOP-19573, to ensure an associated test never fails\nQ: steveloughran opened a new pull request, #7651:\nURL: https://github.com/apache/hadoop/pull/7651\n\n   \r\n   HADOOP-19554. LocalDirAllocator still doesn't always recover from directory deletion\r\n   \r\n   * Pull up recreation logic for all branch paths\r\n   * Lots of logging info\r\n   * Exception test includes the list of paths.\r\n   * tests for the failure recovery and reporting changes\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   New and modified unit test cases.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7651:\nURL: https://github.com/apache/hadoop/pull/7651#issuecomment-2831359212\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   9m 16s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 49s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 35s |  |  trunk passed  |\r\n   | -1 :x: |  shadedclient  |  26m 29s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m  6s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 20s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 20s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 18s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | -1 :x: |  javac  |   0m 18s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 26s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 1 new + 23 unchanged - 0 fixed = 24 total (was 23)  |\r\n   | -1 :x: |  mvnsite  |   0m 33s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 14s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-common in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  spotbugs  |   1m 18s | [/new-spotbugs-hadoop-common-project_hadoop-common.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/new-spotbugs-hadoop-common-project_hadoop-common.html) |  hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | -1 :x: |  shadedclient  |  26m 32s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m  9s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 15s |  |  ASF License check generated no output?  |\r\n   |  |   | 102m 36s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-common-project/hadoop-common |\r\n   |  |  Exceptional return value of java.io.File.mkdirs() ignored in org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(String, long, Configuration, boolean)  At LocalDirAllocator.java:ignored in org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(String, long, Configuration, boolean)  At LocalDirAllocator.java:[line 433] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7651 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux e68470bb8f81 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / da6917e5510e9e52ecfa60f0d95640da8099b1db |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/testReport/ |\r\n   | Max. process+thread count | 550 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We don't recompute the assignment on consumer -> classic member replacement when the consumer member had a regex subscription and the classic member does not.\r\n\r\nWe should explicitly set regex subscription to empty in classicGroupJoinToConsumerGroup", "output": "Regex subscription should be empty for classic members joining mixed group"}
{"instruction": "Answer the question based on the bug.", "input": "Title: High number of Threads Launched when Calling fs.getFileStatus() via proxyUser after Kerberos authentication.\nDescription: We have observed an issue where very large number of threads are being launched when performing concurrent {{fs.getFileStatus(path) operations}} as proxyUser.\r\n\r\nAlthough this issue was observed in our hive services, we were able to isolate and replicate this issue without hive by writing a sample standalone program which first logs in via a principal and keytab and then creates a proxy user and fires concurrent {{fs.getFileStatus(path)}} for a few mins. Eventually when the concurrency increases it tries to create more threads than max available threads(ulimit range) and the process eventually slows down.\r\n{code:java}\r\nUserGroupInformation proxyUserUGI = UserGroupInformation.createProxyUser(\r\n\"hive\", UserGroupInformation.getLoginUser());{code}\r\nIn this particular case, when launching 30 concurrent threads calling , the max number of threads launched by the PID are 6066.\r\n \r\n{code:java}\r\nEvery 1.0s: ps -eo nlwp,pid,args --sort -nlwp | head                                                \nQ: cc [~chinnaraol] , [~pkumarsinha] for your reference", "output": "[~stevel@apache.org] , can i get some help regarding this?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In `DefaultMetricsSystem`, if `miniClusterMode` is set to false by other functions, `newSourceName` exits with a MetricsException", "output": "MetricsException thrown since miniClusterMode off"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Disallow casting geospatial types to/from other data types\nDescription: \nQ: Issue resolved by pull request 52806\n[https://github.com/apache/spark/pull/52806]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Construct FileStatus from the executor side directly\nDescription: https://github.com/apache/spark/pull/50765#discussion_r2357607758", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: some tests failing on third-party stores\nDescription: Some tests against third-party stores fail\r\n- Includes fix for the assumeStoreAwsHosted() logic.\r\n- Documents how to turn off multipart uploads with third-party stores.\nQ: ITestS3AEndpointRegion.testCentralEndpointAndNullRegionWithCRUD with non aws bucket/credentials fails. \r\n\r\nFix: don't do that.\r\n\r\n{code}\r\n[ERROR] testCentralEndpointAndNullRegionWithCRUD(org.apache.hadoop.fs.s3a.ITestS3AEndpointRegion)  Time elapsed: 1.88 s  <<< ERROR!\r\njava.nio.file.AccessDeniedException: s3a://dellecs719/test/testCentralEndpointAndNullRegionWithCRUD/srcdir: getFileStatus on s3a://dellecs719/test/testCentralEndpointAndNullRegionWithCRUD/srcdir: software.amazon.awssdk.services.s3.model.S3Exception: The AWS Access Key Id you provided does not exist in our records. (Service: S3, Status Code: 403, Request ID: 2CZF3PJDSKRYABWS, Extended Request ID: h8T/uz2tPH4zwA6W/dTqqSZfm5lAT9JetkO8moPu9R9L5QkqTP0VwfWrAZWo3J/WMy/nollnxT8=):InvalidAccessKeyId\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:271)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:158)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4088)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3954)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem$MkdirOperationCallbacksImpl.probePathStatus(S3AFileSystem.java:3811)\r\n\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.probePathStatusOrNull(MkdirOperation.java:218)\r\n\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.getPathStatusExpectingDir(MkdirOperation.java:239)\r\n\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:134)\r\n\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:59)\r\n\tat org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation.apply(ExecutingStoreOperation.java:76)\r\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\r\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\r\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2844)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2863)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:3782)\r\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2496)\r\n\tat org.apache.hadoop.fs.s3a.ITestS3AEndpointRegion.assertOpsUsingNewFs(ITestS3AEndpointRegion.java:541)\r\n\tat org.apache.hadoop.fs.s3a.ITestS3AEndpointRegion.testCentralEndpointAndNullRegionWithCRUD(ITestS3AEndpointRegion.java:495)\r\n{code}", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Optimize `sparkapps.sh` to use `kubectl delete --all`\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Support create and rename idempotency on FNS Blob from client side", "output": "ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Cf [https://github.com/apache/kafka/pull/20518#discussion_r2338270305]\r\n\r\nWith AK 4.0.0 release, we officially cut support for direct upgrades of Kafka Streams applications from versions 2.3 and older, and only direct upgrades from 2.4 or newer to 4.0+ are supported.\r\n\r\nThus, we should update `UpgradeFromValues.java` and remove older version, and cleanup the backward compatibility code accordingly.", "output": "Remove unsupported \"upgrade from\" versions"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix flaky RemoteLogManagerTest#testCopyQuota\nDescription: see https://github.com/apache/kafka/actions/runs/16651849478/job/47166854957?pr=20269\nQ: Thanks, [~lianetm]  Didn't yet have a time to take a look on this one\r\nbtw. if we're talking about flaky tests, could you please check this one? It was already waiting for like 3 months :)\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-19299?jql=resolution%20%3D%20Unresolved%20AND%20assignee%20%3D%20currentUser()%20AND%20project%20%3D%2012311720\r\n\r\n[https://github.com/apache/kafka/pull/19927]", "output": "Sure! Sorry that one has been sitting there, I will take a look. Thanks!"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Optimizing the null check logic of Lists#addAll method\nDescription: Optimizing the null check logic of Lists#addAll method", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add downgrade support\nDescription: {{RocksDbStateStore#close}} will need to detect when {{upgrade.from}} is set to a version that doesn't support offsets, and downgrade by porting them out to {{.checkpoint}} and removing the column family.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move BrokerMetadataPublisher to metadata module\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix deadlock in Observation\nDescription: Observation class has been evolved a few times during Spark 3.5 to Spark 4.0.0. Previously it uses locking mechanism (synchronized) between get and onFinish methods to coordinate metrics update and retrieval.\r\n\r\nBut it has a potential deadlocking bug. If get is called before ObservationListener is triggered to call onFinish, get will forever be waiting for metrics because it locks the observation object by synchronized so later onFinish call is locked out from updating the metrics.\r\n\r\nThis locking mechanism was replaced by a promise by SPARK-49423 which is a large refactoring on the observation feature. But in the PR, I don’t see the deadlock bug was mentioned, and there is no bug fix PR proposed to earlier versions. So I think that the bug was not known and the fix is unintentional in Spark 4.0.0. The bug is still in Spark 3.5 branch.\nQ: Issue resolved by pull request 52657\n[https://github.com/apache/spark/pull/52657]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: NPE Cannot invoke org.apache.kafka.connect.runtime.ConnectMetrics$MetricGroup.close()\nDescription: Several tasks in multiple sink connectors in a long-running connect cluster broke spontaneously (within a couple of hours) with the following stacktrace:\r\n{code:java}\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.kafka.connect.runtime.ConnectMetrics$MetricGroup.close()\" because the return value of \"java.util.concurrent.ConcurrentMap.get(Object)\" is null\r\n    at org.apache.kafka.connect.runtime.Worker$ConnectorStatusMetricsGroup.recordTaskRemoved(Worker.java:2333)\r\n    at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:707)\r\n    at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:568)\r\n    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2009)\r\n    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$39(DistributedHerder.java:2059)\r\n    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)\r\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n    at java.base/java.lang.Thread.run(Unknown Source) {code}\r\nRestarting the failed tasks with the REST API lead to another task failure with the following stacktrace:\r\n{code:java}\r\njava.lang.NullPointerException: Cannot invoke \"java.util.Map.size()\" because \"inputMap\" is null\r\n    at org.apache.kafka.common.utils.Utils.castToStringObjectMap(Utils.java:1476)\r\n    at org.apache.k", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update Boost to 1.86.0 in Windows build image\nDescription: HADOOP-19475 has updated Boost to 1.86.0, but it missed the Windows docker image, which doesn't use the same mechanism for dependencies as the Linux ones.\r\n\r\nUpdate Boost in the Windows build Docker image to the same version.\nQ: stoty closed pull request #7601: HADOOP-19538. Update Boost to 1.86.0 in Windows build image\nURL: https://github.com/apache/hadoop/pull/7601", "output": "stoty commented on PR #7601:\nURL: https://github.com/apache/hadoop/pull/7601#issuecomment-2796268824\n\n   FYI @GauthamBanasandra"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade hadoop3 docker scripts to 3.4.2\nDescription: The Hadoop 3.4.2 version has been released, and we need to update the Hadoop Docker image to version 3.4.2.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade commons-validator to 1.10.0\nDescription: In KAFKA-19359, the commons-beanutils transitive dependency was force bumped in the project to avoid related CVEs. The commons-validator already has a new release, which solves this problem:\r\n\r\n[https://github.com/apache/commons-validator/tags]\r\n\r\nThe workaround could be deleted as part of the version bump.\nQ: After bumping the commons-validator, the upgraded commons-beanutils is used:\r\n{code:java}\r\n|    +--- commons-validator:commons-validator:1.10.0\r\n|    |    +--- commons-beanutils:commons-beanutils:1.11.0{code}\r\nThere is still old commons-beanutils in the project, but that comes from checkstyle which is not production related:\r\n{code:java}\r\n\\--- com.puppycrawl.tools:checkstyle:10.20.2\r\n     +--- commons-beanutils:commons-beanutils:1.9.4{code}\r\nUpgrading the checkstyle should be done in another ticket.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: JVM GC Metrics supports fine-grained metrics\nDescription: See [https://bugs.openjdk.org/browse/JDK-8307059] , The Generational ZGC separate the Cycles and Pauses as Minor and Major,like this:\r\n * Old ZGC: \"ZGC Cycles\", \"ZGC Pauses\"\r\n * Generational ZGC: \"ZGC Minor Cycles\", \"ZGC Minor Pauses\", \"ZGC Major Cycles\", \"ZGC Major Pauses\"\r\n\r\nlet us separate it same and give 2 new metric about these, may be better...\r\n\r\n-----------------------------------------------------------------------------------------\r\n\r\nImpl to support get minor/major GC time/count from Hadoop Jmx\nQ: chaijunjie0101 opened a new pull request, #7406:\nURL: https://github.com/apache/hadoop/pull/7406\n\n   ### Description of PR\r\n   https://issues.apache.org/jira/browse/HADOOP-19461\r\n   Add 4 metrics to support Generational ZGC\r\n   MinorGCCount\r\n   MajorGCCount\r\n   MinorGcTimeMillis\r\n   MajorGcTimeMillis\r\n   so we could focous minor or major GC from Hadoop jvm jmx", "output": "test on my cluster, see HADOOP-19461.jpg"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: kafka-consumer-groups.sh --describe --all-groups command times out on large clusters with many consumer groups\nDescription: Description:\r\nWhen running the following command in a Kafka cluster with a large number of consumer groups (over 380) and topics (over 500), the kafka-consumer-groups.sh --describe --all-groups operation consistently times out and fails to return results.\r\n\r\nCommand used:\r\n\r\n./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups\r\nObserved behavior:\r\nThe command fails with a TimeoutException, and no consumer group information is returned. The following stack trace is observed:\r\n\r\njava.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeConsumerGroups, deadlineMs=1753170317381, tries=1, nextAllowedTryMs=1753170317482) timed out at 1753170317382 after 1 attempt(s)\r\n    at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\r\n    at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\r\n    ...\r\nCaused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeConsumerGroups, deadlineMs=..., tries=1, ...) timed out\r\nCaused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: describeConsumerGroups\r\nExpected behavior:\r\nThe command should be able to return the description of all consumer groups, or at least fail more gracefully. Ideally, there should be:\r\n\r\nA way to paginate or batch the describe operation;\r\n\r\nOr configuration options to increase internal timeout thresholds;\r", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SecretManager logs at INFO in bin/hadoop calls\nDescription: When I invoke a CLI command, I now get told information about HMAC keys which are\r\n* utterly meaningless to me\r\n* completely unrelated to what I am doing\r\n\r\n{code}\r\n bin/hadoop s3guard bucket-info $BUCKET\r\n\r\n2025-03-25 12:05:44,838 [main] INFO  token.SecretManager (SecretManager.java:(126)) - Selected hash algorithm: HmacSHA1\r\n2025-03-25 12:05:44,838 [main] INFO  token.SecretManager (SecretManager.java:(131)) - Selected hash key length:64\r\n{code}\r\n\r\nLooks like the changes in YARN-11738 have created this", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Make Maven plugins up-to-date\nDescription: \nQ: Issue resolved by pull request 52622\n[https://github.com/apache/spark/pull/52622]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I'm trying to use the native docker image for SASL PLAIN authentication.\r\n\r\nThe server starts okay but when I connect a client it emits an exception:\r\n\r\n\r\n \r\n{code:java}\r\n[2025-08-06 23:20:47,302] WARN [SocketServer listenerType=BROKER, nodeId=1] Unexpected error from /192.168.178.96 (channelId=192.168.178.96:9092-192.168.178.96:42552-1-1); closing connection (org.apache.kafka.common.network.Selector) java.lang.UnsupportedOperationException: Unable to find suitable Subject#doAs or Subject#callAs implementation at org.apache.kafka.common.internals.UnsupportedStrategy.createException(UnsupportedStrategy.java:40) ~[?:?] at org.apache.kafka.common.internals.UnsupportedStrategy.callAs(UnsupportedStrategy.java:58) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.lambda$callAs$1(CompositeStrategy.java:104) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.performAction(CompositeStrategy.java:78) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.callAs(CompositeStrategy.java:104) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.createSaslServer(SaslServerAuthenticator.java:208) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.handleKafkaRequest(SaslServerAuthenticator.java:533) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.authenticate(SaslServerAuthenticator.java:281) ~[?:?] at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:181) ~[?:?] at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:548) [kafka.Kafka:?] at org.apache.kafka.common.network.Selector.poll(Selector.java:486) [kafka.Kafka:?] at kafka.network.Processor.poll(SocketServer.scala:1017) [kafka.Kafka:?] at kafka.network.Processor.run(SocketServer.scala:921) [kafka.Kafka:?] at java.base/java.lang.Thread.runWith(Thread.java:1596) [kafka.Kafka:?] at java.base/java.lang.Thread.run(Thread.java:1583) [kafka.Kafka:?] at org.graalvm.nativeima", "output": "Native docker image authentication fails with SASL PLAIN"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add Caching Mechanism to HostResolver to Avoid Redundant Hostname Resolutions\nDescription: *Background:*\r\n\r\n \r\n\r\nCurrently, the two implementations of org.apache.hadoop.security.SecurityUtil.HostResolver, *StandardHostResolver and QualifiedHostResolver* in Hadoop performs hostname resolution each time it is called. *Each heartbeat between the AM and RM causes the RM to invoke the* HostResolver#getByName {*}method once{*}. In large-scale clusters running numerous applications, this results in *a high frequency of redundant hostname resolutions.*\r\n\r\n \r\n\r\n*Proposal:*\r\n\r\n \r\n\r\nIntroduce a caching mechanism in HostResolver to store resolved hostnames for a configurable duration. This would:\r\n\r\n•Reduce redundant DNS queries.\r\n\r\n•Improve performance for frequently used hostnames.\r\n\r\n•Allow configuration options for cache size and TTL (Time-to-Live).\r\n\r\n \r\n\r\n*Suggested Implementation:*\r\n\r\n1.{*}Leverage Existing CachedResolver{*}:\r\n\r\nThe NodesListManager.CachedResolver class in Hadoop already implements a caching mechanism for hostname resolution. Instead of introducing an entirely new solution, we propose *extracting the caching logic from* NodesListManager.CachedResolver {*}into a separate reusable utility class{*}.\r\n\r\n2.{*}Create a Shared Caching Utility{*}:\r\n\r\n•Extract the caching logic from NodesListManager.CachedResolver.\r\n\r\n•Implement a new class, e.g., HostnameCache, and place it in the Hadoop Common module to ensure it can be used across different components.\r\n\r\n3.{*}Integrate{*} HostnameCache with *HostResolver &QualifiedHostResolver*:\r\n\r\n•Modify HostResolver to us", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title:  Upgrade buf plugins to v29.5\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The shared resource can be assigned to null, while another member method later dereferences it without a null check. This can cause a runtime failure when fencing operations are triggered.", "output": "Missing null check before using shared resource"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Exclude some files (like .gitkeep) from Rat checks ?\nDescription: *Note:* presumably I need a buy-in from [~chia7712] for this ticket.\r\n\r\n{*}Blocks this tickets{*}: KAFKA-19174, KAFKA-19591 (these two are created separately buy could/should be resolved at once). \r\n\r\n{*}Rationale{*}: Gradle upgrade related PR and Jira ticket are blocked due to a fact that Rat check breaks the build even for such small stuff such as .gitkeep placeholder files.\r\n\r\nSee here for more details: [https://github.com/apache/kafka/pull/19513#issuecomment-3213821218] \r\n{code:java}\r\nRun python .github/scripts/rat.py\r\nFound 1 Rat reports\r\nRead env GITHUB_WORKSPACE: /home/runner/work/kafka/kafka\r\nParsing Rat report file: build/rat/rat-report.xml\r\n6840 approved licenses\r\n1 unapproved licenses\r\nFiles with unapproved licenses:\r\nNotice: File with unapproved license: distribution/.gitkeep\r\nError: Process completed with exit code 1.\r\n{code}", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Exclude some files (like .gitkeep) from Rat checks ?\nDescription: *Note:* presumably I need a buy-in from [~chia7712] for this ticket.\r\n\r\n{*}Blocks this tickets{*}: KAFKA-19174, KAFKA-19591 (these two are created separately buy could/should be resolved at once). \r\n\r\n{*}Rationale{*}: Gradle upgrade related PR and Jira ticket are blocked due to a fact that Rat check breaks the build even for such small stuff such as .gitkeep placeholder files.\r\n\r\nSee here for more details: [https://github.com/apache/kafka/pull/19513#issuecomment-3213821218] \r\n{code:java}\r\nRun python .github/scripts/rat.py\r\nFound 1 Rat reports\r\nRead env GITHUB_WORKSPACE: /home/runner/work/kafka/kafka\r\nParsing Rat report file: build/rat/rat-report.xml\r\n6840 approved licenses\r\n1 unapproved licenses\r\nFiles with unapproved licenses:\r\nNotice: File with unapproved license: distribution/.gitkeep\r\nError: Process completed with exit code 1.\r\n{code}\nQ: (Will be) implemented via KAFKA-19174 (and hence resolving).", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: upgrade bouncycastle to 1.82 due to CVE-2025-8916\nDescription: https://github.com/advisories/GHSA-4cx2-fc23-5wg6\r\n\r\nThought it was tidier to upgrade to latest version even if the fix was a while ago.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Producer performance test failing in parsing aggregate statistics\nDescription: When running benchmark test, some of the producer perf tests are failing in statistics parsing. Logs attached\r\n\r\ncmd: `TC_PATHS=\"tests/kafkatest/benchmarks/core/benchmark_test.py\" bash tests/docker/run_tests.sh`\r\n\r\nJDK: `azul/zulu-openjdk:17`(used azul's jdk because with openjdk tests were failing to run)\nQ: Hello [~schofielaj], I think the main reason is that this is a performance test, so it didn’t finish running. In your test result, the last line is `printWindow()` instead of `printTotal()`, which caused the parse error.\r\n\r\nI tested on both the trunk and the [https://github.com/apache/kafka/pull/20385] branch, and all the tests passed. I think this is not a bug.", "output": "oops. I thought it was azure's"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Context:\r\nhttps://github.com/apache/kafka/pull/20306#discussion_r2262298636", "output": "Have a way to set config to admin client only in TopicBasedRemoteLogMetadataManager"}
{"instruction": "Answer the question based on the bug.", "input": "Title: LocalDirAllocator still doesn't always recover from directory tree deletion\nDescription: In HADOOP-18636, LocalDirAllocator was modified to recreate missing dirs. But it appears that there are still codepaths which don't do that.\r\n\r\nwhen charrypicking this -please follow up with  HADOOP-19573, to ensure an associated test never fails\nQ: hadoop-yetus commented on PR #7651:\nURL: https://github.com/apache/hadoop/pull/7651#issuecomment-2831359212\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   9m 16s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 49s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 35s |  |  trunk passed  |\r\n   | -1 :x: |  shadedclient  |  26m 29s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m  6s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 20s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 20s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 18s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | -1 :x: |  javac  |   0m 18s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 26s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 1 new + 23 unchanged - 0 fixed = 24 total (was 23)  |\r\n   | -1 :x: |  mvnsite  |   0m 33s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 14s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-common in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  spotbugs  |   1m 18s | [/new-spotbugs-hadoop-common-project_hadoop-common.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/new-spotbugs-hadoop-common-project_hadoop-common.html) |  hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | -1 :x: |  shadedclient  |  26m 32s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m  9s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 15s |  |  ASF License check generated no output?  |\r\n   |  |   | 102m 36s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-common-project/hadoop-common |\r\n   |  |  Exceptional return value of java.io.File.mkdirs() ignored in org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(String, long, Configuration, boolean)  At LocalDirAllocator.java:ignored in org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(String, long, Configuration, boolean)  At LocalDirAllocator.java:[line 433] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7651 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux e68470bb8f81 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / da6917e5510e9e52ecfa60f0d95640da8099b1db |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/testReport/ |\r\n   | Max. process+thread count | 550 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7651/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "cnauroth commented on code in PR #7651:\nURL: https://github.com/apache/hadoop/pull/7651#discussion_r2067072005\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java:\n##########\n@@ -393,6 +396,8 @@ int getCurrentDirectoryIndex() {\n      */\n     public Path getLocalPathForWrite(String pathStr, long size,\n         Configuration conf, boolean checkWrite) throws IOException {\n+      LOG.debug(\"searchng for directory for file at {}, size = {}; checkWrite={}\",\n\nReview Comment:\n   Minor typo: \"searching\".\n\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java:\n##########\n@@ -406,27 +411,40 @@ public Path getLocalPathForWrite(String pathStr, long size,\n         pathStr = pathStr.substring(1);\n       }\n       Path returnPath = null;\n-      \n-      if(size == SIZE_UNKNOWN) {  //do roulette selection: pick dir with probability \n-                    //proportional to available size\n-        long[] availableOnDisk = new long[ctx.dirDF.length];\n-        long totalAvailable = 0;\n-        \n-            //build the \"roulette wheel\"\n-        for(int i =0; i < ctx.dirDF.length; ++i) {\n-          final DF target = ctx.dirDF[i];\n-          // attempt to recreate the dir so that getAvailable() is valid\n-          // if it fails, getAvailable() will return 0, so the dir will\n-          // be declared unavailable.\n-          // return value is logged at debug to keep spotbugs quiet.\n-          final boolean b = new File(target.getDirPath()).mkdirs();\n-          LOG.debug(\"mkdirs of {}={}\", target, b);\n-          availableOnDisk[i] = target.getAvailable();\n-          totalAvailable += availableOnDisk[i];\n+\n+      final int dirCount = ctx.dirDF.length;\n+      long[] availableOnDisk = new long[dirCount];\n+      long totalAvailable = 0;\n+\n+      StringBuilder pathNames = new StringBuilder();\n+\n+      //build the \"roulette wheel\"\n+      for (int i =0; i < dirCount; ++i) {\n+        final DF target = ctx.dirDF[i];\n+        // attempt to recreate the dir so that getAvailable() is valid\n+        // if it fails, getAvailable() will return 0, so the dir will\n+        // be declared unavailable.\n+        // return value is logged at debug to keep spotbugs quiet.\n+        final String name = target.getDirPath();\n+        pathNames.append(\" \").append(name);\n+        final File dirPath = new File(name);\n+        if (!dirPath.exists()) {\n+          LOG.debug(\"creating buffer dir {}}\", name);\n+          dirPath.mkdirs();\n\nReview Comment:\n   Do you want to debug-log the boolean return value? It should almost always be `true` now that you added the `exists()` check, but it can still be `false` for edge cases like permissions violations."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Make `KubernetesClientUtils` Java-friendly\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The implementation for KAFKA-15665 is a little too strict - it requires that all replicas in the target replica set be in ISR.\r\n\r\nThis means if we have a reassignment like [1, 2, 3] -> [2, 3, 4] where broker 2 was unhealthy and lagging in replication, reassignment would not complete. \r\n\r\n \r\n\r\nI believe it should work to instead just check the following:\r\n\r\n- Added replicas are in ISR (newTargetISR)\r\n- Resulting ISR (newTargetISR) is not under-min-isr", "output": "KAFKA-15665 prevents safe reassignments from completing"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "1. Using `leaderEpoch=\" + leaderEpoch` feels a bit odd, since not every value of `leaderEpoch` is invalid\r\n\r\n2. `leaderEpoch=null` should be treated as same as `leaderEpoch=-1`, but `OffsetAndMetadata#equals` does not respect it.\r\n\r\n3. `OffsetAndMetadata#metadata` currently has neither integration nor unit tests", "output": "Tweak org.apache.kafka.clients.consumer.OffsetAndMetadata"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update 3.4.2 docs landing page to highlight changes shipped in the release\nDescription: The landing page for the 3.4.2 docs mostly lists changes that already shipped in 3.4.0. Update this to highlight 3.4.2 changes.\nQ: ahmarsuhail opened a new pull request, #7887:\nURL: https://github.com/apache/hadoop/pull/7887\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Updates landing page with 3.4.2 changes.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "ahmarsuhail commented on PR #7887:\nURL: https://github.com/apache/hadoop/pull/7887#issuecomment-3200748949\n\n   @anujmodi2021 could you please review the ABFS changes, and let me know if there's anything else you want to highlight. \r\n   \r\n    @steveloughran anything else in S3A we want to highlight?\r\n   \r\n   I'll merge this in tomorrow and kick off the new build."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "There are a few memory consumers that only support on-heap mode. Including:\r\n\r\n# LongToUnsafeRowMap (for long key hash join)\r\n# ExternalSorter (for non-serializable sort-based shuffle)\r\n\r\nIt's ideal to make them support off-heap when Spark is running with off-heap memory mode. For 3rd accelerator plugins that leverage Spark off-heap memory, having the large allocations in vanilla Spark supporting off-heap mode will significantly ease the memory settings for running the mixed query plan that contains both vanilla Spark and offloaded computations.", "output": "Add off-heap mode support for on-heap-only memory consumers"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This ticket is a follow up to https://issues.apache.org/jira/browse/KAFKA-19668\r\n\r\nWith K19668, we introduced some \"hot fix\", that is actually disabled by default for backward compatibility reasons. A cleaner fix (which required a KIP, and thus cannot go into a bug-fix release), would be to add a new overload of `processValues()` which has the fix enabled, and deprecate the existing `processValues()` methods.", "output": "Provide clean upgrade path from transformValues to processValues"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove unsupported \"upgrade from\" versions\nDescription: Cf [https://github.com/apache/kafka/pull/20518#discussion_r2338270305]\r\n\r\nWith AK 4.0.0 release, we officially cut support for direct upgrades of Kafka Streams applications from versions 2.3 and older, and only direct upgrades from 2.4 or newer to 4.0+ are supported.\r\n\r\nThus, we should update `UpgradeFromValues.java` and remove older version, and cleanup the backward compatibility code accordingly.\nQ: [~isding_l] – Seems there was a race condition. [~nikita-shupletsov] did start to look into this issue already.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix decimal rescaling in LocalDataToArrowConversion\nDescription: \nQ: Issue resolved by pull request 52637\n[https://github.com/apache/spark/pull/52637]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: bulk delete must fully qualify paths before validating them\nDescription: noticed this with iceberg. If you have a path(\"/a/b/c\") where you get the fs, you get a qualified uri back (\"file://\"). but if you call bulkdelete on the path, it fails as the path isn't under file://\r\n\r\nfix: qualify the path under the fs.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add a new Dockerfile to compile Hadoop on latest ubuntu:noble (24.04) with JDK17 as the default compiler.", "output": "[JDK17] Add ubuntu:noble as a build platform with JDK-17 as default"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Create a new docker image based on the lean tarball.\r\n\r\nhadoop-3.4.2-lean.tar.gz", "output": "Create lean docker image"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support recaching when a table is written via a different table implementation (V1 or V2)\nDescription: When a table is cached using one table implementation (e.g., V2) and written through the other (e.g., V1), Spark may not automatically trigger recaching. As a result, the cached data can become stale even though the underlying table content has changed.\r\n\r\n \r\n\r\nThis issue arises because the current recaching mechanism does not consistently handle cross-implementation writes. Given that the community is actively working on Data Source V2 (DSV2), many data sources are expected to have both V1 and V2 implementations for a period of time, making this issue more likely to occur in practice.\r\n\r\n \r\n\r\n*Proposed Fix:*\r\n\r\nEnhance the cache invalidation logic to detect writes that occur through a different table implementation (V1 ↔ V2) and trigger recaching accordingly.\r\n\r\n \r\n\r\n{*}Expected Outcome:{*}{*}{*}\r\n * Cached data remains up to date when a table is written through either V1 or V2 paths.\r\n\r\n * Both logical-plan-based and file-path-based recaching continue to work as expected for V1&V2 connectors", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: UserGroupInformation.java is using a non-support operation in JDK25\nDescription: Hello,\r\n\r\nI'm trying to upgrade my version of ParquetJava and I'm seeing the following error locally\r\n{code:java}\r\n    java.lang.UnsupportedOperationException: getSubject is not supported\r\n\r\n\r\n        at java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\r\n\r\n        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.(FileSystem.java:3852)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.(FileSystem.java:3842)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3630)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541)\r\n\r\n\r\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\r\n\r\n        at org.apache.parquet.hadoop.util.HadoopOutputFile.fromPath(HadoopOutputFi\nQ: Appreciate the link, thanks PJ, will keep a watch on that :)", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Remove JUnit4 Dependency\nDescription: Due to the extensive JUnit4 dependencies in the Hadoop modules, we will attempt to remove JUnit4 dependencies on a module-by-module basis.\nQ: slfan1989 opened a new pull request, #7799:\nURL: https://github.com/apache/hadoop/pull/7799\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19617 - [JDK17] Remove JUnit4 Dependency(MapReduce).\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Junit Test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7799:\nURL: https://github.com/apache/hadoop/pull/7799#issuecomment-3065100266\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 48s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  7s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   1m 53s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   1m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   4m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   4m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 51s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   8m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 50s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   3m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   1m 43s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   1m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   1m 33s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   1m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   3m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 57s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   8m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m  5s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   1m  5s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7799/1/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch failed.  |\r\n   | -1 :x: |  unit  |   0m 48s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7799/1/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core.txt) |  hadoop-mapreduce-client-core in the patch failed.  |\r\n   | +1 :green_heart: |  unit  |   1m  4s |  |  hadoop-mapreduce-client-shuffle in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   9m 55s |  |  hadoop-mapreduce-client-app in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   5m  2s |  |  hadoop-mapreduce-client-hs in the patch passed.  |\r\n   | -1 :x: |  unit  | 135m 47s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7799/1/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt) |  hadoop-mapreduce-client-jobclient in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 55s |  |  hadoop-mapreduce-client-nativetask in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 45s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 340m 22s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapred.TestYARNRunner |\r\n   |   | hadoop.mapred.TestYARNRunner |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7799/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7799 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux ca95605c51de 5.15.0-138-generic #148-Ubuntu SMP Fri Mar 14 19:05:48 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7815ede3f80aa79b97f4e6e9d251c5ae7e72a0dc |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7799/1/testReport/ |\r\n   | Max. process+thread count | 1165 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask U: hadoop-mapreduce-project/hadoop-mapreduce-client |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7799/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support getTables for SparkConnectDatabaseMetaData\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Note:* presumably I need a buy-in from [~chia7712] for this ticket.\r\n\r\n{*}Blocks this tickets{*}: KAFKA-19174, KAFKA-19591 (these two are created separately buy could/should be resolved at once). \r\n\r\n{*}Rationale{*}: Gradle upgrade related PR and Jira ticket are blocked due to a fact that Rat check breaks the build even for such small stuff such as .gitkeep placeholder files.\r\n\r\nSee here for more details: [https://github.com/apache/kafka/pull/19513#issuecomment-3213821218] \r\n{code:java}\r\nRun python .github/scripts/rat.py\r\nFound 1 Rat reports\r\nRead env GITHUB_WORKSPACE: /home/runner/work/kafka/kafka\r\nParsing Rat report file: build/rat/rat-report.xml\r\n6840 approved licenses\r\n1 unapproved licenses\r\nFiles with unapproved licenses:\r\nNotice: File with unapproved license: distribution/.gitkeep\r\nError: Process completed with exit code 1.\r\n{code}", "output": "Exclude some files (like .gitkeep) from Rat checks ?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Upgrade Java SDK to v2.31.66 or higher. \r\n\r\nFollow the qualification process defined in [https://github.com/steveloughran/engineering-proposals/blob/trunk/qualifying-an-SDK-upgrade.md] .\r\n\r\n \r\n\r\n2.31.66+ should now support third party stores, this was previously a blocker to upgrade SDK. \r\n\r\nUpgrade is needed as it has support for some S3 express features, and a fix for https://github.com/aws/aws-sdk-java-v2/issues/5247", "output": "Upgrade Java SDK V2 to v2.31.66+"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Introduce the following frequency estimation functions:\r\n * *APPROX_TOP_K(expr[, k[, maxItemsTracked]])* – Returns an approximate list of the top _k_ most frequent values in the input expression using a probabilistic algorithm.\r\n\r\n * *APPROX_TOP_K_ACCUMULATE(expr[, maxItemsTracked])* – Creates a state object that accumulates frequency statistics for use in later estimation or combination.\r\n\r\n * *APPROX_TOP_K_ESTIMATE(state [, k] ])* – Extracts and returns the approximate top _k_ values and their estimated frequencies from an accumulated state.\r\n\r\n * *APPROX_TOP_K_COMBINE(expr[, maxItemsTracked])* – Merges intermediate APPROX_TOP_K state objects, allowing distributed or parallel computation.", "output": "Frequency estimation functions"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have observed an issue where very large number of threads are being launched when performing concurrent {{fs.getFileStatus(path) operations}} as proxyUser.\r\n\r\nAlthough this issue was observed in our hive services, we were able to isolate and replicate this issue without hive by writing a sample standalone program which first logs in via a principal and keytab and then creates a proxy user and fires concurrent {{fs.getFileStatus(path)}} for a few mins. Eventually when the concurrency increases it tries to create more threads than max available threads(ulimit range) and the process eventually slows down.\r\n{code:java}\r\nUserGroupInformation proxyUserUGI = UserGroupInformation.createProxyUser(\r\n\"hive\", UserGroupInformation.getLoginUser());{code}\r\nIn this particular case, when launching 30 concurrent threads calling , the max number of threads launched by the PID are 6066.\r\n \r\n{code:java}\r\nEvery 1.0s: ps -eo nlwp,pid,args --sort -nlwp | head                                                                       Wed Feb 19 06:12:47 2025\r\nNLWP     PID COMMAND\r\n6066  700718 /usr/lib/jvm/java-17-openjdk/bin/java -cp ./test.jar:/usr/hadoop/*:/usr/hadoop/lib/*:/usr/hadoop-hdfs/* org.apache.hadoop.hive.common.HDFSFileStatusExample hdfs://namenode:8020 principal keytab_location 30 true\r\n\r\n\r\n{code}\r\n \r\n \r\n \r\n \r\nBut the same behaviour is not observed when the same calls are made using the current userUGI instead of proxyUser.\r\n{code:java}\r\nUserGroupInformation currentUserUgi = UserGroupInformation.getCurrentUser();{code}\r\n \r\n\r\nIn this case when launching 30 concurrent threads calling , the max number of threads launched by the PID are 56 and when launched with 500 concurrent threads the max number of threads launched are 524.\r\n{code:java}\r\nEvery 1.0s: ps -eo nlwp,pid,args --sort -nlwp | head                                                       Tue Feb 18 06:23:18 2025NLWP     PID COMMAND\r\n  56  748244 /usr/lib/jvm/java-17-openjdk/bin/java -cp ./test.jar:/usr/hadoop/*:/usr/hadoop", "output": "High number of Threads Launched when Calling fs.getFileStatus() via proxyUser after Kerberos authentication."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix DSv2 in PushVariantIntoScan by adding SupportsPushDownVariants\nDescription: This goes to add DSv2 support to the optimization rule PushVariantIntoScan. The PushVariantIntoScan rule only supports DSv1 Parquet (ParquetFileFormat) source. It limits the effectiveness of variant type usage on DSv2.\nQ: [~dongjoon] Thank you for updating this ticket.", "output": "Thank YOU for the fix. :)"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "noticed this with iceberg. If you have a path(\"/a/b/c\") where you get the fs, you get a qualified uri back (\"file://\"). but if you call bulkdelete on the path, it fails as the path isn't under file://\r\n\r\nfix: qualify the path under the fs.", "output": "bulk delete must fully qualify paths before validating them"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "See https://github.com/apache/kafka/pull/20392#issuecomment-3241457533", "output": "Clean up TaskManagerTest"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration\nDescription: Establish the foundational build infrastructure for RISC-V CRC32 hardware acceleration support while maintaining full backward compatibility with existing software implementations.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SharePartition changes to support renew ack.\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [java.time] hadoop-common\nDescription: \nQ: YanivKunda opened a new pull request, #7556:\nURL: https://github.com/apache/hadoop/pull/7556\n\n   \r\n   ### Description of PR\r\n   \r\n   Migration to java.time classes in hadoop-common\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran existing tests\r\n   \r\n   ### For code changes:\r\n   \r\n   - [V] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7556:\nURL: https://github.com/apache/hadoop/pull/7556#issuecomment-2764944470\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m  3s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 15 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  8s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  35m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 39s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  14m 51s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   5m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m  7s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  4s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |  10m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 56s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   3m 35s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |  10m  1s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javac  |  10m  1s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   9m  4s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | -1 :x: |  javac  |   9m  4s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m 22s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 17 new + 436 unchanged - 7 fixed = 453 total (was 443)  |\r\n   | +1 :green_heart: |  mvnsite  |   4m 24s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 55s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  javadoc  |   3m 44s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  spotbugs  |   2m 26s | [/new-spotbugs-hadoop-common-project_hadoop-common.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/new-spotbugs-hadoop-common-project_hadoop-common.html) |  hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | -1 :x: |  shadedclient  |  11m  3s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  14m 27s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 34s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   5m 40s |  |  hadoop-yarn-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   8m 40s |  |  hadoop-mapreduce-client-core in the patch passed.  |\r\n   | -1 :x: |  unit  |   8m 44s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt) |  hadoop-mapreduce-client-app in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 270m 57s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-common-project/hadoop-common |\r\n   |  |  The class name org.apache.hadoop.util.Clock shadows the simple name of the superclass java.time.Clock  At Clock.java:the simple name of the superclass java.time.Clock  At Clock.java:[lines 32-41] |\r\n   | Failed junit tests | hadoop.mapreduce.v2.app.rm.TestRMContainerAllocator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7556 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d056f058380f 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 670b19020dc64bc90f7f6e6accaf75f5c54c0055 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/testReport/ |\r\n   | Max. process+thread count | 1325 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "HADOOP-19617 removed direct junit 4 dependency.  However, junit 4 is still pulled transitively by other dependencies.", "output": "Exclude junit 4 transitive dependency"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Values set for custom catalogs are not being substituted:\r\n\r\n{code:java}\r\nspark.sql.catalog.mssql=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog\r\nspark.sql.catalog.mssql.user=${env:MSSQL__USER}\r\n{code}\r\n\r\n\r\nFails with:\r\n{code:java}\r\nspark.sql(\"show tables in mssql\").show()\r\n\r\ncom.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '${env:MSSQL__USER}'\r\n{code}", "output": "Enable substitution for SQLConf settings"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use Java-friendly `KubernetesClientUtils` APIs\nDescription: \nQ: Issue resolved by pull request 410\n[https://github.com/apache/spark-kubernetes-operator/pull/410]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update `Operator SDK` to 5.1.4\nDescription: see JOSDK release: https://github.com/operator-framework/java-operator-sdk/releases/tag/v5.1.4\nQ: Issue resolved by pull request 387\n[https://github.com/apache/spark-kubernetes-operator/pull/387]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Prologue:* KAFKA-19664 \r\n\r\n*In more details:* Apache commons bcel Java 25 compatible version will be created soon (and SpotBugs version will follow immediately): https://issues.apache.org/jira/browse/BCEL-377?focusedCommentId=18028106&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18028106\r\n\r\n*Action points:*\r\n - upgrade SpotBugs version (use Java 25 compatible version)\r\n - enable SpotBugs checks for Java 25 Github actions build\r\n\r\n*Related links: *\r\n- [https://issues.apache.org/jira/projects/BCEL/versions/12354966] \r\n- [https://github.com/spotbugs/spotbugs/issues/3564]", "output": "Update SpotBugs version and enable Spotbugs Gradle tasks on Java 25"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A Analaytics-Accelerator: ITestS3AContractOpen.testInputStreamReadNegativePosition() failing\nDescription: New test added in [https://github.com/apache/hadoop/pull/7367,]\r\n\r\nexpects an IndexOutOfBoundsException to be thrown, when offset is negative.\r\n\r\n \r\n\r\nAAL throws IllegalArgumentException. For len, the PR says it's ok to throw either IllegalArgumentException or IndexOutOfBoundsException, so should the same thing happen of offset?", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Rename `spark.executor.(log -> logs).redirectConsoleOutputs`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done.\nDescription: There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header.\r\n\r\nFollowing are types of read we want to identify:\r\n # Direct Read: Read from a given position in remote file. This will be synchronous read\r\n # Normal Read: Read from current seeked position where read ahead was bypassed. This will be synchronous read.\r\n # Prefetch Read: Read triggered from background threads filling up in memory cache. This will be asynchronous read.\r\n # Missed Cache Read: Read triggered after nothing was received from read ahead. This will be synchronous read.\r\n # Footer Read: Read triggered as part of footer read optimization. This will be synchronous.\r\n # Small File Read: Read triggered as a part of small file read. This will be synchronous read.\r\n\r\nWe will add another field in the Tracing Header (Client Request Id) for each request. We can call this field \"Operation Specific Header\" very similar to how we have \"Retry Header\" today. As part of this we will only use it for read operations keeping it empty for other operations. Moving ahead f we need to publish any operation specific info, same header can be used.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip testRenameFileBeingAppended on Windows\nDescription: The AbstractContractAppendTest#testRenameFileBeingAppended() tries to rename the file while the handle is open. This isn't allowed on Windows and thus the call to rename fails.\r\n\r\nThus, we need to skip this test on Windows.\nQ: GauthamBanasandra commented on PR #7666:\nURL: https://github.com/apache/hadoop/pull/7666#issuecomment-2845595219\n\n   Started the Jenkins CI run for Windows - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-win10-x86_64/910/.", "output": "hadoop-yetus commented on PR #7666:\nURL: https://github.com/apache/hadoop/pull/7666#issuecomment-2845725921\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 20s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 18s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 57s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   7m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 44s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  19m  9s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 126m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7666/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7666 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a4db9505b70b 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 87871a3de3e26fd517ef9eb022f723630df278eb |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7666/1/testReport/ |\r\n   | Max. process+thread count | 3149 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7666/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix `FileStreamSinkSuite` flakiness by using `walkFileTree` instead of `walk`\nDescription: \nQ: Issue resolved by pull request 52671\n[https://github.com/apache/spark/pull/52671]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Disallow casting geospatial types to/from other data types\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A : Add option for custom S3 tags while writing and deleting S3 objects\nDescription: Custom S3 object [tags|https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html] can be added to S3 objects while writing and deleting.\r\n\r\n*Use Case:*\r\n\r\nS3 tags can be used to categorize the [objects|https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html] and potentially apply bucket level polices to take some actions.\r\n\r\nFor example : objects can be marked as \"to-be-glacier\" and based on some bucket policy the written objects can be moved to Glacier tier after sometime for cost savings.\r\n\r\nApache iceberg's [S3FileIO|#s3-tags]] also uses S3 Tags for soft deletes.\nQ: Hey [~stevel@apache.org] , I will be working on this ticket.\r\n\r\nTo answer your questions this will be a feature of S3A which will be operated through configs. Any component when interacting to S3 using S3A, if provided with relevant configs (which will be newly created) then the tags will be applied in S3. These configs can be added while creating an object or deleting an object in S3. \r\n\r\nConfig for adding the tag : fs.s3a.object.tag.* (comma separated key and values) or fs.s3a.object.tag.\\{TAG_NAME} (separate configs)\r\n\r\nConfig for deleting the tag : {{fs.s3a.soft.delete.enabled=true}}\r\n\r\nExample : \r\n\r\n1. hadoop fs -Dfs.s3a.object.tag.department=finance,project=alpha -put file.txt s3a://bucket/path/\r\n\r\nor adding conf in different lines : --conf spark.hadoop.fs.s3a.object.tag.department=finance \\\r\n--conf spark.hadoop.fs.s3a.object.tag.project=alpha \\\r\n\r\nThis command will add tags with key as department and project and value as finance and alpha respectively.\r\n\r\n2. hadoop fs \\\r\n-Dfs.s3a.soft.delete.enabled=true \\\r\n-Dfs.s3a.soft.delete.tag.key=archive \\\r\n-Dfs.s3a.soft.delete.tag.value=true \\\r\n-rm s3a://ayshukla-emr-dev/tagged-file27.txt\r\n\r\nIn this tagged-file27.txt will not be deleted. Instead a tag will be added with key as archive and value as true (since those are defined by user).\r\n\r\nHere if no key and value tag are added then a default delete tag can be added. For example key is status and value as deleted.\r\n\r\nDocumented this in the attached pdf.", "output": "ok. \r\n* soft delete is special, as that is mapping a delete call to a tag update\r\n* can you help me get my impala PR for bulk delete in (https://github.com/apache/iceberg/pull/10233) )especially why those tests don't work, as that will reduce delete time by an order of magnitude, with shipping hadoop releases. Then tagging can be done. \r\n\r\nnow, can you put the design up, ideally as a doc in a PR ()e.g. hadoop-aws/src/main/site/..../tagging.md  so that we can review it. thanks"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches`\nDescription: Similar to `Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` resolved in SPARK-54058, `Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite` also seems flaky.\r\n\r\nhttps://github.com/micheal-o/spark/actions/runs/18895266750/job/53931238163\r\n\r\n{code}\r\n[info] - Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) report available offset which is less than end offset during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-42, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info] \tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info] \tat org.scalatest.enablers.Re", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade libxxhash to 0.8.3 in Windows 10\nDescription: The current version of libxxhash - 0.8.1 isn't available on the msys repo. This is causing the Hadoop Jenkins CI for Windows to fail -\r\n\r\n{code}\r\n00:34:06  Step 25/75 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libxxhash-0.8.1-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libxxhash-0.8.1-1-x86_64.pkg.tar.zst\r\n00:34:06   ---> Running in e9a8dd91a514\r\n00:34:15  \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n{code}\r\n\r\nThus, we need to upgrade to 0.8.3.\nQ: GauthamBanasandra merged PR #7689:\nURL: https://github.com/apache/hadoop/pull/7689", "output": "Merged PR to trunk - https://github.com/apache/hadoop/pull/7689."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support `SPARK_VERSION` placeholder in container image names\nDescription: \nQ: Issue resolved by pull request 391\n[https://github.com/apache/spark-kubernetes-operator/pull/391]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Missing null check before using shared resource\nDescription: The shared resource can be assigned to null, while another member method later dereferences it without a null check. This can cause a runtime failure when fencing operations are triggered.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Description:*\r\nTo support modern authentication models (e.g., bearer tokens, OAuth2), we propose adding support in HDFS to propagate an access token via the RPC request header. This enables downstream services (e.g., NameNode, Router) to validate access tokens in a secure and standardized way.\r\n\r\nThe token will be passed in a dedicated field in the {{{}RpcRequestHeaderProto{}}}, mimicking the behavior of an HTTP {{Authorization: Bearer }} header. The caller context or UGI may extract this token and use it for authorization decisions or auditing.\r\n\r\n*Benefits:*\r\n * Enables secure, token-based authentication in multi-tenant environments\r\n\r\n * Lays the foundation for fine-grained, per-request authorization\r\n\r\n*Scope:*\r\n * Add optional {{authorization_token}} field to RPC header\r\n\r\n * Ensure token is thread-local or caller-context scoped\r\n\r\n * Wire it through relevant client and server code paths\r\n\r\n * Provide configuration to enable/disable this feature\r\n\r\n*Notes:*\r\nThis feature is intended to be backward-compatible with existing HDFS clients. If the token is not set, behavior will remain unchanged.\r\n\r\nAt Linkedin, we plan to delegate auth to a custom enforcement point in RBF. The workflow is the client will get an access token and pass that in the RPC. The request and access token will be authorized in the custom authorizer. \r\n\r\n \r\n\r\n \r\n\r\n*PR*\r\n\r\n[https://github.com/apache/hadoop/pull/7803/files#diff-55740268215623b4037dd1bd35200c383576bee23d444096eb9cee751e3728f3]", "output": "Add Support for Propagating Access Token via RPC Header in HDFS"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is an umbrella Jira to migrate exceptions in Spark Connect from plain runtime exceptions to proper Spark Exceptions with error class.", "output": "Improve Error Handling for Spark Connect"}
{"instruction": "Answer the question based on the bug.", "input": "Title: The Content-Security-Policy header must not be overridden\nDescription: [https://github.com/apache/kafka-site/blob/905299a0b7e2e3892d9493c7dcaaf78dce035c00/.htaccess#L13]\r\n\r\nThe Content-Security-Policy header must not be overridden.\r\n\r\nThere is now a standard way to add local exceptions to the CSP:\r\n\r\n[https://infra.apache.org/tools/csp.html]\r\n\r\nPlease update the .htaccess file accordingly.\r\n\r\nPlease note that the policy says that any exceptions must have been approved, and a reference to the approval must be commented in the .htaccess file\nQ: brandboat commented on PR #733:\nURL: https://github.com/apache/kafka-site/pull/733#issuecomment-3447983723\n\n   quickstart local dev env screenshot:", "output": "sebbASF commented on PR #733:\nURL: https://github.com/apache/kafka-site/pull/733#issuecomment-3448072791\n\n   Note that the .htaccess file needs to document why the override is allowed."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve logging about task readiness\nDescription: Kafka Streams processes its assigned tasks in a round robin fashion, however, if a task is considered not ready for processing, it might get skipped. We have observed cases, in which Kafka Streams seems to get stuck on a partition, and restarting the application instance resolves this issue. We suspect, it's related to considering the task for the stuck partition as not ready for processing. (As the ready/not-ready decision is based on in-memory state of KS runtime, bouncing the instance would reset KS runtime into a clean state, unblocking the stuck task.)\r\n\r\nHowever, we currently don't have sufficient logging to reason about this case, and to understand if and why a task might be skipped. We should add more log statement (DEBUG and/or TRACE) to get better visibility. There might be a lurking bug in this logic that we cannot narrow down w/o the corresponding information logging could provide.\nQ: Hi [~qyte] , \r\n\r\nI saw you picked ticket and wanted to check if you are actively working on it? I'm doing a patch for this but forgot to assign this to myself – my apologies for the confusion.\r\n\r\nIf you haven't had a chance to start, I have some capacity and would be happy to pick this up. Please let me know what works for you. Thanks!", "output": "Sure, you can pick it up. I haven't had a chance to start yet to work on it yet."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Über-jira: S3A Hadoop 3.4.3 features\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove Google Analytics from Hadoop Website\nDescription: Hi Hadoop Team,\r\n\r\nThe ASF {_}*Privacy Policy*{_}[1][2] does not permit the use of _*Google Analytics*_ on any ASF websites.\r\n\r\nIt looks like _*Google Analytics*_ was removed from the Hadoop site on *13th Nov 2024* in [Commit 86c0957|https://github.com/apache/hadoop-site/commit/86c09579bff7bf26e86475939d106e900a7da94d] and re-introduced on *20th Feb 2025* in [Commit 27f835e|https://github.com/apache/hadoop-site/commit/27f835ef0369a6bc95a12131af75c696cafb4b4c].\r\n\r\nI'm not sure how this has happened, but I see the following:\r\n* [layouts/partials/footer.html|https://github.com/apache/hadoop-site/blob/asf-site/layouts/partials/footer.html] references *__internal/google_analytics.html_*\r\n* I can't find *__internal/google_analytics.html_*\r\n* It looks like the safest action is to remove the reference to *google_analytics.html* in the *footer.html*\r\n\r\nI have created [PR #67|https://github.com/apache/hadoop-site/pull/67] to remove the reference in the footer, although I have not understood full\nQ: done it; covered in https://cwiki.apache.org/confluence/display/HADOOP2/HowToRelease / publishing\r\n\r\nHope I've done it right ...", "output": "Hi [~stevel@apache.org], from the [hadoop-site README.md|https://github.com/apache/hadoop-site/blob/asf-site/README.md], it looks like there a generation step for the standard (i.e. non-release) content with Hugo. But I know less than you.\r\n\r\nOnce its re-generated, then files like [/content/index.html|https://github.com/apache/hadoop-site/blob/asf-site/content/index.html] should be updated & the GA removed."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add Idle Thread Ratio Metric to MetadataLoader\nDescription: KIP-1229: [https://cwiki.apache.org/confluence/x/ZQteFw]", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h2. *Description*\r\n\r\nThe MirrorCheckpointTask in Mirror Maker 2 commits offsets for active consumer groups on the target cluster, causing consumers to rewind their offsets or skip messages. Typically brokers prevent committing offsets for consumer groups that are not in `EMPTY` state by throwing {{{}UnknownMemberIdException{}}}. In addition, {{MirrorCheckpointTask}} has logic in place to prevent committing offsets older than target for {{EMPTY}} consumer groups. However, due to a bug in {{MirrorCheckpointTask}} code, this prevention check is not enforced and it attempts to commit offsets for {{STABLE}} consumers. These calls can go through if consumers were momentarily disconnected moving the group state to {{{}EMPTY{}}}. This results in consumers' offsets getting reset to older values. If the offset is not available on the target broker (due to retention), the consumers can get reset to \"{{{}earliest\"{}}} or \"{{{}latest\"{}}}, thus reading duplicates or skipping messages. \r\nh2. *Bug location*\r\n\r\n1. In [MirrorCheckpointTask|#L305], we only update the latest target cluster offsets ({{{}idleConsumerGroupsOffset{}}})  if target consumer group state is {{{}EMPTY{}}}.\r\n\r\n2. When {{syncGroupOffset}} is called, we check if the target consumer group is present in  \r\n\r\n{{{}idleConsumerGroupsOffset{}}}. The consumer group won't be present as it's an active group. We assume that this is a new group and start syncing consumer group offsets to target. These calls fail with {_}{{{{{}Unable to sync offsets for consumer group XYZ. This is likely caused by consumers currently using this group in the target cluster. (org.apache.kafka.connect.mirror.MirrorCheckpointTask{}}}}}{_}. When consumers have failed over, the logs typically contain a lot of these messages. These calls can succeed if consumer is momentarily disconnected due to restarts. The code should not assume the lack of consumer group in {{idleConsumerGroupsOffset}} map as a new consumer group.\r\n\r\n3. These erroneous behavior", "output": "MirrorCheckpointTask causes consumers on target cluster to rewind offsets or skip messages due to erroneous offset commits"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add bswap support for RISC-V\nDescription: *Problem*\r\n\r\nWhile building Hadoop native code on the riscv64 architecture, the build fails due to missing support for bswap operations. The standard implementation relies on compiler intrinsics or platform-specific assembly which are not defined for RISC-V targets.\r\n\r\nThis results in compilation errors such as:\r\n\r\n* Error: unrecognized opcode `bswap a4'\r\n* Error: unrecognized opcode `bswap a3'\r\n\r\n*Resolution*\r\n\r\nAdd bswap support for RISC-V.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Spark Connect Python client does not throw a proper error when creating a dataframe from an empty pandas dataframe\nDescription: Spark Connect Python client does not throw a proper error when creating a dataframe from a pandas dataframe with a index and empty data.\r\n\r\nGenerally, spark connect client throws a client-side error `[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from an empty dataset`. when creating a dataframe without data, for example via\r\n{quote}spark.createDataFrame([]).show()\r\n{quote}\r\nor\r\n{quote}df = pd.DataFrame()\r\nspark.createDataFrame(df).show(){quote}\r\nor\r\n{quote}df = pd.DataFrame(\\{\"a\": []})\r\nspark.createDataFrame(df).show(){quote}\r\nThis does not happen when pandas dataframe has an index but no data, e.g.\r\n{quote}df = pd.DataFrame(index=range(5))\r\nspark.createDataFrame(df).show(){quote}\r\nWhat happens instead is that the dataframe is successfully converted to a LocalRelation on the client, is sent to the server, but the server then throws the following exception: `INTERNAL_ERROR: Input data for LocalRelation does not produce a schema. SQLSTATE: XX000`. XX000 is an internal error sql state \nQ: Hi, [~khakhlyuk]. \r\n\r\nApache Spark community has a policy which manages `Fix Version` and `Target Version` like the following. So, please don't set it when you file a JIRA issue.\r\nhttps://spark.apache.org/contributing.html\r\n\r\n{quote}\r\nDo not set the following fields:\r\n- Fix Version. This is assigned by committers only when resolved.\r\n- Target Version. This is assigned by committers to indicate a PR has been accepted for possible fix by the target version.\r\n{quote}", "output": "[~dongjoon] my bad, I'll leave them empty next time, thanks!"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add RPC-level integration tests for StreamsGroupDescribe\nDescription: Add integration tests similar to {{ShareGroupDescribeRequestTest}} and {{ConsumerGroupDescribeRequestTest for StreamsGroupDescribeRequest}}\nQ: https://github.com/apache/kafka/pull/20632", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [java.time] hadoop-common\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I'm planning to add Generate node resolution to the single-pass analyzer. Before that, I need to do minor refactoring to extract common logic from ResolveGenerate rule.", "output": "Extract common Generate resolution logic"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Avoid noisy NPE logs when closing consumer after constructor failures\nDescription: If there's a failure in the kafka consumer constructor, we attempt to close it https://github.com/lianetm/kafka/blob/2329def2ff9ca4f7b9426af159b6fa19a839dc4d/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L540\r\nIn that case, it could be the case that some components may have not been created, so we should consider some null checks to avoid noisy logs about NPE. \r\n\r\nThis noisy logs have been reported with the console share consumer in a similar scenario, so this task is to review and do a similar fix for the Async if needed.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Replace AssumptionViolatedException with TestAbortedException\nDescription: In JUnit 4, {{org.junit.internal.AssumptionViolatedException}} is used to indicate assumption failure and skip the test. However, {{AssumptionViolatedException}} is an implementation in JUnit 4, and in JUnit 5, we can use {{TestAbortedException}} to replace {{{}AssumptionViolatedException{}}}.\r\n\r\n{{TestAbortedException}} is used to indicate that a test has been aborted, and it can be used to replace {{{}AssumptionViolatedException{}}}. However, it is not directly related to assumption failure and is more commonly used in situations where the test needs to be aborted during execution.\nQ: hadoop-yetus commented on PR #7800:\nURL: https://github.com/apache/hadoop/pull/7800#issuecomment-3065373842\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 12 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 48s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m 10s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 28s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   6m 33s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   6m 15s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  11m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 23s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   4m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 24s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   6m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   6m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  12m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 53s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 104m  3s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 53s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m  7s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 45s |  |  hadoop-aliyun in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 45s |  |  hadoop-cos in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 46s |  |  hadoop-huaweicloud in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 10s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 419m  4s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7800/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7800 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ca7de1a90d3f 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 717ce8af91220c2dd9d28d3bdea477758df8ff5a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7800/1/testReport/ |\r\n   | Max. process+thread count | 1894 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-tools/hadoop-aliyun hadoop-cloud-storage-project/hadoop-cos hadoop-cloud-storage-project/hadoop-huaweicloud U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7800/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on PR #7800:\nURL: https://github.com/apache/hadoop/pull/7800#issuecomment-3066745277\n\n   @cnauroth Could you help review this PR? Thank you very much!"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Measures the frequency of calls to {{StateStore#commit}}", "output": "commit-rate metric"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: upgrade jsonschema2pojo due to CVE-2025-3588\nDescription: https://github.com/advisories/GHSA-66rc-vg9f-48m7\r\n\r\nNo fix release is available yet.\r\n\r\nIt looks like we would need a fix release for the older branch (1.0). jsonschema2pojo 1.1 and above needs a version of javax.validation that is incompatible with the version of javax.validation needed by Jersey 2.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-kafka.\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `JUnit` to 6.0.0\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update commons-lang3 to 3.17.0\nDescription: The commons-text version used by Hadoop is incompatible with commons-lang3 3.12.\r\n\r\nThis is actually with an older Hadoop, but with the same {_}commons-lang3{_}, _commons-configuration2_ and _commons-text_ versions as trunk:\r\n{noformat}\r\njava.lang.NoSuchMethodError: 'org.apache.commons.lang3.Range org.apache.commons.lang3.Range.of(java.lang.Comparable, java.lang.Comparable)'\r\n    at org.apache.commons.text.translate.NumericEntityEscaper.(NumericEntityEscaper.java:97)\r\n    at org.apache.commons.text.translate.NumericEntityEscaper.between(NumericEntityEscaper.java:59)\r\n    at org.apache.commons.text.StringEscapeUtils.(StringEscapeUtils.java:271)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.unescapePropertyName(PropertiesConfiguration.java:690)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$PropertiesReader.initPropertyName(PropertiesConfiguration.java:583)\r\n    at org.apache.commons.configuration2.PropertiesConfiguration$Properties\nQ: Interestingly, this doesn't happen with later Hadoop versions, probably the metrics code does not use the same code path and avoids this incompatibility.", "output": "stoty opened a new pull request, #7591:\nURL: https://github.com/apache/hadoop/pull/7591\n\n   ### Description of PR\r\n   \r\n   Update commons-lang3 to 3.17.0\r\n   The commons-text version used by Hadoop is incompatible with commons-langs3 3.12.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   CI\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Protobuf 3.25.5 for docker images\nDescription: HADOOP-19289 upgraded protobuf-java 3.25.5, we should use same version for protobuf installed in docker images.\nQ: hadoop-yetus commented on PR #7780:\nURL: https://github.com/apache/hadoop/pull/7780#issuecomment-3248461577\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/3/console in case of problems.", "output": "hadoop-yetus commented on PR #7780:\nURL: https://github.com/apache/hadoop/pull/7780#issuecomment-3248556711\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7780/4/console in case of problems."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864\nDescription: *CVE-2025-53864:*\r\n\r\nConnect2id Nimbus JOSE + JWT before 10.0.2 allows a remote attacker to cause a denial of service via a deeply nested JSON object supplied in a JWT claim set, because of uncontrolled recursion. NOTE: this is independent of the Gson 2.11.0 issue because the Connect2id product could have checked the JSON object nesting depth, regardless of what limits (if any) were imposed by Gson.\r\n\r\nSeverity: 6.9 (medium)\nQ: rohit-kb opened a new pull request, #7965:\nURL: https://github.com/apache/hadoop/pull/7965\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   1. Bumping nimbus-jose-jwt to 10.4 due to CVEs\r\n   2. com.github.stephenc.jcip:jcip-annotations is being shaded and is no more a transitive dependency from nimbus starting from versions 9.38, so we can add it as an explicit dependency.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "duplicate of HADOOP-19632 for which there are already PRs"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have identified a significant performance regression in Apache Spark's streaming recentProgress method in python notebook starting from version 4.0.0. The time required to fetch recentProgress increases substantially as the number of progress records grows, creating a linear or worse scaling issue. We only observe this behavior in classic pyspark.\r\n\r\nWith the following code, it output charts for time it takes to get recentProgress before and after changes in [this commit|https://github.com/apache/spark/commit/22eb6c4b0a82b9fcf84fc9952b1f6c41dde9bd8d#diff-4d4ed29d139877b160de444add7ee63cfa7a7577d849ab2686f1aa2d5b4aae64]\r\n\r\n```\r\n%python\r\nfrom datetime import datetime\r\nimport time\r\n\r\ndf = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\r\nq = df.writeStream.format(\"noop\").start()\r\nprint(\"begin waiting for progress\")\r\n\r\nprogress_list = []\r\ntime_diff_list = []\r\n\r\nnumProgress = len(q.recentProgress)\r\nwhile numProgress < 70 and q.exception() is None:\r\ntime.sleep(1)\r\nbeforeTime = datetime.now()\r\nprint(beforeTime.strftime(\"%Y-%m-%d %H:%M:%S\") +\": before we got those progress: \"+str(numProgress))\r\nrep = q.recentProgress\r\nnumProgress = len(rep)\r\nafterTime = datetime.now()\r\nprint(afterTime.strftime(\"%Y-%m-%d %H:%M:%S\") +\": after we got those progress: \"+str(numProgress))\r\ntime_diff = (afterTime - beforeTime).total_seconds()\r\nprint(\"Total Time: \"+str(time_diff) +\" seconds\")\r\nprogress_list.append(numProgress)\r\ntime_diff_list.append(time_diff)\r\n\r\nq.stop()\r\nq.awaitTermination()\r\nassert(q.exception() is None)\r\n\r\nimport pandas as pd\r\n\r\nplot_df = pd.DataFrame(\\{'numProgress': progress_list, 'time_diff': time_diff_list})\r\ndisplay(spark.createDataFrame(plot_df).orderBy(\"numProgress\").toPandas().plot.line(x=\"numProgress\", y=\"time_diff\"))\r\n```\r\nSee attachment for the generated graph. Attachment 1 is regression shown in current version. Attachment 2 is regression shown in previous version", "output": "Streaming Query RecentProgress performance regression in Classic Pyspark"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: sendDeferedResponse should also log exception info.\nDescription: sendDeferedResponse should also log exception info.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Following KIP-1100, the metrics with incorrect component names were deprecated. These deprecated metrics should be removed in Kafka 5.0.\r\n\r\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-1100%3A+Rename+org.apache.kafka.server%3Atype%3DAssignmentsManager+and+org.apache.kafka.storage.internals.log.RemoteStorageThreadPool+metrics", "output": "Remove deprecate metrics in AssignmentsManager and RemoteStorageThreadPool"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump Jackson 2.20.0\nDescription: \nQ: Issue resolved in https://github.com/apache/spark/pull/52687", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In structured streaming job, we use transformWithStateInPandas stateful processing and we have com.databricks.sql.streaming.state.RocksDBStateStoreProvider enabled. Everything works as expected.\r\n\r\nBut when we enable:\r\nspark.conf.set(\"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\", \"true\")\r\n\r\nWe get an error: org.apache.spark.SparkUnsupportedOperationException: Synchronous commit is not supported. Use asynchronous commit.\r\n \r\nWe have successfully replicated that in notebook, and also tried the same thing with applyInPandasWithState.\r\n * transformWithStateInPandas - throws Synchronous commit error\r\n * applyInPandasWithState - works as expected\r\n\r\nWe followed this guide: [https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/async-checkpointing] and also used chatGPT for references.\r\n\r\nLooking through the stacktrace, It seems like the issue is where {{TransformWithStateInPandasExec}} uses the sync {{commit()}} instead of the async {{{}commitAsync(){}}}.\r\n\r\n \r\n\r\nPlease check the attached notebook below", "output": "asyncCheckpoint.enabled incompatible with transformWithStateInPandas (StatefulProcessor)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Native KQueue Transport support on BSD/MacOS\nDescription: \nQ: Issue resolved by pull request 52703\n[https://github.com/apache/spark/pull/52703]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Custom Gradle tasks (`unitTest` and `integrationTest`) are not executing tests in Gradle 9\nDescription: Prologue: KAFKA-19174\r\n\r\n\r\n\r\n\r\nSee here for description: https://github.com/apache/kafka/pull/19513#issuecomment-3388760324\nQ: Hi [~dejan2609], could you take a look at [this|https://github.com/apache/kafka/pull/20684]? It might have already resolved this issue.", "output": "Hi [~isding_l] ! Yes, it seems that your PR resolves this issue. \r\n\r\nI will leave a review on your PR (maybe I can save you few lines)."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix compilation error of native libraries on newer GCC\nDescription: Building with {{-Pnative}} by using GCC 14 on Fedora 40 failed due to incompatible changes of GCC.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support logging in UDTFs\nDescription: \nQ: Issue resolved by pull request 52798\n[https://github.com/apache/spark/pull/52798]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: NoSuchElementException (again) in Kafka Streams iterator metrics\nDescription: Since upgrading to Kafka 4.1.0, we again see in our metrics collection:\r\n{code:java}\r\nException thrown from GraphiteReporter#report. Exception was suppressed.\r\njava.util.NoSuchElementException: null\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:1863)\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:393)\r\n\tat org.apache.kafka.streams.internals.metrics.OpenIterators.lambda$add$0(OpenIterators.java:57)\r\n\tat org.apache.kafka.common.metrics.KafkaMetric.metricValue(KafkaMetric.java:81) {code}\r\nI think unfortunately the dynamic registration introduced in KAFKA-19398 ([https://github.com/apache/kafka/pull/20022]) might have reverted the fix from [https://github.com/apache/kafka/pull/18771]\r\n\r\nThere is still a race between removing the metric gauge, and the last iterator being removed from the tracking set.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We are seeing cases where a Kafka Streams (KS) thread stalls for ~20 seconds, resulting in a visible gap in the logs. During this stall, the broker correctly aborts the open transaction (triggered by the 10-second transaction timeout). So far, this is expected behavior. However, when the KS thread resumes, instead of receiving the expected InvalidProducerEpochException (which we already handle gracefully as part of transaction abort), the client is instead hit with an InvalidTxnStateException. KS currently treats this as a fatal error, causing the application to fail.", "output": "Unexpected Fatal error InvalidTxnStateException on Kafka Streams (KIP-890)"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Stop the RSM after closing the remote-log reader threads to handle requests gracefully\nDescription: During shutdown, when the RemoteStorageManager closes first, then the ongoing requests are thrown with error. To handle the ongoing requests gracefully, closing the RSM after closing the remote-log reader thread pools. \r\n\r\n \r\n\r\nhttps://sourcegraph.com/github.com/apache/kafka/-/blob/storage/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogManager.java?L2035", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)\nDescription: In core-default.xml, the property hadoop.security.crypto.buffer.size is currently documented as “The buffer size used by CryptoInputStream and CryptoOutputStream.” It does not specify the legal value constraints.\r\n{code:java}\r\n  \r\nhadoop.security.crypto.buffer.size  \r\n8192  \r\nThe buffer size used by CryptoInputStream and CryptoOutputStream.  \r\n {code}\r\nThe runtime enforces two hidden constraints that are not documented:\r\n1. Minimum value is 512 bytes. Values below 512 cause IllegalArgumentException at stream construction time.\r\n2. Block-size flooring: The effective buffer size is floored to a multiple of the cipher algorithm’s block size (e.g., 16 bytes for AES/CTR/NoPadding).\r\n\r\nAs a result, users may be surprised that:\r\n1. Setting a value like 4100 results in an actual capacity of 4096.\r\n2. Setting values <512 fails fast with IllegalArgumentException.\r\n\r\n*Expected*\r\ncore-default.xml (and user-facing docs) should explicitly document:\r\n1. Minimum legal value: 512 bytes.\r\n\r\n2. The effec\nQ: zzz0706 opened a new pull request, #7907:\nURL: https://github.com/apache/hadoop/pull/7907\n\n   ### Description of PR\r\n   \r\n   Docs-only change to **TransparentEncryption.md** clarifying the hidden constraints for\r\n   `hadoop.security.crypto.buffer.size`:\r\n   \r\n   - **Minimum value is 512 bytes** (values  4096`, `8195 -> 8192`.\r\n   \r\n   This is a minimal wording update (single-sentence addition) to avoid operator surprise, with no behavior change.\r\n   \r\n   **Target branch:** `trunk` (per contributor guide)\r\n   \r\n   **JIRA:** HADOOP-19667\r\n   \r\n   **Files changed:**\r\n   - `hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/TransparentEncryption.md`\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - Docs-only; built site/module locally to validate formatting:", "output": "Thanks for the guidance. I’ve opened a PR against trunk updating the requested file."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In SS TWS, when we do the {{put(array)}} operation in liststate, we put the first element and then merge the remaining elements one by one. so if we want to put an array with 100 elements, it means we need do 1 put + 99 merges. This can result in worse performance than a single put operation for the entire array.\r\n\r\n \r\n\r\nSimilar, we have the same issue in {{merge(array)}}", "output": "Improve the put/merge operation in ListState when t here are multiple values"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Reorganize Python streaming TWS test\nDescription: Reorganize the Python TWS tests:\r\n* moving tws related tests to a new `/streaming` directory\r\n\r\n* further split the Python TWS tests to smaller ones to speed up the CI", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix preservedEntries count in CopyCommitter when preserving directory attributes\nDescription: The preservedEntries count always be 0", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip `test_profile_pandas_*` tests if pandas or pyarrow are unavailable\nDescription: \nQ: Issue resolved by pull request 52549\n[https://github.com/apache/spark/pull/52549]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I found that the remoteFetch thread is holding the file, preventing the disk space from being freed.\r\n\r\n!image-2025-09-24-10-38-25-161.png!  \r\n\r\nHowever, I didn't encounter this problem with the earlier 0.0.1-SNAPSHOT build, which used Kafka 3.8.1. The current issue appears in version 1.0.0 together with Kafka 3.9.1.\r\n\r\n!image-2025-09-24-10-38-36-572.png!\r\n\r\nI'm not sure whether this issue is caused by the tiered storage plugin or by Kafka itself. Could someone help take a look?", "output": "Disk space is not being freed"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Bump Jackson 2.20.0\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Race condition between log segment flush and file deletion causing log dir to go offline\nDescription: h1. Context\r\n\r\nWe are using Kafka v3.7.1 with Zookeeper, our brokers are configured with multiple disks in a JBOD setup, routine intra-broker data rebalancing is performed using Cruise Control to manage disk utilization. During these rebalance operations, a race condition between a log segment flush operation and the file deletion that is part of the replica's directory move. This race leads to a `NoSuchFileException` when the flush operation targets a file path that has just been deleted by the rebalance process. This exception incorrectly forces the broker to take the entire log directory offline.\r\nh1. Logs / Stack trace\r\n{code:java}\r\n2025-07-23 19:03:30,114 WARN Failed to flush file /var/lib/kafka-08/topic_01-12/00000000024420850595.snapshot (org.apache.kafka.\r\ncommon.utils.Utils)\r\njava.nio.file.NoSuchFileException: /var/lib/kafka-08/topic_01-12/00000000024420850595.snapshot\r\n        at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\r\n        at java\nQ: (https://issues.apache.org/jira/browse/KAFKA-13403) was fixed to swallow the first ``NoSuchFileException`` WARN in the above stacktrace, but not the underlying exception.", "output": "(https://issues.apache.org/jira/browse/KAFKA-15391) is similar but different, it swallows `NoSuchFileException` for race condition on log directory move/delete, but not on the segment file level."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade SpotBugs Version to Support JDK 17 Compilation\nDescription: The current SpotBugs version used in the project is 4.2.2 (SpotBugs) and 4.2.0 (SpotBugs Maven Plugin), which is not fully compatible with JDK 17 compilation. To ensure proper functionality of the code quality check tool in a JDK 17 environment, SpotBugs needs to be upgraded to the latest version that supports JDK 17.\nQ: slfan1989 opened a new pull request, #8028:\nURL: https://github.com/apache/hadoop/pull/8028\n\n   ### Description of PR\r\n   \r\n   JIRA: [JDK17] Upgrade SpotBugs Version to Support JDK 17 Compilation.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "slfan1989 commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3397979702\n\n   @cnauroth @szetszwo After upgrading to JDK 17, I found that spotbug could not run properly because the current version does not support JDK 17. To resolve this issue, I upgraded the versions of the two related plugins. The changes have been tested locally, and the results are as expected."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Remove powermock dependency\nDescription: The powermock dependency is specified in\r\n- hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml\r\n- hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml\r\n\r\nbut not used anywhere.  We should remove it.\nQ: Will include this in HADOOP-19677.", "output": "Reopen for removing unused powermock dependencies."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [ABFS] Rename/Create path idempotency client-level resolution\nDescription: CreatePath and RenamePath APIs are idempotent as subsequent retries on same resource don’t change the server state. However, when client experiences connection break on the CreatePath and the RenamePath APIs, client cannot make sense if the request is accepted by the server or not. \r\n\r\nOn connection failure, the client retries the request. The server might return 404 (sourceNotFound) in case of RenamePath API and 409 (pathAlreadyExists) in case of CreatePath (overwrite=false) API. Now the client doesn’t have a path forward. Reason being, in case of CreatePath, client doesn’t know if the path was created on the original request or the path was already there for some other request, in case of RenamePath, client doesn’t know if the source was removed because of the original-try or it was not there on the first place.\nQ: hadoop-yetus commented on PR #7364:\nURL: https://github.com/apache/hadoop/pull/7364#issuecomment-2640770705\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 42s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 131m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7364 |\r\n   | JIRA Issue | HADOOP-19450 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7d4c7cf69c8d 5.15.0-125-generic #135-Ubuntu SMP Fri Sep 27 13:53:58 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e2eba214503db812a4f2f7e19f6e1417ba227929 |\r\n   | Default Java | Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/1/testReport/ |\r\n   | Max. process+thread count | 541 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7364:\nURL: https://github.com/apache/hadoop/pull/7364#issuecomment-2640874156\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 49s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 38s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 58s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 42s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 131m 55s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7364 |\r\n   | JIRA Issue | HADOOP-19450 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 272fc60f98e1 5.15.0-125-generic #135-Ubuntu SMP Fri Sep 27 13:53:58 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 086f2440907ac4afdac54a324d89f5448d48e5d7 |\r\n   | Default Java | Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/2/testReport/ |\r\n   | Max. process+thread count | 635 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: `SparkAppDriverConf` should respect `sparkVersion` of `SparkApplication` CRD\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When the Factory service is stopped, we're currently missing the code to close the factory. My miss, this got lost in the move to the new factory code.", "output": "S3A Analytics-Accelerator: AAL stream factory not being closed"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve logging about task readiness\nDescription: Kafka Streams processes its assigned tasks in a round robin fashion, however, if a task is considered not ready for processing, it might get skipped. We have observed cases, in which Kafka Streams seems to get stuck on a partition, and restarting the application instance resolves this issue. We suspect, it's related to considering the task for the stuck partition as not ready for processing. (As the ready/not-ready decision is based on in-memory state of KS runtime, bouncing the instance would reset KS runtime into a clean state, unblocking the stuck task.)\r\n\r\nHowever, we currently don't have sufficient logging to reason about this case, and to understand if and why a task might be skipped. We should add more log statement (DEBUG and/or TRACE) to get better visibility. There might be a lurking bug in this logic that we cannot narrow down w/o the corresponding information logging could provide.", "output": "Patch Available"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Refactor KafkaRaftClient to use event scheduler framework\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix flaky DescribeStreamsGroupTest testDescribeStreamsGroupWithShortTimeout()\nDescription: According to [Develocity|https://develocity.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&tests.container=org.apache.kafka.tools.streams.DescribeStreamsGroupTest&tests.sortField=FLAKY], {{testDescribeStreamsGroupWithShortTimeout}} is very flaky.\nQ: Hi [~kirktrue] , maybe I could help with this issue if you haven't started to handle this.", "output": "There is already a PR this this: [https://github.com/apache/kafka/pull/20320] – we did not have a ticket, so it was filed a MINOR... \\cc [~alisa23]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix `gradlew` content: add missing parts (old Groovy template version is being referenced)\nDescription: {panel:bgColor=#ffffce}\r\n*_Prologue: Kafka developers can't commit jar's into Gir repo (that includes Gradle wrapper jar_*\r\nRelated links:\r\n * https://issues.apache.org/jira/browse/KAFKA-2098?focusedCommentId=14481979&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-14481979\r\n * https://issues.apache.org/jira/browse/KAFKA-1490\r\n * [https://stackoverflow.com/questions/39856714/gradle-wrapper-without-the-jar]\r\n * [https://discuss.gradle.org/t/how-hand-compile-gradle-wrapper-jar-when-gradle-cannot-be-installed-and-cannot-check-jar-files-into-git/4813]{panel}\r\n \r\n\r\n*(i) Intro:* Groovy file _*unixStartScript.txt*_ (that servers as a template for a {_}*gradlew*{_}) path/module was changed in Gradle version 8.8.0\r\n\r\n _*(x) Problem description:*_\r\n # at the moment Kafka trunk branch is using Gradle version [8.14.1|https://github.com/apache/kafka/blob/8deb6c6911616f887ebb2678f3f12ee1da09a618/gradle/wrapper/gradle-wrapper.properties] but thing is that _*gradlew*_ is referencing Gradle 8.7.0 template file _*unixStartScript.txt*_\r\n # it means that _*gradlew*_ is missing all recent changes for a template file _*unixStartScript.txt*_\r\n\r\n*_Related GitHub Gradle links for unixStartScript.txt:_*\r\n * Gradle = 8.8.0 version:\r\n ** file path: [https://github.com/gradle/gradle/blob/v8.8.0/platforms/jvm/plugins-application/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt]\r\n ** Git history for versions [8.8.0, 8.14.1]: [https://github.com/gradle/g", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: `ci-complete` needs to work with active branches after the JDK is updated\nDescription: The JDK version used by `ci-complete` is inconsistent with other active branches, which is causing the build report task to fail\nQ: Perhasp we could use multiple branches to handle this case.\r\n\r\n \r\n{code:java}\r\njobs:\r\n  upload-build-scan-main:\r\n    if: github.event.workflow_run.head_branch == 'trunk'\r\n    runs-on: ubuntu-latest\r\n    strategy:\r\n      matrix:\r\n    # ... rest of the steps\r\n\r\n  upload-build-scan-other:\r\n    if: github.event.workflow_run.head_branch != 'trunk'\r\n    runs-on: ubuntu-latest\r\n    strategy:\r\n      matrix:  \r\n   # ... rest of the steps{code}", "output": "Another idea to be considered (on): https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/github-code-transformation-workflow-advanced.html\r\nFYI [~mingyen066]"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "CreatePath and RenamePath APIs are idempotent as subsequent retries on same resource don’t change the server state. However, when client experiences connection break on the CreatePath and the RenamePath APIs, client cannot make sense if the request is accepted by the server or not. \r\n\r\nOn connection failure, the client retries the request. The server might return 404 (sourceNotFound) in case of RenamePath API and 409 (pathAlreadyExists) in case of CreatePath (overwrite=false) API. Now the client doesn’t have a path forward. Reason being, in case of CreatePath, client doesn’t know if the path was created on the original request or the path was already there for some other request, in case of RenamePath, client doesn’t know if the source was removed because of the original-try or it was not there on the first place.", "output": "[ABFS] Rename/Create path idempotency client-level resolution"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "org.opentest4j.AssertionFailedError: Condition not met within timeout 60000. Timed out waiting for active restoring task ==> expected:  but was: \r\n\tat app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n\tat app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n\tat app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)\r\n\tat app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)\r\n\tat app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214)\r\n\tat app//org.apache.kafka.test.TestUtils.lambda$waitForCondition$4(TestUtils.java:447)\r\n\tat app//org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:495)\r\n\tat app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:444)\r\n\tat app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:428)\r\n\tat app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:418)\r\n\tat app//org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitForActiveRestoringTask(IntegrationTestUtils.java:611)\r\n\tat app//org.apache.kafka.streams.integration.RestoreIntegrationTest.shouldInvokeUserDefinedGlobalStateRestoreListener(RestoreIntegrationTest.java:689)\r\n\tat java.base@17.0.16/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat java.base@17.0.16/java.util.Optional.ifPresent(Optional.java:178)\r\n\tat java.base@17.0.16/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)\r\n\tat java.base@17.0.16/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)\r\n\tat java.base@17.0.16/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)\r\n\tat java.base@17.0.16/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)\r\n\tat java.base@17.0.16/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)\r\n\tat java.base@17.0.16/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:992)\r\n\tat ", "output": "Fix flaky RestoreIntegrationTest#shouldInvokeUserDefinedGlobalStateRestoreListener"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Regenerate benchmark results after upgrading to Scala 2.13.17\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix the negative remote bytesLag and segmentsLag metric\nDescription: * Each broker rotates the segment independently.\r\n * When the leadership changes to the next replica, then the highest offset uploaded to the remote storage might already be higher than the baseOffset of the current leader active segment.\r\n * During this case, the {{onlyLocalLogSegmentsSize}} and {{onlyLocalLogSegmentsCount}} returns 0, so the metrics shows negative value (0 - activeSegment.size).", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Producer performance test failing in parsing aggregate statistics\nDescription: When running benchmark test, some of the producer perf tests are failing in statistics parsing. Logs attached\r\n\r\ncmd: `TC_PATHS=\"tests/kafkatest/benchmarks/core/benchmark_test.py\" bash tests/docker/run_tests.sh`\r\n\r\nJDK: `azul/zulu-openjdk:17`(used azul's jdk because with openjdk tests were failing to run)", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Document skip.platformToolsetDetection option in BUILDING.txt\nDescription: Readme.md has a complete command line example for building Hadoop on Windows.\r\nHowever, that one doesn't work in the docker image, because that doesn't have full Visual Studio install, and misses the expected programs.\r\n\r\nAdd to README.md that the *-Dskip.platformToolsetDetection* maven option is needed when building from the docker image.\nQ: stoty opened a new pull request, #7600:\nURL: https://github.com/apache/hadoop/pull/7600\n\n   ### Description of PR\r\n   \r\n   The command lines for Windows in BULIDING.txt don't work.\r\n   This fixes them so that they work out of the box in the Windows build Docker  image.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran the example command in the Windows build docker image\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "stoty commented on PR #7600:\nURL: https://github.com/apache/hadoop/pull/7600#issuecomment-2796073693\n\n   Found this while testing the Windows image @GauthamBanasandra ."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Share consumer changes to support renew ack.\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I think this is new to the V2 SDK, or at least our region logic there.\r\nWhen you try to connect to a bucket without specifying the region, then even if you're running in the same region of the store a HEAD request is still made to US Central. This adds latency, makes us-central a SPoF and if the VM/container has network rules which blocks such access, we actually timeout and eventually fail.\r\n\r\nWhile Machine configurations should ideally have the fs.s3a.endpoint.region setting configured, that information is actually provided as IAM metadata. Therefore it would be possible \r\n\r\nThis is actually included in the default region chain according to the SDK docs \"If running in EC2, check the EC2 metadata service for the region\", so maybe this isn't being picked up because\r\n\r\n# cross region access is being checked for first.\r\n# the region chain we are setting off doesn't check the EC2 metadata service.\r\n\r\nThe SDK region chain does do the right thing within AWS infra. How do we restore that while still supporting remote deployments?", "output": "S3A: region resolution within AWS infra always goes to us-east-2"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Throw better error to indicate that TWS is only supported with RocksDB state store provider\nDescription: When user tries to use TWS with HDFS as state store, they should get an error explicitly requiring to use RocksDB state provider.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Upgrade com.google.guava library to 32.1.3-jre to fix:\r\n * [CVE-2023-2976|https://github.com/advisories/GHSA-7g45-4rm6-3mm3]\r\n * [CVE-2020-8908|https://github.com/advisories/GHSA-5mg8-w23w-74h3]\r\n\r\nLink to PR:\r\n[https://github.com/apache/hadoop/pull/7473]", "output": "Upgrade to com.google.guava 32.1.3-jre to fix CVE-2023-2976 and CVE-2020-8908"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Deadlock in multipart upload\nDescription: Reproduced while testing system resilience and turning S3 network off (introduced a network partition to the list of IP addresses S3 uses) - but given it's seemingly timers related stack traces, I'd guess it could happen any time?\r\n\r\n\r\n{code:java}\r\nFound one Java-level deadlock:\r\n=============================\r\n\"sdk-ScheduledExecutor-2-3\":\r\n  waiting to lock monitor 0x00007f5c880a8630 (object 0x0000000315523c78, a java.lang.Object),\r\n  which is held by \"sdk-ScheduledExecutor-2-4\"\r\n\"sdk-ScheduledExecutor-2-4\":\r\n  waiting to lock monitor 0x00007f5c7c016700 (object 0x0000000327800000, a org.apache.hadoop.fs.s3a.S3ABlockOutputStream),\r\n  which is held by \"io-compute-blocker-15\"\r\n\"io-compute-blocker-15\":\r\n  waiting to lock monitor 0x00007f5c642ae900 (object 0x00000003af0001d8, a java.lang.Object),\r\n  which is held by \"sdk-ScheduledExecutor-2-3\"\r\nJava stack information for the threads listed above:\r\n===================================================\r\n\"sdk-ScheduledExecutor-2-3\":\r\n        at \nQ: Good Find!\r\n\r\nlooks like one thread has blocked in the initMultipartUpload request due to the far end not responding, and the timeout thread is trying to close() the stream in a different thread. and that is causing the deadlock.\r\n\r\nmaybe we could pull the initMultipartUpload() call out of the sync block, so the network IO isn't being done there. Tricky though...I wouldn't want to make this a last minute change for 3.4.2. That output stream is complicated beast.", "output": "maybe move off simple sync to try to acquire a lock with timeouts? it'd make close() more robust, even if the action on timeout is failure. which, given there are clearly network problems, is the right thing to do."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "hadoop-aws tests which need to be parameterized on a class level\r\nare configured to do so through the @ParameterizedClass tag.\r\nFilesystem contract test suites in hadoop-common have\r\nalso been parameterized as appropriate.\r\n\r\nThere are custom JUnit tags declared in org.apache.hadoop.test.tags,\r\nwhich add tag strings to test suites/cases declaring them.\r\nThey can be used on the command line and in IDEs to control\r\nwhich tests are/are not executed.\r\n\r\n@FlakyTest \"flaky\"\r\n@LoadTest \"load\"\r\n@RootFilesystemTest \"rootfilesystem\"\r\n@ScaleTest \"scale\"\r\n\r\nFor anyone migrating tests to JUnit 5\r\n* Methods which subclass an existing test case MUST declare the @Test\r\n  tag again -it is no longer inherited.\r\n* All overridden setup/teardown methods MUST be located and\r\n  @BeforeEach/@AfterEach attribute added respectively\r\n* Subclasses of a parameterized test suite MUST redeclare themselves\r\n  as a @ParameterizedClass, and the binding mechanism again.\r\n* Parameterized test suites SHOULD declare a pattern to generate an\r\n  informative parameter value string for logs, IDEs and stack traces, e.g.\r\n  @ParameterizedClass(name=\"performance-{0}\")\r\n* Test suites SHOULD add a org.apache.hadoop.test.tags tag to\r\n  declare what kind of test it is. These tags are inherited, so it\r\n  may be that only shared superclasses of test suites need to be tagged.\r\n  The abstract filesystem contract tests are NOT declared as integration\r\n  tests -implementations MUST do so if they are integration tests.", "output": "S3A: ITests to run under JUnit5"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Implementing Create and Mkdir file system APIs for FNS(HNS Disabled) accounts on Blob Endpoint involves a lot of checks and marker file creations to handle implicit explicit cases of paths involved in these APIs.\r\n\r\nThis Jira proposes a few optimizations to reduce the network calls wherever possible and in case where create/mkdir is bound to fail, it should fail faster before doing any post checks,", "output": "ABFS: [FnsOverBlob][Optimizations] Reduce Network Calls In Create and Mkdir Flow"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move ClientQuotaMetadataManager to metadata module\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: JoinWithIncompleteMetadataIntegrationTest fails in isolated run of one parameter\nDescription: When I run JoinWithIncompleteMetadataIntegrationTest, with only the new protocol enabled, it fails (since it does not run long enough for the exception to be triggered by the timer).\r\n\r\n \r\n\r\nWhen the test is run for both protocols, the second run passes because it uses an existing group ID and that is why it why streams is shutting down.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Expose consumer CorruptRecordException\nDescription: As part of our analysis around KAFKA-19430, we realized it's not possible to handle a {{CorruptRecordException}} in Streams because it's not exposed to the client; instead, a generic {{KafkaException}} ** is thrown\r\n\r\nThe proposed solution is to expose {{CorruptRecordException}} with information about affected TopicPartition and offset we're trying read from\r\n\r\nKIP: https://cwiki.apache.org/confluence/x/NQrxFg\nQ: Since the clients are supposed to skip a corrupted message, it would also be beneficial if the partition and maybe the offset is added to the `CorruptRecordException`. Parsing the exception message is uncool and currently I don't see another way to get the partition?", "output": "Definitely parsing of String message is not a solution. The idea here was just to expose exception, but let me check if more changes for consumer are required"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "AWS SDK 2.30.0+ added checksum headers to all requests\r\nhttps://github.com/aws/aws-sdk-java-v2/discussions/5802\r\n\r\nThis apparently causes a lot of regressions, especially with third party stores, as reported in icebeg\r\n\r\nhttps://github.com/apache/iceberg/pull/12264\r\n\r\nnote the disclaimer that the SDK is for AWS S3 storage only\r\n\r\nbq. the AWS SDKs and CLI are designed for usage with official AWS services. We may introduce and enable new features by default, such as these new default integrity protections, prior to them being supported or otherwise handled by third-party service implementations.\r\n\r\nI understand why the team doesn't bother testing with other people's stores, but we have to support them and need to consider all new SDK features are potentially trouble, so disable them by default.", "output": "S3A: disable strong integrity checksum feature from recent AWS SDKs"}
{"instruction": "Answer the question based on the bug.", "input": "Title: LdapAuthenticationHandler supports configuring multiple ldapUrls.\nDescription: Currently, LdapAuthenticationHandler supports only a single ldap server url, this can obviously cause issues if the ldap instance goes down. This JIRA attempts to improve this by allowing users to list multiple ldap server urls, and performing a failover if we detect any issues.\nQ: github-actions[bot] closed pull request #7772: HADOOP-19598. LdapAuthenticationHandler supports configuring multiple ldapUrls.\nURL: https://github.com/apache/hadoop/pull/7772", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add application-id as a tag to the ClientState JMX metric\nDescription: As a follow-on to the improvements introduced in [KIP-1091|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1091%3A+Improved+Kafka+Streams+operator+metrics] it would be useful to add the application-id as a tag to the `client-state` metric. This allows Kafka Streams developers and operators to connect metrics containing a `thread-id` (which embeds the `application-id`) across separate deployments of Kafka Streams instances, which are members of the same logical application.\r\n\r\nKIP-1221 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1221%3A+Add+application-id+tag+to+Kafka+Streams+state+metric]", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: MapOutputTrackerMaster.shufflestatuses is mistakenly cleaned by Shuffle cleanup\nDescription: MapOutputTrackerMaster.shufflestatuses can be mistakenly cleaned by Shuffle Cleanup feature, leading to SparkException (crashing the SparkContext) by the subsequent access to that already removed shuffle metadata. A real case (limited to local cluster currently) is the ongoing subquery could access the shuffle metadata which has been already cleanedup after the main query completes. See the detailed discussion at: [https://github.com/apache/spark/pull/52213#discussion_r2415632474].", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Potential Long.MAX_VALUE overflow in sessionExpirationTimeNanos calculation in SaslServerAuthenticator \nDescription: There is a potential risk of Long.MAX_VALUE overflow in the sessionExpirationTimeNanos calculation within the SaslServerAuthenticator class.\r\nLocation:\r\n !image-2025-08-01-10-12-04-784.png! \r\nThe calculation sessionExpirationTimeNanos = authenticationEndNanos + 1000 * 1000 * retvalSessionLifetimeMs can potentially overflow when:\r\nretvalSessionLifetimeMs is very large \r\nauthenticationEndNanos is already a large value\r\nThe multiplication 1000 * 1000 * retvalSessionLifetimeMs exceeds Long.MAX_VALUE - authenticationEndNanos", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve Netty usage patterns\nDescription: \nQ: Thank you for making this umbrella, [~dongjoon]. Making it resolved sounds good to me", "output": "Thank you, [~yao]."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce APPROX_PERCENTILE_COMBINE\nDescription: *APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Close stale PRs updated over 100 days ago.\nDescription: Close stale PRs on GitHub which updated over 100 days ago, base on the voting thread: https://lists.apache.org/thread/7x6c029fp9k62bxt2995dz6q6j6sg0bh\nQ: hadoop-yetus commented on PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#issuecomment-3252923998\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  yamllint  |   0m  0s |  |  yamllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  44m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/artifact/out/blanks-eol.txt) |  The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  88m 49s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7930 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets yamllint |\r\n   | uname | Linux 8be7cf7a0c05 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5e03454c04c43efa24691ef3d9245d9333a46e70 |\r\n   | Max. process+thread count | 536 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#issuecomment-3253473455\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  yamllint  |   0m  0s |  |  yamllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  44m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  87m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7930 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets yamllint |\r\n   | uname | Linux 013fcfcc6e1b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bae392d8dfb7f2edb9de956122a07626d8802b20 |\r\n   | Max. process+thread count | 531 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use CRD `v1` instead of `v1beta1` in `benchmark` script\nDescription: \nQ: Issue resolved by pull request 396\n[https://github.com/apache/spark-kubernetes-operator/pull/396]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade AWS SDK to 2.35.4\nDescription: Upgrade to a recent version of 2.33.x or later while off the critical path of things.\r\n\r\n\r\nHADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.", "output": "In Progress"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{{BloomMapFile.Writer#initBloomFilter(Configuration)}} computes the Bloom filter vector size as:\r\n{code:java}\r\nint numKeys = conf.getInt(IO_MAPFILE_BLOOM_SIZE_KEY, IO_MAPFILE_BLOOM_SIZE_DEFAULT);\r\nfloat errorRate = conf.getFloat(IO_MAPFILE_BLOOM_ERROR_RATE_KEY, IO_MAPFILE_BLOOM_ERROR_RATE_DEFAULT);\r\nint vectorSize = (int) Math.ceil(\r\n  (double)(-HASH_COUNT * numKeys) /\r\n  Math.log(1.0 - Math.pow(errorRate, 1.0 / HASH_COUNT))\r\n); {code}\r\nWhen {{io.mapfile.bloom.error.rate}} is *≤ 0* or {*}≥ 1{*}:\r\n * {{Math.pow(errorRate, 1/k)}} produces *NaN* (negative base with non-integer exponent) or an invalid value;\r\n * {{Math.log(1 - NaN)}} becomes {*}NaN{*};\r\n * {{Math.ceil(NaN)}} cast to {{int}} yields {*}0{*}, so {{{}vectorSize == 0{}}};\r\n * constructing {{DynamicBloomFilter}} subsequently fails, and {{BloomMapFile.Writer}} construction fails (observed as assertion failure in tests).\r\n\r\nThe code misses input validation for {{io.mapfile.bloom.error.rate}} which should be strictly within {{{}(0, 1){}}}. With invalid values, the math silently degrades to NaN/0 and fails at runtime.\r\n\r\n*Reproduction*\r\n\r\nInjected values: {{io.mapfile.bloom.error.rate = 0,-1}}\r\n\r\nTest: {{org.apache.hadoop.io.TestBloomMapFile#testBloomMapFileConstructors}}\r\n{code:java}\r\n[INFO] Running org.apache.hadoop.io.TestBloomMapFile\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.358 s <<< FAILURE! - in org.apache.hadoop.io.TestBloomMapFile\r\n[ERROR] org.apache.hadoop.io.TestBloomMapFile.testBloomMapFileConstructors  Time elapsed: 0.272 s  <<< FAILURE!\r\njava.lang.AssertionError: testBloomMapFileConstructors error !!!\r\n        at org.apache.hadoop.io.TestBloomMapFile.testBloomMapFileConstructors(TestBloomMapFile.java:287{code}", "output": "BloomMapFile: invalid io.mapfile.bloom.error.rate (≤0 or ≥1) causes NaN/zero vector size and writer construction failure"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade Spark to `4.1.0-preview3`\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: CheckStyle version upgrade: 10 -->> 12\nDescription: *Task:* upgrade checkstyle version from *10.20.2* to *12.0.1*\r\n\r\n*Related link:* [https://checkstyle.org/releasenotes.html#Release_12.0.1]\r\n\r\n *Note:* difference between versions is quite big:\r\n * version *10.20.2* (published in Novemeber 2024)\r\n * version *12.0.1* (published in Octoober 2025)\r\n * git diff between versions: [https://github.com/checkstyle/checkstyle/compare/checkstyle-10.20.2...checkstyle-12.0.1]\nQ: @Anyone: feel free to check PR: [https://github.com/apache/kafka/pull/20726]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "With KIP-848, consumer HB interval config moved to the broker, so currently, the consumer HB request manager starts with a 0ms interval (mainly to send a first HB right away after the consumer subscribe + poll). Once a response is received, the consumer takes the interval from the response and starts using it. \r\n\r\nThat 0ms initial interval makes that the HB mgr poll continuously executes logic on a tight loop that may not really be needed. It mostly has to wait for a response (or a failure).\r\n\r\nProbably worse than this, is the impact on the app thread, given that pollTimeout takes into account the maxTimeToWait from the network thread, that is directly impacted by the timeToNextHeartbeat\r\n * [https://github.com/apache/kafka/blob/388739f5d847d7a16e389d9891f806547f023476/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1764-L1766]\r\n * [https://github.com/apache/kafka/blob/781bc7a54b8c4f7c86f0d6bb9ef8399d86d0735e/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L255]\r\n\r\nWe should review and consider setting a non-zero initial interval (while we wait for the actual interval from the broker). One option to consider would be using the request timeout maybe (just a first thought)\r\n\r\nHigh level goals here would be to:\r\n * maintain the behaviour of sending a first HB without delay \r\n * ensure no unneeded activity on the HB mgr poll in the background, in tight loop, while we're just waiting for the first HB response with an interval\r\n * ensure the app thread poll timeout is not affected", "output": "Improve heartbeat request manager initial HB interval "}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Ticket for KIP-1224.\r\n\r\n* Add a new allowed value for {{group.coordinator.append.linger.ms}} and {{share.coordinator.append.linger.ms}} of -1.\r\n* When {{append.linger.ms}} is set to -1, use the flush strategy outlined in the KIP.\r\n* Add batch-linger-time metrics.\r\n* Add batch-flush-time metrics.", "output": "KIP-1224: Adaptive append.linger.ms for the group coordinator and share coordinator"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve `ConfOptionDocGenerator` to generate a sorted doc by config key\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "SPARK-52689 added a DataSourceV2 API that sends operation metrics along with the commit, via a map of string, long.  It would be cleaner to model it as a proper object so that it is more clear what metrics Spark sends, and to handle future cases where metrics may not be long values.\r\n\r\nSuggestion from [~aokolnychyi]", "output": "Model DSV2 Commit Operation Metrics API"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h3. *Title*\r\n\r\nAdd {{acks}} dimension to {{BrokerTopicMetrics}} to measure per-acks produce performance impact\r\n----\r\nh3. *Summary*\r\n\r\nCurrently, Kafka’s {{BrokerTopicMetrics}} only tracks produce metrics such as {{{}BytesInPerSec{}}}, {{{}MessagesInPerSec{}}}, and {{ProduceRequestsPerSec}} at the topic level. However, these metrics do not distinguish between different producer acknowledgment ({{{}acks{}}}) configurations ({{{}acks=0{}}}, {{{}acks=1{}}}, {{{}acks=-1{}}}).\r\nIn high-throughput environments, different {{acks}} levels have significantly different impacts on broker CPU, I/O, and network utilization.\r\nThis proposal introduces an additional {{acks}} label to existing topic-level metrics, enabling more granular visibility into broker performance under various producer reliability modes.\r\n----\r\nh3. *Motivation*\r\n\r\nThe current aggregated produce metrics make it difficult to assess the performance and stability implications of different {{acks}} settings on brokers.\r\nFor example, asynchronous ({{{}acks=0{}}}) and fully acknowledged ({{{}acks=-1{}}}) produces can have very different effects on disk I/O, request queues, and replication latency, but these effects are hidden in current metrics.\r\n\r\nBy introducing an {{acks}} dimension, operators and performance engineers can:\r\n * Quantify the resource cost of different producer acknowledgment strategies.\r\n\r\n * Analyze how {{acks}} configuration affects cluster throughput, replication load, and latency.\r\n\r\n * Perform fine-grained benchmarking and capacity planning.\r\n\r\n----\r\nh3. *Proposed Changes*\r\n # *Extend {{BrokerTopicStats}}*\r\nAdd a new {{perTopicAcksStats}} structure to track metrics per {{(topic, acks)}} combination:\r\n\r\n{code:java}\r\nval perTopicAcksStats = new Pool[(String, Short), BrokerTopicMetrics](\r\nSome((key) => new BrokerTopicMetrics(Some(s\"${key._1},ack=${key._2}\")))\r\n){code}\r\n # *Instrument Produce Handling*\r\nIn {{{}KafkaApis.handleProduceRequest{}}}, extract the producer {{acks}} value and record metr", "output": "Add acks dimension to BrokerTopicMetrics for produce requests"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consistency of command-line arguments for remaining CLI tools\nDescription: This implements KIP-1147 for kafka-cluster.sh, kafka-leader-election.sh and kafka-streams-application-reset.sh.\nQ: Hi [~schofielaj] , I saw that this Jira ticket is unassigned. I'd be happy to take it on if that would be alright.", "output": "Sure."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Streams open iterator tracking has high contention on metrics lock\nDescription: We run Kafka Streams 4.1.0 with custom processors that heavily use state store range iterators.\r\n\r\nWhile investigating disappointing performance, we found a surprising source of lock contention.\r\n\r\nOver the course of about a 1 minute profiler sample, the {{org.apache.kafka.common.metrics.Metrics}} lock is taken approximately 40,000 times and blocks threads for about 1 minute.\r\n\r\nThis appears to be because our state stores generally have no iterators open, except when their processor is processing a record, in which case it opens an iterator (taking the lock through {{OpenIterators.add}} into {{{}Metrics.registerMetric{}}}), does a tiny bit of work, and then closes the iterator (again taking the lock through {{OpenIterators.remove}} into {{{}Metrics.removeMetric{}}}).\r\n\r\nSo, stream processing threads takes a globally shared lock twice per record, for this subset of our data. I've attached a profiler thread state visualization with our findings - the red bar indicates the thread was bloc\nQ: Thanks [~mjsax] for taking a look.\r\n\r\nWe have a product requirement to compute a streaming-min and streaming-max operation over a grouped aggregate. For example, \"earliest record due date for each user\" or \"latest record created date for each user\".\r\n\r\nTo do this, we take the input stream,\r\n{code:java}\r\nK1 = U1 V1\r\nK2 = U1 V2\r\nK3 = U2 V3\r\nK4 = U2 V4 {code}\r\nand reorganize the records so the group-key and value are the key prefix, like\r\n{code:java}\r\nU1 V1 K1 = K1\r\nU1 V2 K2 = K2\r\nU2 V3 K3 = K3\r\nU2 V4 K4 = K4{code}\r\nand put it in a state store. Then, to determine the minimum or maximum, we do a prefix range scan to take the first or last record for the group U1 or U2.\r\n\r\nIt might be possible to reduce the number of range scans by caching the minimum and maximum values by key, to know if the max or min possibly changed and skip the iterator if not, but then we need a second state store duplicating the winning record per user. We assumed the cost of opening an iterator is roughly equal to the cost of a key lookup, but maybe this is not a good assumption.\r\n\r\nRegardless, to me, the current semantics for this metric seems wrong. If the store is open, with no iterators currently, the correct value for the metric is explicitly \"0\" not \"null / unregister\". The current setup makes it difficult to graph, since our dashboards will interpret \"null\" as \"missing data\" which is distinct from a present 0.\r\n\r\nI would expect the metric to be unregistered only when the state store is closed or otherwise we are sure no new iterators will ever be created.", "output": "This metric is a little bit tricky... (for context [KIP-989|https://cwiki.apache.org/confluence/display/KAFKA/KIP-989%3A+Improved+StateStore+Iterator+metrics+for+detecting+leaks]) – if we would report `0` (or `-1`), the issue is, that if you setup an alert that computes \"currentTime minus metricValue\" you get false-positives, as the iterator open time computation would report a high value (many years). Your alert would need to be conditional, what is a struggle as far as I know. While a dashboard can render `0` it would blow out your \"y-axis\" on the dashboard to a very high value, too, and it seems it would make it very hard to actually read the dashboard?\r\n\r\nWe actually reported `null` originally, but this also caused issues: https://issues.apache.org/jira/browse/KAFKA-17954 – so we decided to de-register the metric when it becomes empty.\r\n{quote} otherwise we are sure no new iterators will ever be created.\r\n{quote}\r\nNot sure what you mean by this?\r\n\r\nFor your use case: how many values per group do you get? Would it be possible to do an `aggregation` per group, and compute a `List` over all values per group? This would allow you to maintain this list with a key-lookup per update, avoiding a range scan (of course, this only works if the list is small enough, to avoid too large records...)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve `SparkSubmit` to invoke `exitFn` with the root cause instead of `SparkUserAppException`\nDescription: I hit this when I ran a pipeline that had no flows:\r\n```\r\norg.apache.spark.SparkUserAppException: User application exited with 1\r\n\tat org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:127)\r\n\tat org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1028)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:226)\r\n\tat org.apache.spark.deploy.Spar\nQ: Issue resolved by pull request 52770\n[https://github.com/apache/spark/pull/52770]", "output": "I collected this as a subtask of SPARK-51727 in order to improve the visibility."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()\nDescription: We have evidence that `IOStatisticsSupport.snapshotIOStatistics()` can hang, specifically on the statistics collected by an S3AInputStream, whose statistics are merged in to the FS stats in close();\r\n\r\n{code}\r\njdk.internal.misc.Unsafe.park(Native Method)\r\njava.util.concurrent.locks.LockSupport.park(LockSupport.java:341)\r\njava.util.concurrent.ForkJoinTask.awaitDone(ForkJoinTask.java:468)\r\njava.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:687)\r\njava.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:927)\r\njava.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\njava.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)\r\norg.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap.entrySet(EvaluatingStatisticsMap.java:166)\r\njava.util.Collections$UnmodifiableMap.entrySet(Collections.java:1529)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.copyMap(IOStatisticsBinding.java:172)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:216)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:199)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSnapshot.snapshot(IOStatisticsSnapshot.java:165)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSnapshot.(IOStatisticsSnapshot.java:125)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSupport.snapshotIOStatistics(IOStatisticsSupport.java:49)\r\n{code}\r\n\r\nthe code in question is calling `parallelStream()`, which uses", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade hadoop3 docker scripts to 3.4.2\nDescription: The Hadoop 3.4.2 version has been released, and we need to update the Hadoop Docker image to version 3.4.2.\nQ: slfan1989 opened a new pull request, #8005:\nURL: https://github.com/apache/hadoop/pull/8005\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "slfan1989 commented on PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#issuecomment-3355515590\n\n   @jojochuang @adoroszlai @ayushtkn Hadoop 3.4.2 has been released, and we are preparing a corresponding Docker image for Hadoop 3.4.2. I have created this PR to complete the Docker image release. Could you please review this PR? Thank you very much!"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1\nDescription: The project is currently using {{restrict-imports-enforcer-rule}} version {*}2.0.0{*}, which was released in *2021* and is now outdated. Since then, several new versions have been released with important bug fixes, performance improvements, and enhanced compatibility. To maintain a modern and stable build environment, it is necessary to upgrade to version {*}2.6.1{*}.\nQ: slfan1989 opened a new pull request, #8012:\nURL: https://github.com/apache/hadoop/pull/8012\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19715. Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   CI.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #8012:\nURL: https://github.com/apache/hadoop/pull/8012#issuecomment-3379348657\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 43s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 25s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 25s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  mvnsite  |   0m 25s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/branch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  shadedclient  |   3m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 24s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 24s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 22s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   0m 25s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-javadoc-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 24s | [/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-javadoc-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  shadedclient  |   4m 41s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 28s |  |  ASF License check generated no output?  |\r\n   |  |   |  13m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8012 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux bcae851086b8 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f59839a3b410ba91d777948cc1b8683d10006e31 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/testReport/ |\r\n   | Max. process+thread count | 55 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8012/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Handle overlap batch alignment on partition reassignment\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: CheckStyle version upgrade: 10 -->> 12\nDescription: *Task:* upgrade checkstyle version from *10.20.2* to *12.0.1*\r\n\r\n*Related link:* [https://checkstyle.org/releasenotes.html#Release_12.0.1]\r\n\r\n *Note:* difference between versions is quite big:\r\n * version *10.20.2* (published in Novemeber 2024)\r\n * version *12.0.1* (published in Octoober 2025)\r\n * git diff between versions: [https://github.com/checkstyle/checkstyle/compare/checkstyle-10.20.2...checkstyle-12.0.1]", "output": "Patch Available"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Support of CSE-KMS in version 3.4.1 seems to be broken\nDescription: Seem like on Commit c54bf19 (moving to aws sdk v2) the support of CSE-KMS was dropped or the way to configure it was changed without updating documentation. \r\n\r\n[https://github.com/apache/hadoop/commit/81d90fd65b3c0c7ac66ebae78e22dcb240a0547b#diff-a8dabc9bdb3ac3b04f92eadd1e3d9a7076d8983ca4fb7d1d146a1ac725caa309]\r\n\r\nin file: DefaultS3ClientFactory.java\r\n\r\nfor example code that was removed:\r\n\r\n!image-2025-07-21-18-13-16-765.png!\r\n\r\n \r\nFYI: [~stevel@apache.org]\nQ: Hey [~igreenfi] , releases 3.4.0 and 3.4.1 did not have support for CSE. The upcoming 3.4.2 release will support CSE again. \r\n\r\nThis is the CSE PR: https://github.com/apache/hadoop/pull/6884", "output": "marking as resolved by HADOOP-18708.\r\n\r\n[~igreenfi] if You could checkout and test the branch-3.4.2  release and make sure that CSE works for you, we can be confident that the final release works too. Please do try this -or at least test the next RC. thanks"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Frequency estimation functions\nDescription: Introduce the following frequency estimation functions:\r\n * *APPROX_TOP_K(expr[, k[, maxItemsTracked]])* – Returns an approximate list of the top _k_ most frequent values in the input expression using a probabilistic algorithm.\r\n\r\n * *APPROX_TOP_K_ACCUMULATE(expr[, maxItemsTracked])* – Creates a state object that accumulates frequency statistics for use in later estimation or combination.\r\n\r\n * *APPROX_TOP_K_ESTIMATE(state [, k] ])* – Extracts and returns the approximate top _k_ values and their estimated frequencies from an accumulated state.\r\n\r\n * *APPROX_TOP_K_COMBINE(expr[, maxItemsTracked])* – Merges intermediate APPROX_TOP_K state objects, allowing distributed or parallel computation.\nQ: To improve the visibility of this feature, I linked this to SPARK-51166 and marked as `Resolved` according to the subtasks. I believe we can add more subtasks (if needed) until we release Apache Spark 4.1.0 release. Thank you, [~yhuang95] and [~Gengliang.Wang].", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add the changes from KIP-1147 into upgrade.html. Rather than have 5 PRs make tiny changes, it's probably more efficient to do this in one PR.", "output": "Update upgrade.html with the changes in Apache Kafka 4.2"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "SPIP: https://docs.google.com/document/d/1xVTLijvDTJ90lZ_ujwzf9HvBJgWg0mY6cYM44Fcghl0/edit?tab=t.0#heading=h.4iogryr5qznc", "output": "Metrics & semantic modeling in Spark"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In KAFKA-19359, the commons-beanutils transitive dependency was force bumped in the project to avoid related CVEs. The commons-validator already has a new release, which solves this problem:\r\n\r\n[https://github.com/apache/commons-validator/tags]\r\n\r\nThe workaround could be deleted as part of the version bump.", "output": "Upgrade commons-validator to 1.10.0"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "KIP-939 has been paused but some of the client code and new API's have already been merged to trunk. We need to revert the changes in the 4.2 release branch.", "output": "Revert KIP-939 API's and Client Code for 4.2"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Fix Case Sensitivity Issue for hdi_isfolder metadata\nDescription: In the blob endpoint, we determine whether the path is a file, or a directory based on the metadata attribute hdi_isfolder. When creating a directory, we set hdi_isfolder to true. Currently, our method for checking if the path is a directory involves a case-sensitive equality check. Consequently, if someone configures a directory with Hdi_isfolder, the driver will not recognize that path as a directory. We need to address this issue because, in the backend, hdi_isfolder and Hdi_isfolder are considered the same metadata attribute. Therefore, the solution involves modifying our equality check to be case-insensitive, ensuring that the driver correctly identifies directories regardless of case variations in the metadata attribute.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: MirrorMaker2 Offset Replication Issue\nDescription: I am using *Apache Kafka 4.0* with *MirrorMaker 2* to link the primary cluster ({*}clusterA{*}) to the secondary cluster ({*}clusterB{*}).\r\nThe secondary cluster will not have any producers or consumers until a disaster recovery event occurs, at which point all producers and consumers will switch to it.\r\n\r\n*Setup:*\r\n * Dedicated standalone MirrorMaker 2 node\r\n * {{IdentityReplicationPolicy}} (no topic renaming)\r\n * No clients connected to secondary cluster under normal operation\r\n\r\n*MirrorMaker 2 config:*\r\n {{# Cluster aliases\r\nclusters = clusterA, clusterB\r\n\r\n# Bootstrap servers\r\nclusterA.bootstrap.servers = serverA-kafka-1:9092\r\nclusterB.bootstrap.servers = serverB-kafka-1:9092\r\n\r\n# Replication policy\r\nreplication.policy.class=org.apache.kafka.connect.mirror.IdentityReplicationPolicy\r\n\r\n# Offset/Checkpoint sync\r\nemit.checkpoints.enabled=true\r\nemit.checkpoints.interval.seconds=5\r\nsync.group.offsets.enabled=true\r\nsync.group.offsets.interval.seconds=5\r\noffset.lag.max=10\r\nrefresh.topics.interval.seconds=5}}\r\n----\r\nh3. Test results:\r\n # *Produce 300 messages when MirrorMaker is running*\r\n*Expected:* Topic offset synced within a minute\r\n*Result:* ✅ Passed\r\n\r\n # *Consume 100 messages when MirrorMaker is running, then terminate the consumer*\r\n*Expected:* Consumer offset synced\r\n*Result:* ❌ Failed — offset is not synced to clusterB\r\n\r\n # *Restart MirrorMaker after test #2*\r\n*Expected:* Consumer offset synced\r\n*Result:* ✅ Passed\r\n\r\n # *Repeat test #2 — consume 100 messages when Mirro", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix testRenameFileWithFullQualifiedPath on Windows\nDescription: The Apache Commons Net FTP library is used for FTPFileSystem. In *TestFTPFileSystem#testRenameFileWithFullQualifiedPath()*, the FS operations (such as touch and rename) are made using absolute paths. However, the library expects relative paths.\r\nThis caused *FTPFileSystem#getFileStatus()* to throw FileNotFoundException since the library was trying to look for the absolute path under which the FileSystem was mounted.\r\n\r\nThis worked fine on Linux as it just appended the absolute path under the FileSystem's mount path -\r\n\r\n !image-2025-04-27-23-05-05-212.png! \r\n\r\nHowever, this fails on Windows since suffixing the absolute path under the FileSystem's mount path doesn't yield a valid path due to the drive letter in the absolute path.\r\n\r\nConsider the following illustration -\r\n+On Linux+\r\n{text}\r\npath1 => /mnt/d/a/b\r\npath2 => /mnt/d/x/y\r\npath1 + path2 yields a valid path => /mnt/d/a/b/mnt/d/x/y\r\n{text}\r\n\r\n+On Windows+\r\n{text}\r\npath1 => C:\\a\\b\r\npath2 => C:\\x\\y\r\npath1 + path2 doesn't yield a va\nQ: hadoop-yetus commented on PR #7654:\nURL: https://github.com/apache/hadoop/pull/7654#issuecomment-2833646931\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 45s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 36s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 16s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 45s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 36s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 56s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 207m 22s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7654/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7654 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c1bf5de9a12a 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 12929572d9070158a679f0a295fccd3b6a3d1850 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7654/1/testReport/ |\r\n   | Max. process+thread count | 1269 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7654/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "GauthamBanasandra commented on PR #7654:\nURL: https://github.com/apache/hadoop/pull/7654#issuecomment-2833872827\n\n   Started the CI run for Windows - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-win10-x86_64/905/."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use Swift 6.2 for all Linux CIs\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: KAFKA-15665 prevents safe reassignments from completing\nDescription: The implementation for KAFKA-15665 is a little too strict - it requires that all replicas in the target replica set be in ISR.\r\n\r\nThis means if we have a reassignment like [1, 2, 3] -> [2, 3, 4] where broker 2 was unhealthy and lagging in replication, reassignment would not complete. \r\n\r\n \r\n\r\nI believe it should work to instead just check the following:\r\n\r\n- Added replicas are in ISR (newTargetISR)\r\n- Resulting ISR (newTargetISR) is not under-min-isr", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Drop temporary functions in Pandas UDF tests\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix preservedEntries count in CopyCommitter when preserving directory attributes\nDescription: The preservedEntries count always be 0\nQ: yuw1 opened a new pull request, #7555:\nURL: https://github.com/apache/hadoop/pull/7555\n\n   …ving directory attributes\r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   The preservedEntries parameter is not updated when preserve dir is running and is always 0.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   The log output of the existing test case can check the problem\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7555:\nURL: https://github.com/apache/hadoop/pull/7555#issuecomment-2764577171\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  27m  3s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 154m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7555/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7555 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d27f44be1b70 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 14978fffaadabdb1b168c2e9047aacc4561c565e |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7555/1/testReport/ |\r\n   | Max. process+thread count | 700 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7555/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In this PR I propose to make ResolvedCollation evaluable. By making ResolvedCollation evaluable, it can now pass the canEvaluateWithinJoin check, allowing the optimizer to use efficient hash joins for queries with inline COLLATE in join conditions (before this change we would fallback to BroadcastNestedLoopJoin).", "output": "Make ResolvedCollation evaluable"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: AbfsConfiguration should store account type information (HNS or FNS)\nDescription: Currently, both {{AbfsClient}} and {{AzureBlobFileSystemStore}} store information about whether the account is FNS or HNS (i.e., whether namespace is enabled). This information should instead be stored at the {{AbfsConfiguration}} level, allowing both the client and the store to retrieve it from there when needed.", "output": "In Progress"}
{"instruction": "Answer the question based on the bug.", "input": "Title: RunJar throws UnsupportedOperationException on Windows\nDescription: On Windows, run {{hadoop jar}} (with any jar). One immediately gets the following exception:\r\n\r\n{code}\r\nException in thread \"main\" java.lang.UnsupportedOperationException: 'posix:permissions' not supported as initial attribute\r\n    at java.base/sun.nio.fs.WindowsSecurityDescriptor.fromAttribute(WindowsSecurityDescriptor.java:358)\r\n    at java.base/sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:497)\r\n    at java.base/java.nio.file.Files.createDirectory(Files.java:690)\r\n    at java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:135)\r\n    at java.base/java.nio.file.TempFileHelper.createTempDirectory(TempFileHelper.java:172)\r\n    at java.base/java.nio.file.Files.createTempDirectory(Files.java:966)\r\n    at org.apache.hadoop.util.RunJar.run(RunJar.java:296)\r\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:245)\r\n{code}\r\n\r\nI'm running Windows 11 with OpenJDK 11.0.26.\r\n\r\nThis bug does not exist in 3.3.6.\nQ: We changed how we create the temp directory in HADOOP-19031, and this seems to have broken Windows. This issue does not exist on Linux or macOS.", "output": "[~hexiaoqiao] could you please review this and comment? Thanks."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Kafka repo to use JUnit6\nDescription: As JUnit6 was released [https://docs.junit.org/6.0.0/release-notes], it would be great to upgrade repo with such version. Currently {*}5.13.1{*}.\nQ: JUnit 6 requires Java 17 while Kafka still supports Java 11 for some modules. So, we cannot move to JUnit 6 for all modules. It may be confusing to use different JUnit versions for different modules.", "output": "Unless JUnit 6 brings major enhancements, I'd rather stick to JUnit 5 across the full code base instead of using different versions."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Since upgrading to Kafka Streams 4.1.0, we again see in our metrics collection:\r\n{code:java}\r\nException thrown from GraphiteReporter#report. Exception was suppressed.\r\njava.util.NoSuchElementException: null\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:1863)\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:393)\r\n\tat org.apache.kafka.streams.internals.metrics.OpenIterators.lambda$add$0(OpenIterators.java:57)\r\n\tat org.apache.kafka.common.metrics.KafkaMetric.metricValue(KafkaMetric.java:81) {code}\r\nIt seems that unfortunately the dynamic metric registration introduced in [https://github.com/apache/kafka/pull/20022] undid the fix from [https://github.com/apache/kafka/pull/18771]\r\n\r\nThere is still a race between the metric gauge being removed, and the last open iterator being removed from the tracking set. The same fix from #18771 would fix the issue again.", "output": "NoSuchElementException (again) in Kafka Streams iterator metrics"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Comply with ASF website policy \nDescription: Fix issue about 'Apache Project Website Checks' shows at https://whimsy.apache.org/site/project/hadoop", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Trigger Docker image builds in release script\nDescription: In the release process, you release manager has to manually trigger the Docker Build Test Workflow for both the jvm and native images ([https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=34840886#ReleaseProcess-CreateJVMApacheKafkaDockerArtifacts(Forversions>=3.7.0)|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=34840886#ReleaseProcess-CreateJVMApacheKafkaDockerArtifacts(Forversions>=3.7.0)]).\r\n\r\nThe release script could trigger these jobs automatically using the GitHub REST API.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: add a lower bound to num.replica.fetchers\nDescription: `num.replica.fetchers` parameter does not enforce a lower bound, so both `num.replica.fetchers=0` or `num.replica.fetchers=-1` are valid when starting a broker. However, setting `num.replica.fetchers=-1` will result in only one fetcher being created, while setting `num.replica.fetchers=0` will cause a `java.lang.ArithmeticException: / by zero` during replication startup", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add ContinuousMemorySink for RTM testing\nDescription: \nQ: Issue resolved by pull request 52550\n[https://github.com/apache/spark/pull/52550]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Move Gauge#value to MetricValueProvider\nDescription: from: https://github.com/apache/kafka/pull/3705#discussion_r140830112\nQ: we may need to keep the method in `Guave` to avoid possible compatibility issue (see KAFKA-6174)", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Make Maven plugins up-to-date\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Exclude junit 4 transitive dependency\nDescription: HADOOP-19617 removed direct junit 4 dependency.  However, junit 4 is still pulled transitively by other dependencies.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Prologue:* this issue was discovered while working on Gradle upgrade (from 8 to 9); related JIRA tickets:\r\n * KAFKA-19174\r\n * KAFKA-19654\r\n\r\n*Environment:*\r\n * Develocity instance (version 2025.2.4): [https://develocity.apache.org/scans?search.rootProjectNames=kafka]\r\n * Gradle version 8.14.3, GitHub Actions for Gradle builds: 4.3.0\r\n * Gradle version 9.0.0, GitHub Actions for Gradle builds: 4.4.3\r\n * note: Github Actions for Gradle is using wrapper Gradle version (definition is here: .github/actions/setup-gradle/action.yml)\r\n\r\n*Scenario:*\r\n * upgrade {{com.gradle.develocity}} Gradle plugin version from a current version (3.19) to any recent version (3.19.1 and/or more recent)\r\n * GitHub Action build breaks (with this error: \"build scan failed to be published\") for both Gradle 8.14.3 and Gradle 9.0.0\r\n * (!) note for commit that contains Gradle 9.0.0: Github Actions build shows wrong Gradle version (8.14.3)\r\n\r\n*Test cases:*\r\n * test case [1]\r\n ** baseline: Gradle 8.14.3 /com.gradle.develocity 3.19\r\n ** git commit: last main branch (trunk) commit\r\n ** build result -->> everything works fine (/) (no surprises here, it's a trunk after all)\r\n * test case [2]\r\n ** Gradle version 8.14.3 / com.gradle.develocity 3.19.2\r\n ** git commit: [https://github.com/apache/kafka/pull/20450/commits/d930867e8c2b4dd3f79f032e7b17328cf0ea97ef]\r\n ** Github actions error: (x) [https://github.com/apache/kafka/actions/runs/17714050963]\r\n * test case [3]\r\n ** Gradle version 9.0.0 / com.gradle.develocity 3.19\r\n ** git commit: [https://github.com/apache/kafka/pull/20450/commits/3b624732469835abc54ce06c2a554361578e7c6a]\r\n ** build result -->> everything works fine (/)\r\n ** Giithub actions build: ([https://github.com/apache/kafka/actions/runs/17714609830/job/50338068122?pr=20450])\r\n ** scan for Java 17: [https://develocity.apache.org/s/o2v23hp2575mu]\r\n ** scan for Java 24: [https://develocity.apache.org/s/nui35ngzalmrm]\r\n * test case [4]\r\n ** Gradle version 9.0.0 / com.gradle.develocity 3.19.2\r\n *", "output": "Develocity Gradle plugin version can't be upgraded (CI build fails with error: \"build scan failed to be published\")"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Running the native image with `KAFKA_SASL_ENABLED_MECHANISMS=OAUTHBEARER` results in an exception that prevents it starting:\r\n\r\n\r\n \r\n{code:java}\r\n[2025-08-06 22:55:09,656] ERROR Exiting Kafka due to fatal exception during startup. (kafka.Kafka$) org.apache.kafka.common.KafkaException: org.apache.kafka.common.KafkaException: Could not find a public no-argument constructor for org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredLoginCallbackHandler at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:184) ~[?:?] at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:188) ~[?:?] at org.apache.kafka.common.network.ChannelBuilders.serverChannelBuilder(ChannelBuilders.java:105) ~[?:?] at kafka.network.Processor.(SocketServer.scala:883) ~[?:?] at kafka.network.Acceptor.newProcessor(SocketServer.scala:791) ~[kafka.Kafka:?] at kafka.network.Acceptor.$anonfun$addProcessors$1(SocketServer.scala:757) ~[kafka.Kafka:?] at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:190) ~[kafka.Kafka:?] at kafka.network.Acceptor.addProcessors(SocketServer.scala:756) ~[kafka.Kafka:?] at kafka.network.DataPlaneAcceptor.configure(SocketServer.scala:472) ~[?:?] at kafka.network.SocketServer.createDataPlaneAcceptorAndProcessors(SocketServer.scala:222) ~[?:?] at kafka.network.SocketServer.$anonfun$new$16(SocketServer.scala:149) ~[?:?] at kafka.network.SocketServer.$anonfun$new$16$adapted(SocketServer.scala:149) ~[?:?] at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619) ~[kafka.Kafka:?] at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617) ~[kafka.Kafka:?] at scala.collection.AbstractIterable.foreach(Iterable.scala:935) ~[kafka.Kafka:?] at kafka.network.SocketServer.(SocketServer.scala:149) ~[?:?] at kafka.server.BrokerServer.startup(BrokerServer.scala:274) ~[?:?] at kafka.server.KafkaRaftServer.$anonfun$startup$2(KafkaRaftServer.scala:96) ~[?:?] at kafka.server.Kafk", "output": "Native docker image fails to start when using SASL OAUTHBEARER mechanism"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*CVE-2025-53864:*\r\n\r\nConnect2id Nimbus JOSE + JWT before 10.0.2 allows a remote attacker to cause a denial of service via a deeply nested JSON object supplied in a JWT claim set, because of uncontrolled recursion. NOTE: this is independent of the Gson 2.11.0 issue because the Connect2id product could have checked the JSON object nesting depth, regardless of what limits (if any) were imposed by Gson.\r\n\r\nSeverity: 6.9 (medium)", "output": "Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively\nDescription: It has been observed that certain requests taking more time than expected to complete hinders the performance of whole workload. Such requests are known as tailing requests. They can be taking more time due to a number of reasons and the prominent among them is a bad network connection. In Abfs driver we cache network connections and keeping such bad connections in cache and reusing them can be bad for perf.\r\n\r\nIn this effort we try to identify such connections and close them so that new good connetions can be established and perf can be improved. There are two parts of this effort.\r\n # Identifying Tailing Requests: This involves profiling all the network calls and getting percentiles value optimally. By default we consider p99 as the tail latency and all the future requests taking more than tail latency will be considere as Tailing requests.\r\n\r\n # Proactively Killing Socket Connections: With Apache client, we can now kill the socket connection and fail the tailing request. Such failur\nQ: hadoop-yetus commented on PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#issuecomment-3448411577\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  11m  8s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  13m 53s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 43 new + 3 unchanged - 0 fixed = 46 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 34 new + 1472 unchanged - 0 fixed = 1506 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 33 new + 1413 unchanged - 0 fixed = 1446 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 177 unchanged - 1 fixed = 181 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 12s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  70m  0s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:[line 533] |\r\n   |  |  new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:[line 55] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:[line 81] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8043 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux d98c9c1604f2 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7dcac93eec4dc5a48d643ae81372c581b6c3bebf |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/testReport/ |\r\n   | Max. process+thread count | 614 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #8043:\nURL: https://github.com/apache/hadoop/pull/8043#issuecomment-3449602331\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 43 new + 3 unchanged - 0 fixed = 46 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 35 new + 1472 unchanged - 0 fixed = 1507 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 34 new + 1413 unchanged - 0 fixed = 1447 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 177 unchanged - 1 fixed = 181 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  60m 12s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:new org.apache.hadoop.fs.azurebfs.services.AbfsAHCHttpOperation(URL, String, List, Duration, Duration, long, AbfsApacheHttpClient, AbfsClient)  At AbfsAHCHttpOperation.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(int, TracingContext)  At AbfsRestOperation.java:[line 533] |\r\n   |  |  new org.apache.hadoop.fs.azurebfs.services.AbfsTailLatencyTracker(AbfsConfiguration) may expose internal representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:representation by storing an externally mutable object into AbfsTailLatencyTracker.configuration  At AbfsTailLatencyTracker.java:[line 55] |\r\n   |  |  Unknown bug pattern CT_CONSTRUCTOR_THROW in new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:new org.apache.hadoop.fs.azurebfs.services.SlidingWindowHdrHistogram(long, int, int, int, int, long, int, AbfsRestOperationType)  At SlidingWindowHdrHistogram.java:[line 81] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8043 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 9b5c6baa74d5 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / de244d215362fca4d8ba16b3d01a9f39a3ff0e81 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/testReport/ |\r\n   | Max. process+thread count | 637 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8043/3/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove invalid `licenses` field of `hadoop-aliyun` SBOM by upgradding `cyclonedx` to 2.9.1\nDescription: \nQ: dongjoon-hyun commented on PR #7990:\nURL: https://github.com/apache/hadoop/pull/7990#issuecomment-3322518878\n\n   Thank you, @slfan1989 .", "output": "hadoop-yetus commented on PR #7990:\nURL: https://github.com/apache/hadoop/pull/7990#issuecomment-3325310193\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  60m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  20m 20s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m  2s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  11m 38s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   9m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 185m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  49m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  21m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  21m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  18m 49s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  18m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  21m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |  11m 53s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   9m  0s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  88m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 511m 54s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 880m 54s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.TestReconstructStripedFileWithRandomECPolicy |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7990 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 3a691ff72698 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f944112d4216160cc394f6ea7bd013f7b92796a9 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/testReport/ |\r\n   | Max. process+thread count | 2369 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7990/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*BEFORE*\r\n{code}\r\nStarting Operator...\r\nWARNING: A terminally deprecated method in sun.misc.Unsafe has been called\r\nWARNING: sun.misc.Unsafe::allocateMemory has been called by io.netty.util.internal.PlatformDependent0$2 (file:/opt/spark-operator/operator/spark-kubernetes-operator.jar)\r\nWARNING: Please consider reporting this to the maintainers of class io.netty.util.internal.PlatformDependent0$2\r\nWARNING: sun.misc.Unsafe::allocateMemory will be removed in a future release\r\n25/10/09 03:35:46 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:35:46 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.\r\n{code}\r\n\r\n*AFTER*\r\n{code}\r\nStarting Operator...\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Configuring operator with 50 reconciliation threads.\r\n25/10/09 03:32:21 INFO   o.a.s.k.o.SparkOperator Adding Operator JosdkMetrics to metrics system.                                                                    │\r\n{code}", "output": "Set `io.netty.noUnsafe` to `true` to avoid JEP-498 warnings"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support `spark.logConf` configuration\nDescription: \nQ: Issue resolved by pull request 403\n[https://github.com/apache/spark-kubernetes-operator/pull/403]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: MirrorMaker2 Offset Replication Issue\nDescription: I am using *Apache Kafka 4.0* with *MirrorMaker 2* to link the primary cluster ({*}clusterA{*}) to the secondary cluster ({*}clusterB{*}).\r\nThe secondary cluster will not have any producers or consumers until a disaster recovery event occurs, at which point all producers and consumers will switch to it.\r\n\r\n*Setup:*\r\n * Dedicated standalone MirrorMaker 2 node\r\n * {{IdentityReplicationPolicy}} (no topic renaming)\r\n * No clients connected to secondary cluster under normal operation\r\n\r\n*MirrorMaker 2 config:*\r\n {{# Cluster aliases\r\nclusters = clusterA, clusterB\r\n\r\n# Bootstrap servers\r\nclusterA.bootstrap.servers = serverA-kafka-1:9092\r\nclusterB.bootstrap.servers = serverB-kafka-1:9092\r\n\r\n# Replication policy\r\nreplication.policy.class=org.apache.kafka.connect.mirror.IdentityReplicationPolicy\r\n\r\n# Offset/Checkpoint sync\r\nemit.checkpoints.enabled=true\r\nemit.checkpoints.interval.seconds=5\r\nsync.group.offsets.enabled=true\r\nsync.group.offsets.interval.seconds=5\r\noffset.lag.max=10\r\nrefresh.topics.\nQ: Hi [~geric] Please see the other related tickets in this area:\r\n * https://issues.apache.org/jira/browse/KAFKA-16364\r\n * https://issues.apache.org/jira/browse/KAFKA-16291 \r\n * https://issues.apache.org/jira/browse/KAFKA-15564 \r\n * [https://github.com/apache/kafka/pull/15423] \r\n\r\nFor a detailed explanation of this behavior, please see this conference talk: \r\n\r\n[https://current.confluent.io/2024-sessions/mirrormaker-2s-offset-translation-isnt-exactly-once-and-thats-okay] \r\n\r\nFarther from the end of the topic (lag is ~200) the translation is worse (lag can double to ~400). Because you have only 300 messages in the topic, it looks like the offset never gets translated, or translates to 0 or 1.\r\n\r\nWith a larger example (1000 messages produced, 800 messages consumed) I would expect translation to lead to a downstream consumer lag of ~400. Also be aware that resetting the consumer offsets may not be sufficient to clear the MM2 state in the checkpoints topics. Make sure to use a new consumer group for further experiments with the same MM2 instance.\r\n\r\nFor applications, you either need to let your consumers reach 0 lag prior to cut-over, or you need to tolerate some re-delivery on the destination side.\r\n\r\nHope this helps!", "output": "Hi Greg, \r\nThanks for replying. \r\n\r\nIn a planned flip to DR, is there a way for me to force sync the consumer offsets?\r\n\r\nExample: to delete all the consumers and re-sync the offset?\r\n\r\nThanks\r\n\r\nGeric"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve error message when the path contains double slash without a preceding authority or bucket name\nDescription: Currently, we display the following error message when a path contains a double slash without a preceding authority or bucket name. \r\n{code:java}\r\njava.lang.IllegalArgumentException: Wrong FS: s3://test_file.json.gz, expected: s3://test_bucket/ at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:824) at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:683){code}\r\nThe path used in the above case is s3://test_bucket{*}//{*}test_file.json.gz. \r\n\r\nIt is expected that Hadoop will treat the path name preceding the double slash (//) as a bucket or authority because, according to HADOOP-8087, a relative reference that begins with two slash characters is termed a network-path reference.\r\n\r\nCreated this Jira to evaluate printing the full path in the error message or to revise the error message when the path contains a double slash without a preceding authority or bucket name for easier debugging.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use Java `Set.of` instead of `Collections.emptySet`\nDescription: \nQ: Issue resolved by pull request 52711\n[https://github.com/apache/spark/pull/52711]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When a task stops, [{{WorkerSinkTask#closeAllPartitions()}}|https://github.com/apache/kafka/blob/84caaa6e9da06435411510a81fa321d4f99c351f/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L664] runs and the [task’s {{preCommit}} is invoked|https://github.com/apache/kafka/blob/3c7f99ad31397a6a7a4975d058891f236f37d02d/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L444]. At that time, the {{currentOffsets}} argument may include {*}revoked partitions{*}.\r\n\r\nThis appears to be caused by:\r\n * {{WorkerSinkTask}} does not remove revoked partitions from {{currentOffsets}}\r\n * {{WorkerSinkTask#closeAllPartitions()}} passes its {{currentOffsets}} to {{SinkTask#preCommit(...)}} _as-is_ (i.e., without filtering).\r\n\r\nDuring normal iterations, {{SinkTask#preCommit(...)}} receives {{{}KafkaConsumer#assignment(){}}}, so revoked partitions are *not* included.\r\n\r\nHaving revoked partitions included *only* at stop is confusing behavior. If this behavior is specified, Could we add a brief note to the {{SinkTask#preCommit(...)}} Javadoc to clarify this behavior?", "output": "Revoked partitions are included in currentOffsets passed to preCommit on task stop"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "[KIP-724: Drop support for message formats v0 and v1|https://cwiki.apache.org/confluence/display/KAFKA/KIP-724%3A+Drop+support+for+message+formats+v0+and+v1]  and Kafka 4.0.0 removed support for the dynamic configs like message.format.version.\r\n\r\nWhen the user upgrades from 3.x to 4.x it can cause the dynamic configuration validation to fail if there are dynamic configurations that are no longer supported.\r\n{code:java}\r\nCaused by: org.apache.kafka.common.errors.InvalidConfigurationException: Unknown topic config name: message.format.version {code}\r\nOne solution for solving this problem is to have a well define list of allowed dynamic configuration. Any config that doesn't match this list will be automatically deleted.\r\n\r\nThis deletion can be done implicitly when applying the ConfigRecord record. This behavior can be further guarded by checking that the metadata version is greater than 4.0. This also means that upgrading the metadata version to 4.0 would cause all of the removed config to get deleted.", "output": "Delete dynamic config that were removed by Kafka"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [ABFS] AzureADAuthenticator should be able to retry on UnknownHostException\nDescription: When Hadoop is requested to perform operations against ADLS Gen2 storage, *AbfsRestOperation* attempts to obtain an access token from Microsoft. Underneath the hood, it uses a simple *java.net.HttpURLConnection* HTTP client.\r\n\r\nOccasionally, environments may run into network intermittent issues, including DNS-related {*}UnknownHostException{*}. Technically, the HTTP client throws *IOException* whose cause is {*}UnknownHostException{*}. *AzureADAuthenticator* in its turn catches {*}IOException{*}, sets *httperror = -1* and then checks whether the error is recoverable and can be retried. However, it's neither an instance of {*}MalformedURLException{*}, nor an instance of {*}FileNotFoundException{*}, nor a recoverable status code ({*}= 500 && != 501 && != 505{*}), hence a retry never occurs which is sensitive for our project causing problems with state recovery.\r\n\r\nThe final exception stack trace on the client side looks as follows (Apache Spark application, tenant ID is redacted):\r\n{code:java}\r\nJob aborted due to stage failure: Task 14 in stage 384.0 failed 4 times, most recent failure: Lost task 14.3 in stage 384.0 TID 3087 10.244.91.7 executor 29 : Status code: -1 error code: null error message: Auth failure: HTTP Error -1; url='https://login.microsoftonline.com/$TENANT_ID/oauth2/v2.0/token' AzureADAuthenticator.getTokenCall threw java.net.UnknownHostException: login.microsoftonline.com\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation AbfsRest", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Test failures during AWS SDK upgrade\nDescription: 1. The logic to skip vector IO contract tests doesn't work  if the analytics stream is set on a per-bucket basis for the test bucket\r\n2. tests with SSE-C are failing. Test bucket is normally set up to use SSE-KMS, FWIW\r\n\r\n{code}\r\n  \r\n    fs.s3a.bucket.stevel-london.encryption.algorithm\r\n    SSE-KMS\r\n  \r\n\r\n\r\n{code}\r\n\r\nthis only happens when the analytics stream is set for the test bucket fs.s3a.bucket.stevel-london.input.stream.type=analytics; set it globally all is good\r\n\r\n+ various other tests. FInal pr message\r\n{code}\r\nCode changes related to HADOOP-19485.\r\n\r\nAwsSdkWorkarounds no longer needs to cut back on transfer manager logging\r\n(HADOOP-19272) :\r\n- Remove log downgrade and change assertion to expect nothing to be logged.\r\n- remove false positives from log.\r\n\r\nITestS3AEndpointRegion failure:\r\n- Change in state of AwsExecutionAttribute.ENDPOINT_OVERRIDDEN\r\n  attribute requires test tuning to match.\r\n\r\nSome tests against third-party stores fail\r\n- Includes fix for the assumeStoreAws\nQ: examples\r\n{code}\r\n\r\n[ERROR] ITestS3AContractVectoredRead.testStopVectoredIoOperationsCloseStream  Time elapsed: 0.398 s  <<< FAILURE!\r\njava.lang.AssertionError: Expected an exception of type class java.io.InterruptedIOException\r\n        at org.apache.hadoop.test.LambdaTestUtils.intercept(LambdaTestUtils.java:411)\r\n        at org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.testStopVectoredIoOperationsCloseStream(ITestS3AContractVectoredRead.java:202)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\r\n        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\r\n        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)\r\n        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)\r\n        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.lang.Thread.run(Thread.java:750)\r\n\r\n[ERROR] ITestS3AContractVectoredRead.testStopVectoredIoOperationsUnbuffer  Time elapsed: 0.453 s  <<< FAILURE!\r\njava.lang.AssertionError: Expected an exception of type class java.io.InterruptedIOException\r\n        at org.apache.hadoop.test.LambdaTestUtils.intercept(LambdaTestUtils.java:411)\r\n        at org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.testStopVectoredIoOperationsUnbuffer(ITestS3AContractVectoredRead.java:224)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\r\n        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\r\n        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)\r\n        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)\r\n        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.lang.Thread.run(Thread.java:750)\r\n{code}", "output": "note that disabling all vector IO tests is dangerous as the stream MUST support the core contract tests...it is only the s3a input stream parallel reads an IOStats collection which should be skipped"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Similarly to \r\nhttps://issues.apache.org/jira/browse/KAFKA-17870\r\n\r\nExtending existing topics with a request that asks to create too many partitions can lead to OOM error and the client receiving an UnlknownServerException.\r\n\r\n \r\n\r\nWe should limit the number of new partitions similarly as when creating new topics.", "output": "Disallow create too many partitions also in createPartitions"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Maven site task fails with Java 17\nDescription: If I try to build site with Java 17, I get an IllegalArgumentException from analyze-report\r\n{code}\r\n$ mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site\r\n...\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report: IllegalArgumentException -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:347)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.i\nQ: MikaelSmith opened a new pull request, #7936:\nURL: https://github.com/apache/hadoop/pull/7936\n\n   ### Description of PR\r\n   Updates maven-dependency-plugin to 3.8.1 to fix an IllegalArgumentException when running the `site:site` Maven task with Java 17.\r\n   \r\n   ### How was this patch tested?\r\n   Ran `mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site` locally with openjdk-17 and it succeeded.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7936:\nURL: https://github.com/apache/hadoop/pull/7936#issuecomment-3262110832\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   9m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  15m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   6m  7s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  94m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  19m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m  3s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 20s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   8m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   9m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m  5s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 21s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 663m 19s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 840m 24s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapreduce.v2.TestUberAM |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7936 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 33f1385abb00 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e89a211da8c19aed6f68dbd9ea4418f6e657d615 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/testReport/ |\r\n   | Max. process+thread count | 3913 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Classify errors for AvroOptions boolean casting failure\nDescription: IllegalArgumentException is thrown when AvroOptions option requiring boolean is given to be not boolean. Should classify the error.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-resourceestimator.\nDescription: \nQ: slfan1989 opened a new pull request, #7548:\nURL: https://github.com/apache/hadoop/pull/7548\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19438. Upgrade JUnit from 4 to 5 in hadoop-resourceestimator.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7548:\nURL: https://github.com/apache/hadoop/pull/7548#issuecomment-2760047235\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 59s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/1/artifact/out/blanks-eol.txt) |  The patch has 4 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  9s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 33s | [/patch-unit-hadoop-tools_hadoop-resourceestimator.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/1/artifact/out/patch-unit-hadoop-tools_hadoop-resourceestimator.txt) |  hadoop-resourceestimator in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  75m 39s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.resourceestimator.service.TestResourceEstimatorService |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7548 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c32e5cb0de61 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 294d118c73af4343bc03498e359b42e6bdd0cadc |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/1/testReport/ |\r\n   | Max. process+thread count | 553 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-resourceestimator U: hadoop-tools/hadoop-resourceestimator |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7548/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analaytics-Accelerator: ITestS3AContractOpen.testInputStreamReadNegativePosition() failing\nDescription: New test added in [https://github.com/apache/hadoop/pull/7367,]\r\n\r\nexpects an IndexOutOfBoundsException to be thrown, when offset is negative.\r\n\r\n \r\n\r\nAAL throws IllegalArgumentException. For len, the PR says it's ok to throw either IllegalArgumentException or IndexOutOfBoundsException, so should the same thing happen of offset?\nQ: [~stevel@apache.org] what do you think? when offset is negative, is it ok to throw IllegalArgumentException?", "output": "I think it is somebody screwing up, so maybe we don't have to worry too much about the specifics. \r\n\r\nship it and see"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Many pieces of code still use old Java date/time classes, and can benefit from migrating to the \"new\" (Java 8) {{java.time}} APIs for several reasons, including:\r\n* Non thread-safe SimpleDateFormat - forces ThreadLocal usages and is generally bad practice these days (see YARN-11116)\r\n* Redundant object creation - e.g. creating a {{java.util.Date}} just to get an epoch millis value\r\n* Use of {{Calendar}}, {{TimeZone}}, etc.\r\n* All sorts of date/time arithmetics\r\n\r\nI propose 3 types of changes in this area:\r\n1) Internal changes - should be trivial to change without user-facing changes\r\n2) User-facing APIs - should be done _extending_ current code, with the question of deprecating existing code discussed separately\r\n3) External dependencies - value of change vs. cost (complexity/performance) should be discussed on a case-by-case basis.", "output": "Migrate usage of old Java date/time classes to java.time"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: improve the docs of \"clusterId\" for AddRaftVoterOptions and RemoveRaftVoterOptions\nDescription: 1. The cluster id is optional, but users are not aware of its purpose\r\n2. the RPC documentation is also missing\r\n3. there are no integration tests", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add new StateStore APIs\nDescription: Just the new APIs, with no modifications to Streams behaviour. This should keep review nice and simple, as it will just be lots of call-site method renames.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We're currently using libopenssl 3.1.0 which is needed for rsync 3.2.7 on Windows for the Yetus build validation.\r\nHowever, libopenssl 3.1.0 is no longer available for download on the msys2 site -\r\n\r\n{code}\r\nPS D:\\projects\\github\\apache\\hadoop> Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.0-2-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.0-2-x86_64.pkg.tar.zst\r\nInvoke-WebRequest:\r\n404 Not Found\r\n\r\n404 Not Found\r\nnginx/1.26.3\r\n{code}\r\n\r\nThus, we need to upgrade libopenssl to the next available version - 3.1.1 to mitigate this issue.", "output": "Upgrade libopenssl to 3.1.1 for rsync on Windows"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Fix debug logs here. \r\n\r\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java#L350", "output": "[ABFS] Fix logging in FSDataInputStream buffersize as that is not used and confusing the customer"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Support WebIdentityTokenFileCredentialsProvider\nDescription: The current default s3 credential provider chain is set in the order of \r\n{code:java}\r\norg.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,software.amazon.awssdk.auth.credentials.EnvironmentVariableCredentialsProvider,org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider{code}\r\nRefer [code ref |https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml#L1450]for more details.\r\n\r\n \r\n\r\nThis works perfectly fine when used in AWS EC2, EMR Serverless, but not with AWS EKS pods.\r\n\r\n \r\n\r\nFor EKS pods, It is recommended to use\r\n{code:java}\r\nsoftware.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider , software.amazon.awssdk.auth.credentials.ContainerCredentialsProvider (PodIdentity is enabled){code}\r\nWebIdentityTokenFileCredentialsProvider is an AWS credentials provider that enables applications to obtain temporary AWS credentials by assuming an IAM rol\nQ: steveloughran commented on code in PR #7802:\nURL: https://github.com/apache/hadoop/pull/7802#discussion_r2205569618\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSCredentialProviderList.java:\n##########\n@@ -197,7 +197,17 @@ public AwsCredentials resolveCredentials() {\n       } catch (SdkException e) {\n         lastException = e;\n         LOG.debug(\"No credentials provided by {}: {}\",\n-            provider, e.toString(), e);\n+            provider, e);\n\nReview Comment:\n   seems good\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/CredentialProviderListFactory.java:\n##########\n@@ -184,6 +198,8 @@ private static Map initCredentialProvidersMap() {\n         EC2_IAM_CREDENTIALS_V2);\n     v1v2CredentialProviderMap.put(ENVIRONMENT_CREDENTIALS_V1,\n         ENVIRONMENT_CREDENTIALS_V2);\n+    v1v2CredentialProviderMap.put(WEB_IDENTITY_CREDENTIALS_V1,\n\nReview Comment:\n   no need to care about v1 to v2 migration; this wasn't something in use. if it was, we'd have had complaints about", "output": "shameersss1 commented on PR #7802:\nURL: https://github.com/apache/hadoop/pull/7802#issuecomment-3071617785\n\n   > -1 to adding to the default chain, it changes behaviour in a way which isn't obvious to anyone looking at a system.\r\n   > \r\n   > if you deploy on an older release, auth will fail. asking for an explicit declaration of the provider makes it work everywhere.\r\n   \r\n   \r\n   Make sense. I will change the narrative of the PR to Fix credential provider chain List"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add a unique identifier in FNS-Blob user agent to get their usage through telemetry", "output": "ABFS: [FNSOverBlob] Add Distinct String In User Agent to Get Telemetry for FNS-Blob"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add `SparkConf.getAllWithPrefix(String, String => K)` API\nDescription: We need to set some config related to S3 for our inner Spark. The implementation of the function show below.\r\n\r\n{code:java}\r\nprivate def setS3Configs(conf: SparkConf): Unit = {\r\n    val S3A_PREFIX = \"spark.fs.s3a\"\r\n    val SPARK_HADOOP_S3A_PREFIX = \"spark.hadoop.fs.s3a\"\r\n    val s3aConf = conf.getAllWithPrefix(S3A_PREFIX)\r\n    s3aConf\r\n      .foreach(\r\n        confPair => {\r\n          val keyWithoutPrefix = confPair._1\r\n          val oldKey = S3A_PREFIX + keyWithoutPrefix\r\n          val newKey = SPARK_HADOOP_S3A_PREFIX + keyWithoutPrefix\r\n          val value = confPair._2\r\n          (newKey, value)\r\n        })\r\n  }\r\n\r\n{code}\r\n\r\nThese code seems redundant and complicated. The reason is getAllWithPrefix only return the suffix part.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: HADOOP-19467: [ABFS][FnsOverBlob] Fixing Config Name in Documenatation\nDescription: There is a typo in config name added as part of fns-blob documentation.\r\nThis will fix that typo", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Jira for KIP-1188: https://cwiki.apache.org/confluence/x/2IkvFg", "output": "New ConnectorClientConfigOverridePolicy with allowlist of configurations"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "as title", "output": "add compression type and level to log_compaction_test.py"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Replica assignment is a complex issue, as it depends on how a kafka cluster is run, maintained, and used. KAFKA-19507 aims to enhance the default assignment policy, and in my opinion, the best approach is to make the system flexible enough to allow users to customize the policy according to their specific needs", "output": "Allow to configure custom `ReplicaPlacer` implementation"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Kafka Streams processes its assigned tasks in a round robin fashion, however, if a task is considered not ready for processing, it might get skipped. We have observed cases, in which Kafka Streams seems to get stuck on a partition, and restarting the application instance resolves this issue. We suspect, it's related to considering the task for the stuck partition as not ready for processing. (As the ready/not-ready decision is based on in-memory state of KS runtime, bouncing the instance would reset KS runtime into a clean state, unblocking the stuck task.)\r\n\r\nHowever, we currently don't have sufficient logging to reason about this case, and to understand if and why a task might be skipped. We should add more log statement (DEBUG and/or TRACE) to get better visibility. There might be a lurking bug in this logic that we cannot narrow down w/o the corresponding information logging could provide.", "output": "Improve logging about task readiness"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We last upgraded SBT two years ago. Let's upgrade SBT to the latest version.", "output": "Upgrade SBT to 1.11.7"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade os-maven-plugin to 1.7.1 to support riscv64\nDescription: *Problem*\r\n\r\nWhen building Hadoop on the  riscv64 architecture, the current version of os-maven-plugin (1.7.0) fails with the following error:\r\n\r\n    os.detected.arch: unknown\r\n\r\n    org.apache.maven.MavenExecutionException: unknown os.arch: riscv64\r\n\r\nThis indicates that version 1.7.0 does not recognize or support the riscv64 target, causing native or plugin-based build failures.\r\n\r\n*Resolution*\r\n\r\nUpgrading os-maven-plugin to version 1.7.1 resolves the issue. The newer version includes updated architecture detection logic and supports riscv64 without error.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `FlatBuffers` to v25.9.23\nDescription: \nQ: Issue resolved by pull request 254\n[https://github.com/apache/spark-connect-swift/pull/254]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/pull/20530#discussion_r2345531638", "output": "ListOffsetsHandler should kick out the unsupported timestamp type when receiving an UnsupportedVersionException"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Release hadoop-thirdparty 1.5.0", "output": "Release hadoop-thirdparty 1.5.0"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Drop temporary functions in Arrow UDF tests\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analytics-Accelerator: Update LICENSE-binary\nDescription: update LICENSE-binary to include AAL dependency\nQ: ahmarsuhail commented on PR #7982:\nURL: https://github.com/apache/hadoop/pull/7982#issuecomment-3306787342\n\n   @steveloughran PR to add in license binary.", "output": "hadoop-yetus commented on PR #7982:\nURL: https://github.com/apache/hadoop/pull/7982#issuecomment-3307163564\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  46m  8s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  90m 19s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7982/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7982 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets shellcheck shelldocs |\r\n   | uname | Linux 8e1ed683c4d3 5.15.0-151-generic #161-Ubuntu SMP Tue Jul 22 14:25:40 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bf18e340d1dc826e0c031dc1e88e59a58fcb59d7 |\r\n   | Max. process+thread count | 530 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7982/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [CONNECT] Supporting large LocalRelations\nDescription: h1. Problem description\r\n\r\nLocalRelation is a Catalyst logical operator used to represent a dataset of rows inline as part of the LogicalPlan. LocalRelations represent dataframes created directly from Python and Scala objects, e.g., Python and Scala lists, pandas dataframes, csv files loaded in memory, etc.\r\n\r\nIn Spark Connect, local relations are transferred over gRPC using LocalRelation (for relations under 64MB) and CachedLocalRelation (larger relations over 64MB) messages.\r\n\r\nCachedLocalRelations currently have a hard size limit of 2GB, which means that spark users can’t execute queries with local client data, pandas dataframes, csv files of over 2GB.\r\nh1. Design\r\n\r\nIn Spark Connect, the client needs to serialize the local relation before transferring it to the server. It serializes data via an Arrow IPC stream as a single record batch and schema as a json string. It then embeds data and schema as LocalRelation\\{schema,data} proto message.\r\nSmall local relations (under 64MB) are sent directly as part of the ExecutePlanRequest.\r\n\r\n!image-2025-10-15-13-50-04-179.png!\r\n\r\nLarger local relations are first sent to the server via addArtifact and stored in memory or on disk via BlockManager. Then an ExecutePlanRequest is sent containing CachedLocalRelation\\{hash}, where hash is the artifact hash. The server retrieves the cached LocalRelation from the BlockManager via the hash, deserializes it, adds it to the LogicalPlan and then executes it.\r\n\r\n!image-2025-10-15-13-50-44-333.png!", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The landing page for the 3.4.2 docs mostly lists changes that already shipped in 3.4.0. Update this to highlight 3.4.2 changes.", "output": "Update 3.4.2 docs landing page to highlight changes shipped in the release"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Document controller.quorum.auto.join.enable config in upgrade.html\nDescription: \nQ: [~kevinwu2412] Please feel free to unassign it if you don’t have the bandwidth. I will find another contributor to take it over. :)", "output": "[~chia7712] Someone else can take it over and I can give an initial review. It would be good to make others aware of this new functionality for KRaft at a high level."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently in our production environment, the most common problem with Kafka is disk full failure. When the failure occurs, Kafka cannot provide any services, and can only be recovered by manually deleting disk data or expanding the disk capacity and then restarting the service.\r\n\r\nIn contrast, RocketMQ and RabbitMQ already support disk threshold configuration (such as {{diskMaxUsedSpaceRatio}} , {{disk_free_limit}} ). When the disk usage exceeds the limit, the system rejects new produce, but the service remains available.\r\n\r\nSo we hope to implement a similar strategy in Kafka to prevent the disk full failure.\r\n\r\nKIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1201%3A+Add+disk+threshold+config+to+prevent+disk+full+failure", "output": "Add disk usage threshold to prevent disk full failure"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [ReadAheadV2] Implement Read Buffer Manager V2 with improved aggressiveness\nDescription: \nQ: hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3131423891\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 59s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 7 new + 3 unchanged - 9 fixed = 10 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 48s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 22s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 17s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 26s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  81m 35s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 06ad120d8f97 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 28cb97fde1eda2fa096401499e9ee1dbccbdef2b |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3145123984\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  5s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 5 new + 3 unchanged - 9 fixed = 8 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 26s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  79m 28s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   |  |  Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time  Unsynchronized access at ReadBufferManagerV2.java:81% of time  Unsynchronized access at ReadBufferManagerV2.java:[line 617] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c66f9b95f564 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7670846f293f7f629cacdd968ccc1cbd9e70ff25 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/testReport/ |\r\n   | Max. process+thread count | 709 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Establish the foundational build infrastructure for RISC-V CRC32 hardware acceleration support while maintaining full backward compatibility with existing software implementations.", "output": "Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Generator | Fix order of arguments to assertEquals in unit tests\nDescription: This sub-task is intended to fix the order of arguments passed in assertions in test cases within the generator package.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `JaCoCo` to 0.8.14 for official Java 25 support\nDescription: \nQ: Issue resolved by pull request 398\n[https://github.com/apache/spark-kubernetes-operator/pull/398]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Removed build support for the following EOL (End-of-Life) operating system versions (e.g., CentOS 7, Debian 9, etc.).\r\n\r\nCleaned up redundant code and dependency configurations in the Dockerfile related to these operating systems.\r\n\r\nOptimized the build logic to ensure that currently supported OS versions build successfully.", "output": "[JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. "}
{"instruction": "Answer the question based on the bug.", "input": "Title: Acked record on new topic not immediately visible to consumer\nDescription: h2. Steps to reproduce\r\n * program uses a single broker (we see the issue with an in-JVM embedded kafka server)\r\n * create a new topic with 1 partition\r\n * produce a record with {{acks=all}}\r\n * await acknowledgement from the broker\r\n * start a consumer (configured to read from beginning of topic)\r\n * spuriously, _the consumer never sees the record_\r\n\r\nThe problem seems to occur more on a very busy server (e.g. a build server running many tests in parallel). We have not seen it on a modern laptop.\r\n\r\nA delay of 10ms after receiving the acknowledgement, and before starting the consumer, makes the record always visible.\r\n\r\nWe observe this problem in ~1 in 6 runs of [zio-kafka|https://github.com/zio/zio-kafka]'s test suite, when run from a GitHub action. Since we run the test-suite for 3 different JVM versions, more than half of the zio-kafka builds fails due to this issue.\r\nh2. Expected behavior\r\n\r\nA record, produced with {{{}acks=all{}}}, of which producing was acknowledged, should be i\nQ: [~erikvanoosten] Can you provide the code for the consumer application? I am pretty confident that the problem as described in the issue would have been caught by automated tests. I wonder whether the consuming application design is making this occur.", "output": "Yes! The problem happens in the unit test of the open source zio-kafka library. Here are some pointer to tests that sometimes fail to consume any record (they time-out after 2 minutes):\r\n * [https://github.com/zio/zio-kafka/blob/7a97a9a197223dcdc5841c6313ebb486f55cc816/zio-kafka-test/src/test/scala/zio/kafka/consumer/SubscriptionsSpec.scala#L22-L54]\r\n * [https://github.com/zio/zio-kafka/blob/7a97a9a197223dcdc5841c6313ebb486f55cc816/zio-kafka-test/src/test/scala/zio/kafka/consumer/ConsumerSpec.scala#L170-L203]\r\n * [https://github.com/zio/zio-kafka/blob/7a97a9a197223dcdc5841c6313ebb486f55cc816/zio-kafka-test/src/test/scala/zio/kafka/consumer/ConsumerSpec.scala#L250-L289]\r\n\r\n(there are a few more)\r\n\r\nThese test have in common that they follow the steps described in this issue."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation.", "output": "Introduce APPROX_PERCENTILE_COMBINE"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove workaround for protoc on M1 mac and unused property build.platform\nDescription: \nQ: jojochuang merged PR #7783:\nURL: https://github.com/apache/hadoop/pull/7783", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove broken Centos 7 C++ precommit checks from CI\nDescription: The Netty update in HADOOP-19335 required a newer grpc compiler which requires newer libraries than even the add-on C++ environment has in Centos 7.\r\n\r\nCentos 7 is EOL, and fixing Centos 7 would require rebuilding and replacing the C++ library at which point why even bother ?", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix race condition issue related to ObservedMetrics\nDescription: In Spark Connect environment, QueryExecution#observedMetrics can be called by two threads concurrently.\r\n\r\n \r\n\r\nThread1(ObservationManager)\r\n{code:java}\r\nprivate def tryComplete(qe: QueryExecution): Unit = {\r\n  val allMetrics = qe.observedMetrics\r\n  qe.logical.foreach {\r\n    case c: CollectMetrics =>\r\n      allMetrics.get(c.name).foreach { metrics =>\r\n        val observation = observations.remove((c.name, c.dataframeId))\r\n        if (observation != null) {\r\n          observation.setMetricsAndNotify(metrics)\r\n        }\r\n      }\r\n    case _ =>\r\n  }\r\n}\r\n{code}\r\nThread2(SparkConnectPlanExecution)\r\n{code:java}\r\nprivate def createObservedMetricsResponse(\r\n    sessionId: String,\r\n    observationAndPlanIds: Map[String, Long],\r\n    dataframe: DataFrame): Option[ExecutePlanResponse] = {\r\n  val observedMetrics = dataframe.queryExecution.observedMetrics.collect {\r\n    case (name, row) if !executeHolder.observations.contains(name) =>\r\n      val values = SparkConnectPlanExecution.toObservedMetricsVa\nQ: Issue resolved by pull request 52575\n[https://github.com/apache/spark/pull/52575]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Enable ProducerPerformance to abort transaction randomly\nDescription: While testing KAFKA-18884, I noticed there are no official tools available to measure the performance of aborted transactions. `ProducerPerformance` should serve this purpose by allowing us to configure a ratio of transactions to abort\nQ: Very nice. Of course, this will have an impact on consumption too, but adding this to `ProducerPerformance` is great.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix observations on Spark Connect with plan cache\nDescription: \nQ: Issue resolved by pull request 52616\r\nhttps://github.com/apache/spark/pull/52616", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Make a new hadoop-thirdparty release to go with the 3.4.2 release", "output": "Release Hadoop Third-Party 1.4.0"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Cloning sessions is a common operation in Spark applications (e.g., for creating isolated execution contexts). The current approach of duplicating cached data can significantly increase memory footprint, especially when:\r\n * Sessions are cloned frequently\r\n * Cached relations contain large datasets\r\n * Multiple clones exist simultaneously\r\n\r\n \r\n\r\nAn improvement can be made by implementing reference counting as opposed to data replication for the block manager entries that reference cached local relations.", "output": "Reduce memory footprint from cached local relations upon cloning"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Investigate stalling share consumer poll when empty response is returned early\nDescription: This is related to https://issues.apache.org/jira/browse/KAFKA-19259. If an empty ShareFetch response is returned before the poll delay has elapsed, it seems possible that the application thread will continue to wait rather than promptly sending another ShareFetch request.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Recently, `OracleJoinPushdownIntegrationSuite` frequently fails.\r\n\r\nhttps://github.com/beliefer/spark/actions/runs/18904457292/job/53959322233#logs\r\n{code:java}\r\n[info] org.apache.spark.sql.jdbc.v2.join.OracleJoinPushdownIntegrationSuite *** ABORTED *** (10 minutes, 15 seconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 599 times over 10.012648813883333 minutes. Last failure message: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info]   https://docs.oracle.com/error-help/db/ora-12541/. (DockerJDBCIntegrationSuite.scala:214)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at", "output": "Fix Flaky Test:`OracleJoinPushdownIntegrationSuite`"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update to ApacheDS 2.0.0.AM27 and ldap-api 2.1.7\nDescription: ApacheDS 2.0.0AM26 (or at least its test integration) is incompatible with JDK21+.\r\n\r\nUpdate to 2.0.0.AM27 and the corresponding ldap-api version.\r\n\r\nThis also requires migrating some dependent tests from Junit 4 to Junit 5.\r\n\r\n{noformat}\r\n[ERROR] org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler -- Time elapsed: 1.426 s <<< ERROR!\r\njava.lang.NoSuchMethodError: 'void sun.security.x509.X509CertInfo.set(java.lang.String, java.lang.Object)'\r\n        at org.apache.directory.server.core.security.CertificateUtil.setInfo(CertificateUtil.java:96)\r\n        at org.apache.directory.server.core.security.CertificateUtil.generateSelfSignedCertificate(CertificateUtil.java:194)\r\n        at org.apache.directory.server.core.security.CertificateUtil.createTempKeyStore(CertificateUtil.java:337)\r\n        at org.apache.directory.server.factory.ServerAnnotationProcessor.instantiateLdapServer(ServerAnnotationProcessor.java:158)\r\n        at org.apache.directory.server.factor\nQ: hadoop-yetus commented on PR #7628:\nURL: https://github.com/apache/hadoop/pull/7628#issuecomment-2812365688\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 16s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  36m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 45s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 16s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +0 :ok: |  spotbugs  |   0m 40s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 56s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 56s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  3s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  15m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 16s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +0 :ok: |  spotbugs  |   0m 38s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 22s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 34s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m  5s |  |  hadoop-auth in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  2s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 209m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7628/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7628 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux 33f8fa3aa330 5.15.0-134-generic #145-Ubuntu SMP Wed Feb 12 20:08:39 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 67f932e041415206999492760101b4d1429a3211 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7628/1/testReport/ |\r\n   | Max. process+thread count | 700 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-auth U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7628/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on PR #7628:\nURL: https://github.com/apache/hadoop/pull/7628#issuecomment-2816562572\n\n   LGTM."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "KAFKA-19716", "output": "Backport KAFKA-19716 fix to 4.0 and 4.1"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This sub-task is intended to fix the order of arguments passed in assertions in test cases within the tools package.", "output": "Tools | Fix order of arguments to assertEquals in unit test"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: OOM when loading large uncompacted __consumer_offsets partitions with transactional workload\nDescription: When loading a large poorly compacted __consumer_offsets partition with a transactional workload (alternating transaction offset commits and commit markers), we create and discard lots of TimelineHashMaps. These accumulate in the SnapshotRegistry's rollback mechanism. Overall, memory usage is linear in the size of the partition.\r\n\r\nWe can commit the snapshot every N offsets in {{{}CoordinatorLoaderImpl{}}}, to allow the memory to be reclaimed.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK 17] Implementation of JAXB-API has not been found on module path or classpath\nDescription: When i try to run the *org.apache.hadoop.yarn.webapp.TestWebApp* tests over JDK17 i am facing the following errors:\r\n\r\n{noformat}\r\nCaused by: A MultiException has 2 exceptions.  They are:\r\n1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver\r\n{noformat}\r\n\r\nRepro steps:\r\n- run: ./start-build-env.sh ubuntu_24\r\n- in docker run: mvn clean install -Pjdk17+ -T 32 -DskipTests \r\n- in hadoop-yarn-common modul run: mvn test -Dtest=org.apache.hadoop.yarn.webapp.TestWebApp\r\n\r\nI found a similar error here:\r\nhttps://issues.apache.org/jira/browse/HDDS-5068\r\n\r\nBased on my understanding the problem is the JDK11+ environments does not have the JAXB runtime, so we have to explicit provide them. Maybe this is just a temporal solution, till the whole Jakarta upgrade can be done.\r\n\r\n{panel:title=Full error log}\r\n{code}\r\n[ERROR] testRobotsText  Time elapsed: 0.064 s  (ApplicationHandler.java:261)\r\n\tat org.glassfish.jersey.servlet.WebComponent.(WebComponent.java:314)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:154)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:360)\r\n\tat javax.servlet.GenericServlet.init(GenericServlet.java:244)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:632)\r\n\t... 104 more\r\nCaused by: javax.xml.bind.JA", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I believe for this to occur you need\r\n # transformValues on a KTable, followed by a KTable join or leftJoin\r\n # The transformValues is not materialized (no store name given)\r\n # The transformValues accesses at least one extra store\r\n\r\nTested on 3.6.1 and 3.9.1\r\n\r\nExample code:\r\n{code:java}\r\n@Component\r\nclass TestCase {\r\n    private static final StoreBuilder> TRANSFORMER_STORE =\r\n          Stores.timestampedKeyValueStoreBuilder(\r\n                Stores.persistentTimestampedKeyValueStore(\"transformer-store\"),\r\n                Serdes.String(),\r\n                Serdes.String()\r\n          );\r\n\r\n    private final StreamsBuilder streamsBuilder;\r\n\r\n    TestCase(StreamsBuilder streamsBuilder) {\r\n       this.streamsBuilder = streamsBuilder;\r\n    }\r\n\r\n    @PostConstruct\r\n    void configure() {\r\n       streamsBuilder.addStateStore(TRANSFORMER_STORE);\r\n\r\n       var aggregateTable = streamsBuilder\r\n             .stream(\"input\", Consumed.with(Serdes.String(), Serdes.String()).withName(\"input-to-stream\"))\r\n             .toTable(Named.as(\"to-table\"), MaterializedAs.keyValue(\"aggregate-store\",\r\n                   Serdes.String(), Serdes.String()))\r\n             .transformValues(MyTransformer::new,\r\n                   Materialized.with(Serdes.String(), Serdes.String()),\r\n                   Named.as(\"my-transformer\"), TRANSFORMER_STORE.name());\r\n\r\n       aggregateTable\r\n             .join(aggregateTable,\r\n                   (value, _) -> value,\r\n                   Named.as(\"after-transformer\"),\r\n                   Materialized.>as(\"after-transformer-store\")\r\n                         .withKeySerde(Serdes.String())\r\n                         .withValueSerde(Serdes.String()))\r\n             .toStream(Named.as(\"aggregate-to-stream\"))\r\n             .to(\"output\", Produced.with(Serdes.String(), Serdes.String()).withName(\"output-to-topic\"));\r\n\r\n       System.out.println(streamsBuilder.build().describe().toString());\r\n    }\r\n\r\n    private static class MyTransformer implements ValueTransformerWith", "output": "Kafka Streams join after unmaterialized transformValues on KTable with extra store fails"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Reduce memory footprint from cached local relations upon cloning\nDescription: Cloning sessions is a common operation in Spark applications (e.g., for creating isolated execution contexts). The current approach of duplicating cached data can significantly increase memory footprint, especially when:\r\n * Sessions are cloned frequently\r\n * Cached relations contain large datasets\r\n * Multiple clones exist simultaneously\r\n\r\n \r\n\r\nAn improvement can be made by implementing reference counting as opposed to data replication for the block manager entries that reference cached local relations.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "If I try to build site with Java 17, I get an IllegalArgumentException from analyze-report\r\n{code}\r\n$ mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site\r\n...\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report: IllegalArgumentException -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:347)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute", "output": "Maven site task fails with Java 17"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: NPE in `Processor#init()` accessing state store\nDescription: As reported on the dev mailing list, we introduced a regression bug via https://issues.apache.org/jira/browse/KAFKA-13722 in 4.1 branch. We did revert the commit ([https://github.com/apache/kafka/commit/f13a22af0b3a48a4ca1bf2ece5b58f31e3b26b7d]) for 4.1 release, and want to fix-forward for 4.2 release.\r\n\r\nStacktrace:\r\n{code:java}\r\n15:29:05 ERROR [STREAMS] KafkaStreams - stream-client [app1] Encountered the following exception during processing and the registered exception handler opted to SHUTDOWN_CLIENT. The streams client is going to shut down now.\r\norg.apache.kafka.streams.errors.StreamsException: failed to initialize processor random-value-processor\r\n        at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:132) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:141) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.StreamTask.initializeTopology(StreamTask.java:1109) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.StreamTask.completeRestoration(StreamTask.java:297) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.TaskManager.tryToCompleteRestoration(TaskManager.java:955) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.StreamThread.initializeAndRestorePhase(StreamThread.java:", "output": "Patch Available"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Build multi-arch hadoop image\nDescription: Build {{apache/hadoop}} Docker image for both amd64 and arm64.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce Catalyst server-side geospatial execution classes\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix scheduled job for numpy 2.1.3\nDescription: \nQ: Issue resolved by pull request 52633\n[https://github.com/apache/spark/pull/52633]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: When TaskSchedulerImpl searches for an executor to schedule an unschedulable task, it should exclude executors that are pending to remove\nDescription: Call kill executor\r\n\r\n!image-2025-10-30-17-22-25-127.png|width=1255,height=241!\r\n\r\nContainer 18 didn't kill it\r\n \r\n!image-2025-10-30-17-24-03-632.png|width=1087,height=183!\r\n\r\n \r\n\r\nPending container causing  task can't be scheduled .", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We create—and wait on—{{{}CheckAndUpdatePositionsEvents{}}} in {{updateFetchPositions()}} even though, in a stable system, 99.9%+ of the time there are no updates to perform.", "output": "Reduce number of events generated in AsyncKafkaConsumer.updateFetchPositions()"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove addMetric(MetricName metricName, Measurable measurable) method.\nDescription: see：https://github.com/apache/kafka/pull/20543#discussion_r2371009016", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "HADOOP-19455. S3A: Enable logging of SDK client metrics\r\n\r\nTo log the output of the AWS SDK metrics, set the log\r\n`org.apache.hadoop.fs.s3a.DefaultS3ClientFactory` to `TRACE`.", "output": "HADOOP-19455. S3A: Enable logging of SDK client metrics"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Build multi-arch hadoop image\nDescription: Build {{apache/hadoop}} Docker image for both amd64 and arm64.\nQ: slfan1989 commented on PR #8023:\nURL: https://github.com/apache/hadoop/pull/8023#issuecomment-3385915162\n\n   LGTM.", "output": "slfan1989 commented on PR #8023:\nURL: https://github.com/apache/hadoop/pull/8023#issuecomment-3385930578\n\n   @adoroszlai Thank you for the contribution. If there are no additional comments, I’ll proceed to merge this PR shortly."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Jackson to 2.16.0\nDescription: \nQ: Oh didn't check existing jira, will close this as duplicate. Upgraded to this version as it was non-vulnerable and did not have any breaking changes.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Deprecated the config log.dir\nDescription: Currently, {{log.dir}} and {{log.dirs}} exhibit similar behavior. With the changes introduced in KIP-1161, we now have an opportunity to simplify and clean up their usage for better consistency.\r\n\r\nsee the discussion: https://github.com/apache/kafka/pull/20334#discussion_r2291866372", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Spark Connect Python client does not throw a proper error when creating a dataframe from an empty pandas dataframe\nDescription: Spark Connect Python client does not throw a proper error when creating a dataframe from a pandas dataframe with a index and empty data.\r\n\r\nGenerally, spark connect client throws a client-side error `[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from an empty dataset`. when creating a dataframe without data, for example via\r\n{quote}spark.createDataFrame([]).show()\r\n{quote}\r\nor\r\n{quote}df = pd.DataFrame()\r\nspark.createDataFrame(df).show(){quote}\r\nor\r\n{quote}df = pd.DataFrame(\\{\"a\": []})\r\nspark.createDataFrame(df).show(){quote}\r\nThis does not happen when pandas dataframe has an index but no data, e.g.\r\n{quote}df = pd.DataFrame(index=range(5))\r\nspark.createDataFrame(df).show(){quote}\r\nWhat happens instead is that the dataframe is successfully converted to a LocalRelation on the client, is sent to the server, but the server then throws the following exception: `INTERNAL_ERROR: Input data for LocalRelation does not produce a schema. SQLSTATE: XX000`. XX000 is an internal error sql state and the error is not actionable enough for the user.\r\nThis should be fixed.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-dynamometer-infra.\nDescription: \nQ: hadoop-yetus commented on PR #7588:\nURL: https://github.com/apache/hadoop/pull/7588#issuecomment-2788154603\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m  9s | [/results-checkstyle-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-infra.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-infra.txt) |  hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 19s |  |  hadoop-dynamometer-infra in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  68m 46s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7588 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f46e86b46d53 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1b6245c1788616bf1f05550cc65575cc64b0b6c8 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/1/testReport/ |\r\n   | Max. process+thread count | 557 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra U: hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7588:\nURL: https://github.com/apache/hadoop/pull/7588#issuecomment-2790033076\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  1s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 37s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 29s |  |  hadoop-dynamometer-infra in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 113m 25s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7588 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2323db061423 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f01f24d8bb816e1a22bba65bea25a1cb29ea7985 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/2/testReport/ |\r\n   | Max. process+thread count | 699 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra U: hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade libopenssl to 3.1.1 for rsync on Windows\nDescription: We're currently using libopenssl 3.1.0 which is needed for rsync 3.2.7 on Windows for the Yetus build validation.\r\nHowever, libopenssl 3.1.0 is no longer available for download on the msys2 site -\r\n\r\n{code}\r\nPS D:\\projects\\github\\apache\\hadoop> Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.0-2-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.0-2-x86_64.pkg.tar.zst\r\nInvoke-WebRequest:\r\n404 Not Found\r\n\r\n404 Not Found\r\nnginx/1.26.3\r\n{code}\r\n\r\nThus, we need to upgrade libopenssl to the next available version - 3.1.1 to mitigate this issue.\nQ: GauthamBanasandra opened a new pull request, #7487:\nURL: https://github.com/apache/hadoop/pull/7487\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   * We're currently using libopenssl 3.1.0 which\r\n     is needed for rsync 3.2.7 on Windows for the\r\n     Yetus build validation.\r\n   * However, libopenssl 3.1.0 is no longer\r\n     available for download on the msys2 site.\r\n   * This PR upgrades libopenssl to the next\r\n     available version - 3.1.1 to mitigate this\r\n     issue.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   * Jenkins CI validation.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "GauthamBanasandra commented on PR #7487:\nURL: https://github.com/apache/hadoop/pull/7487#issuecomment-2708470616\n\n   Jenkins CI validation for Windows is in progress - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-win10-x86_64/839/."}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade nimbus-jose-jwt due to CVE-2025-53864\nDescription: https://www.cve.org/CVERecord?id=CVE-2025-53864\nQ: Hi [~fanningpj] , working on a patch for this. I had raised Jira for the same https://issues.apache.org/jira/browse/HADOOP-19632.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Backport KAFKA-19716 fix to 4.0 and 4.1\nDescription: KAFKA-19716", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We get warnings like \r\n{noformat}\r\nWARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module\r\nWARNING: Restricted methods will be blocked in a future release unless native access is enabled\r\n{noformat}\r\non JDK21+.\r\n\r\nWhile this works now, it's better to add this early, and avoid breakage later.\r\n\r\nThis also cleans up the the console output.", "output": "Add --enable-native-access=ALL-UNNAMED JVM option"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SHOW DATABASES AS JSON\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Read Buffer Manager used today was introduced way back and has been stable for quite a while.\r\nRead Buffer Manager to be introduced as part of https://issues.apache.org/jira/browse/HADOOP-19596 will introduce many changes incrementally over time. While the development goes on and we are able to fully stabilise the optimized version we need the current flow to be functional and undisturbed. \r\n\r\nThis work item is to isolate that from new code by refactoring ReadBufferManager class to have 2 different implementations with same public interfaces: ReadBufferManagerV1 and ReadBufferManagerV2.\r\n\r\nThis will also introduce new configs that can be used to toggle between new and old code.", "output": "ABFS: [ReadAheadV2] Refactor ReadBufferManager to isolate new code with the current working code"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: AWS4SignerType failing after S3 service changes\nDescription: With the latest change to the AWS S3 integrity, all existing releases of hadoop using the v2 SDK \r\n    fs.s3a.bucket.signing-algorithm\r\n    AWS4SignerType\r\n  \r\n{code}\r\n\r\n\r\nrejected at far end by\r\n{code}\r\n\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: Missing required header for this request: x-amz-content-sha256 (Service: S3, Status Code: 400, Request ID: 7M0MNNE8NBAGZWFH, Extended Request ID: A/0oyoZ52GLklg+GuV70vqPNEAK350ZF1rTJaNtSajWSXLT5bUCC/Gu6VbN8Pu+AeaMboIIGyGHeFLXOKUwdVQ==)\r\n{code}\r\n\r\n\r\nPresumably some change in 2.30.0 restored compatibility, but this means that all shipping 3.4.x releases do not work.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Netty to 4.2.6.Final\nDescription: \nQ: Issue resolved by pull request 52552\n[https://github.com/apache/spark/pull/52552]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We need to add {{sql_confs}} to {{DefineOutput}} in SDP to support following use case\r\n\r\nSQL file #1\r\n{code:java}\r\nSET some_conf = some_value;\r\nCREATE STREAMING TABLE st; {code}\r\nSQL file #2\r\nCREATE FLOW INSERT INTO st FROM ....\r\n \r\nSince we don't store {{sql_confs}} at the Dataset level, the conf would be ignored. We should store it at dataset level and will merge the {{sql_conf}} stored at the dataset level to its corresponding flow when we construct the DataflowGraph.", "output": "Add support to track `spark_conf` at the dataset level"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Incorrect jitter value in StreamsGroupHeartbeatRequestManager and AbstractHeartbeatRequestManager\nDescription: see https://github.com/apache/kafka/pull/14873#discussion_r2442794887, \r\n\r\nIn below code snippets\r\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L116\r\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/StreamsGroupHeartbeatRequestManager.java#L333\r\n\r\nwe use max.poll.interval.ms (default 300000) as the jitter value. Although RequestState.remainingBackoffMs guards with Math.max(0, …), which won’t make the value become negative, using a jitter that isn’t in (0–1) is unexpected.\r\n\r\nIn addition, we should validate that ExponentialBackoff receives a jitter strictly within (0, 1) to prevent this scenario in the future.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Make ResolvedCollation evaluable\nDescription: In this PR I propose to make ResolvedCollation evaluable. By making ResolvedCollation evaluable, it can now pass the canEvaluateWithinJoin check, allowing the optimizer to use efficient hash joins for queries with inline COLLATE in join conditions (before this change we would fallback to BroadcastNestedLoopJoin).\nQ: Issue resolved by pull request 52779\n[https://github.com/apache/spark/pull/52779]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{code:java}\r\nList data = Arrays.asList(\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95)\r\n);\r\n \r\nStructType schema = DataTypes.createStructType(new StructField[] {\r\nDataTypes.createStructField(\"timestamp\", DataTypes.StringType, false),\r\nDataTypes.createStructField(\"id\", DataTypes.StringType, false),\r\nDataTypes.createStructField(\"value\", DataTypes.DoubleType, false)\r\n});\r\n \r\nDataset df = spark.createDataFrame(data, schema);\r\n \r\n// Show the input data\r\nSystem.out.println(\"Input data:\");\r\ndf.show();\r\n \r\n// Perform the aggregation\r\nDataset result = df.groupBy(\"id\")\r\n.agg(\r\navg(\"value\").as(METADATA_COL_METRICVALUE),\r\nsum(\"value\").as(METADATA_COL_SUM_VALUE)\r\n);\r\n \r\n// Show the results\r\nSystem.out.println(\"Aggregation results:\");\r\nresult.show();\r\n \r\n// Collect the results\r\nList results = result.collectAsList();\r\n \r\n// Print the results\r\nSystem.out.println(\"Number of results: \" + results.size());\r\nfor (Row row : results) {\r\nSystem.out.println(\"Metric value: \" + row.getDouble(row.fieldIndex(METADATA_COL_METRICVALUE)));\r\nSystem.out.println(\"Sum value: \" + row.getDouble(row.fieldIndex(METADATA_COL_SUM_VALUE)));\r\n}\r\n \r\n// Verify the results\r\nassertEquals(1, results.size(), \"Expected 1 aggregated result\");\r\n \r\nRow resultRow = results.get(0);\r\ndoublesumValue = resultRow.getDouble(resultRow.fieldIndex(METADATA_COL_SUM_VALUE));\r\ndoubleexpectedSum = 799.6; // 8 * 99.95\r\n \r\nSystem.out.println(\"Expected sum: \" + expectedSum);\r\nSystem.out.println(\"Actual sum: \" + sumValue);\r\nSystem.out.println", "output": "Spark aggregation is incorrect (floating point error)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Create lean docker image\nDescription: Create a new docker image based on the lean tarball.\r\n\r\nhadoop-3.4.2-lean.tar.gz\nQ: steveloughran commented on PR #8013:\nURL: https://github.com/apache/hadoop/pull/8013#issuecomment-3371636931\n\n   I did check the url resolved, BTW.\r\n   \r\n   Note that in #7980 packaging will change where we move hadoop-aws and hadoop azure to common/lib, with all dependencies except bundle.jar; that'll come iff you do a \"-Paws-sdk\" build. And the other cloud modules will come in if you explicitly ask for them.\r\n   \r\n   Still a WiP; hope to be done ASAP with hadoop 3.4.3 like this. No more \"let's strip the build\" work, instead just choose the build options for a release.", "output": "adoroszlai commented on PR #8013:\nURL: https://github.com/apache/hadoop/pull/8013#issuecomment-3371660323\n\n   Thanks @steveloughran for the review.  Pushed 4cb319a9a98350bb0711029cf13c170f3c9ce043 to `docker-hadoop-3.4.2-lean`."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The test `SmokeTestDriverIntegrationTest` failed many time in recent runs:\r\n\r\n[https://develocity.apache.org/scans/tests?search.names=Git%20branch&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&search.values=trunk&tests.container=org.apache.kafka.streams.integration.SmokeTestDriverIntegrationTest&tests.sortField=FLAKY]\r\n\r\n \r\n\r\nSample run: https://github.com/apache/kafka/actions/runs/17471808549/job/49622305615?pr=20266", "output": "Flaky Test org.apache.kafka.streams.integration.SmokeTestDriverIntegrationTest"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [ABFS] AzureADAuthenticator should be able to retry on UnknownHostException\nDescription: When Hadoop is requested to perform operations against ADLS Gen2 storage, *AbfsRestOperation* attempts to obtain an access token from Microsoft. Underneath the hood, it uses a simple *java.net.HttpURLConnection* HTTP client.\r\n\r\nOccasionally, environments may run into network intermittent issues, including DNS-related {*}UnknownHostException{*}. Technically, the HTTP client throws *IOException* whose cause is {*}UnknownHostException{*}. *AzureADAuthenticator* in its turn catches {*}IOException{*}, sets *httperror = -1* and then checks whether the error is recoverable and can be retried. However, it's neither an instance of {*}MalformedURLException{*}, nor an instance of {*}FileNotFoundException{*}, nor a recoverable status code ({*}= 500 && != 501 && != 505{*}), hence a retry never occurs which is sensitive for our project causing problems with state recovery.\r\n\r\nThe final exception stack trace on the client side looks as follows (Apache Spark application, tenant ID is redacted):\r\n{code\nQ: Hi [~Serhii Nesterov] \r\nThanks for reporting.\r\nHowever I do see UnknownHostException getting retried. \r\nTo confirm on this, I ingested an UnknownHostException [here just after token fetch|https://github.com/apache/hadoop/blob/d3690f07377e792b2a5d4a78b082d38e80497d44/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java#L349] and saw request getting retried in debug logs.\r\n\r\nCan you please check once again.", "output": "services.AbfsClient (AbfsRestOperation.java:signRequest(565)) - Authenticating request with OAuth2 access token\r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenUsingClientCreds(112)) - AADToken: starting to fetch token using client creds for client ID \r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenCall(367)) - Retrying getTokenSingleCall. RetryCount = 1\r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenCall(367)) - Retrying getTokenSingleCall. RetryCount = 2\r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenCall(367)) - Retrying getTokenSingleCall. RetryCount = 3\r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenCall(367)) - Retrying getTokenSingleCall. RetryCount = 4\r\noauth2.AzureADAuthenticator (AzureADAuthenticator.java:getTokenCall(367)) - Retrying getTokenSingleCall. RetryCount = 5\r\n\r\nThe exception which you shared above is thrown outside retry loop as follows\r\n\r\nAuth failure: HTTP Error -1; url='' AzureADAuthenticator.getTokenCall threw java.net.UnknownHostException : login.microsoftonline.com\r\norg.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator$HttpException: HTTP Error -1; url='' AzureADAuthenticator.getTokenCall threw java.net.UnknownHostException : login.microsoftonline.com\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:410)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:323)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:289)\r\nat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:494)\r\nat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:465)"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The coordinator runtime can be overly optimistic about compression ratios when filling up batches and can often overflow `max.message.bytes` of the topic. When this happens, all the group coordinator requests that wrote to the batch fail with an UNKNOWN_SERVER_ERROR. This error is non-retriable and disruptive to client applications.", "output": "RecordTooLargeExceptions in group coordinator when offsets.topic.compression.codec is used"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove broken Centos 7 C++ precommit checks from CI\nDescription: The Netty update in HADOOP-19335 required a newer grpc compiler which requires newer libraries than even the add-on C++ environment has in Centos 7.\r\n\r\nCentos 7 is EOL, and fixing Centos 7 would require rebuilding and replacing the C++ library at which point why even bother ?\nQ: pan3793 commented on PR #7493:\nURL: https://github.com/apache/hadoop/pull/7493#issuecomment-2710815286\n\n   This sounds like a silent breaking change. I know CentOS 7 has been sunset, but ... we have dozens of thousands of Hadoop nodes running on CentOS 7 as of today ...\r\n   \r\n   Please at least discuss the drop support of CentOS 7 in the mailing list ...", "output": "stoty commented on PR #7493:\nURL: https://github.com/apache/hadoop/pull/7493#issuecomment-2711075128\n\n   I have started a discussion on dev@hadoop.apache.org , @pan3793 ."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade Protobuf 3.25.5 for docker images\nDescription: HADOOP-19289 upgraded protobuf-java 3.25.5, we should use same version for protobuf installed in docker images.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Apache Client Connection Pool Relook\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve the put/merge operation in ListState when t here are multiple values\nDescription: In SS TWS, when we do the {{put(array)}} operation in liststate, we put the first element and then merge the remaining elements one by one. so if we want to put an array with 100 elements, it means we need do 1 put + 99 merges. This can result in worse performance than a single put operation for the entire array.\r\n\r\n \r\n\r\nSimilar, we have the same issue in {{merge(array)}}", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Let approx_top_k_accumulate/combine/estimate handle NULLs\nDescription: As a follow-up of https://issues.apache.org/jira/browse/SPARK-53947, let approx_top_k_accumulate/combine/estimate handle NULLs.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Delete Operation\nDescription: We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Ingress Related operations on blob Endpoint (Create + Append + Flush filesystem calls)\r\n\r\nThis Jira tracks implementing these tests.\nQ: manika137 opened a new pull request, #7376:\nURL: https://github.com/apache/hadoop/pull/7376\n\n   ### Description of PR\r\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19446\r\n   We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint. This PR adds tests for the negative scenarios identified for the delete operation particularly.\r\n   \r\n   Yet to add the test results...", "output": "hadoop-yetus commented on PR #7376:\nURL: https://github.com/apache/hadoop/pull/7376#issuecomment-2648641086\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 49s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 118m 23s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7376/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7376 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 9fb3e2ec763a 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6262ad8b1f8e25aeffd762fed8491b5ecc581768 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7376/1/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7376/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: make container build work on macOS Tahoe\nDescription: macOS Tahoe includes a native container daemon and runtime and it is supposed to be near feature-compatible with docker. In principle, it should be possible to run the container build using the container command line on macOS Tahoe.\r\n\r\nIt would be great if we can add this functionality to the build script so folks can build on macOS natively without creating another VM.\nQ: This could be someone's good hack project.", "output": "this means you can run linux in it? cool. puts it on a par with windows -though that has the advantage you can just dual boot the machine to linux or just replace windows entirely"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix `initialize` to add `CREATE` option additionally in `DriverRunner`\nDescription: When submitting jobs to standalone cluster using restful api, we get errors like:\r\n\r\n```\r\n25/10/30 16:02:59 INFO DriverRunner: Killing driver process!\r\n25/10/30 16:02:59 INFO CommandUtils: Redirection to /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stdout closed: Stream closed\r\n25/10/30 16:02:59 WARN Worker: Driver driver-20251030160259-0020 failed with unrecoverable exception: java.nio.file.NoSuchFileException: /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stderr\r\n\r\n```\r\n\r\nWe need to fix the usage of `Files.writeString` in DriverRunner\nQ: I'd like to fix it.", "output": "Issue resolved by pull request 52789\n[https://github.com/apache/spark/pull/52789]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [ABFS] Enable rename and create recovery from client transaction id over DFS endpoint\nDescription: We have implemented create and rename recovery using client transaction IDs over the DFS endpoint ([HADOOP-19450] [ABFS] Rename/Create path idempotency client-level resolution - ASF JIRA). Since the backend changes were not fully rolled out, we initially implemented the changes with the flag disabled. With this update, we aim to enable the flag, which will start sending client transaction IDs. In case of a failure, we will attempt to recover from the failed state if possible. Here are the detailed steps and considerations for this process:\r\n\r\n1. **Implementation Overview**: We introduced a mechanism for create and rename recovery via client transaction IDs to enhance reliability and data integrity over the DFS endpoint. This change was initially flagged as disabled due to incomplete backend rollouts.\r\n\r\n2. **Current Update**: With the backend changes now in place, we are ready to enable the flag. This will activate the sending of client transaction IDs, allowing us to track and manage transactions more effectively.\r\n\r\n3. **Failure Recovery**: The primary advantage of enabling this flag is the potential for recovery from failed states. If a transaction fails, we can use the client transaction ID to attempt a recovery, minimizing data loss and ensuring continuity.\r\n\r\n4. **Next Steps**: We will proceed with enabling the flag and closely monitor the system's performance. Any issues or failures will be documented and addressed promptly to ensure a smooth transition.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Enable messageTemplate propagation to SparkThrowable \nDescription: The goal is to add a new *default* method, getDefaultMessageTemplate, to the public SparkThrowable interface. This gives clients a consistent, machine-readable *default* template for error rendering, while leaving them free to localize or otherwise transform the message.\nQ: I am working on this", "output": "Issue resolved by pull request 52559\n[https://github.com/apache/spark/pull/52559]"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation\nDescription: This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-V platforms supporting the Zbc extension (CLMUL instructions) in bulk_crc32_riscv.c.\r\nKey changes:\r\n * Implements optimized CRC32 and CRC32C routines using CLMUL instructions for zlib and Castagnoli polynomials.\r\n * Automatically switches to hardware acceleration when Zbc is available, otherwise falls back to generic table-based software implementation.\r\n * Maintains compatibility with platforms lacking Zbc support.\r\n\r\nThis optimization improves CRC performance on RISC-V CPUs with Zbc extension.\nQ: PeterPtroc opened a new pull request, #7896:\nURL: https://github.com/apache/hadoop/pull/7896\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   - Implements a CLMUL-based 16B fold + Barrett reduction algorithm, adapted from riscv-crc32-clmul.\r\n   - True interleaved (round-robin) multi-block pipeline (1–3 blocks) to increase ILP.\r\n   - Small buffers and tails fall back to the existing table-based software path.\r\n   - Runtime gating:\r\n     - Double-checked detection: “zbc” in /proc/cpuinfo AND a SIGILL-safe CLMUL probe.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - Build (native profile):\r\n     - mvn -pl hadoop-common-project/hadoop-common -am -Pnative -DskipTests clean install\r\n   - Run benchmark:\r\n     - mvn -Pnative -DskipTests -Dexec.classpathScope=test -Dexec.mainClass=org.apache.hadoop.util.Crc32PerformanceTest \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7896:\nURL: https://github.com/apache/hadoop/pull/7896#issuecomment-3222853817\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  27m 44s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  7s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 45s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 105m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 47s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 12s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  24m 53s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 47s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 223m 28s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7896 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 4727479d09e6 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 58baa6eb96c75dd288767f650918073c5c094a6f |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/testReport/ |\r\n   | Max. process+thread count | 3133 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support getCatalogs for SparkConnectDatabaseMetaData\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: bit_count results incorrect results for negative byte/short/int values\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob] Remove Duplicates from Blob Endpoint Listing Across Iterations\nDescription: On FNS-Blob, List Blobs API is known to return duplicate entries for the non-empty explicit directories. One entry corresponds to the directory itself and another entry corresponding to the marker blob that driver internally creates and maintains to mark that path as a directory. We already know about this behaviour and it was handled to remove such duplicate entries from the set of entries that were returned as part current list iterations.\r\n\r\nDue to possible partition split if such duplicate entries happen to be returned in separate iteration, there is no handling on this and caller might get back the result with duplicate entries as happening in this case. The logic to remove duplicate was designed before the realization of partition split came.\r\n\r\nThis PR fixes this bug", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The TransformWithState StateServer's {{parseProtoMessage}} method uses {{read}} (InputStream/FilterInputStream) which only reads all available data and may not return the full message. We should be using the [readFully DataInputStream API|https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/DataInput.html#readFully(byte%5B%5D)] instead, which will continue fetching until it fills up the provided buffer.\r\n\r\nIn addition to the linked API above, this StackOverflow post also illustrates the difference between the two APIs: [https://stackoverflow.com/a/25900095]\r\n\r\nWithout this change, it is possible for the state server to fail to fully read large proto messages (e.g., those containing a large state value update) and run into a parsing error.\r\n\r\n \r\n\r\nAffected versions identified by the tags on the original PR, it seems to have been present since the state server was introduced: [https://github.com/apache/spark/commit/def42d44405af5df78c3039ac5ad0f8a0469efaa]\r\n\r\n \r\n\r\nIn practice this seems like an uncommon scenario (bug was identified/confirmed with a 512KB string state value update which likely produces a proto message much larger than typical use cases)", "output": "Python streaming transform_with_state StateServer does not fully read large state values"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Refactor ParameterizedQuery arguments validation\nDescription: * In this issue I propose to refactor ParameterizedQuery arguments validation to ParameterizedQueryArgumentsValidator so it can be reused between single-pass and fixed-point analyzer implementations. We also remove one redundant case from the ParameterizedQueryArgumentsValidator.isNotAllowed (Alias shouldn't call isNotAllowed again as it introduces unnecessary overhead to the method) to improve performance.\nQ: Issue resolved by pull request 52744\n[https://github.com/apache/spark/pull/52744]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Additional tests in ShareConsumerTest\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support AQE in stateless streaming workloads\nDescription: We have been disabling AQE for streaming workloads since stateful operators do not work with AQE. But applying AQE in stateless workloads is still beneficial and we have been blocking the case too aggressively.\r\n\r\nWe have been observed streaming workloads which can enjoy the benefits of AQE e.g. stream-static join with large static table, MERGE INTO in ForeachBatch sink, etc. To accelerate the workloads, we would want to support AQE in stateless streaming workloads.\nQ: PR is up for review. Will rename the PR.", "output": "Issue resolved by pull request 52642\n[https://github.com/apache/spark/pull/52642]"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update GitHub Actions workflow to use the latest versions of actions\nDescription: The current GitHub Actions workflows in the Hadoop repository are using outdated versions of GitHub Actions, such as {{{}actions/checkout@v3{}}}. To ensure better security, performance, and compatibility, we should update them to the latest stable versions.\nQ: Bcoderx6 opened a new pull request, #7454:\nURL: https://github.com/apache/hadoop/pull/7454\n\n   Updated GitHub Actions workflows to use the latest stable versions of various actions\r\n   \r\n   - Updated actions/checkout to v4\r\n   - Updated other outdated GitHub Actions dependencies\r\n   - Ensured compatibility and successful workflow execution", "output": "hadoop-yetus commented on PR #7454:\nURL: https://github.com/apache/hadoop/pull/7454#issuecomment-2695116996\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  13m 39s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  yamllint  |   0m  0s |  |  yamllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  37m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  86m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7454/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7454 |\r\n   | JIRA Issue | HADOOP-19477 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets yamllint |\r\n   | uname | Linux 2481122392a4 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7334f1f9a04d312c0755f5e30215c2b167bafa7f |\r\n   | Max. process+thread count | 610 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7454/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The following command from BUILDING.txt does not work anymore:\r\n{code:java}\r\ndocker build -t hadoop-windows-10-builder -f .\\dev-support\\docker\\Dockerfile_windows_10 .\\dev-support\\docker\\ {code}\r\nSeveral dependencies are missing and vcpkg points to an outdated 7-zip package.", "output": "Dockerfile_windows_10 cannot be build"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default\nDescription: Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler.\nQ: hadoop-yetus commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3341301544\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 38s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  37m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 39s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 53s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  98m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8001 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 01b5b1df2935 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7ccea846897ea6a8209a2238c06933afb4c489bc |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/console |\r\n   | versions | git=2.43.7 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "pan3793 commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3342170970\n\n   Previously, there were concerns about having many versions of Linux dist Dockerfiles, how about upgrading Debian 10 to 13 directly?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone.\r\n\r\nAfter the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will take time, and during this period, other jobs are still catching up with the new realm configuration. However, the namenode down disrupts all of them.\r\n\r\n \r\n{code:java}\r\nERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception\r\njava.lang.IllegalArgumentException: Illegal principal name hive/xxxxx@yyyy.COM\r\norg.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM         at org.apache.hadoop.security.User.(User.java:51)\r\n        at org.apache.hadoop.security.User.(User.java:43)\r\n        at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1417)\r\n        at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1401)\r\n        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier.getUser(AbstractDelegationTokenIdentifier.java:80)\r\n        at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.getUser(DelegationTokenIdentifier.java:81)\r\n        at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.toString(DelegationTokenIdentifier.java:91)\r\n        at java.lang.String.valueOf(String.java:2994)\r\n        at java.lang.StringBuilder.append(StringBuilder.java:137)\r\n        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.formatTokenId(AbstractDelegationTokenSecretManager.java:58)\r\n        at org.apache.hadoop.security.token.delegation.AbstractDelegationTo", "output": "Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade libxxhash to 0.8.3 in Windows 10\nDescription: The current version of libxxhash - 0.8.1 isn't available on the msys repo. This is causing the Hadoop Jenkins CI for Windows to fail -\r\n\r\n{code}\r\n00:34:06  Step 25/75 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libxxhash-0.8.1-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libxxhash-0.8.1-1-x86_64.pkg.tar.zst\r\n00:34:06   ---> Running in e9a8dd91a514\r\n00:34:15  \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n{code}\r\n\r\nThus, we need to upgrade to 0.8.3.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "New golden files for edge-cases discovered during the Analyzer support and development.", "output": "Add golden files for Analyzer edge cases"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce APPROX_PERCENTILE_ESTIMATE\nDescription: *APPROX_PERCENTILE_ESTIMATE(state [, k] ])* : Returns the approximate percentile value(s) from a previously accumulated sketch state.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add built-in TTL (Time-to-Live) support for Kafka Streams State Stores\nDescription: In business cases Kafka Streams users frequently need *per-key Time-To-Live (TTL)* behavior for state stores such as keyValueStore or kTables , typically to model cache-like or deduplication scenarios.\r\n\r\nToday, achieving this requires *manual handling* using:\r\n * Custom timestamp tracking per key,\r\n\r\n * Punctuators to periodically scan and remove expired entries, and\r\n\r\n * Manual emission of tombstones to maintain changelog consistency.\r\n\r\nThese workarounds are:\r\n * {*}Inconsistent across applications{*}, and\r\n\r\n * {*}Operationally costly{*}, as each developer must reimplement the same logic.\r\n\r\n*Proposal* is to introduce a *built-in TTL mechanism* for Kafka Streams state stores, allowing automatic expiration of records after a configured duration.\r\n\r\nIntroduction to new Api's like : \r\n\r\nStoreBuilder withTTL(Duration ttl);\r\nMaterialized withTtl(Duration ttl);\r\n\r\nWhen configured:\r\n * Each record’s timestamp (from event-time or processing-time) is tracked.\r\n\r\n * Expired keys are automat\nQ: Is there any difference to https://issues.apache.org/jira/browse/KAFKA-4212 – or is this ticket a duplicate?", "output": "Hello [~mjsax] \r\n\r\nThis ticket obviously is relates to KAFKA-4212 and went through the code pushed & discussions, but the concept is not a duplicate. KAFKA-4212 introduced a specific TTL store, whereas with this the proposal is to have  *general TTL support for all state stores* with automatic eviction and changelog tombstones.\r\n\r\nCurrently, i implement TTL manually using a timestampedkeyvalstore:\r\n\r\ncontext.schedule(Duration.ofDays(scheduledFrequencyDays), PunctuationType.WALL_CLOCK_TIME, timestamp - > {\r\n    try (var iterator = stateStore.all()) {\r\n        while (iterator.hasNext()) {\r\n            var entry = iterator.next();\r\n            if (entry.value.timestamp() + Duration.ofDays(retentionDays).toMillis() <= timestamp)\r\n\r\n{                 stateStore.delete(entry.key);             }\r\n\r\n        }\r\n    }\r\n});\r\n\r\n \r\n\r\nRight now i consider wallclocktime only to trigger cleanup at fixed time irrespective of incoming records to puntuate ~ StreAM_tIME, independent of new records.\r\n\r\nWith this idea is to simplify things as below\r\n\r\n{{Stores.persistentKeyValueStore(\"state-store\")\r\n.withTtl(Duration.ofDays(retentionDays));}}\r\n * TTL evictionwould be {*}automatic{*}.\r\n\r\n * expired entries would generate {*}changelog tombstones{*}.\r\n\r\n * no manual scheduling or iteration required.\r\n\r\n * to work consistently for all state stores, not just a specific store type.\r\n\r\n * .withTtl(duration){{{}{}}} is {*}optional at the API level{*}: passing null would mean {*}no TTL is applied{*}, and the store behaves like a normal persistent store.\r\n\r\n * Any store without a TTL set will still function normally, maintaining {*}backward compatibility{*}."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hi, we run Debezium connectors on Kafka Connect. We identified several \"zombie\" records that are delivered by the connectors during or after the rebalance. Since the downstream consumers require ordering, this issue breaks several things where previous primitives were build upon.\r\n\r\nHere are an overview of the setup:\r\n * Connector type: Debezium Mongo Connector\r\n * Kafka Connect version: 3.2\r\n * Number of workers: 3-4\r\n * Kafka producer configs: at-least once settings, ack=all, max inflight requests=1\r\n\r\nThe following conclusion are based on our investigation:\r\n{quote}When a Kafka Connect worker (part of a connector cluster) is overloaded or degraded, the connector on it may become temporarily unhealthy. The Kafka Connect cluster will rebalance the connector by \"moving\" it to another worker. When the connector is started on the new worker, the events will resume normally without any data loss and depending on the previously committed offsets, there might be a small amount of duplicate events due to replay but eventually the total ordering is still guaranteed. \r\n\r\nHowever, the producer of the old worker may not have been gracefully shut down. When the old worker recovered, some old events that were already placed in the producer's internal queue got sent out to Kafka before the producer was forcefully closed. This caused the \"out-of-band\" duplicate events, which we referred to as \"ghost duplicates\" or \"zombie records.\r\n{quote}\r\nCan you verify our conclusion and do you have any recommendation for the potential fix or prevention?", "output": "Kafka Connect connectors sent out zombie records during rebalance"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Document skip.platformToolsetDetection option in BUILDING.txt\nDescription: Readme.md has a complete command line example for building Hadoop on Windows.\r\nHowever, that one doesn't work in the docker image, because that doesn't have full Visual Studio install, and misses the expected programs.\r\n\r\nAdd to README.md that the *-Dskip.platformToolsetDetection* maven option is needed when building from the docker image.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Flaky test ShareConsumerTest. testShareGroupMaxSizeConfigExceeded\nDescription: The test has been observed as flaky in the recent runs: [https://develocity.apache.org/scans/tests?search.names=CI%20workflow%2CGit%20repository&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI%2Chttps:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.consumer.ShareConsumerTest&tests.sortField=FLAKY&tests.test=testShareGroupMaxSizeConfigExceeded()%5B1%5D] \r\n\r\n \r\nAfter tracing the failures, the root cause was narrowed down to commit [https://github.com/apache/kafka/commit/87657fdfc721055835f5b1f22151c461e85eab4a]. This change introduced a new try/catch around {{handleCompletedAcknowledgements()}} in {{{}ShareConsumerImpl.java{}}}. The side effect is that all exceptions coming from the commit callback — including GroupMaxSizeReachedException — are now swallowed and only logged, preventing the test from ever receiving the exception.\r\n - If the ack callback fi\nQ: Will pick this up as part of the KIP-1222 work.", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The plugin adds few useful commands to browser dependencies tree in SBT.", "output": "add sbt-dependency-graph to SBT plugins"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-dynamometer-infra.\nDescription: \nQ: slfan1989 opened a new pull request, #7588:\nURL: https://github.com/apache/hadoop/pull/7588\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19429. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-dynamometer-infra.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7588:\nURL: https://github.com/apache/hadoop/pull/7588#issuecomment-2788154603\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m  9s | [/results-checkstyle-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-infra.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-dynamometer_hadoop-dynamometer-infra.txt) |  hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 19s |  |  hadoop-dynamometer-infra in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  68m 46s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7588 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f46e86b46d53 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1b6245c1788616bf1f05550cc65575cc64b0b6c8 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/1/testReport/ |\r\n   | Max. process+thread count | 557 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra U: hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7588/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Protocol schema and public API changes\nDescription: This just adds the RPC schema and public API changes to let the implementation progress with multiple teams.", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Session mode compileScala reports missing classes java main directory\nDescription: *Problem*\r\nWhen running *{{:core:compileScala}}* under {*}{{keepAliveMode=SESSION}}{*}, joint compilation adds a non-existent directory ({_}{{.../core/build/classes/java/main}}{_}) to {{{}javac{}}}’s classpath.\r\nThis triggers:\r\n{code:java}\r\nUnexpected javac output: warning: [path] bad path element \".../core/build/classes/java/main\": no such file or directory\r\nerror: warnings found and -Werror specified {code}\r\nThe same task succeeds under {{{}keepAliveMode=DAEMON{}}}.\r\n\r\n*Current Workaround*\r\nAdded a scoped compiler flag ({{{}-Xlint:-path{}}}) only under {{keepAliveMode=SESSION}} to suppress the {{[path]}} warning and allow the build to succeed.\r\n\r\n*Steps to Reproduce*\r\n{code:java}\r\n./gradlew clean :core:compileScala -PkeepAliveMode=session   # fails before workaround  \r\n./gradlew clean :core:compileScala -PkeepAliveMode=daemon    # succeeds   {code}\r\n*Goal of This Ticket*\r\nIdentify why SESSION-mode joint compilation includes the missing {{classes/java/main}} entry in {{{}javac{}}}’s classpath, align its behavior with DAEMON mode, and remove the temporary suppression once the classpath is corrected.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: the current assignments shown by ReassignPartitionsCommand should include the log directories\nDescription: https://github.com/apache/kafka/blob/trunk/tools/src/main/java/org/apache/kafka/tools/reassign/ReassignPartitionsCommand.java#L572\r\nhttps://github.com/apache/kafka/blob/trunk/tools/src/main/java/org/apache/kafka/tools/reassign/ReassignPartitionsCommand.java#L931\r\n\r\nSince we always pass `Map.of`, the log directories are always treated as \"any\"", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: ITestS3ACommitterMRJob failing on Junit5\nDescription: NPE in test200 of ITestS3ACommitterMRJob.\r\n\r\nCause is\r\n* test dir setup and propagation calls Path.getRoot().toUri() which returns the /home dir\r\n* somehow the locatedFileStatus of that path being 1 so a codepath in FileInputFormat NPEs.\r\n\r\nFix is in test code, though FileInputFormat has its NPE messages improved to help understand what is going wrong.\nQ: steveloughran commented on PR #7968:\nURL: https://github.com/apache/hadoop/pull/7968#issuecomment-3293282933\n\n   ```\r\n   [INFO] Running org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob\r\n   [INFO] Running org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob\r\n   [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.97 s", "output": "steveloughran commented on PR #7968:\nURL: https://github.com/apache/hadoop/pull/7968#issuecomment-3293285487\n\n   fyi @slfan1989 @ahmarsuhail @mukund-thakur\r\n   one of the final nits of junit5 migration"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Make sure we don't upload empty files in RocksDB snapshot\nDescription: This change is to make sure the RocksDB snapshot zip file doesn't contain empty files (except RocksDB log file). Other files such as OPTION, manifest, metadata are not expected to be empty in any situation.\r\n\r\nIntroduced a conf {{verifyNonEmptyFilesInZip}} to make sure we can turn this off if needed.\nQ: Issue resolved by pull request 52774\n[https://github.com/apache/spark/pull/52774]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "If there is any failure in construction of KafkaShareConsumer, then it logs a few NPEs while closing.\r\n{code:java}\r\n[2025-07-31 21:45:46,484] ERROR [ShareConsumer clientId=console-share-consumer, groupId=test_1] Failed to release assignment before closing consumer (org.apache.kafka.clients.consumer.internals.ShareConsumerImpl) java.lang.NullPointerException: Cannot invoke \"org.apache.kafka.clients.consumer.internals.events.ApplicationEventHandler.add(org.apache.kafka.clients.consumer.internals.events.ApplicationEvent)\" because \"this.applicationEventHandler\" is null at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.sendAcknowledgementsAndLeaveGroup(ShareConsumerImpl.java:936) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.lambda$close$4(ShareConsumerImpl.java:882) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.common.utils.Utils.swallow(Utils.java:1042) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.close(ShareConsumerImpl.java:881) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.(ShareConsumerImpl.java:335) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.(ShareConsumerImpl.java:209) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator.create(ShareConsumerDelegateCreator.java:49) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.(KafkaShareConsumer.java:383) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.(KafkaShareConsumer.java:376) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.(KafkaShareConsumer.java:357) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.tools.consumer.ConsoleShareConsumer.run(ConsoleShareConsumer.java:75) [kafka-tools-4.1.0.jar:?] at org.apache.kafka.tools.consumer.ConsoleShareConsumer.main(Cons", "output": "Fix NPE messages in ConsoleShareConsumer."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "My team recently updated spark dependency version from 3.5.5 to 4.0.0\r\nThis included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic).\r\n\r\nAfter this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and 2. read from database, write to database.\r\n\r\nI have performed comparison between application versions with spark 3.5.5 and 4.0.0 in cluster mode, local mode and one comparison (with use of synthetic file) using spark-shell only.\r\nIn case of spark-shell I had more or less the same throughput for 3.5.5 and 4.0.0 but in case of our app used in cluster / local mode - both of these scenarios had better throughput with 3.5.5.\r\n\r\nI have observed that with 4.0.0 there are longer delays (when compared with 3.5.5) between log lines\r\n\"Running task x in stage y\"\r\nand\r\n\"Finished task x in stage y\".\r\n\r\nIs this throughput degradation a known issue? Could it be related to this task - [SPARK-48456] [M1] Performance benchmark - ASF JIRA ?\r\n\r\n(I'll also mention that we are using checkpointing (in case it might be important here))", "output": "Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add support for KLL quantiles functions based on DataSketches\nDescription: Documentation reference: [https://datasketches.apache.org/docs/KLL/KLLSketch.html].\r\n\r\nDataSketches code API reference: [https://apache.github.io/datasketches-java/6.1.0/org/apache/datasketches/kll/KllLongsSketch.html] \r\n\r\nReference PR for recently adding Theta sketches: [https://github.com/apache/spark/pull/51298]\nQ: Here is a pull request to implement the functionality: [https://github.com/apache/spark/pull/52800|https://github.com/apache/spark/pull/52800]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This ticket serves as an umbrella (Uber JIRA) to track all work related to enabling and improving support for the RISC-V architecture within the project.\r\n * Platform-specific optimizations and compatibility adjustments\r\n * Any follow-up bug fixes or enhancements specific to RISC-V\r\n * Test and CI coverage for RISC-V environments\r\n * Documentation updates related to RISC-V support", "output": "RISC-V Architecture Support"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update Surefire plugin to 3.5.3\nDescription: Hadoop currently uses the old 3.0.0-M4  surefire version, which swallows all exceptions thrown by @BeforeAll methods.\r\n\r\nThis leads to some tests not being run and not being reported as failing/erroring.\r\n\r\nUpdate to the latest version, which fixes this issue.\nQ: stoty opened a new pull request, #7574:\nURL: https://github.com/apache/hadoop/pull/7574\n\n   ### Description of PR\r\n   \r\n   Hadoop currently uses the old 3.0.0-M2  surefire version, which swallows all errors in the test class setup @BeforeAll methods.\r\n   This leads to important test not being run and not being reported as failing/erroring.\r\n   Update to the latest version, which fixes this issue.\r\n   \r\n   Note that this may well cause new test failures, which were previously unreported, but that's not a problem with this patch.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran it on JDK24, where some previously silently failing test properly errored out.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7574:\nURL: https://github.com/apache/hadoop/pull/7574#issuecomment-2775113259\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  81m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |   2m 31s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 14s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 28s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  87m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7574/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7574 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 9c5f5fcc3e4c 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 651b504e4396bfb5c3482d5f68957a4483f59a8c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7574/1/testReport/ |\r\n   | Max. process+thread count | 525 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7574/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: google gs connector registration failing\nDescription: Surfaced during HADOOP-19696 and work with all the cloud connectors on the classpath.\r\n\r\nThere's a missing dependency causing the gcs connector to fail to register *when the first filesystem is instantiated*, because the service registration process loads the gcs connector class, instantiates one and asks for its schema.\r\n\r\nAs well as a sign of a problem, it's better to just add an entry in core-default.xml as this saves all classloading overhead. It's what the other asf bundled ones do.\nQ: Trying to list local root  \r\n\r\nIf there are dependencies needed in the HADOOP-19696 let's make sure they get into common/lib, but this registration process mustn't fail this way, so let's just have a fs.gs.impl declararation in core-default.xml\r\n\r\n{code}\r\n bin/hadoop fs -ls file:///\r\n2025-09-17 15:03:47,688 [main] WARN  fs.FileSystem (FileSystem.java:loadFileSystems(3539)) - Cannot load filesystem\r\njava.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.gs.GoogleHadoopFileSystem Unable to get public no-arg constructor\r\n        at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)\r\n        at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:679)\r\n        at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1240)\r\n        at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)\r\n        at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)\r\n        at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)\r\n        at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:3522)\r\n        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3562)\r\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)\r\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\r\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\r\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:373)\r\n        at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:347)\r\n        at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:265)\r\n        at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:248)\r\n        at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:105)\r\n        at org.apache.hadoop.fs.shell.Command.run(Command.java:192)\r\n        at org.apache.hadoop.fs.FsShell.run(FsShell.java:327)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)\r\n        at org.apache.hadoop.fs.FsShell.main(FsShell.java:390)\r\nCaused by: java.lang.NoClassDefFoundError: com/google/auth/Credentials\r\n        at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)\r\n        at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3373)\r\n        at java.base/java.lang.Class.getConstructor0(Class.java:3578)\r\n        at java.base/java.lang.Class.getConstructor(Class.java:2271)\r\n        at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:666)\r\n        at java.base/java.util.ServiceLoader$1.run(ServiceLoader.java:663)\r\n        at java.base/java.security.AccessController.doPrivileged(AccessController.java:569)\r\n        at java.base/java.util.ServiceLoader.getConstructor(ServiceLoader.java:674)\r\n        ... 20 more\r\nCaused by: java.lang.ClassNotFoundException: com.google.auth.Credentials\r\n        at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n        at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n        ... 28 more\r\nFound 20 items\r\n----------   1 root admin          0 2025-08-16 19:44 file:///.file\r\n...\r\n{code}", "output": "Hi [~stevel@apache.org]. Not fully caught up on this one, but there is a {{fs.gs.impl}} entry in core-default.xml now. Was that all we needed?\r\n\r\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml#L4508"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add Support for Propagating Access Token via RPC Header in HDFS\nDescription: *Description:*\r\nTo support modern authentication models (e.g., bearer tokens, OAuth2), we propose adding support in HDFS to propagate an access token via the RPC request header. This enables downstream services (e.g., NameNode, Router) to validate access tokens in a secure and standardized way.\r\n\r\nThe token will be passed in a dedicated field in the {{{}RpcRequestHeaderProto{}}}, mimicking the behavior of an HTTP {{Authorization: Bearer }} header. The caller context or UGI may extract this token and use it for authorization decisions or auditing.\r\n\r\n*Benefits:*\r\n * Enables secure, token-based authentication in multi-tenant environments\r\n\r\n * Lays the foundation for fine-grained, per-request authorization\r\n\r\n*Scope:*\r\n * Add optional {{authorization_token}} field to RPC header\r\n\r\n * Ensure token is thread-local or caller-context scoped\r\n\r\n * Wire it through relevant client and server code paths\r\n\r\n * Provide configuration to enable/disable this feature\r\n\r\n*Notes:*\r\nThis feature is intended to be backward-compatible with existing HDFS clients. If the token is not set, behavior will remain unchanged.\r\n\r\nAt Linkedin, we plan to delegate auth to a custom enforcement point in RBF. The workflow is the client will get an access token and pass that in the RPC. The request and access token will be authorized in the custom authorizer. \r\n\r\n \r\n\r\n \r\n\r\n*PR*\r\n\r\n[https://github.com/apache/hadoop/pull/7803/files#diff-55740268215623b4037dd1bd35200c383576bee23d444096eb9cee751e3728f3]", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, the consumer attempts to update positions for all partitions it owns that don't have a position, even when the partition may be already being revoked. This could lead to NoOffsetForPartitionException for such partitions (if there is no reset strategy defined). There are 2 main issues around this:\r\n * is probably unneeded work to fetch offsets and update positions for partitions being revoked. At that point we're actually not allowing fetching from there anymore\r\n * throwing NoOffsetForPartitionException may lead applications to take action to set positions themselves, which will most probably fail (the partition will most probably not owned by the consumer anymore since it was already being revoked when the NoOffsetForPartitionException was generated). We've seen this on Kafka Streams, that handled NoOffsetForPartitionException by calling seek to set positions.   \r\n\r\nThis task is to review if there are no other implications I may be missing? Then fix to ensure we don't update positions for partitions being revoked (aligned with how we don't allow fetching from them)", "output": "Consumer NoOffsetForPartitionException for partitions being revoked"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Block unsupported aggregates in both signatures of `visitAggregateFunction`\nDescription: \nQ: Issue resolved by pull request 52573\n[https://github.com/apache/spark/pull/52573]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Listing Optimizations to avoid multiple iteration over list response.\nDescription: On blob endpoint, there are a couple of handling that is needed to be done on client side.\r\nThis involves:\r\n # Parsing of xml response and converting them to VersionedFileStatus list\r\n # Removing duplicate entries for non-empty explicit directories coming due to presence of the marker files\r\n # Trigerring Rename recovery on the previously failed rename indicated by the presence of pending json file.\r\n\r\nCurrently all three are done in a separate iteration over whole list. This is to pbring all those things to a common place so that single iteration over list reposne can handle all three.\nQ: hadoop-yetus commented on PR #7421:\nURL: https://github.com/apache/hadoop/pull/7421#issuecomment-2695179276\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  24m  1s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  24m 12s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 17 new + 12 unchanged - 0 fixed = 29 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 14s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 2 new + 10 unchanged - 0 fixed = 12 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 15s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 generated 2 new + 10 unchanged - 0 fixed = 12 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 23s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 23s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  84m 22s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7421 |\r\n   | JIRA Issue | HADOOP-19474 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5e009114837d 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ec1419b47587389823a2b6175322bb8541a25e34 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/4/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7421:\nURL: https://github.com/apache/hadoop/pull/7421#issuecomment-2695226615\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  10m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 57s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  35m 58s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 18 new + 12 unchanged - 0 fixed = 30 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 2 new + 10 unchanged - 0 fixed = 12 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 generated 2 new + 10 unchanged - 0 fixed = 12 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 50s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 135m  4s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7421 |\r\n   | JIRA Issue | HADOOP-19474 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b3d24f968b2c 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b37c3d2bdb5f8771b1dbad0a179be307cf3444b1 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/testReport/ |\r\n   | Max. process+thread count | 761 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Disallow JUnit4 Imports After JUnit5 Migration\nDescription: As our project has fully migrated to JUnit5, we should now enforce a rule that prevents the import and usage of JUnit4 classes (such as org.junit.Test, org.junit.Assert, etc.) to ensure consistency, avoid regressions, and allow safe removal of legacy dependencies.\r\n\r\nThis task involves identifying and eliminating any remaining JUnit4 imports, and introducing static code analysis or linting rules to ban future usage.\nQ: slfan1989 commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3297850797\n\n   @TaoYang526 We are currently working on upgrading the project to JUnit 5, and I have added a validation rule to prevent users from reintroducing JUnit 4 dependencies. During the review, I found that the testAsyncScheduleThreadExit method used JUnit 4 features, so I made some modifications. Could you please take a look and let me know if the changes are reasonable?", "output": "hadoop-yetus commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3300118538\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 33s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  47m 14s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 57s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 16s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 32s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 51s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  36m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  73m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 45s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  48m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 25s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 47s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  37m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  73m 40s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 503m 15s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   2m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 930m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7976 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux 946810b75b42 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 78ed6e9e068b8c35d3eb372e09440128bf4fd0d8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/testReport/ |\r\n   | Max. process+thread count | 3137 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [ReadAheadV2] Increase Prefetch Aggressiveness to improve sequential read performance\nDescription: Various analyses done in the past have shown a need for significant improvement in the performance of sequential reads. The current implementation clearly shows the lack of parallelism that is needed to cater to high throughput sequential read workloads. \r\nMore details on updated design and results of POC benchmarking will be added here soon.\nQ: Yes, in complete agreement to your thoughts here [~stevel@apache.org] \r\nThis change will only be effective for sequential read patterns. As soon as a random read pattern is detected, we switch off trigerring prefetches.\r\n\r\nI am working on a design doc based on a POC which I did. Soon will be adding the doc here and start working on formal PRs\r\nThanks", "output": "ok, look at the filesystem spec for openFIle and don't do this if the read policy is random, vectored, parquet, orc, but only for : whole-file, sequential, adaptive (plus whatever default you want).\r\n\r\nparquet now opens files as \"parquet, vector, random, adaptive\" in order of preference"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FNSOverBlob] Add Distinct String In User Agent to Get Telemetry for FNS-Blob\nDescription: Add a unique identifier in FNS-Blob user agent to get their usage through telemetry\nQ: anmolanmol1234 commented on code in PR #7713:\nURL: https://github.com/apache/hadoop/pull/7713#discussion_r2107198302\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsClient.java:\n##########\n@@ -355,6 +356,25 @@ public void verifyUserAgentClusterType() throws Exception {\n       .contains(DEFAULT_VALUE_UNKNOWN);\n   }\n \n+  @Test\n+  // Test to verify that the user agent string for FNS-Blob accounts\n+  public void verifyUserAgentForFNSBlob() throws Exception {\n\nReview Comment:\n   add similar test for dfs and validate that no extra string is added.", "output": "hadoop-yetus commented on PR #7713:\nURL: https://github.com/apache/hadoop/pull/7713#issuecomment-2909654906\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 57s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7713/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 39s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 15s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7713/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7713 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 4c92dd0dbf3c 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d538ce5d82c431548dc0e50a39f720682ef480ad |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7713/1/testReport/ |\r\n   | Max. process+thread count | 680 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7713/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Native profile fails to build on SLES 15\nDescription: Hadoop build fails to find pthreads on SLES 15 builds while linking rpc. It looks like it checks for SunRPC library via rpc/rpc.h, but instead finds tirpc and sets it up incorrectly.\r\n\r\n{code}\r\nPerforming C SOURCE FILE Test CMAKE_HAVE_LIBC_PTHREAD failed with the following output:\r\nChange Dir: /grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/gmake cmTC_4ddc0/fast && /usr/bin/gmake  -f CMakeFiles/cmTC_4ddc0.dir/build.make CMakeFiles/cmTC_4ddc0.dir/build\r\ngmake[1]: Entering directory '/grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeTmp'\r\nBuilding C object CMakeFiles/cmTC_4ddc0.dir/src.c.o\r\n/usr/bin/gcc-8 -DCMAKE_HAVE_LIBC_PTHREAD   -o CMakeFiles/cmTC_4ddc0.dir/src.c.o -c /grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeT\nQ: MikaelSmith opened a new pull request, #7789:\nURL: https://github.com/apache/hadoop/pull/7789\n\n   ### Description of PR\r\n   \r\n   On SLES 15, failing to add tirpc results in failures linking pthreads. tirpc is present and appears and the newer RPC library option (with support for IPv6). Use it when available.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Builds on RHEL 8, RHEL 9, Ubuntu 22, and SLES 15.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7789:\nURL: https://github.com/apache/hadoop/pull/7789#issuecomment-3049613739\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  34m 51s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | -1 :x: |  shadedclient  |  37m 50s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |   1m 24s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 23s |  |  hadoop-pipes in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 31s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  43m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7789 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets |\r\n   | uname | Linux f4434e8fb98a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bdd0879bbd4e216137aaa8afda7b182766588173 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_412-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/1/testReport/ |\r\n   | Max. process+thread count | 98 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-pipes U: hadoop-tools/hadoop-pipes |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7789/1/console |\r\n   | versions | git=2.9.5 maven=3.9.10 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Hive should support IPv6\nDescription: \nQ: Opened under the wrong project", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add a missing test for ContinuousMemorySink\nDescription: \nQ: Issue resolved by pull request 52602\n[https://github.com/apache/spark/pull/52602]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: ITestS3AConfiguration.testDirectoryAllocatorDefval() failing\nDescription: while working on HADOOP-19554 I added a per-bucket setting for fs.s3a.buffer.dir\r\n\r\nafter this, ITestS3AConfiguration.testDirectoryAllocatorDefval() would fail in a test run of the entire class, but not if run alone.\r\n\r\nCauses\r\n* dir allocator map of config key to allocator is static; previous uses tainted outcome\r\n* per-bucket settings were't being overridden. This is complicated by the fact that \"unset\" isn't a setting, therefore can't be forced in. Instead some whitespace needs to be set.\nQ: steveloughran opened a new pull request, #7699:\nURL: https://github.com/apache/hadoop/pull/7699\n\n   \r\n   \r\n   * trim the buffer dir string before the probe (more robust code anyway)\r\n   * tests to set this, and remove any instantiated context mappers\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   in the ide until eventually the stack trace went away\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "steveloughran commented on PR #7699:\nURL: https://github.com/apache/hadoop/pull/7699#issuecomment-2894602926\n\n   testing: s3 london with a per-bucket fs.s3a.buffer.dir setting"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update SpotBugs version and enable Spotbugs Gradle tasks on Java 25\nDescription: *Prologue:* KAFKA-19664 \r\n\r\n*In more details:* Apache commons bcel Java 25 compatible version will be created soon (and SpotBugs version will follow immediately): https://issues.apache.org/jira/browse/BCEL-377?focusedCommentId=18028106&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18028106\r\n\r\n*Action points:*\r\n - upgrade SpotBugs version (use Java 25 compatible version)\r\n - enable SpotBugs checks for Java 25 Github actions build\r\n\r\n*Related links: *\r\n- [https://issues.apache.org/jira/projects/BCEL/versions/12354966] \r\n- [https://github.com/spotbugs/spotbugs/issues/3564]\nQ: *{{Apache commons-bcel}}* Java 25 compatible version should be released soon: https://github.com/apache/commons-bcel/releases/tag/commons-bcel-6.11.0-RC1", "output": "*Apache* *{{commons-bcel}}* new version has been released; Spotbugs version should be released soon:\r\n - [https://github.com/spotbugs/spotbugs/pull/3763]\r\n - [https://github.com/spotbugs/spotbugs/milestone/31?closed=1] \r\n - [https://github.com/spotbugs/spotbugs/discussions/3771]"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip `test_in_memory_data_source` in Python 3.14\nDescription: \nQ: Issue resolved by pull request 52769\n[https://github.com/apache/spark/pull/52769]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: LdapAuthenticationHandler supports configuring multiple ldapUrls.\nDescription: Currently, LdapAuthenticationHandler supports only a single ldap server url, this can obviously cause issues if the ldap instance goes down. This JIRA attempts to improve this by allowing users to list multiple ldap server urls, and performing a failover if we detect any issues.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Python streaming transform_with_state StateServer does not fully read large state values\nDescription: The TransformWithState StateServer's {{parseProtoMessage}} method uses {{read}} (InputStream/FilterInputStream) which only reads all available data and may not return the full message. We should be using the [readFully DataInputStream API|https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/DataInput.html#readFully(byte%5B%5D)] instead, which will continue fetching until it fills up the provided buffer.\r\n\r\nIn addition to the linked API above, this StackOverflow post also illustrates the difference between the two APIs: [https://stackoverflow.com/a/25900095]\r\n\r\nWithout this change, it is possible for the state server to fail to fully read large proto messages (e.g., those containing a large state value update) and run into a parsing error.\r\n\r\n \r\n\r\nAffected versions identified by the tags on the original PR, it seems to have been present since the state server was introduced: [https://github.com/apache/spark/commit/def42d44405af5df78c3039ac5ad0f8a0469efaa]\r\n\r\n \r\n\r\nIn prac\nQ: Issue resolved via [https://github.com/apache/spark/pull/52539]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In HDFS-15652, make block size from NNThroughputBenchmark configurable. Benchmarking.md should also be updated.", "output": "Update the command usage of NNThroughputBenchmark by adding the \"-blockSize\" option."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Suppress `StatusRecorder` warning messages\nDescription: \nQ: Issue resolved by pull request 397\n[https://github.com/apache/spark-kubernetes-operator/pull/397]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager\nDescription: We have observed this across multiple workload runs that when we start reading data from input stream. The first read which came to input stream has to be read synchronously even if we trigger prefetch request for that particular offset. Most of the times we end up doing extra work of checking if the prefetch is trigerred, removing prefetch from the pending queue and go ahead to do a direct remote read in workload thread itself.\r\n\r\nTo avoid all this overhead, we will always bypass read ahead for the very first read of each input stream and trigger read aheads for second read onwards.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged.\r\nWe plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules.", "output": "Fix SpotBugs warnings introduced after SpotBugs version upgrade."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix Unit Test Failure in ITestAzureBlobFileSystemMainOperation\nDescription: The unit test failure in ITestAzureBlobFileSystemMainOperation is due to the missing {{@BeforeEach}} and {{@AfterEach}} annotations. This occurs because in JUnit 5, if a subclass overrides the parent class's {{setup}} and {{tearDown}} methods, the {{@BeforeEach}} and {{@AfterEach}} annotations must be explicitly added.\nQ: slfan1989 commented on PR #7769:\nURL: https://github.com/apache/hadoop/pull/7769#issuecomment-3026867793\n\n   > +1\r\n   > Thanks for fixing this. But this might already get covered in #7653\r\n   \r\n   @anujmodi2021 Thank you very much for your help with the review! You are right; this is the same as the work completed in #7653, so we can go ahead and close this PR.", "output": "slfan1989 closed pull request #7769: HADOOP-19601. Fix Unit Test Failure in ITestAzureBlobFileSystemMainOperation.\nURL: https://github.com/apache/hadoop/pull/7769"}
{"instruction": "Answer the question based on the bug.", "input": "Title: State Store Row Checksum implementation\nDescription: Introduce row checksum creation and verification for state store, both {*}HDFS and RocksDB state store{*}. This will help detect corruption at the row level and help prevent corruption.\nQ: Issue resolved by pull request 52809\n[https://github.com/apache/spark/pull/52809]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Turn on Gradle reproducible builds feature\nDescription: *Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923] \r\n\r\n*Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future)\r\n\r\n*Related Gradle issues and links:*\r\n *  [https://github.com/gradle/gradle/issues/34643]\r\n * [https://github.com/gradle/gradle/issues/30871]  \r\n * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives]\r\n * [https://docs.gradle.org/9.1.0/userguide/upgrading_major_version_9.html#reproducible_archives_by_default] \r\n * [https://docs.gradle.org/9.1.0/dsl/org.gradle.api.tasks.bundling.Tar.html#org.gradle.api.tasks.bundling.Tar:preserveFileTimestamps]  \r\n\r\n \r\n\r\n*Definition of done (at the minimum):*\r\n * *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc) : [https://kafka.apache.org/quickstart]\nQ: Hi [~naveenthuvana] \r\n\r\nMy suggestion for you is to try to build with *reproducibleFileOrder = true* (this is a default, so it can be left out) and *preserveFileTimestamps = true*", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Building with {{-Pclover}} fails with\r\n{code}\r\n[INFO] Instrumentation error\r\ncom.atlassian.clover.api.CloverException: /home/jenkins/hadoop/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ;\r\n...\r\nFailed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory\r\n{code}\r\n\r\nIt doesn't seem to like a double semicolon in ITestS3APutIfMatchAndIfNoneMatch.java that was added in HADOOP-19256.", "output": "Clover breaks on double semicolon"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: docker_build_test.py does not run tests when using --kafka-archive.\nDescription: When we want to run docker_sanity_test.py for a local image, we need to pass a local archive image via --kafka-archive to docker_build_test.py.\r\nBut currently it fails to run the test(docker_sanity_test.py) when we pass --kafka-archive.\r\nCurrently the tests run only when we pass a URL via --kafka-url.\r\n\r\n \r\n{code:java}\r\npython docker_build_test.py kafka/test --image-tag=4.2.0-SNAPSHOT --image-type=jvm --kafka-archive=/Users/shivsundarr/dev/opensource/kafka/core/build/distributions/kafka_2.13-4.2.0-SNAPSHOT.tgz{code}\r\nAfter building the image, got the following output when it attempted to run the tests.\r\n{code:java}\r\nTraceback (most recent call last):\r\n  File \"/Users/shivsundarr/dev/opensource/kafka/docker/docker_build_test.py\", line 50, in run_docker_tests\r\n    execute([\"wget\", \"-nv\", \"-O\", f\"{temp_dir_path}/kafka.tgz\", kafka_url])\r\n  File \"/Users/shivsundarr/dev/opensource/kafka/docker/common.py\", line 24, in execute\r\n    if subprocess.run(command).returncode != 0:\r\n       ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/shivsundarr/.pyenv/versions/3.12.7/lib/python3.12/subprocess.py\", line 548, in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/shivsundarr/.pyenv/versions/3.12.7/lib/python3.12/subprocess.py\", line 1026, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"/Users/shivsundarr/.pyenv/versions/3.12.7/lib/python3.12/subprocess.py\", line 1885, in _execute_child\r\n    self.pid = _f", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()\nDescription: We have evidence that `IOStatisticsSupport.snapshotIOStatistics()` can hang, specifically on the statistics collected by an S3AInputStream, whose statistics are merged in to the FS stats in close();\r\n\r\n{code}\r\njdk.internal.misc.Unsafe.park(Native Method)\r\njava.util.concurrent.locks.LockSupport.park(LockSupport.java:341)\r\njava.util.concurrent.ForkJoinTask.awaitDone(ForkJoinTask.java:468)\r\njava.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:687)\r\njava.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:927)\r\njava.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\njava.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)\r\norg.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap.entrySet(EvaluatingStatisticsMap.java:166)\r\njava.util.Collections$UnmodifiableMap.entrySet(Collections.java:1529)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.copyMap(IOStatisticsBinding.java:172)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBindin\nQ: steveloughran opened a new pull request, #8006:\nURL: https://github.com/apache/hadoop/pull/8006\n\n   \r\n   Reworked how entrySet() and values() work, using .forEach() iterators after reviewing what ConcurrentHashMap does internally; it does a (safe) traverse.\r\n   \r\n   Add EvaluatingStatisticsMap.forEach() implementation which maps the passed in BiConsumer down to the evaluators.forEach, evaluating each value as it goes.\r\n   \r\n   Use that in IOStatisticsBinding.snapshot() code.\r\n   \r\n   Tests for all this.\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "steveloughran commented on PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006#issuecomment-3357467095\n\n   tested s3 london args ` -Dparallel-tests -DtestsThreadCount=8 -Dscale`\r\n   ```\r\n   [ERROR] Failures: \r\n   [ERROR]   ITestS3APrefetchingInputStream.testReadLargeFileFully:130 [Maxiumum named action_executor_acquired.max] \r\n   Expecting:                                                                                                                                                                                                                                             \r\n                                                                                                                                                                                                                                                      \r\n   to be greater than:                                                                                                                                                                                                                                    \r\n               \r\n   ```"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "During shutdown, when the RemoteStorageManager closes first, then the ongoing requests are thrown with error. To handle the ongoing requests gracefully, closing the RSM after closing the remote-log reader thread pools. \r\n\r\n \r\n\r\nhttps://sourcegraph.com/github.com/apache/kafka/-/blob/storage/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogManager.java?L2035", "output": "Stop the RSM after closing the remote-log reader threads to handle requests gracefully"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Reload DSv2 tables in views created using plans on each access\nDescription: The current problem is that the view definition in the session catalog captures the analyzed plan that references DataSourceV2Relation and Table. If a connector doesn’t have an internal cache and produces a new Table object after TableCatalog$load, Table referenced in the view will become orphan and there will be no way to refresh it unless that Table instance auto refreshes on each scan (super dangerous).", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Regex subscription should be empty for classic members joining mixed group\nDescription: We don't recompute the assignment on consumer -> classic member replacement when the consumer member had a regex subscription and the classic member does not.\r\n\r\nWe should explicitly set regex subscription to empty in classicGroupJoinToConsumerGroup", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done.\nDescription: There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header.\r\n\r\nFollowing are types of read we want to identify:\r\n # Direct Read: Read from a given position in remote file. This will be synchronous read\r\n # Normal Read: Read from current seeked position where read ahead was bypassed. This will be synchronous read.\r\n # Prefetch Read: Read triggered from background threads filling up in memory cache. This will be asynchronous read.\r\n # Missed Cache Read: Read triggered after nothing was received from read ahead. This will be synchronous read.\r\n # Footer Read: Read triggered as part of footer read optimization. This will be synchronous.\r\n # Small File Read: Read triggered as a part of small file read. This will be synchronous read.\r\n\r\nWe will add another field in the Tracing Header (Client Request Id) for each r\nQ: hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135193019\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 14 new + 1 unchanged - 0 fixed = 15 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m 59s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 51s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c64bcd4ab2e9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ea1572a40346a9ddfa4e80f4f1a4925308205175 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135296997\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 17 new + 1 unchanged - 0 fixed = 18 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 28s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 28s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m 59s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 30s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7be945cb4d05 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 42ecdd05eb6954bdfd26d5022bf4c8a517c607ba |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/testReport/ |\r\n   | Max. process+thread count | 540 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "ApacheDS 2.0.0AM26 (or at least its test integration) is incompatible with JDK21+.\r\n\r\nUpdate to 2.0.0.AM27 and the corresponding ldap-api version.\r\n\r\nThis also requires migrating some dependent tests from Junit 4 to Junit 5.\r\n\r\n{noformat}\r\n[ERROR] org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler -- Time elapsed: 1.426 s <<< ERROR!\r\njava.lang.NoSuchMethodError: 'void sun.security.x509.X509CertInfo.set(java.lang.String, java.lang.Object)'\r\n        at org.apache.directory.server.core.security.CertificateUtil.setInfo(CertificateUtil.java:96)\r\n        at org.apache.directory.server.core.security.CertificateUtil.generateSelfSignedCertificate(CertificateUtil.java:194)\r\n        at org.apache.directory.server.core.security.CertificateUtil.createTempKeyStore(CertificateUtil.java:337)\r\n        at org.apache.directory.server.factory.ServerAnnotationProcessor.instantiateLdapServer(ServerAnnotationProcessor.java:158)\r\n        at org.apache.directory.server.factory.ServerAnnotationProcessor.createLdapServer(ServerAnnotationProcessor.java:318)\r\n        at org.apache.directory.server.factory.ServerAnnotationProcessor.createLdapServer(ServerAnnotationProcessor.java:351)\r\n        at org.apache.directory.server.core.integ.FrameworkRunner.run(FrameworkRunner.java:139)\r\n{noformat}", "output": "Update to ApacheDS 2.0.0.AM27 and ldap-api 2.1.7"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We create—and wait on—{{{}CreateFetchRequestEvent{}}}s in {{{}AsyncKafkaConsumer.pollForRecords(){}}}. The inter-thread communication involved in sending the event, notifying the event future, and blocking on the event future results in high CPU usage. Investigate if it is possible to reduce/eliminate the need for the event future.", "output": "Remove waiting for event completion in AsyncKafkaConsumer.pollForRecords()"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Apache Client Connection Pool Relook\nDescription: \nQ: hadoop-yetus commented on PR #7817:\nURL: https://github.com/apache/hadoop/pull/7817#issuecomment-3095586717\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 3 unchanged - 0 fixed = 7 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 20s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  93m 52s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7817 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b6b106bf7326 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b9e21c77c34a6e66ccf1d38b82b777973ecc38f2 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/1/testReport/ |\r\n   | Max. process+thread count | 556 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7817:\nURL: https://github.com/apache/hadoop/pull/7817#issuecomment-3116394363\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 23s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  7s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 17s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  81m 28s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7817 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 8f951cfe3624 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b8310f78cee7139a2ece37578800a0c6037aec5e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/2/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In our production environment, we encountered a scenario where a broker failed to start due to checkpoint creation failure on a single disk (caused by disk corruption or filesystem errors). According to Kafka's design, such disk-level failures should be isolated via {{{}logDirFailureChannel{}}}, allowing other healthy disks to continue serving traffic. However, upon reviewing the {{CheckpointFileWithFailureHandler}} implementation, we observed that while methods like {{{}write{}}}, {{{}read{}}}, and {{writeIfDirExists}} handle {{IOException}} by routing the affected {{log.dir}} to {{{}logDirFailureChannel{}}}, the checkpoint initialization process lacks this fault-tolerant behavior. Should checkpoint creation adopt the same failure-handling logic? If this is not an intentional design, I will submit a PR to fix this issue.", "output": "CLONE - Broker Startup: Handle Checkpoint Creation Failure via logDirFailureChannel"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Adding request priority for prefetches\nDescription: Adding low traffic request priority (behind a config flag) for prefetches to reduce load on server during throttling", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "as title", "output": "Rewrite AbortedTxn by generated protocol"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Includes fix for CVE-2025-53864", "output": "Upgrade nimbusds to 10.0.2"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Make sure we don't upload empty files in RocksDB snapshot\nDescription: This change is to make sure the RocksDB snapshot zip file doesn't contain empty files (except RocksDB log file). Other files such as OPTION, manifest, metadata are not expected to be empty in any situation.\r\n\r\nIntroduced a conf {{verifyNonEmptyFilesInZip}} to make sure we can turn this off if needed.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support `spark.kubernetes.allocation.maximum`\nDescription: \nQ: Issue resolved by pull request 52615\n[https://github.com/apache/spark/pull/52615]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "code\r\n{code}\r\n  public void testABFSConstructor() throws Throwable {\r\n    new AzureBlobFileSystem().close();\r\n  }\r\n{code}\r\n\r\nstack\r\n{code}\r\n[ERROR] org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor -- Time elapsed: 0.003 s <<< ERROR!\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getClient()\" because \"this.abfsStore\" is null\r\n        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.close(AzureBlobFileSystem.java:800)\r\n        at org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor(TestRuntimeValid.java:49)\r\n{code}", "output": "ABFS: NPE when close() called on uninitialized filesystem"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)\nDescription: The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone.\r\n\r\nAfter the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will take time, and during this period, other jobs are still catching up with the new realm configuration. However, the namenode down disrupts all of them.\r\n\r\n \r\n{code:java}\r\nERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception\r\njava.lang.IllegalArgumentException: Illegal principal name hive/xxxxx@yyyy.COM\r\norg.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM         at org.apache.hadoop.security.User.(User.java:51)\r\n        at org.apache.hadoop.\nQ: You’re right [~stevel@apache.org]. My user version is 3.1.1, so it is missing this HDFS-17138 fix, which clearly addresses my user scenarios. Thanks for checking so quickly. I should have checked it, but unfortunately I taken your time :)", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-datajoin.\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Prefer to detect Java Home from env JAVA_HOME on finding jmap\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteWithOutdatedEtag() fails when no encryption method is set. \r\n \r\nThis is because it does  \r\ncreateFileWithFlags(fs, path, SMALL_FILE_BYTES, true, null);\r\n \r\nand then to overwrite the file, also does\r\n \r\ncreateFileWithFlags(fs, path, SMALL_FILE_BYTES, true, null);\r\n \r\nWhen no encryption is used, the eTAG is the md5 of the object, and so will always be the same, and won't result in the 412 conditional write failure. \r\n \r\nTest passes when using SSE-KMS, as when using encryption, eTag is no longer the md5 of the object content, and changes on every write. \r\n \r\n \r\nFix is simple enough, change the object content on the second write.", "output": "S3A: testIfMatchOverwriteWithOutdatedEtag() fails when not using SSE-KMS"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Enable all disabled tests on Linux\nDescription: \nQ: Issue resolved by pull request 256\n[https://github.com/apache/spark-connect-swift/pull/256]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade Netty to 4.2.6.Final\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: KRaft voter auto join will add a removed voter immediately\nDescription: In v4.2.0, we are able to auto join a controller with the configuration `controller.quorum.auto.join.enable=true` set ([KIP-853|https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes#KIP853:KRaftControllerMembershipChanges-Controllerautojoining](KAFKA-19078)). This is a good improvement for controller addition, but it has a UX issue, which is that when a controller is removed via removeVoterRequest, it will be added immediately due to `controller.quorum.auto.join.enable=true`. In the KIP, we also mention you have to stop the controller before removing the controller:\r\n\r\n \r\n{noformat}\r\ncontroller.quorum.auto.join.enable:\r\n\r\nControls whether a KRaft controller should automatically join the cluster \r\nmetadata partition for its cluster id. If the configuration is set to \r\ntrue the controller must be stopped before removing the controller with kafka-metadata-quorum remove-controller.{noformat}\r\n \r\n\r\nThis is not a user friendly behavior in my opinion\nQ: If we only allow anto-join voter to join the cluster once (it can be retried until success), is it simpler than using removed voter RPC?", "output": "Do you mean the auto-join can only join the cluster once? But if the node is already a controller, and now it gets removed by remote voter RPC, it will auto join the voters immediately, which doesn't fix the problem."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hi, I was playing with Share groups on the 4.1.0-rc2 and found it a little opaque to detect a failure to auto create the `__share_group_state` topic due to the replication factor not being satisfiable.\r\n\r\nI start up a cluster like this (taken from the dockerhub instructions):\r\n{code:java}\r\npodman run --rm \\\r\n  -p 9092:9092 \\\r\n  -e KAFKA_NODE_ID=1 \\\r\n  -e KAFKA_PROCESS_ROLES=broker,controller \\\r\n  -e KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \\\r\n  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\r\n  -e KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER \\\r\n  -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \\\r\n  -e KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9093 \\\r\n  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\r\n  -e KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1 \\\r\n  -e KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1 \\\r\n  -e KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0 \\\r\n  -e KAFKA_NUM_PARTITIONS=3 \\\r\n  apache/kafka:4.1.0-rc2{code}\r\nand then run the commands:\r\n{code:java}\r\nrobeyoun:kafka_2.13-4.1.0$ bin/kafka-features.sh --bootstrap-server localhost:9092 upgrade --feature share.version=1\r\nshare.version was upgraded to 1.\r\nrobeyoun:kafka_2.13-4.1.0$ bin/kafka-topics.sh --create --topic my_topic --bootstrap-server localhost:9092\r\nWARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\r\nCreated topic my_topic.\r\nrobeyoun:kafka_2.13-4.1.0$ bin/kafka-console-share-consumer.sh --bootstrap-server localhost:9092 --topic quickstart-events\r\n[2025-08-12 09:50:13,130] WARN Share groups and KafkaShareConsumer are part of a preview feature introduced by KIP-932, and are not recommended for use in production. (org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator)\r\n[2025-08-12 09:50:13,357] WARN [ShareConsumer clientId=console-share-consumer, groupId=console-share-consumer] The metadata response from the cluster reported a reco", "output": "Log auto topic creation failures more visibly"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Several tasks in multiple sink connectors in a long-running connect cluster broke spontaneously (within a couple of hours) with the following stacktrace:\r\n{code:java}\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.kafka.connect.runtime.ConnectMetrics$MetricGroup.close()\" because the return value of \"java.util.concurrent.ConcurrentMap.get(Object)\" is null\r\n    at org.apache.kafka.connect.runtime.Worker$ConnectorStatusMetricsGroup.recordTaskRemoved(Worker.java:2333)\r\n    at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:707)\r\n    at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:568)\r\n    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2009)\r\n    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$39(DistributedHerder.java:2059)\r\n    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)\r\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n    at java.base/java.lang.Thread.run(Unknown Source) {code}\r\nRestarting the failed tasks with the REST API lead to another task failure with the following stacktrace:\r\n{code:java}\r\njava.lang.NullPointerException: Cannot invoke \"java.util.Map.size()\" because \"inputMap\" is null\r\n    at org.apache.kafka.common.utils.Utils.castToStringObjectMap(Utils.java:1476)\r\n    at org.apache.kafka.common.config.AbstractConfig.(AbstractConfig.java:112)\r\n    at org.apache.kafka.common.config.AbstractConfig.(AbstractConfig.java:146)\r\n    at org.apache.kafka.connect.runtime.TaskConfig.(TaskConfig.java:51)\r\n    at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:661)\r\n    at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:568)\r\n    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2009)\r\n    at org.apache.kafk", "output": "NPE Cannot invoke org.apache.kafka.connect.runtime.ConnectMetrics$MetricGroup.close()"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h2. Description\r\nh3. Summary\r\n\r\nChange the data type of *{{log.segment.bytes}}* configuration from *{{int}}* to *{{long}}* to allow segment sizes beyond the current 2GB limit imposed by the integer maximum value.\r\nh3. Current Limitation\r\n\r\nThe {{*log.segment.bytes*}} configuration currently uses an *{{int}}* data type, which limits the maximum segment size to ~2GB (2,147,483,647 bytes). This constraint becomes problematic for modern high-capacity storage deployments.\r\nh3. Background: Kafka Log Segment Structure\r\n\r\nEach Kafka topic partition consists of multiple log segments stored as separate files on disk. For each segment, Kafka maintains three core files:\r\n * {*}{{.log}} files{*}: Contain the actual message data\r\n * {*}{{.index}} files{*}: Store mappings between message offsets and their physical positions within the log file, allowing Kafka to quickly locate messages by their offset without scanning the entire log file\r\n * {*}{{.timeindex}} files{*}: Store mappings between message timestamps and their corresponding offsets, enabling efficient time-based retrieval of messages\r\n\r\nh3. Motivation\r\n # {*}Modern Hardware Capabilities{*}: Current deployments often use high-capacity storage (e.g., EPYC servers with 4×15TB drives) where 2GB segments are inefficiently small\r\n # {*}File Handle Optimization{*}: Large Kafka deployments with many topics can have 50-100k open files across all segment types (.log, .index, .timeindex files). Each segment requires open file handles, and larger segments would reduce the total number of files and improve caching efficiency\r\n # {*}Performance Benefits{*}: Fewer segment rotations in high-traffic scenarios would reduce I/O overhead and improve overall performance. Sequential disk operations are much faster than random access patterns\r\n # {*}Storage Efficiency{*}: Reducing segment file proliferation improves filesystem metadata performance and reduces inode usage on high-volume deployments\r\n # {*}Community Interest{*}: Similar requests", "output": " Change log.segment.bytes configuration type from int to long to support segments larger than 2GB"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support recaching when a table is written via a different table implementation (V1 or V2)\nDescription: When a table is cached using one table implementation (e.g., V2) and written through the other (e.g., V1), Spark may not automatically trigger recaching. As a result, the cached data can become stale even though the underlying table content has changed.\r\n\r\n \r\n\r\nThis issue arises because the current recaching mechanism does not consistently handle cross-implementation writes. Given that the community is actively working on Data Source V2 (DSV2), many data sources are expected to have both V1 and V2 implementations for a period of time, making this issue more likely to occur in practice.\r\n\r\n \r\n\r\n*Proposed Fix:*\r\n\r\nEnhance the cache invalidation logic to detect writes that occur through a different table implementation (V1 ↔ V2) and trigger recaching accordingly.\r\n\r\n \r\n\r\n{*}Expected Outcome:{*}{*}{*}\r\n * Cached data remains up to date when a table is written through either V1 or V2 paths.\r\n\r\n * Both logical-plan-based and file-path-based recaching continue to work as expected for V1&V2 co\nQ: cc [~vli-databricks]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [ReadAheadV2] Refactor ReadBufferManager to isolate new code with the current working code\nDescription: Read Buffer Manager used today was introduced way back and has been stable for quite a while.\r\nRead Buffer Manager to be introduced as part of https://issues.apache.org/jira/browse/HADOOP-19596 will introduce many changes incrementally over time. While the development goes on and we are able to fully stabilise the optimized version we need the current flow to be functional and undisturbed. \r\n\r\nThis work item is to isolate that from new code by refactoring ReadBufferManager class to have 2 different implementations with same public interfaces: ReadBufferManagerV1 and ReadBufferManagerV2.\r\n\r\nThis will also introduce new configs that can be used to toggle between new and old code.\nQ: anujmodi2021 opened a new pull request, #7801:\nURL: https://github.com/apache/hadoop/pull/7801\n\n   This is the first PR in series of work done under Parent Jira: [HADOOP-19596](https://issues.apache.org/jira/browse/HADOOP-19596) to improve the performance of sequential reads in ABFS Driver. \r\n   Please refer to Parent JIRA for more details.\r\n   \r\n   ### Description of PR\r\n   Jira: https://issues.apache.org/jira/browse/HADOOP-19613\r\n   \r\n   Read Buffer Manager used today was introduced way back and has been stable for quite a while.\r\n   Read Buffer Manager to be introduced as part of [HADOOP-19596](https://issues.apache.org/jira/browse/HADOOP-19596) will introduce many changes incrementally over time. While the development goes on and we are able to fully stabilise the optimized version we need the current flow to be functional and undisturbed. \r\n   \r\n   This work item is to isolate that from new code by refactoring ReadBufferManager class to have 2 different implementations with same public interfaces: ReadBufferManagerV1 and ReadBufferManagerV2.\r\n   \r\n   This will also introduce new configs that can be used to toggle between new and old code. \r\n   \r\n   ### How was this patch tested?\r\n   Existing tests were modified to work with the Refactored Classes.\r\n   More tests will be added with coming up PRs where new implementation will be introduced.\r\n   Test suite result added.", "output": "anujmodi2021 commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3072020510\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 818, Failures: 0, Errors: 0, Skipped: 165\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 821, Failures: 0, Errors: 0, Skipped: 117\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 660, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 818, Failures: 0, Errors: 0, Skipped: 176\r\n   [WARNING] Tests run: 133, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 664, Failures: 0, Errors: 0, Skipped: 134\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 657, Failures: 0, Errors: 0, Skipped: 225\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 661, Failures: 0, Errors: 0, Skipped: 147\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 659, Failures: 0, Errors: 0, Skipped: 165\r\n   [WARNING] Tests run: 133, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 692, Failures: 0, Errors: 0, Skipped: 172\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 657, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "* We're currently using libopenssl 3.1.2 which\r\n  is needed for rsync 3.2.7 on Windows for the\r\n  Yetus build validation.\r\n* However, libopenssl 3.1.2 is no longer\r\n  available for download on the msys2 site.\r\n* This PR upgrades libopenssl to the next\r\n  available version - 3.1.4 to mitigate this\r\n  issue.", "output": "Upgrade libopenssl to 3.1.4 for rsync on Windows"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Please see the KIP: https://cwiki.apache.org/confluence/x/6oqMEw", "output": "Remove deprecated metrics in Rename AssignmentsManager and RemoteStorageThreadPool"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix scheduled job for numpy 2.1.3\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "HADOOP-19483 releases thirdparty 1.4.0; upgrade hadoop to it", "output": "Upgrade to hadoop-thirdparty 1.4.0"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `Gradle` to 9.2.0\nDescription: \nQ: Issue resolved by pull request 411\n[https://github.com/apache/spark-kubernetes-operator/pull/411]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade\nDescription: After https://issues.apache.org/jira/browse/HADOOP-19425\r\n\r\nmost of the integration tests are getting skipped. All tests need to be fixed with this PR", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Avoid function conflicts in test_pandas_grouped_map\nDescription: \nQ: Issue resolved by pull request 52811\n[https://github.com/apache/spark/pull/52811]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Deadlock in multipart upload\nDescription: Reproduced while testing system resilience and turning S3 network off (introduced a network partition to the list of IP addresses S3 uses) - but given it's seemingly timers related stack traces, I'd guess it could happen any time?\r\n\r\n\r\n{code:java}\r\nFound one Java-level deadlock:\r\n=============================\r\n\"sdk-ScheduledExecutor-2-3\":\r\n  waiting to lock monitor 0x00007f5c880a8630 (object 0x0000000315523c78, a java.lang.Object),\r\n  which is held by \"sdk-ScheduledExecutor-2-4\"\r\n\"sdk-ScheduledExecutor-2-4\":\r\n  waiting to lock monitor 0x00007f5c7c016700 (object 0x0000000327800000, a org.apache.hadoop.fs.s3a.S3ABlockOutputStream),\r\n  which is held by \"io-compute-blocker-15\"\r\n\"io-compute-blocker-15\":\r\n  waiting to lock monitor 0x00007f5c642ae900 (object 0x00000003af0001d8, a java.lang.Object),\r\n  which is held by \"sdk-ScheduledExecutor-2-3\"\r\nJava stack information for the threads listed above:\r\n===================================================\r\n\"sdk-ScheduledExecutor-2-3\":\r\n        at \nQ: maybe move off simple sync to try to acquire a lock with timeouts? it'd make close() more robust, even if the action on timeout is failure. which, given there are clearly network problems, is the right thing to do.", "output": "hi, is there a workaround we could use? e.g. can we adjust the timeout to not close and cause the deadlock?\r\n\r\nwhile the risk of S3 (or network routes to it) being not available is fairly low, the impact of this is rather high as it freezes the whole system and does not recover after the network recovers"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [ReadAheadV2] Increase Prefetch Aggressiveness to improve sequential read performance\nDescription: Various analyses done in the past have shown a need for significant improvement in the performance of sequential reads. The current implementation clearly shows the lack of parallelism that is needed to cater to high throughput sequential read workloads. \r\nMore details on updated design and results of POC benchmarking will be added here soon.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, if we call DataFrameWriter.saveAsTable(tableName) with the SaveMode equal to Overwrite and the DataSource is V1, the table is dropped from catalog directly in the DataFrameWriter and only then the CreateTable command is created. \r\n\r\nIn case if we want to somehow change the default behavior using Spark extensions, for instance, postpone the table dropping to decrease the time interval, when the old table is dropped and the new one is not created, such hardcoded behavior in DataFrameWriter makes it almost impossible if we want to use the original saveAsTable method. \r\n\r\nFor instance, if we call df.write.mode(\"overwrite\").insertInto(table_name) then it's possible to control the table removal logic if we provide our own implementation of the spark.sql.sources.commitProtocolClass. In case of saveAsTable it makes no sense, because a table is unconditionally dropped before handling the CreateTable command.\r\n\r\nTherefore, is it possible to extract the table dropping logic from DataFrameWriter to the corresponding LogicalPlan commands (as it was done for V2 Datasources) in order to give more flexibility to Spark extensions?", "output": "Extract dropping table from catalog from DataFrameWriter in case of overwriting table during creation"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Kafka broker freezes and gets fenced during rolling restart with KRaft mode\nDescription: After upgrading our Kafka clusters to *Kafka 3.9.0* with *KRaft mode enabled* in production, we started observing strange behavior during rolling restarts of broker nodes — behavior we had never seen before.\r\n\r\nWhen a broker is {*}gracefully shut down by the KRaft controller{*}, it immediately restarts. Shortly afterward, while it is busy {*}replicating missing data{*}, the broker suddenly {*}freezes for approximately 20–50 seconds{*}. During this time, it produces {*}no logs, no metrics{*}, and *no heartbeat messages* to the controllers (see the timestamps below).\r\n\r\n \r\n{code:java}\r\n2025-07-30 09:21:27,224 INFO [Broker id=8] Skipped the become-follower state change for my-topic-215 with topic id Some(yO4CQayIRbyESrHHVPdOrQ) and partition state LeaderAndIsrPartitionState(topicName='my-topic', partitionIndex=215, controllerEpoch=-1, leader=4, leaderEpoch=54, isr=[4, 8], partitionEpoch=102, replicas=[4, 8], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 54. (state.change.logger) [kafka-8-metadata-loader-event-handler]\r\n\r\n2025-07-30 09:21:51,887 INFO [Broker id=8] Transitioning 427 partition(s) to local followers. (state.change.logger) [kafka-8-metadata-loader-event-handler]       {code}\r\n \r\n\r\nAfter this “hanging” period, the broker *resumes normal operation* without emitting any error or warning messages — as if nothing happened. However, during this gap, because the broker fails to send heartbeats to", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `JUnit` to 6.0.0\nDescription: \nQ: Issue resolved by pull request 383\n[https://github.com/apache/spark-kubernetes-operator/pull/383]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Make HadoopArchives support human-friendly units about blocksize and partsize.\nDescription: You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).\nQ: fuchaohong opened a new pull request, #7611:\nURL: https://github.com/apache/hadoop/pull/7611\n\n   You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).", "output": "hadoop-yetus commented on PR #7611:\nURL: https://github.com/apache/hadoop/pull/7611#issuecomment-2801557527\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 26s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-archives.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-archives.txt) |  hadoop-tools/hadoop-archives: The patch generated 2 new + 87 unchanged - 0 fixed = 89 total (was 87)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   1m 36s | [/patch-unit-hadoop-tools_hadoop-archives.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/1/artifact/out/patch-unit-hadoop-tools_hadoop-archives.txt) |  hadoop-archives in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 131m 39s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.TestHadoopArchives |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7611 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 73fa39924ad9 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d8951df897280187f60aac9cf80b2a3230e27aab |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/1/testReport/ |\r\n   | Max. process+thread count | 707 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-archives U: hadoop-tools/hadoop-archives |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "https://github.com/advisories/GHSA-4cx2-fc23-5wg6\r\n\r\nThought it was tidier to upgrade to latest version even if the fix was a while ago.", "output": "upgrade bouncycastle to 1.82 due to CVE-2025-8916"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Mark the minOneMessage as false when delayedRemoteFetch is present in the first partition\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add tree node pattern bits for supported expressions in ParameterizedQuery argument list\nDescription: In this PR I propose that we add tree node pattern bits for supported expressions in ParameterizedQuery argument list to prepare implementation of parameters in single-pass framework.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Fixing some typos, details and adding links for better readability in the documentation files for ABFS driver.", "output": "ABFS: [FnsOverBlob] Updating Documentations of Hadoop Drivers for Azure"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade AWS SDK to 2.35.4\nDescription: Upgrade to a recent version of 2.33.x or later while off the critical path of things.\r\n\r\n\r\nHADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.\nQ: steveloughran opened a new pull request, #7882:\nURL: https://github.com/apache/hadoop/pull/7882\n\n   ### How was this patch tested?\r\n   \r\n   Testing in progress; still trying to get the ITests working.\r\n   \r\n   JUnit5 update complicates things here, as it highlights that minicluster tests aren't working.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "pan3793 commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201364651\n\n   > JUnit5 update complicates things here, as it highlights that minicluster tests aren't working.\r\n   \r\n   I found `hadoop-client-runtime` and `hadoop-client-minicluster` broken during integration with Spark, HADOOP-19652 plus YARN-11824 recovers that, is it the same issue?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Disallow JUnit4 Imports After JUnit5 Migration\nDescription: As our project has fully migrated to JUnit5, we should now enforce a rule that prevents the import and usage of JUnit4 classes (such as org.junit.Test, org.junit.Assert, etc.) to ensure consistency, avoid regressions, and allow safe removal of legacy dependencies.\r\n\r\nThis task involves identifying and eliminating any remaining JUnit4 imports, and introducing static code analysis or linting rules to ban future usage.\nQ: hadoop-yetus commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3300118538\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m 33s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  47m 14s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 57s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 16s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |  23m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 32s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 51s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  36m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  73m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 45s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  48m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 25s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 47s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |  37m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  73m 40s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 503m 15s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   2m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 930m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7976 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux 946810b75b42 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 78ed6e9e068b8c35d3eb372e09440128bf4fd0d8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/testReport/ |\r\n   | Max. process+thread count | 3137 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7976/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on PR #7976:\nURL: https://github.com/apache/hadoop/pull/7976#issuecomment-3300985791\n\n   > +1. Thanks @slfan1989 .\r\n   > \r\n   > I was going to suggest also banning `org.hamcrest`, but it looks like there is still a tiny amount of hamcrest remaining in YARN. Maybe this is a topic for a different PR.\r\n   > \r\n   > ```\r\n   > > grep -r --include '*.java' 'org.hamcrest' *\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppListControllerTest.java:import static org.hamcrest.MatcherAssert.assertThat;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppListControllerTest.java:import static org.hamcrest.core.Is.is;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppDetailsControllerTest.java:import static org.hamcrest.MatcherAssert.assertThat;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppDetailsControllerTest.java:import static org.hamcrest.core.Is.is;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppStoreControllerTest.java:import static org.hamcrest.MatcherAssert.assertThat;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppStoreControllerTest.java:import static org.hamcrest.core.Is.is;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestResourceCalculatorProcessTree.java:import static org.hamcrest.MatcherAssert.assertThat;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestResourceCalculatorProcessTree.java:import static org.hamcrest.core.IsInstanceOf.instanceOf;\r\n   > hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestResourceCalculatorProcessTree.java:import static org.hamcrest.core.IsSame.sameInstance;\r\n   > ```\r\n   \r\n   @cnauroth Thank you for reviewing the code! I’ll submit a separate PR to replace the usage of `org.hamcrest.`."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Bug reported by Matt over common-dev discussion.\r\n\r\n> What seems to be the issue is that the timer tasks are cleaned up but\r\n> the timer threads themselves are never actually cleaned up. This will\r\n> eventually lead to an OOM since nothing is collecting these. I was\r\n> able to reproduce this locally in 3.3.6 and 3.4.1 but I believe that\r\n> it would affect any version that relies on autothrottling for ABFS.\r\n>\r\n> I was also able to make a quick fix as well as confirm a workaround --\r\n> the long term fix would be to include `timer.cancel()` and\r\n> `timer.purge()` in a method for AbfsClientThrottlingAnalyzer.java. The\r\n> short term workaround is to disable autothrottling and rely on Azure\r\n> to throttle the connections as needed with the below configuration.", "output": "[Bug Report] Thread leak in ABFS AbfsClientThrottlingAnalyzer"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Revisit gradle/spotbugs-exclude.xml\nDescription: The PR https://github.com/apache/kafka/pull/20294  will add a number of exclusions to the SpotBugs configurations, and it would be good to revisit them later to ensure that we are not overlooking potential bugs", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analaytics-Accelerator: ITestS3AContractOpen.testInputStreamReadNegativePosition() failing\nDescription: New test added in [https://github.com/apache/hadoop/pull/7367,]\r\n\r\nexpects an IndexOutOfBoundsException to be thrown, when offset is negative.\r\n\r\n \r\n\r\nAAL throws IllegalArgumentException. For len, the PR says it's ok to throw either IllegalArgumentException or IndexOutOfBoundsException, so should the same thing happen of offset?\nQ: I think it is somebody screwing up, so maybe we don't have to worry too much about the specifics. \r\n\r\nship it and see", "output": "any PR planned ?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Streaming List Path Result Should Happen Inside Retry Loop\nDescription: Listing APIs on both DFS and Blob Endpoints return response as part of response body and has to be read from an Socket Input Stream.\r\nAny network error occuring while reading the stream should be retried.\r\n\r\nToday, this parsing happens in client and such errors are not retried.\r\nThis change fixes this behavior\nQ: hadoop-yetus commented on PR #7582:\nURL: https://github.com/apache/hadoop/pull/7582#issuecomment-2780668739\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m  0s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  40m 25s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7582/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 40s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 43s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 153m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7582/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7582 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d4259183eced 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e3f272aaa285701ddea68e9fca0364940811e166 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7582/1/testReport/ |\r\n   | Max. process+thread count | 588 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7582/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "anmolanmol1234 commented on code in PR #7582:\nURL: https://github.com/apache/hadoop/pull/7582#discussion_r2030561973\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -1501,9 +1504,13 @@ public ListResponseData parseListPathResults(AbfsHttpOperation result, URI uri)\n       listResponseData.setContinuationToken(\n           getContinuationFromResponse(result));\n       return listResponseData;\n+    } catch (AbfsDriverException ex) {\n+      // Throw as it is to avoid multiple wrapping.\n+      LOG.error(\"Unable to deserialize list results for Uri {}\", uri != null ? uri.toString(): \"NULL\", ex);\n+      throw ex;\n     } catch (IOException ex) {\n-      LOG.error(\"Unable to deserialize list results for Uri {}\", uri.toString(), ex);\n-      throw new AbfsDriverException(ERR_DFS_LIST_PARSING, ex);\n+      LOG.error(\"Unable to deserialize list results for Uri {}\", uri != null ? uri.toString(): \"NULL\", ex);\n+      throw new AbfsDriverException(ex);\n\nReview Comment:\n   throw new AbfsDriverException(ERR_DFS_LIST_PARSING, ex); add the message here as well"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "It should include following tasks\r\n # rewrite by java\r\n # move to server module", "output": "Move `KRaftClusterTest` from core module to server module"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Unify kraft shutdown logic in poll methods\nDescription: Currently, different KRaft replica states handle polling during graceful shutdown differently. This Jira aims to unify their logic.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix dependency exclusion list of hadoop-client-runtime.\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add basic logging support\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "If during rebalance a failure occurs in the StateUpdater thread, and this failure leads to the thread shutdown, the Stream thread may get into an infinite wait state during it's shutdown.\r\n\r\nSee the attached test that reproduces the issue.", "output": "Failures in the StateUpdater thread may lead to inability to shut down a stream thread"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The test has been observed as flaky in the recent runs: [https://develocity.apache.org/scans/tests?search.names=CI%20workflow%2CGit%20repository&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI%2Chttps:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.consumer.ShareConsumerTest&tests.sortField=FLAKY&tests.test=testShareGroupMaxSizeConfigExceeded()%5B1%5D] \r\n\r\n \r\nAfter tracing the failures, the root cause was narrowed down to commit [https://github.com/apache/kafka/commit/87657fdfc721055835f5b1f22151c461e85eab4a]. This change introduced a new try/catch around {{handleCompletedAcknowledgements()}} in {{{}ShareConsumerImpl.java{}}}. The side effect is that all exceptions coming from the commit callback — including GroupMaxSizeReachedException — are now swallowed and only logged, preventing the test from ever receiving the exception.\r\n - If the ack callback fires while {{poll()}} is executing → exception is caught & swallowed → test times out\r\n - If the callback fires outside that path → exception escapes → test passes\r\n\r\nSo the same test randomly passes/fails depending on scheduling of the ack callback.\r\n\r\nAlso, the test testAcknowledgementCommitCallbackThrowsException has been marked as Flaky for the same Root cause.", "output": "Flaky test ShareConsumerTest. testShareGroupMaxSizeConfigExceeded"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Make open telemetry object instantiation configurable\nDescription: After the 3.7 release of Kafka, the java client will emit many warning messages and more objects deployed due to an open telemetry object instantiation (possibly due to KIP-714). It appears the open telemetry feature was enabled by default and there's no way for a user to disable it. \r\n\r\nUsers receive the INFO message defined [here |https://github.com/apache/kafka/blob/3.7.2/clients/src/main/java/org/apache/kafka/common/telemetry/internals/ClientTelemetryReporter.java#L479] after activating INFO log level for \"org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter\" class. This log line appears lots of times.\r\n\r\nIt appears that the occurrence of this info messages shows ClientTelemetry objects are being created if the feature is not explicitly disabled. Users would like this feature to be opt-in and not enabled by default as it is.\nQ: The user has the ability to disable this feature - enable.metrics.push=false", "output": "[~apoorvmittal10] You are entirely correct that KIP-714 tells us that client metrics are enabled by default on the clients and can be disabled by config. Of course, metrics push will only occur for a modern client, connected to a cluster with client telemetry enabled, and a valid metrics subscription. I wonder whether there is scope to adjust the logging."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove workaround for protoc on M1 mac and unused property build.platform\nDescription: \nQ: slfan1989 commented on PR #7783:\nURL: https://github.com/apache/hadoop/pull/7783#issuecomment-3064755942\n\n   @cnauroth Can you please review this PR? From my perspective, this seems reasonable.", "output": "jojochuang merged PR #7783:\nURL: https://github.com/apache/hadoop/pull/7783"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done.\nDescription: There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header.\r\n\r\nFollowing are types of read we want to identify:\r\n # Direct Read: Read from a given position in remote file. This will be synchronous read\r\n # Normal Read: Read from current seeked position where read ahead was bypassed. This will be synchronous read.\r\n # Prefetch Read: Read triggered from background threads filling up in memory cache. This will be asynchronous read.\r\n # Missed Cache Read: Read triggered after nothing was received from read ahead. This will be synchronous read.\r\n # Footer Read: Read triggered as part of footer read optimization. This will be synchronous.\r\n # Small File Read: Read triggered as a part of small file read. This will be synchronous read.\r\n\r\nWe will add another field in the Tracing Header (Client Request Id) for each r\nQ: hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135296997\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 17 new + 1 unchanged - 0 fixed = 18 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 28s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 28s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m 59s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 30s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7be945cb4d05 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 42ecdd05eb6954bdfd26d5022bf4c8a517c607ba |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/testReport/ |\r\n   | Max. process+thread count | 540 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135629353\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  2s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  2s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 58s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 18 new + 1 unchanged - 0 fixed = 19 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  46m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 42s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 148m 56s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5f43a0e97825 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 224f712fee069bd839f8c27a979367e61cec8c17 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/testReport/ |\r\n   | Max. process+thread count | 596 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "ShareConsumerConfig and ConsumerConfig are inherent at the moment.\r\nThe drawback is the config logic is mixed, for example: \r\nShareAcknowledgementMode.\r\nWe can decouple to prevent the logic is complicated in the future.", "output": "Decouple ConsumerConfig and ShareConsumerConfig"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Delete the UID in the dev container that is the same as the host user\nDescription: \nQ: Hi, is this a beginner issue to work on? Could anyone let me know where to start?", "output": "cnauroth closed pull request #7781: HADOOP-19606. Delete the UID in the dev container that is the same as the host user\nURL: https://github.com/apache/hadoop/pull/7781"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add tests to verify read/readstream options are actually respected by the flow that executes the read/readstream dataframe.\r\n\r\nTrivial test example might be:\r\n{code:python}\r\n@materialized_view def mv_from_csv():\r\n   return spark.read.option(\"delimiter\", \"|\").csv(\"/my/table.csv\")\r\n{code}\r\nI suspect that today, the read/readstream options will not be respected ([1|https://github.com/apache/spark/blob/master/sql/pipelines/src/main/scala/org/apache/spark/sql/pipelines/graph/FlowAnalysis.scala#L120], [2)|#L131].\r\n\r\nIf true, a solution might be to copy over the options in the `UnresolvedRelation` into either the DataFrameReader that is constructed or the `streamingReadOptions`/`batchReadOptions` argument.", "output": "[SDP] Test (and fix) read/readstream options are respected for pipelines"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc\r\ninstruction sets, with full functional verification and performance testing completed.\r\n\r\nThe implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction.\r\n\r\nKey Features: \r\n1. Runtime Hardware Detection\r\nThe PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime.\r\n\r\n2. Performance Improvement\r\nHardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation.", "output": "Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update num.recovery.threads.per.data.dir configs\nDescription: The default value of num.recovery.threads.per.data.dir is now 2 according to KIP-1030. We should update config files which are still setting 1.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update Surefire plugin to 3.5.3\nDescription: Hadoop currently uses the old 3.0.0-M4  surefire version, which swallows all exceptions thrown by @BeforeAll methods.\r\n\r\nThis leads to some tests not being run and not being reported as failing/erroring.\r\n\r\nUpdate to the latest version, which fixes this issue.\nQ: hadoop-yetus commented on PR #7574:\nURL: https://github.com/apache/hadoop/pull/7574#issuecomment-2775113259\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  81m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |   2m 31s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 14s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 28s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  87m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7574/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7574 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 9c5f5fcc3e4c 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 651b504e4396bfb5c3482d5f68957a4483f59a8c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7574/1/testReport/ |\r\n   | Max. process+thread count | 525 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7574/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7574:\nURL: https://github.com/apache/hadoop/pull/7574#issuecomment-2775737687\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  81m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |   2m 31s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 13s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 29s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  87m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7574/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7574 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 6a50f92a40a8 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 651b504e4396bfb5c3482d5f68957a4483f59a8c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7574/2/testReport/ |\r\n   | Max. process+thread count | 535 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7574/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Log auto topic creation failures more visibly\nDescription: Hi, I was playing with Share groups on the 4.1.0-rc2 and found it a little opaque to detect a failure to auto create the `__share_group_state` topic due to the replication factor not being satisfiable.\r\n\r\nI start up a cluster like this (taken from the dockerhub instructions):\r\n{code:java}\r\npodman run --rm \\\r\n  -p 9092:9092 \\\r\n  -e KAFKA_NODE_ID=1 \\\r\n  -e KAFKA_PROCESS_ROLES=broker,controller \\\r\n  -e KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \\\r\n  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\r\n  -e KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER \\\r\n  -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \\\r\n  -e KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9093 \\\r\n  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\r\n  -e KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1 \\\r\n  -e KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1 \\\r\n  -e KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0 \\\r\n  -e KAFKA_NUM_PARTITIONS=3 \\\r\n  apache/kafka:4.1.0-rc2{code}\r\nand then run the comma\nQ: Hi [~schofielaj]. Yes the docker images work smoothly when you don't set any of the configuration environment variables, as the default configuration file tunes the replication factors/isrs down to 1. The docs make it pretty clear that if you are using environment variable configuration that you are responsible for configuring {_}everything{_}.\r\n\r\nI guess the examples of environment variable configuration [https://hub.docker.com/r/apache/kafka#overriding-the-default-broker-configuration] could be updated since they already contain the equivalent settings for the transaction state topic. I imagine these examples should represent a working single-node configuration that users can then modify.", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have implemented create and rename recovery using client transaction IDs over the DFS endpoint ([HADOOP-19450] [ABFS] Rename/Create path idempotency client-level resolution - ASF JIRA). Since the backend changes were not fully rolled out, we initially implemented the changes with the flag disabled. With this update, we aim to enable the flag, which will start sending client transaction IDs. In case of a failure, we will attempt to recover from the failed state if possible. Here are the detailed steps and considerations for this process:\r\n\r\n1. **Implementation Overview**: We introduced a mechanism for create and rename recovery via client transaction IDs to enhance reliability and data integrity over the DFS endpoint. This change was initially flagged as disabled due to incomplete backend rollouts.\r\n\r\n2. **Current Update**: With the backend changes now in place, we are ready to enable the flag. This will activate the sending of client transaction IDs, allowing us to track and manage transactions more effectively.\r\n\r\n3. **Failure Recovery**: The primary advantage of enabling this flag is the potential for recovery from failed states. If a transaction fails, we can use the client transaction ID to attempt a recovery, minimizing data loss and ensuring continuity.\r\n\r\n4. **Next Steps**: We will proceed with enabling the flag and closely monitor the system's performance. Any issues or failures will be documented and addressed promptly to ensure a smooth transition.", "output": "[ABFS] Enable rename and create recovery from client transaction id over DFS endpoint"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Pin robotframework version\nDescription: {{hadoop-runner}} installs {{robotframework}} without version definition.  Re-building the image for unrelated changes may unexpectedly upgrade {{robotframework}}, too.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: TopologyTestDriver spends most of its time forcing changes to disk with persistent state stores\nDescription: We are using TopologyTestDriver \r\n\r\nIt looks like the test driver is syncing the entire state directory to disk for every record processed:\r\n\r\n!image-2025-09-08-14-07-05-768.png!\nQ: I apologize, this was actually a correlated change on our end, not a regression in Kafka Streams.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip `test_to_feather` in Python 3.14\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implement transform in column API in PySpark\nDescription: related to https://issues.apache.org/jira/browse/SPARK-53779", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add bitmap_and_agg aggregation function\nDescription: Introduce a function analogous to bitmap_or_agg, but performing a bitwise AND operation instead of OR.\r\n\r\nSpecifically, the bitmap_and_agg function should output a bitmap that represents the bitwise AND of all bitmaps in the input column. The input column must contain bitmaps generated from bitmap_construct_agg(). \r\n\r\nExample:\r\n{code:java}\r\n>>> from pyspark.sql import functions as sf\r\n>>> df = spark.createDataFrame([(\"30\",),(\"70\",),(\"F0\",)], [\"a\"])\r\n>>> df.select(sf.bitmap_and_agg(sf.to_binary(df.a, sf.lit(\"hex\")))).show()\r\n\r\n+--------------------------------+\r\n|bitmap_and_agg(to_binary(a, hex))|\r\n+--------------------------------+\r\n|            [30 00 00 00 00 0...|\r\n+--------------------------------+{code}", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used.\r\nThis is to stop any user explicitly enabling the config to enable RBMV2.", "output": "ABFS: Read Buffer Manager V2 should not be allowed untill implemented"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use Swift 6.2 as the minimum supported version\nDescription: \nQ: Issue resolved by pull request 257\n[https://github.com/apache/spark-connect-swift/pull/257]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Relax state directory file system restrictions\nDescription: The implementation of permission restriction by https://issues.apache.org/jira/browse/KAFKA-10705 is very restrictive on groups. Existing group write permissions should be kept.\r\n\r\nWe could also make this configurable, which would require a KIP.\r\n\r\nKIP-1230 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1230%3A+Add+config+for+file+system+permissions]", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`\nDescription: HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found.\r\n\r\nI think the cause is:\r\n * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration`\r\n * However, code paths to load AWS credential provider call `S3AUtils.getInstanceFromReflection`, which uses the classloader that loaded the S3AUtils class. That's likely to be the built-in application classloader, which won't be able to load classes in a Spark job JAR.\r\n\r\nAnd the fix seems small:\r\n * Change `S3AUtils.getInstanceFromReflection` to load classes using the `Configuration`'s classloader. Luckily we already have the Configuration in this method.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "If there's a failure in the kafka consumer constructor, we attempt to close it https://github.com/lianetm/kafka/blob/2329def2ff9ca4f7b9426af159b6fa19a839dc4d/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L540\r\nIn that case, it could be the case that some components may have not been created, so we should consider some null checks to avoid noisy logs about NPE. \r\n\r\nThis noisy logs have been reported with the console share consumer in a similar scenario, so this task is to review and do a similar fix for the Async if needed.", "output": "Avoid noisy NPE logs when closing consumer after constructor failures"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The JDK version used by `ci-complete` is inconsistent with other active branches, which is causing the build report task to fail", "output": "`ci-complete` needs to work with active branches after the JDK is updated"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix SpotBugs warnings introduced after SpotBugs version upgrade.\nDescription: Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged.\r\nWe plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules.\nQ: Hi [~slfan1989] \r\nThanks for tracking this.\r\n\r\nWe do have a bunch of PRs open that are facing issue reported here.\r\nWhat are the expectations here? Do we need to address all warnings with the PR itself or they can be ignored and taken later as part of this Jira?\r\n\r\nI think later would be better. It will help keep the changes in PR limited to what is being done and will ease the review process.", "output": "I agree with your point. We’ll work on submitting a common PR that includes a SpotBugs rule to temporarily suppress the new static analysis warnings and restore the state back to the 4.2.0 level."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support changing stateless shuffle partitions upon restart of streaming query\nDescription: We have been having a huge restriction on the number of shuffle partitions in streaming - once the streaming query runs, there is no way but discard the checkpoint to change the number of shuffle partitions. There has been consistent requests for unblocking this.\r\n\r\nThe main reason of the limitation is due to the fact the stateful operator has fixed partitions and we should make sure it is unchanged. While the invariant is not changed, there is no technical reason to also disallow changing of the number of shuffle partitions in stateless shuffle e.g. stream-static join, MERGE INTO, etcetc.\nQ: Issue resolved by pull request 52645\n[https://github.com/apache/spark/pull/52645]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Retrieve dependency version from pom.xml for DependencyOverrides in SparkBuild.scala\nDescription: Currently, version strings for some dependencies are hard coded in DependencyOverrides in SparkBuild, so developers need to keep consistent with version strings specified in pom.xml manually.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][Tests] Update Test Scripts to Run Tests with Blob Endpoint\nDescription: Following Cobination of test Suites will run as a part of CI after blob endpoint support has been added.\r\n\r\nHNS-OAuth-DFS\r\nHNS-SharedKey-DFS\r\nNonHNS-SharedKey-DFS\r\nAppendBlob-HNS-OAuth-DFS\r\nNonHNS-SharedKey-Blob\r\nNonHNS-OAuth-DFS\r\nNonHNS-OAuth-Blob\r\nAppendBlob-NonHNS-OAuth-Blob\r\nHNS-Oauth-DFS-IngressBlob\r\nNonHNS-Oauth-DFS-IngressBlob\nQ: anmolanmol1234 commented on code in PR #7344:\nURL: https://github.com/apache/hadoop/pull/7344#discussion_r1938879600\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -524,10 +524,10 @@ public boolean isDfsToBlobFallbackEnabled() {\n   public void validateConfiguredServiceType(boolean isHNSEnabled)\n       throws InvalidConfigurationValueException {\n     // TODO: [FnsOverBlob][HADOOP-19179] Remove this check when FNS over Blob is ready.\n-    if (getFsConfiguredServiceType() == AbfsServiceType.BLOB) {\n-      throw new InvalidConfigurationValueException(FS_DEFAULT_NAME_KEY,\n-          \"Blob Endpoint Support not yet available\");\n-    }\n+//    if (getFsConfiguredServiceType() == AbfsServiceType.BLOB) {\n\nReview Comment:\n   or this change has been removed as part of documentation PR", "output": "anmolanmol1234 commented on code in PR #7344:\nURL: https://github.com/apache/hadoop/pull/7344#discussion_r1938880019\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsPermission.java:\n##########\n@@ -68,7 +69,7 @@ public boolean equals(Object obj) {\n    * @return a permission object for the provided string representation\n    */\n   public static AbfsPermission valueOf(final String abfsSymbolicPermission) {\n-    if (abfsSymbolicPermission == null) {\n+    if (StringUtils.isEmpty(abfsSymbolicPermission)) {\n\nReview Comment:\n   already added in rename PR"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When I run JoinWithIncompleteMetadataIntegrationTest, with only the new protocol enabled, it fails (since it does not run long enough for the exception to be triggered by the timer).\r\n\r\n \r\n\r\nWhen the test is run for both protocols, the second run passes because it uses an existing group ID and that is why it why streams is shutting down.", "output": "JoinWithIncompleteMetadataIntegrationTest fails in isolated run of one parameter"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add built-in TTL (Time-to-Live) support for Kafka Streams State Stores\nDescription: In business cases Kafka Streams users frequently need *per-key Time-To-Live (TTL)* behavior for state stores such as keyValueStore or kTables , typically to model cache-like or deduplication scenarios.\r\n\r\nToday, achieving this requires *manual handling* using:\r\n * Custom timestamp tracking per key,\r\n\r\n * Punctuators to periodically scan and remove expired entries, and\r\n\r\n * Manual emission of tombstones to maintain changelog consistency.\r\n\r\nThese workarounds are:\r\n * {*}Inconsistent across applications{*}, and\r\n\r\n * {*}Operationally costly{*}, as each developer must reimplement the same logic.\r\n\r\n*Proposal* is to introduce a *built-in TTL mechanism* for Kafka Streams state stores, allowing automatic expiration of records after a configured duration.\r\n\r\nIntroduction to new Api's like : \r\n\r\nStoreBuilder withTTL(Duration ttl);\r\nMaterialized withTtl(Duration ttl);\r\n\r\nWhen configured:\r\n * Each record’s timestamp (from event-time or processing-time) is tracked.\r\n\r\n * Expired keys are automat\nQ: Dear [~ankursinha07] -\r\n * I see the positives in implementing TTL concept since it's augmenting developer's productivity by eliminating repetitive TTL boilerplate and punctuator code. \r\n * It ensures changelog consistency with automatic tombstones.\r\n * Opt-in design maintains existing store behavior.\r\n\r\nFew aspects which we should thought about - \r\n * Eviction tasks might increase I/O load on RocksDB, especially in large stores.\r\n * For operational needs - A metrics/tracing hooks (e.g., number of expired keys, sweep duration, lag in eviction) to monitor TTL behavior.\r\n * During changelog restoration, TTL-expired records should not be reloaded — implementation must ensure cleanup consistency across restore cycles.\r\n * Deleting many keys generates tombstones - the compaction strategy must efficiently reclaim disk space.\r\n\r\n \r\n\r\noverall, built-in TTL is a meaningful option simplifying state management and improving reliability in long-running Kafka Streams applications. With careful attention to eviction performance and observability, this feature could significantly elevate the developer experience.", "output": "Is there any difference to https://issues.apache.org/jira/browse/KAFKA-4212 – or is this ticket a duplicate?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Various analyses done in the past have shown a need for significant improvement in the performance of sequential reads. The current implementation clearly shows the lack of parallelism that is needed to cater to high throughput sequential read workloads. \r\nMore details on updated design and results of POC benchmarking will be added here soon.", "output": "ABFS: [ReadAheadV2] Increase Prefetch Aggressiveness to improve sequential read performance"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "DescribeTopicsOptions.partitionSizeLimitPerResponse is a prerequisite for supporting pagination, which is useful for requests involving a large number of topics.\r\n\r\nCurrently, when calling Admin#describeTopics(TopicCollection) with topic IDs, KafkaAdminClient uses a MetadataRequest under the hood. However, MetadataRequest does not support the partitionSizeLimitPerResponse option. This means that pagination is effectively unsupported for topic ID–based requests.\r\n\r\nIn contrast, partitionSizeLimitPerResponse is supported by DescribeTopicPartitionsRequest. However, this RPC currently only supports topic names, not topic IDs. To enable full pagination support across both topic names and topic IDs, we would need to extend DescribeTopicPartitionsRequest to accept topic IDs as an additional field.", "output": "Support partitionSizeLimitPerResponse for topicId–based describeTopics request"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "- The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11.\r\n\r\n - The powermock dependency is specified in the pom below. We should replace it with mockito since it is known to be incompatible with JDK17.\r\n -* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml", "output": "[JDK17] Remove mockito-all 1.10.19 and powermock"}
{"instruction": "Answer the question based on the bug.", "input": "Title: kafka-broker-api tool should support to get controller api version\nDescription: like tool kafka-broker-api-versions.sh, we can get all RPC version from broker.\r\n\r\nIt should also support for controller.\r\n\r\nKIP: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1220%3A+kafka-broker-api-versions+tool+support+controllers]\nQ: I will prepare KIP ASAP.", "output": "If all we want is to list the RPC versions of the \"voters\", this can be done with `DescribeQuorumRequest`. That means we don't need a KIP to support `bootstrap.controller`. By contrast, we would need a KIP if we wanted to list all the RPCs of \"observers\"\r\n\r\n[~taijuwu] [~schofielaj] WDYT?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Revoked partitions are included in currentOffsets passed to preCommit on task stop\nDescription: When a task stops, [{{WorkerSinkTask#closeAllPartitions()}}|https://github.com/apache/kafka/blob/84caaa6e9da06435411510a81fa321d4f99c351f/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L664] runs and the [task’s {{preCommit}} is invoked|https://github.com/apache/kafka/blob/3c7f99ad31397a6a7a4975d058891f236f37d02d/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L444]. At that time, the {{currentOffsets}} argument may include {*}revoked partitions{*}.\r\n\r\nThis appears to be caused by:\r\n * {{WorkerSinkTask}} does not remove revoked partitions from {{currentOffsets}}\r\n * {{WorkerSinkTask#closeAllPartitions()}} passes its {{currentOffsets}} to {{SinkTask#preCommit(...)}} _as-is_ (i.e., without filtering).\r\n\r\nDuring normal iterations, {{SinkTask#preCommit(...)}} receives {{{}KafkaConsumer#assignment(){}}}, so revoked partitions are *not* included.\r\n\r\nHaving revoked partitions included *only* at stop is confusing behavior. \nQ: Added clarification in https://github.com/apache/kafka/pull/20756.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Disable topic autocreation for streams consumers.\nDescription: Currently we disable it only for [the main consumer|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1793], but not for the [restore|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1832] or [global|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1865] consumers.\nQ: Hi, [~goyarpit]. \r\nyes, please", "output": "[~nikita-shupletsov]  I saw in main consumer we made this change after override . So should we allow the overrride for this field  or not ?. I will make the changes accordingly."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade nimbusds to 10.0.2\nDescription: Includes fix for CVE-2025-53864\nQ: [~ananysin] the version is wrong - we need at least 10.0.2 to fix https://www.cve.org/CVERecord?id=CVE-2025-53864\r\n\r\nI would suggest using the latest - 10.4", "output": "Sure [~fanningpj]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Exclude `.idea` directory from `Spotless` check\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Publish multi-arch hadoop-runner image to GitHub\nDescription: Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image to GitHub Container Registry.  Build for both amd64 and arm64.\nQ: adoroszlai opened a new pull request, #8021:\nURL: https://github.com/apache/hadoop/pull/8021\n\n   ## What changes were proposed in this pull request?\r\n   \r\n   Add workflow to publish `apache/hadoop-runner` (`jdk11-u2204` in this case) to GitHub Container Registry.\r\n   \r\n   https://issues.apache.org/jira/browse/HADOOP-19720\r\n   \r\n   ## How was this patch tested?\r\n   \r\n   [Workflow run](https://github.com/adoroszlai/hadoop/actions/runs/18353000644) in my fork for push to branch `docker-hadoop-runner-HADOOP-19720-jdk11-u2204` built [image](https://github.com/adoroszlai/hadoop/pkgs/container/hadoop-runner/538747134?tag=HADOOP-19720-jdk11-u2204) `ghcr.io/adoroszlai/hadoop-runner:HADOOP-19720-jdk11-u2204`.  It has both amd64 and arm64 arch.\r\n   \r\n   ```bash\r\n   $ docker run -it --rm ghcr.io/adoroszlai/hadoop-runner:HADOOP-19720-jdk11-u2204 bash -c 'uname -a; cat /etc/lsb-release; java -version'\r\n   Linux 8099f50d7322 6.8.0-65-generic #68~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 15 18:06:34 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n   DISTRIB_ID=Ubuntu\r\n   DISTRIB_RELEASE=22.04\r\n   DISTRIB_CODENAME=jammy\r\n   DISTRIB_DESCRIPTION=\"Ubuntu 22.04.5 LTS\"\r\n   openjdk version \"11.0.28\" 2025-07-15\r\n   OpenJDK Runtime Environment Temurin-11.0.28+6 (build 11.0.28+6)\r\n   OpenJDK 64-Bit Server VM Temurin-11.0.28+6 (build 11.0.28+6, mixed mode, sharing)\r\n   ```", "output": "slfan1989 commented on PR #8021:\nURL: https://github.com/apache/hadoop/pull/8021#issuecomment-3383664657\n\n   @adoroszlai Many thanks for your contribution. Could we consider upgrading the image to ubuntu 24.04?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-fs2img.\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: NPE in dependency-check of hadoop-thirdparty\nDescription: the dependency checker of hadoop-thirdparty PRs fails with an NPE in the plugin.\r\n{code}\r\nError:  Failed to execute goal org.owasp:dependency-check-maven:6.1.5:aggregate (default-cli) on project hadoop-thirdparty: Fatal exception(s) analyzing Apache Hadoop Third-party Libs: One or more exceptions occurred during analysis:\r\nError:  \tUpdateException: java.util.concurrent.ExecutionException: java.lang.NullPointerException\r\nError:  \t\tcaused by ExecutionException: java.lang.NullPointerException\r\nError:  \t\tcaused by NullPointerException: null\r\n{code}\r\n\r\n\r\nThere is a much newer version of the library, but it is a java11 release. we will need to find the most recent release without this issue that is java8 only.\r\n\r\nAgain, this highlights why trying to continue build on java8 is futile", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce the framework for adding ST functions in Scala\nDescription: \nQ: Issue resolved by pull request 52803\n[https://github.com/apache/spark/pull/52803]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade to hadoop-thirdparty 1.4.0\nDescription: HADOOP-19483 releases thirdparty 1.4.0; upgrade hadoop to it\nQ: cnauroth commented on code in PR #7560:\nURL: https://github.com/apache/hadoop/pull/7560#discussion_r2021689285\n\n\n##########\nLICENSE-binary:\n##########\n@@ -243,7 +243,7 @@ com.google.guava:listenablefuture:9999.0-empty-to-avoid-conflict-with-guava\n com.microsoft.azure:azure-storage:7.0.0\n com.nimbusds:nimbus-jose-jwt:9.37.2\n com.zaxxer:HikariCP:4.0.3\n-commons-beanutils:commons-beanutils:1.9.4\n+commons-beanutils:commons-beanutils:1.9.4j\n\nReview Comment:\n   Typo?", "output": "hadoop-yetus commented on PR #7560:\nURL: https://github.com/apache/hadoop/pull/7560#issuecomment-2767686551\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m  8s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 50s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  18m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 17s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 18s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |  10m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 20s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  1s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  29m 41s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 25s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |  17m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  8s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  11m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 24s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 59s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  31m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 523m 38s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7560/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 54s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 690m 57s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.client.api.impl.TestYarnClient |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClient |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMProxy |\r\n   |   | hadoop.yarn.client.api.impl.TestNMClient |\r\n   |   | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7560/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7560 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux cb9454d4fedf 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0052185cd761c63b90219fdfd5ebcc28fb0b5bc8 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7560/1/testReport/ |\r\n   | Max. process+thread count | 3503 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7560/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob][BugFix] IsNonEmptyDirectory Check should loop on listing using updated continuation token\nDescription: As part of recent change, loop over listing was added when checking if a directory is empty or not.\r\nContinuation token was not getting updated in subsequent listblob calls leading to infinite loop of list calls.\r\n\r\nCaused by: https://issues.apache.org/jira/browse/HADOOP-19572", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade gson due to security fixes\nDescription: not sure why https://github.com/google/gson/pull/2588 didn't attract a CVE\r\n\r\nlinked to https://issues.apache.org/jira/browse/HADOOP-19632 - nimbus-jose-jwt uses gson\nQ: pjfanning opened a new pull request, #7833:\nURL: https://github.com/apache/hadoop/pull/7833\n\n   Update LICENSE-binary\r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   https://issues.apache.org/jira/browse/HADOOP-19643\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7833:\nURL: https://github.com/apache/hadoop/pull/7833#issuecomment-3134038336\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 23s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  33m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m 49s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 49s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  51m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 37s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |  30m 48s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |  15m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 51s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 22s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  56m  3s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 339m 51s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 16s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 629m  8s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.federation.policies.amrmproxy.TestLocalityMulticastAMRMProxyPolicy |\r\n   |   | hadoop.crypto.key.kms.server.TestKMSAudit |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7833 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux 3d08bdf50c29 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 04292f85b2cde016fe4e4c12bfe4c8cd1c040dd4 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/testReport/ |\r\n   | Max. process+thread count | 3066 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix Unit Test Failure in ITestAzureBlobFileSystemMainOperation\nDescription: The unit test failure in ITestAzureBlobFileSystemMainOperation is due to the missing {{@BeforeEach}} and {{@AfterEach}} annotations. This occurs because in JUnit 5, if a subclass overrides the parent class's {{setup}} and {{tearDown}} methods, the {{@BeforeEach}} and {{@AfterEach}} annotations must be explicitly added.\nQ: slfan1989 closed pull request #7769: HADOOP-19601. Fix Unit Test Failure in ITestAzureBlobFileSystemMainOperation.\nURL: https://github.com/apache/hadoop/pull/7769", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Handle module JVM options in a centralized manner\nDescription: Hadoop currently adds some module options needed for Java9+ in an ad-hoc manner.\r\n\r\nThese should be centralized, and automatically added whenever a new JVM is launched.\r\n\r\n(and in the startup scripts)\r\n\r\n\r\nThe list of options in HBase is a superset of those needed by Hadoop, and could be used as a starting point.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Relax offset commit validation to allow member epochs since assignment\nDescription: h2. Fencing offset commits\r\n\r\nIn the Kafka protocol, when a consumer commits offsets or a producer tries to add offsets to a transaction, it includes its epoch/generation of the consumer group. The point of this is for the group coordinator to fence against zombie commit requests, that is, commit requests that include an offset for a partition that was since reassigned to a different member. If such a guard was not in place, a zombie offset commit may overwrite offsets of the new owner, or its offsets may be committed to the consumer offset topic but not be included in the result of the new owners offset fetch request.\r\n\r\nIn KIP-848, when receiving an offset commit request that includes the client-side member epoch and a member ID, the group coordinator performs the check\r\n\r\n{{Client-Side Member Epoch == Broker-Side Member Epoch}}\r\n\r\nand, if the check fails, returns a {{STALE_MEMBER_EPOCH}} error for regular offset commits and a {{ILLEGAL_GENERATION}} for transactional offset commits. If the member epoch sent in the request is the current broker-side member epoch, KIP-848 guarantees that the partition cannot also be owned by a different member at the same or a larger epoch. Therefore, this is sufficient for fencing zombie commits. Note that we assume zombie commits will always contain offsets for partitions that were owned by the member at the member epoch sent in the request. Commit requests that commit offsets for partitions that are _not_ owned by the member in that epoch,", "output": "Patch Available"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add disk usage threshold to prevent disk full failure\nDescription: Currently in our production environment, the most common problem with Kafka is disk full failure. When the failure occurs, Kafka cannot provide any services, and can only be recovered by manually deleting disk data or expanding the disk capacity and then restarting the service.\r\n\r\nIn contrast, RocketMQ and RabbitMQ already support disk threshold configuration (such as {{diskMaxUsedSpaceRatio}} , {{disk_free_limit}} ). When the disk usage exceeds the limit, the system rejects new produce, but the service remains available.\r\n\r\nSo we hope to implement a similar strategy in Kafka to prevent the disk full failure.\r\n\r\nKIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1201%3A+Add+disk+threshold+config+to+prevent+disk+full+failure", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923] \r\n\r\n*Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future)\r\n\r\n*Related Gradle issues and links:*\r\n *  [https://github.com/gradle/gradle/issues/34643]\r\n * [https://github.com/gradle/gradle/issues/30871]  \r\n * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives]\r\n * [https://docs.gradle.org/9.1.0/userguide/upgrading_major_version_9.html#reproducible_archives_by_default] \r\n * [https://docs.gradle.org/9.1.0/dsl/org.gradle.api.tasks.bundling.Tar.html#org.gradle.api.tasks.bundling.Tar:preserveFileTimestamps]  \r\n\r\n \r\n\r\n*Definition of done (at the minimum):*\r\n * *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc) : [https://kafka.apache.org/quickstart]", "output": "Turn on Gradle reproducible builds feature"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed.\r\n{code:java}\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[216,50] package org.checkerframework.checker.nullness.qual does not exist\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[2509,30] cannot find symbol\r\n[ERROR]   symbol:   class NonNull\r\n[ERROR]   location: class org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.AsyncThreadFactory\r\n {code}\r\nI checked the usage in the related modules, and we should use *org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.NonNull* instead of directly using {*}org.checkerframework.checker.nullness.qual.NonNull{*}.", "output": "Resolve build error caused by missing Checker Framework (NonNull not recognized)"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*Problem*\r\nWhen running *{{:core:compileScala}}* under {*}{{keepAliveMode=SESSION}}{*}, joint compilation adds a non-existent directory ({_}{{.../core/build/classes/java/main}}{_}) to {{{}javac{}}}’s classpath.\r\nThis triggers:\r\n{code:java}\r\nUnexpected javac output: warning: [path] bad path element \".../core/build/classes/java/main\": no such file or directory\r\nerror: warnings found and -Werror specified {code}\r\nThe same task succeeds under {{{}keepAliveMode=DAEMON{}}}.\r\n\r\n*Current Workaround*\r\nAdded a scoped compiler flag ({{{}-Xlint:-path{}}}) only under {{keepAliveMode=SESSION}} to suppress the {{[path]}} warning and allow the build to succeed.\r\n\r\n*Steps to Reproduce*\r\n{code:java}\r\n./gradlew clean :core:compileScala -PkeepAliveMode=session   # fails before workaround  \r\n./gradlew clean :core:compileScala -PkeepAliveMode=daemon    # succeeds   {code}\r\n*Goal of This Ticket*\r\nIdentify why SESSION-mode joint compilation includes the missing {{classes/java/main}} entry in {{{}javac{}}}’s classpath, align its behavior with DAEMON mode, and remove the temporary suppression once the classpath is corrected.", "output": "Session mode compileScala reports missing classes java main directory"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix TestTextCommand on Windows\nDescription: The following 2 tests are failing in TestTextCommand -\r\n1. testDisplayForNonWritableSequenceFile\r\n2. testDisplayForSequenceFileSmallMultiByteReads\r\n\r\nThese tests create a file and flush a string ending with \"\\n\". However, for verification, the test expects \"\\r\\n\" as the line ending on Windows. Thus, the test fails.\nQ: hadoop-yetus commented on PR #7679:\nURL: https://github.com/apache/hadoop/pull/7679#issuecomment-2869010098\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 16s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 16s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  44m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 55s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 21s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 12s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m  7s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 227m 19s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7679/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7679 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux e93e5cdd2cc7 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a1fdc69daaa1f5f477df15bd17e3c25c517ea058 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7679/1/testReport/ |\r\n   | Max. process+thread count | 3135 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7679/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "Merged PR to trunk - https://github.com/apache/hadoop/pull/7679."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: S3 Express bucket failure of conditional overwrite of multiparts is 200 + error, not 412\nDescription: The S3 Express response to create conflicts of multipart uploads is not a 412/419, it is 200 + an xml error.\r\n\r\n\r\n{code}\r\n\r\nError \r\n\r\nCode: PreconditionFailed\r\nMessage: At least one of the pre-conditions you specified did not hold\r\nCondition: If-None-Match\r\nRequestId: 01e17cb4f5000197899662aa050968218250cedb\r\nHostId: K1OApth\r\n\r\n{code}\r\n\r\nThis is raised an exception in the SDK, but it isn't mapping to a 419/412, the way we expect, so is mapped to\r\na generic AWSS3IOException.\r\n\r\n{code}\r\n\r\nAWSS3IOException: Completing multipart upload on test/testIfNoneMatchConflictOnMultipartUpload: software.amazon.awssdk.services.s3.model.S3Exception: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 200, Request ID: 0110480881000197899bc814050902966399617b, Extended Request ID: y1E4oDawYBQ7XsAVFA):PreconditionFailed: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 200, Request ID: 0110480881000197899bc814050902966399617b, Extended Request ID: y1E4oDawYBQ7XsAVFA)\r\n{code}\r\n\r\nIf this is expected behaviour of S3 express:\r\n* docs need updating\r\n* unless it is on the SDK roadmap, we should catch and map to condition failures, presumably RemoteFileChangedException,", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title:  Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c\nDescription: ## Description\r\n \r\nThere is a duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c` at lines 29-30.\r\n\r\n## Current Code\r\n```c\r\n#include \"gcc_optimizations.h\"\r\n#include \"gcc_optimizations.h\"\r\n```\nQ: hadoop-yetus commented on PR #7899:\nURL: https://github.com/apache/hadoop/pull/7899#issuecomment-3224065363\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  28m 36s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 57s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 50s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 107m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  14m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 54s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 43s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  23m 14s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 223m 13s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7899 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux e8837fccd16c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c1d9fb6a820785db99554892fe312f5135201bf3 |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/testReport/ |\r\n   | Max. process+thread count | 1253 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7899/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "PeterPtroc commented on PR #7899:\nURL: https://github.com/apache/hadoop/pull/7899#issuecomment-3224106808\n\n   I'll push an empty commit to re-run the tests."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Optimize `sparkapps.sh` to use `kubectl delete --all`\nDescription: \nQ: Issue resolved by pull request 400\n[https://github.com/apache/spark-kubernetes-operator/pull/400]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "A lot of ByteBuf leaks are reported when running with JDK23.\r\n\r\nI have not idea why those are not detected with JDK8, 11 or 17. Maybe something about the logging setup ?", "output": "Fix ByteBuf leaks in TestShuffleChannelHandler"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support multiple files in `pyFiles`\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{code:java}\r\n[warn]  Resolution\r\n of the configuration :tools:tools-api:runtimeClasspath was attempted \r\nfrom a context different than the project context. Have a look at the \r\ndocumentation to understand why this is a problem and how it can be \r\nresolved. This behavior has been deprecated. (1)    - [warn]  Resolution\r\n of the configuration :tools:tools-api:runtimeClasspath was attempted \r\nfrom a context different than the project context. Have a look at the \r\ndocumentation to understand why this is a problem and how it can be \r\nresolved. This behavior has been deprecated.This will fail with an error in Gradle 9.0.        - Locations- ``- `:core:releaseTarGz` {code}\r\nThe issue was introduced by [https://github.com/apache/kafka/pull/13454]\r\n\r\n \r\n\r\n`tools-api` is already in core module runtime path, so adding it to `releaseTarGz` causes the resolution conflicts, which will be a fatal error in gradle 9", "output": "Address the compileClasspath resolution warnings for the `releaseTarGz` task"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix file permission errors as per the platform\nDescription: The file permission denial error message in Linux systems end with \"(Permission denied)\" particularly.\r\nHowever, an error message in the same scenario on Windows ends with \"(Access is denied)\" error.\r\n\r\nThis results in a bug in *org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker* and also leads to a unit test failure *org.apache.hadoop.fs.TestFsShellCopy#testPutSrcFileNoPerm*.\r\n\r\nThus, we need to make the appropriate check in accordance with the platform.\nQ: slfan1989 commented on PR #7767:\nURL: https://github.com/apache/hadoop/pull/7767#issuecomment-3081984805\n\n   @GauthamBanasandra Thank you for your contribution — I believe this PR is ready to be merged.", "output": "GauthamBanasandra commented on PR #7767:\nURL: https://github.com/apache/hadoop/pull/7767#issuecomment-3082098761\n\n   Yes @slfan1989 . It's ready. But since this PR changes a source file, I'm trying to get the Yetus to run locally on this one, so that we run all the tests.\n   \n   The Jenkins build agents for Windows are failing due to low disk space. Hence I'm working on getting Yetus to run locally.\n   \n   I should be able to merge this by next week."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update streams documentation with KIP-1147 changes\nDescription: docs/streams/quickstart.html and docsk/streams/developer-guide/datatypes.html both need to be updated to change the flags for kafka-console-consumer.sh to rename --property to --formatter-property.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [ReadAheadV2] Refactor ReadBufferManager to isolate new code with the current working code\nDescription: Read Buffer Manager used today was introduced way back and has been stable for quite a while.\r\nRead Buffer Manager to be introduced as part of https://issues.apache.org/jira/browse/HADOOP-19596 will introduce many changes incrementally over time. While the development goes on and we are able to fully stabilise the optimized version we need the current flow to be functional and undisturbed. \r\n\r\nThis work item is to isolate that from new code by refactoring ReadBufferManager class to have 2 different implementations with same public interfaces: ReadBufferManagerV1 and ReadBufferManagerV2.\r\n\r\nThis will also introduce new configs that can be used to toggle between new and old code.", "output": "In Progress"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, we have a feature to exclude weak ciphers from *HttpServer2* and *SSLFactory* using the *ssl.server.exclude.cipher.list property*. \r\nWith this feature, we can also define an inclusion list of ciphers using the *ssl.server.include.cipher.list property*. \r\nIf the inclusion list is populated, any cipher not present in the list will not be allowed. \r\nIf a cipher is present in both the exclusion and inclusion lists, it will be excluded.\r\nNote that SSLFactory does not support regex-based cipher patterns, unlike HttpServer2.", "output": "Include cipher feature for HttpServer2 and SSLFactory"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "It was noticed that `ExplodeBase.eval` returns an IterableOnce[InternalRow].  The current implementation creates a pre-allocated array, populates the array appropriately, and returns the Array.  This works as the is an implicit conversion from Array to IterableOnce.\r\n\r\nHowever Allocating and populating an array does not seem to provide benefits over exposing an iterator over the input data type.\r\n\r\nA proposed PR removes the creation and population of the array and instead returns a IterableOnce object that iterates over the underlying input.", "output": "ExplodeBase.eval Iterate directly on input"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: SDK reads content twice during PUT to S3 Express store.\nDescription: The commit on trunk {{60c7d4fea010}} and on branch 3.4 {{f3ec55b}} fixes the logging, but does not\r\naddress the underlying issue.\r\n\r\nh2. problem\r\n\r\nDuring PUT calls, even of 0 byte objects, our UploadContentProver is reporting a recreation of \r\nof the input stream of an UploadContentProvider, as seen by our logging at info of this happening\r\n\r\n{code}\r\n bin/hadoop fs -touchz $v3/4\r\n2025-03-26 13:38:53,377 [main] INFO  impl.UploadContentProviders (UploadContentProviders.java:newStream(289)) - Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-stevel/s3a/s3ablock-0001-659277820991634509.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-03-26T13:38:53.355, streamCreationCount=2, currentStream=null}\r\n{code}\r\n\r\nThis code was added in HADOOP-19221, S3A: Unable to recover from failure of multipart block upload attempt \"Status Code: 400; Error Code: RequestTimeout\"; it logs at INFO because it is considered both rare and serious enough that we should log it, based on our\nQ: This exists on older releases; i'd just not noticed. That is why we MUST test  against so many different endpoints.\r\n\r\nA lot of the s3 express code path in the SDK is  different from the S3 standard client, and so needs to be tested independently.\r\n\r\n\r\nFor the SDK upgrade I will downgrade the log to debug and leave it to the SDK team to fix themselves.", "output": "[~iwasakims] not fixed in our code...this appears to be an AWS SDK bug. has it gone away there?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hi,\r\n\r\nI have a test case that contains a topology with 2 time windows and chained emitStrategy calls. The standalone reproduction use case is attached to this issue\r\nThe problem is that about 25% of the time the test fails. About 75% of the time the test succeeds. I followed the conclusions of \r\n!https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype|width=16,height=16! KAFKA-19810\r\nto make sure that I am sending events at the correct times (at least I think my calculations are correct). I really need to get to the bottom of why the test fails intermittently, as the test somewhat reflects a real life topology of a project I am working on, and we cannot have an intermittently failing test in our build pipeline. \r\nGreg", "output": "Intermittent test failures when using chained emit strategy on window close"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade sbt-pom-reader from 2.4.0 to 2.5.0\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: SDK reads content twice during PUT to S3 Express store.\nDescription: The commit on trunk {{60c7d4fea010}} and on branch 3.4 {{f3ec55b}} fixes the logging, but does not\r\naddress the underlying issue.\r\n\r\nh2. problem\r\n\r\nDuring PUT calls, even of 0 byte objects, our UploadContentProver is reporting a recreation of \r\nof the input stream of an UploadContentProvider, as seen by our logging at info of this happening\r\n\r\n{code}\r\n bin/hadoop fs -touchz $v3/4\r\n2025-03-26 13:38:53,377 [main] INFO  impl.UploadContentProviders (UploadContentProviders.java:newStream(289)) - Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-stevel/s3a/s3ablock-0001-659277820991634509.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-03-26T13:38:53.355, streamCreationCount=2, currentStream=null}\r\n{code}\r\n\r\nThis code was added in HADOOP-19221, S3A: Unable to recover from failure of multipart block upload attempt \"Status Code: 400; Error Code: RequestTimeout\"; it logs at INFO because it is considered both rare and serious enough that we should log it, based on our\nQ: [~iwasakims] not fixed in our code...this appears to be an AWS SDK bug. has it gone away there?", "output": "[~stevel@apache.org] oops. I misunderstood by just seeing only the first line of [commit message|https://github.com/apache/hadoop/commit/f3ec55b5911d1e5e7da438c64f614a59a7db62a3]."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove invalid `licenses` field of `hadoop-aliyun` SBOM by upgradding `cyclonedx` to 2.9.1\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade scala-xml to 2.4.0\nDescription: \nQ: Issue resolved in https://github.com/apache/spark/pull/52659", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "All static classes in Field except TaggedFieldsSection are not really being used. We should remove them.", "output": "Remove all static classes in Field except TaggedFieldsSection"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Store decimal precision loss conf in arithmetic expressions\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add `Apache Iceberg` example\nDescription: \nQ: Issue resolved by pull request 394\n[https://github.com/apache/spark-kubernetes-operator/pull/394]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove unsupported \"upgrade from\" versions\nDescription: Cf [https://github.com/apache/kafka/pull/20518#discussion_r2338270305]\r\n\r\nWith AK 4.0.0 release, we officially cut support for direct upgrades of Kafka Streams applications from versions 2.3 and older, and only direct upgrades from 2.4 or newer to 4.0+ are supported.\r\n\r\nThus, we should update `UpgradeFromValues.java` and remove older version, and cleanup the backward compatibility code accordingly.\nQ: Hi [~mjsax], if you're not working on this, may I take it? Thanks.", "output": "[~isding_l] – Seems there was a race condition. [~nikita-shupletsov] did start to look into this issue already."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Log auto topic creation failures more visibly\nDescription: Hi, I was playing with Share groups on the 4.1.0-rc2 and found it a little opaque to detect a failure to auto create the `__share_group_state` topic due to the replication factor not being satisfiable.\r\n\r\nI start up a cluster like this (taken from the dockerhub instructions):\r\n{code:java}\r\npodman run --rm \\\r\n  -p 9092:9092 \\\r\n  -e KAFKA_NODE_ID=1 \\\r\n  -e KAFKA_PROCESS_ROLES=broker,controller \\\r\n  -e KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \\\r\n  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\r\n  -e KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER \\\r\n  -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \\\r\n  -e KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9093 \\\r\n  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\r\n  -e KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1 \\\r\n  -e KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1 \\\r\n  -e KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0 \\\r\n  -e KAFKA_NUM_PARTITIONS=3 \\\r\n  apache/kafka:4.1.0-rc2{code}\r\nand then run the commands:\r\n{code:java}\r\nrobeyoun:kafka_2.13-4.1.0$ bin/kafka-features.sh --bootstrap-server localhost:9092 upgrade --feature share.version=1\r\nshare.version was upgraded to 1.\r\nrobeyoun:kafka_2.13-4.1.0$ bin/kafka-topics.sh --create --topic my_topic --bootstrap-server localhost:9092\r\nWARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\r\nCreated topic my_topic.\r\nrobeyoun:kafka_2.13-4.1.0$ bin/k", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Deduplicate the variables in PythonArrowInput\nDescription: \nQ: Issue resolved by pull request 52621\n[https://github.com/apache/spark/pull/52621]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: ITests to run under JUnit5\nDescription: hadoop-aws tests which need to be parameterized on a class level\r\nare configured to do so through the @ParameterizedClass tag.\r\nFilesystem contract test suites in hadoop-common have\r\nalso been parameterized as appropriate.\r\n\r\nThere are custom JUnit tags declared in org.apache.hadoop.test.tags,\r\nwhich add tag strings to test suites/cases declaring them.\r\nThey can be used on the command line and in IDEs to control\r\nwhich tests are/are not executed.\r\n\r\n@FlakyTest \"flaky\"\r\n@LoadTest \"load\"\r\n@RootFilesystemTest \"rootfilesystem\"\r\n@ScaleTest \"scale\"\r\n\r\nFor anyone migrating tests to JUnit 5\r\n* Methods which subclass an existing test case MUST declare the @Test\r\n  tag again -it is no longer inherited.\r\n* All overridden setup/teardown methods MUST be located and\r\n  @BeforeEach/@AfterEach attribute added respectively\r\n* Subclasses of a parameterized test suite MUST redeclare themselves\r\n  as a @ParameterizedClass, and the binding mechanism again.\r\n* Parameterized test suites SHOULD declare a patt\nQ: hadoop-yetus commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3089822111\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 16s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  44m  7s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  20m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 33s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 26s |  |  branch/hadoop-client-modules/hadoop-client-integration-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 30s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 11s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 46s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 55s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   3m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 42s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 35s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  hadoop-client-modules/hadoop-client-integration-tests has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 20s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 18s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  18m 32s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 28s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 26s |  |  hadoop-client-integration-tests in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 211m 27s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7814 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 63d1f6ad482c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f6e890ab053f9169cc683ac5ac5046f4bc4be111 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/testReport/ |\r\n   | Max. process+thread count | 1509 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-client-modules/hadoop-client-integration-tests U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3100023345\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 32 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 47s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  19m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   4m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   3m 36s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 33s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  branch/hadoop-client-modules/hadoop-client-integration-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 28s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   2m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  7s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 43s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   2m  2s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/4/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 13 new + 18 unchanged - 9 fixed = 31 total (was 27)  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   3m 26s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 45s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  hadoop-client-modules/hadoop-client-integration-tests has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 27s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  18m 25s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  90m 23s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 59s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 33s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 33s |  |  hadoop-client-integration-tests in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 46s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 265m 19s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7814 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux e5cbce971dc1 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b2d347f58b50e511ffc02a66625fe9ae7dbceafe |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/4/testReport/ |\r\n   | Max. process+thread count | 3309 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-client-modules/hadoop-client-integration-tests U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add --enable-native-access=ALL-UNNAMED JVM option\nDescription: We get warnings like \r\n{noformat}\r\nWARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module\r\nWARNING: Restricted methods will be blocked in a future release unless native access is enabled\r\n{noformat}\r\non JDK21+.\r\n\r\nWhile this works now, it's better to add this early, and avoid breakage later.\r\n\r\nThis also cleans up the the console output.\nQ: stoty opened a new pull request, #7627:\nURL: https://github.com/apache/hadoop/pull/7627\n\n   ### Description of PR\r\n   \r\n   Add the --enable-native-access=ALL-UNNAMED option to --enable-native-access=ALL-UNNAMED\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Run a test my JDK24 branch. The warning was no longer displayed\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7627:\nURL: https://github.com/apache/hadoop/pull/7627#issuecomment-2812034325\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  29m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |  49m 25s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |  38m 25s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 26s | [/patch-unit-hadoop-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7627/1/artifact/out/patch-unit-hadoop-project.txt) |  hadoop-project in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 28s |  |  ASF License check generated no output?  |\r\n   |  |   | 120m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7627/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7627 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 259fca8f8e8d 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / eb3b538b54f0e6f3c2c50d48bc39827bf26f7c8c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7627/1/testReport/ |\r\n   | Max. process+thread count | 98 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7627/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade AWS SDK to 2.35.4\nDescription: Upgrade to a recent version of 2.33.x or later while off the critical path of things.\r\n\r\n\r\nHADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.\nQ: pan3793 commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201364651\n\n   > JUnit5 update complicates things here, as it highlights that minicluster tests aren't working.\r\n   \r\n   I found `hadoop-client-runtime` and `hadoop-client-minicluster` broken during integration with Spark, HADOOP-19652 plus YARN-11824 recovers that, is it the same issue?", "output": "steveloughran commented on PR #7882:\nURL: https://github.com/apache/hadoop/pull/7882#issuecomment-3201641390\n\n   @pan3793 maybe. \r\n   \r\n   what is unrelated is out the box the SDK doesn't do bulk delete with third party stores which support it (Dell ECS).\r\n   ```\r\n   org.apache.hadoop.fs.s3a.AWSBadRequestException: bulkDelete on job-00-fork-0001/test/org.apache.hadoop.fs.contract.s3a.ITestS3AContractBulkDelete: software.amazon.awssdk.services.s3.model.InvalidRequestException: Missing required header for this request: Content-MD5 (Service: S3, Status Code: 400, Request ID: 0c07c87d:196d43d824a:d5329:91d, Extended Request ID: 85e1d41b57b608d4e58222b552dea52902e93b05a12f63f54730ae77769df8d1) (SDK Attempt Count: 1):InvalidRequest: Missing required header for this request: Content-MD5 (Service: S3, Status Code: 400, Request ID: 0c07c87d:196d43d824a:d5329:91d, Extended Request ID: 85e1d41b57b608d4e58222b552dea52902e93b05a12f63f54730ae77769df8d1) (SDK Attempt Count: 1)\r\n   --\r\n   \r\n   ```"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have observed this across multiple workload runs that when we start reading data from input stream. The first read which came to input stream has to be read synchronously even if we trigger prefetch request for that particular offset. Most of the times we end up doing extra work of checking if the prefetch is trigerred, removing prefetch from the pending queue and go ahead to do a direct remote read in workload thread itself.\r\n\r\nTo avoid all this overhead, we will always bypass read ahead for the very first read of each input stream and trigger read aheads for second read onwards.", "output": "ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Global stream thread ignores all exceptions\nDescription: {{globalStreamThread}} in {{KafkaStreams}} class ignores all exceptions and fails to apply the user-provided {{StreamsUncaughtExceptionHandler}} in:\r\n{code:java}\r\npublic void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler userStreamsUncaughtExceptionHandler)\r\n{code}\r\nThis can lead to streams being stuck in faulty state after an exception is thrown during their initialization phase (e.g. failure to load the RocksDB native library). The exception isn't logged, so debugging such problem is difficult.\r\n\r\nFrom my understanding of the {{b62d8b97}} commit message, the following code is unnecessary as the {{globalStreamThread}} can't be replaced and the issue description from KAFKA-12699 doesn't apply to it. Removing it should help.\r\n{code:java}\r\nif (globalStreamThread != null) {\r\n    globalStreamThread.setUncaughtExceptionHandler((t, e) -> { }\r\n    );\r\n}\r\n{code}\nQ: You're right, now I see that the first call to {{setUncaughtExceptionHandler}} on {{globalStreamThread}} is using the new implementation and sets it right. I might have incorrectly pinpointed my issue.\r\n\r\nI've verified that the problem I've encountered happens when {{initialize()}} call in {{GlobalStreamThread:276}} throws an {{ExceptionInInitializerError}} due to RocksDB failing to load the library in static initializer of {{{}DBOptions{}}}. This exception was only caught inside the {{java.lang.Thread}} exception handler of the {{{}globalStreamThread{}}}.", "output": "[~mikkolaj] [~mjsax] I want to help Kafka development for the first time. I have the ticket but I don't know what to do exactly. Can you guide me on what to do about it?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use CRD `v1` instead of `v1beta1` in `benchmark` script\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: the `record-size` and `throughput`arguments don't work in TestRaftServer\nDescription: see https://github.com/apache/kafka/blob/656242775c321c263a3a01411b560098351e8ec4/core/src/main/scala/kafka/tools/TestRaftServer.scala#L118\r\n\r\nwe always hard code the `recordsPerSec` and `recordSize`", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Adding low traffic request priority (behind a config flag) for prefetches to reduce load on server during throttling", "output": "ABFS: Adding request priority for prefetches"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update `Operator SDK` to 5.1.4\nDescription: see JOSDK release: https://github.com/operator-framework/java-operator-sdk/releases/tag/v5.1.4", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Version 1.2.1 brings in better memory management, readVectored and SSE-C support.", "output": "S3A: AAL - Update to version 1.2.1"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Ingress Operations\nDescription: We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Ingress Related operations on blob Endpoint (Create + Append + Flush filesystem calls)\r\n\r\nThis Jira tracks implementing these tests.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default\nDescription: Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler.\nQ: vinayakumarb opened a new pull request, #8001:\nURL: https://github.com/apache/hadoop/pull/8001\n\n   This commit introduces support for Debian 12 (Bookworm) and Debian 13 (Trixie) as build platforms, following the approach established for Ubuntu 24.\r\n   \r\n   Key changes include:\r\n   - Creation of `Dockerfile_debian_12` and `Dockerfile_debian_13` based on `Dockerfile_ubuntu_24`, with appropriate base images and package resolver arguments.\r\n   - Updates to `dev-support/docker/pkg-resolver/packages.json` to include package definitions for `debian:12` and `debian:13`.\r\n   - Addition of `debian:12` and `debian:13` to `dev-support/docker/pkg-resolver/platforms.json`.\r\n   - Modification of `BUILDING.txt` to list `debian_12` and `debian_13` as supported OS platforms.", "output": "hadoop-yetus commented on PR #8001:\nURL: https://github.com/apache/hadoop/pull/8001#issuecomment-3341301544\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 38s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  hadolint  |   0m  0s |  |  hadolint was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  jsonlint  |   0m  0s |  |  jsonlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  37m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 39s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 53s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  98m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8001 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets hadolint shellcheck shelldocs jsonlint |\r\n   | uname | Linux 01b5b1df2935 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7ccea846897ea6a8209a2238c06933afb4c489bc |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8001/1/console |\r\n   | versions | git=2.43.7 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "As our project has fully migrated to JUnit5, we should now enforce a rule that prevents the import and usage of JUnit4 classes (such as org.junit.Test, org.junit.Assert, etc.) to ensure consistency, avoid regressions, and allow safe removal of legacy dependencies.\r\n\r\nThis task involves identifying and eliminating any remaining JUnit4 imports, and introducing static code analysis or linting rules to ban future usage.", "output": "[JDK17] Disallow JUnit4 Imports After JUnit5 Migration"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Kafka Source RTM support\nDescription: \nQ: Issue resolved by pull request 52729\n[https://github.com/apache/spark/pull/52729]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. \nDescription: Removed build support for the following EOL (End-of-Life) operating system versions (e.g., CentOS 7, Debian 9, etc.).\r\n\r\nCleaned up redundant code and dependency configurations in the Dockerfile related to these operating systems.\r\n\r\nOptimized the build logic to ensure that currently supported OS versions build successfully.\nQ: slfan1989 opened a new pull request, #7822:\nURL: https://github.com/apache/hadoop/pull/7822\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19636. [JDK17] Remove EOL OS Support and Clean Up Dockerfile.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7822:\nURL: https://github.com/apache/hadoop/pull/7822#issuecomment-3108826927\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7822/1/console in case of problems."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title:  Change log.segment.bytes configuration type from int to long to support segments larger than 2GB\nDescription: h2. Description\r\nh3. Summary\r\n\r\nChange the data type of *{{log.segment.bytes}}* configuration from *{{int}}* to *{{long}}* to allow segment sizes beyond the current 2GB limit imposed by the integer maximum value.\r\nh3. Current Limitation\r\n\r\nThe {{*log.segment.bytes*}} configuration currently uses an *{{int}}* data type, which limits the maximum segment size to ~2GB (2,147,483,647 bytes). This constraint becomes problematic for modern high-capacity storage deployments.\r\nh3. Background: Kafka Log Segment Structure\r\n\r\nEach Kafka topic partition consists of multiple log segments stored as separate files on disk. For each segment, Kafka maintains three core files:\r\n * {*}{{.log}} files{*}: Contain the actual message data\r\n * {*}{{.index}} files{*}: Store mappings between message offsets and their physical positions within the log file, allowing Kafka to quickly locate messages by their offset without scanning the entire log file\r\n * {*}{{.timeindex}} files{*}: Store mappings between message timestamps and their corresponding offsets, enabling efficient time-based retrieval of messages\r\n\r\nh3. Motivation\r\n # {*}Modern Hardware Capabilities{*}: Current deployments often use high-capacity storage (e.g., EPYC servers with 4×15TB drives) where 2GB segments are inefficiently small\r\n # {*}File Handle Optimization{*}: Large Kafka deployments with many topics can have 50-100k open files across all segment types (.log, .index, .timeindex files). Each segment requires open file handles, and la", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Reorganize Python streaming TWS test\nDescription: Reorganize the Python TWS tests:\r\n* moving tws related tests to a new `/streaming` directory\r\n\r\n* further split the Python TWS tests to smaller ones to speed up the CI\nQ: Issue resolved by pull request 52691\n[https://github.com/apache/spark/pull/52691]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use Java `(Map|Set).of` instead of `Collections.(empty|singleton)(Set|Map)`\nDescription: \nQ: Issue resolved by pull request 401\n[https://github.com/apache/spark-kubernetes-operator/pull/401]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "As a follow-up of https://issues.apache.org/jira/browse/SPARK-53947, let approx_top_k_accumulate/combine/estimate handle NULLs.", "output": "Let approx_top_k_accumulate/combine/estimate handle NULLs"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Modification of share fetch to accommodate piggybacked renewals.\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Error if an empty topic is created when there is a regex source KS\nDescription: If there is a KS application that uses a regex source, and we create a new topic that matches that regex, but produce no messages, the application will get into an {{ERROR}} state.\r\n\r\n \r\n\r\nif we take {}[RegexSourceIntegrationTest#testRegexRecordsAreProcessedAfterNewTopicCreatedWithMultipleSubtopologies|#L206{}}}], but without producing any messages to {{TEST-TOPIC-2}} the problem will reproduce:\r\n{quote}{{org.apache.kafka.streams.errors.StreamsException: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TEST-TOPIC-2-0}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:981)}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:898)}}\r\n{{Caused by: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TEST-TOPIC-2-0}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamTask.findOffsetAndMetadata(StreamTask.java:480)}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamTask.committableOffsetsAndMetadata(StreamTask.java:511)}}\r\n{{    at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:454)}}\r\n{{    at org.apache.kafka.streams.processor.internals.TaskExecutor.commitTasksAndMaybeUpdateCommittableOffsets(TaskExecutor.java:145)}}\r\n{{    at org.apache.kafka.streams.processor.internals.TaskManager.commitTasksAndMaybeUpdateCommittableOffsets(TaskManager.java:2025)}}\r\n{{    at org.apache.kafka.stream", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Both G1GC and ZGC needs extra object header (usually 16 bytes in 64bit system) when allocating region or ZPage for java long array, so the really bytes allocated is pageSize+16.\r\n\r\nSo we need consider the object header when allocating spark pages.", "output": "Add utility functions to detect JVM GCs"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hadoop currently adds some module options needed for Java9+ in an ad-hoc manner.\r\n\r\nThese should be centralized, and automatically added whenever a new JVM is launched.\r\n\r\n(and in the startup scripts)\r\n\r\n\r\nThe list of options in HBase is a superset of those needed by Hadoop, and could be used as a starting point.", "output": "Handle module JVM options in a centralized manner"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Enable `spark.io.compression.lzf.parallel.enabled` by default\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: MetaDataShell log error message for SNAPSHOT_HEADER and SNAPSHOT_FOOTER\nDescription: ```\r\n\r\n./bin/kafka-metadata-shell.sh -s /tmp/kafka-meta/bootstrap.checkpoint\r\n\r\n17:31:43 Loading...\r\n\r\n[2025-09-08 17:32:19,271] ERROR Ignoring control record with type SNAPSHOT_HEADER at offset 0 (org.apache.kafka.metadata.util.SnapshotFileReader)\r\n\r\n[2025-09-08 17:32:19,274] ERROR Ignoring control record with type SNAPSHOT_FOOTER at offset 5 (org.apache.kafka.metadata.util.SnapshotFileReader) Starting...\r\n```", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use difference error message when kill on idle timeout\nDescription: \nQ: Issue resolved by pull request 52749\n[https://github.com/apache/spark/pull/52749]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [pb-upgrade] RouterAdminProtocolTranslatorPB use ShadedProtobufHelper\nDescription: RouterAdminProtocolTranslatorPB should also use ShadedProtobufHelper.\nQ: hadoop-yetus commented on PR #7407:\nURL: https://github.com/apache/hadoop/pull/7407#issuecomment-2670667030\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m  1s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  42m  3s |  |  hadoop-hdfs-rbf in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 184m 59s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7407/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7407 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 51178602b2e3 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 037618232be0184eb90aa942991e58b2c6d854dd |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7407/1/testReport/ |\r\n   | Max. process+thread count | 3372 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-rbf U: hadoop-hdfs-project/hadoop-hdfs-rbf |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7407/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hfutatzhanghb commented on PR #7407:\nURL: https://github.com/apache/hadoop/pull/7407#issuecomment-2671353554\n\n   @steveloughran Sir, cc. Thanks a lot."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade os-maven-plugin to 1.7.1 to support riscv64\nDescription: *Problem*\r\n\r\nWhen building Hadoop on the  riscv64 architecture, the current version of os-maven-plugin (1.7.0) fails with the following error:\r\n\r\n    os.detected.arch: unknown\r\n\r\n    org.apache.maven.MavenExecutionException: unknown os.arch: riscv64\r\n\r\nThis indicates that version 1.7.0 does not recognize or support the riscv64 target, causing native or plugin-based build failures.\r\n\r\n*Resolution*\r\n\r\nUpgrading os-maven-plugin to version 1.7.1 resolves the issue. The newer version includes updated architecture detection logic and supports riscv64 without error.\nQ: leiwen2025 opened a new pull request, #7796:\nURL: https://github.com/apache/hadoop/pull/7796\n\n   \r\n   \r\n   ### Description of PR\r\n   When building Hadoop on the  riscv64 architecture, the current version of os-maven-plugin (1.7.0) fails with the following error:\r\n   \r\n   > os.detected.arch: unknown\r\n   > org.apache.maven.MavenExecutionException: unknown os.arch: riscv64\r\n   \r\n   This indicates that version 1.7.0 does not recognize or support the riscv64 target, causing native or plugin-based build failures.\r\n   \r\n   Upgrading os-maven-plugin to version 1.7.1 resolves the issue. The newer version includes updated architecture detection logic and supports riscv64 without error.\r\n   \r\n   ### How was this patch tested?\r\n   mvn package -Pdist,native -DskipTests -Dtar\r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7796:\nURL: https://github.com/apache/hadoop/pull/7796#issuecomment-3060513576\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   9m 46s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  20m  1s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 25s | [/branch-compile-hadoop-project-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/artifact/out/branch-compile-hadoop-project-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-project in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 25s | [/branch-compile-hadoop-project-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/artifact/out/branch-compile-hadoop-project-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-project in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 12s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  22m 36s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m  9s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m  7s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m  8s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m  8s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  32m  3s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 24s | [/patch-unit-hadoop-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/artifact/out/patch-unit-hadoop-project.txt) |  hadoop-project in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 25s |  |  ASF License check generated no output?  |\r\n   |  |   |  66m 56s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7796 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux e4e5439ea320 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a55ccaf8c85912a95820a71e54c528862a4fe660 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/testReport/ |\r\n   | Max. process+thread count | 114 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7796/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Update all integration tests using IQv2 to use both streams and classic group protocol to receive correct endpoint to partition information for all members of the group.\r\n\r\n \r\n\r\nAC: All existing integration tests with IQv2 are passing", "output": "Update Kafka Streams integration tests using IQv2 to new streams protocol"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Share Partition Lag Persistence and Retrieval\nDescription: Ticket for KIP-1226. The KIP proposes introducing the concept of lag for share partitions, which will be persisted in the __share_group_state topic and can be retrieved using admin.listShareGroupOffsets\nQ: Hi [~chiragwadhwa55] ，\r\n\r\nI could help with some of the sub-tasks if you'd like to share the workload, thanks~", "output": "Hi [~chiragwadhwa55]  i am।happy to help in any of the subtasks Let me know if i can be of any help"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Handle JDK-8225499 IpAddr.toString() format changes in tests\nDescription: The IP address string format has changed in JDK14\r\n[https://bugs.openjdk.org/browse/JDK-8225499|https://bugs.openjdk.org/browse/JDK-8232369]\r\n\r\n\r\n\r\nHDFS-15685 adjust some tests for it, but not all.\nQ: stoty opened a new pull request, #7502:\nURL: https://github.com/apache/hadoop/pull/7502\n\n   ### Description of PR\r\n   \r\n   Change the expected IPaddr strings according to the JDK version in some tests\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran the changed tests on JDK8 and JDK17.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7502:\nURL: https://github.com/apache/hadoop/pull/7502#issuecomment-2722677852\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  18m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 47s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  36m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 31s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 14s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 44s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  15m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 39s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 45s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  38m  4s |  |  hadoop-mapreduce-client-app in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 51s | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/1/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) |  hadoop-hdfs-rbf in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 46s |  |  ASF License check generated no output?  |\r\n   |  |   | 280m 58s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7502 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 97c7a921b1f6 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4ed6e2dc2bba937d153bdc7e9584e60e534183c1 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/1/testReport/ |\r\n   | Max. process+thread count | 565 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-hdfs-project/hadoop-hdfs-rbf U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Migrate ViewFileSystemBaseTest to Junit 5\nDescription: Its children have been converted to Junit 5, but the parent has not.\r\nThis breaks most of the child test classes (at least with the latest JUnit5+Surefire).\r\nThe breakage may not happen with the current old Surefire, but it is more likely that it is just silently ignored.\nQ: stoty opened a new pull request, #7646:\nURL: https://github.com/apache/hadoop/pull/7646\n\n   ### Description of PR\r\n   \r\n   migrate ViewFileSystemBaseTest.java to Junit 5\r\n   This fixes the failures in the child class that have already been migrated\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Run mvn test for the affected test classes.\r\n   \r\n   Note that TestViewFileSystemHdfs.testTrashRootsAfterEncryptionZoneDeletion is failing, but that one\r\n   is not run on trunk, and is probably a pre-existing issue unrelated to the Junit 5 migration issues.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7646:\nURL: https://github.com/apache/hadoop/pull/7646#issuecomment-2827031446\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 19s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 48s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 56s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   7m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m 12s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/2/artifact/out/blanks-eol.txt) |  The patch has 4 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 37s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/2/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 33 new + 26 unchanged - 33 fixed = 59 total (was 59)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 54s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 15s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  12m 59s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 120m 39s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7646 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5e8d2a84d7fc 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0e69dd3cb3a71ac8531bf6be81c5b8ab15066b90 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/2/testReport/ |\r\n   | Max. process+thread count | 2635 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "As reported on the dev mailing list, we introduced a regression bug via https://issues.apache.org/jira/browse/KAFKA-13722 in 4.1 branch. We did revert the commit ([https://github.com/apache/kafka/commit/f13a22af0b3a48a4ca1bf2ece5b58f31e3b26b7d]) for 4.1 release, and want to fix-forward for 4.2 release.\r\n\r\nStacktrace:\r\n{code:java}\r\n15:29:05 ERROR [STREAMS] KafkaStreams - stream-client [app1] Encountered the following exception during processing and the registered exception handler opted to SHUTDOWN_CLIENT. The streams client is going to shut down now.\r\norg.apache.kafka.streams.errors.StreamsException: failed to initialize processor random-value-processor\r\n        at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:132) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:141) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.StreamTask.initializeTopology(StreamTask.java:1109) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.StreamTask.completeRestoration(StreamTask.java:297) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.TaskManager.tryToCompleteRestoration(TaskManager.java:955) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.StreamThread.initializeAndRestorePhase(StreamThread.java:1417) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:1219) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:934) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:894) [kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\nCaused by: java.la", "output": "NPE in `Processor#init()` accessing state store"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove `ignore.symbol.file` Javac option\nDescription: \nQ: Issue resolved by pull request 52802\n[https://github.com/apache/spark/pull/52802]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The following 2 tests are failing in TestTextCommand -\r\n1. testDisplayForNonWritableSequenceFile\r\n2. testDisplayForSequenceFileSmallMultiByteReads\r\n\r\nThese tests create a file and flush a string ending with \"\\n\". However, for verification, the test expects \"\\r\\n\" as the line ending on Windows. Thus, the test fails.", "output": "Fix TestTextCommand on Windows"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Admin#describeReplicaLogDirs has following issues:\r\n\r\n1. Admin#describeReplicaLogDirs does not propagate the CLUSTER_AUTHORIZATION_FAILED\r\n\r\n2. the single directory-level error is propagated to other directories result\r\n\r\n{code:java}\r\n                        if (logDirInfo.error() != null)\r\n                            handleFailure(new IllegalStateException(\r\n                                \"The error \" + logDirInfo.error().getClass().getName() + \" for log directory \" + logDir + \" in the response from broker \" + brokerId + \" is illegal\"));\r\n{code}", "output": "Admin#describeReplicaLogDirs does not handle error correctly"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Update {{ProcessorStateManager}} to start passing offsets to {{StateStore#commit}} and loading offsets using {{{}StateStore#getCommittedOffsets{}}}.", "output": "Implement new behaviour in ProcessorStateManager"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add `Apache Iceberg` example\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade com.huaweicloud:esdk-obs-java for CVE-2023-3635\nDescription: The {{com.huaweicloud:esdk-obs-java}} dependency , used exclusively by the {{hadoop-huaweicloud}} uses {{com.squareup.okio:okio:1.17.2}} which has [CVE-2023-3635|https://nvd.nist.gov/vuln/detail/cve-2023-3635].\r\nUpgrading it will use a newer fixed version of {{okio}}, which will mitigate the vulnerability.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title:  StreamsOnTasksAssignedCallbackNeededEvent could not be completed \nDescription: In several tests, for example PlaintextAdminIntegrationTest, we see this exception silently triggered when the consumer is closed.\r\n\r\n \r\n{code:java}\r\norg.apache.kafka.common.errors.TimeoutException: StreamsOnTasksAssignedCallbackNeededEvent could not be completed before the consumer closed[2025-09-05 14:17:40,972] ERROR [Consumer clientId=consumer-stream_group_id_1-1, groupId=stream_group_id_1] Reconciliation failed: callback invocation failed for tasks LocalAssignment{localEpoch=0, activeTasks={subtopology-0=[0, 1, 2]}, standbyTasks={}, warmupTasks={}} (org.apache.kafka.clients.consumer.internals.StreamsMembershipManager:1077)java.util.concurrent.CompletionException: org.apache.kafka.common.errors.TimeoutException: StreamsOnTasksAssignedCallbackNeededEvent could not be completed before the consumer closed\tat java.base/java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:368)\tat java.base/java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:3\nQ: Hey, in case it helps, assuming the contract for onTaskAssigned is the same as onPartitionsAssigned, it should only be triggered within a call to poll, so we don't want it triggered on close even if there is a pending event for it (so this log you shared would be expected, in the case where close happens when there is a callback needed event in the queue but no poll before closing) \r\n\r\nNote that on the consumer there is no processing of background events on close, intentionally, as we don't want to be triggering all callbacks or processing more errors. On close it's just the onPartitionsREvoked/Lost that are expected, so they are simply triggered directly on the app thread ([https://github.com/apache/kafka/blob/b92d47d48791dc379d67239489fcf1bf8016b6b1/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1481] )\r\n\r\nNot sure about the contract regarding streams callbacks, but if it's the same as with the consumer callbacks, we probably just need to ensure that the streamsTaskRevoked/Lost are triggered on close I would expect.", "output": "I'm closing this, since it's noise and as disucssed wih Lianet, the noise is hard avoid"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce Geography and Geometry data types to PySpark API\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52627.", "output": "Issue resolved by pull request 52627\n[https://github.com/apache/spark/pull/52627]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: NPE when close() called on uninitialized filesystem\nDescription: code\r\n{code}\r\n  public void testABFSConstructor() throws Throwable {\r\n    new AzureBlobFileSystem().close();\r\n  }\r\n{code}\r\n\r\nstack\r\n{code}\r\n[ERROR] org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor -- Time elapsed: 0.003 s <<< ERROR!\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getClient()\" because \"this.abfsStore\" is null\r\n        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.close(AzureBlobFileSystem.java:800)\r\n        at org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor(TestRuntimeValid.java:49)\r\n{code}", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Test failures during AWS SDK upgrade\nDescription: 1. The logic to skip vector IO contract tests doesn't work  if the analytics stream is set on a per-bucket basis for the test bucket\r\n2. tests with SSE-C are failing. Test bucket is normally set up to use SSE-KMS, FWIW\r\n\r\n{code}\r\n  \r\n    fs.s3a.bucket.stevel-london.encryption.algorithm\r\n    SSE-KMS\r\n  \r\n\r\n\r\n{code}\r\n\r\nthis only happens when the analytics stream is set for the test bucket fs.s3a.bucket.stevel-london.input.stream.type=analytics; set it globally all is good\r\n\r\n+ various other tests. FInal pr message\r\n{code}\r\nCode changes related to HADOOP-19485.\r\n\r\nAwsSdkWorkarounds no longer needs to cut back on transfer manager logging\r\n(HADOOP-19272) :\r\n- Remove log downgrade and change assertion to expect nothing to be logged.\r\n- remove false positives from log.\r\n\r\nITestS3AEndpointRegion failure:\r\n- Change in state of AwsExecutionAttribute.ENDPOINT_OVERRIDDEN\r\n  attribute requires test tuning to match.\r\n\r\nSome tests against third-party stores fail\r\n- Includes fix for the assumeStoreAws\nQ: note that disabling all vector IO tests is dangerous as the stream MUST support the core contract tests...it is only the s3a input stream parallel reads an IOStats collection which should be skipped", "output": "All tests working with SSE-C failing in read or copy\r\n\r\n{code}\r\n[ERROR] Tests run: 10, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 10.624 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.scale.ITestS3AHugeFilesSSECDiskBlocks\r\n[ERROR] org.apache.hadoop.fs.s3a.scale.ITestS3AHugeFilesSSECDiskBlocks.test_040_PositionedReadHugeFile  Time elapsed: 1.949 s  <<< ERROR!\r\njava.io.IOException: job-00/test/tests3ascale/disk/src/hugefile: Stream is closed!\r\n        at org.apache.hadoop.fs.s3a.impl.streams.AnalyticsStream.throwIfClosed(AnalyticsStream.java:235)\r\n        at org.apache.hadoop.fs.s3a.impl.streams.AnalyticsStream.seek(AnalyticsStream.java:78)\r\n        at org.apache.hadoop.fs.FSInputStream.read(FSInputStream.java:85)\r\n        at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:124)\r\n        at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:129)\r\n        at org.apache.hadoop.fs.s3a.scale.AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile(AbstractSTestS3AHugeFiles.java:490)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\r\n        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\r\n        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)\r\n        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)\r\n        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.lang.Thread.run(Thread.java:750)\r\n{code}"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: AWS SDK 2.30+ incompatible with third party stores\nDescription: Changes in the SDK related to a new AWS SDK Feature, Default integritY Protection {{https://docs.aws.amazon.com/sdkref/latest/guide/feature-dataintegrity.html}} break all intereraction with third party S3 Stores.\r\n\r\nSee {{https://github.com/aws/aws-sdk-java-v2/issues/5801}}\r\n\r\nThere are documented mechanisms to turn this off\r\n* an environment variable\r\n* a change in ~/..aws/config\r\n* a system property\r\n* looks like a new builder option, though the docs don't cover it.\r\n\r\nThat checksum builder option looks like the only viable strategy, but it will need testing. maybe even make it a property which can be enabled/disabled, with tests and docs.\r\n\r\nWhoever wants a 2.30.x feature either gets to do the fixing and testing. For 3.4.2 it'll be an older release.\nQ: Pretty much all thirdparty s3 compatible stores are broken. We're fixing it in Ozone here: HDDS-12488", "output": "jojochuang commented on PR #7494:\nURL: https://github.com/apache/hadoop/pull/7494#issuecomment-2711972076\n\n   Hi Steve, this is unrelated but I noticed that the v2 s3 client has a close() method, and looking at the tutorial doc it seems to expect users to close it at the end. Looking at Hadoop S3A code I found a few test code where it is not closed. That may require your attention."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Reduce memory footprint from cached local relations upon cloning\nDescription: Cloning sessions is a common operation in Spark applications (e.g., for creating isolated execution contexts). The current approach of duplicating cached data can significantly increase memory footprint, especially when:\r\n * Sessions are cloned frequently\r\n * Cached relations contain large datasets\r\n * Multiple clones exist simultaneously\r\n\r\n \r\n\r\nAn improvement can be made by implementing reference counting as opposed to data replication for the block manager entries that reference cached local relations.\nQ: Issue resolved by pull request 52651\n[https://github.com/apache/spark/pull/52651]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see JOSDK release: https://github.com/operator-framework/java-operator-sdk/releases/tag/v5.1.4", "output": "Update `Operator SDK` to 5.1.4"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We are using a Kafka Streams topology that continuously writes large volumes of data into a RocksDB state store with stable throughput. In parallel, another thread executes Interactive Query (IQ) requests against the same local state store.\r\n\r\nWhen the number of IQ requests in the queue grows (≈50+), the application enters a {*}deadlock state{*}.\r\n\r\n*Investigation:*\r\nUsing a thread dump, we discovered a lock inversion between RocksDB operations:\r\n * {{RocksDBStore.put}}\r\n\r\n ** blocked on {{org.apache.kafka.streams.query.Position@4ba00b6c}}\r\n\r\n ** holding {{org.apache.kafka.streams.state.internals.RocksDBStore@414cff0e}}\r\n\r\n * {{RocksDBStore.range}}\r\n\r\n ** blocked on {{org.apache.kafka.streams.state.internals.RocksDBStore@414cff0e}}\r\n\r\n ** holding {{org.apache.kafka.streams.query.Position@4ba00b6c}}\r\n\r\nThis indicates that {*}{{put}} and {{range}} acquire the same locks but in different order{*}, which leads to deadlock under concurrent load.\r\n\r\n*Expected Behavior:*\r\nKafka Streams API should guarantee deadlock-free operation. Store writes ({{{}put{}}}) and IQ reads ({{{}range{}}}) should not block each other in a way that leads to lock inversion.\r\n\r\n*Steps to Reproduce:*\r\n # Create a Kafka Streams topology with a RocksDB state store receiving continuous writes.\r\n\r\n # In a parallel thread, issue a high number of Interactive Query {{range}} requests (≈50+ queued).\r\n\r\n # Observe that the system eventually enters deadlock.\r\n\r\n *  \r\n\r\n*Impact:*\r\n * Application stops processing data.\r\n\r\n * Interactive Queries fail indefinitely.\r\n\r\n * Requires manual restart to recover.\r\n\r\n*Notes:*\r\n * Appears to be a lock ordering bug in {{{}RocksDBStore{}}}.\r\n\r\n * Expected the Streams API to coordinate thread-safety and prevent such deadlocks.", "output": "Deadlock in Kafka Streams when processing Interactive Queries and state store updates concurrently"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Acquisition lock timeout renewal in share consumer explicit mode\nDescription: Ticket for KIP-1222. The KIP proposes adding a new AcknowledgementType to be used in ShareFetch and ShareAcknowledge RPCs which will allow renewal of acquisition lock timeout with causing a state transition of the record in the share partition.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The current default s3 credential provider chain is set in the order of \r\n{code:java}\r\norg.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,software.amazon.awssdk.auth.credentials.EnvironmentVariableCredentialsProvider,org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider{code}\r\nRefer [code ref |https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml#L1450]for more details.\r\n\r\n \r\n\r\nThis works perfectly fine when used in AWS EC2, EMR Serverless, but not with AWS EKS pods.\r\n\r\n \r\n\r\nFor EKS pods, It is recommended to use\r\n{code:java}\r\nsoftware.amazon.awssdk.auth.credentials.WebIdentityTokenFileCredentialsProvider , software.amazon.awssdk.auth.credentials.ContainerCredentialsProvider (PodIdentity is enabled){code}\r\nWebIdentityTokenFileCredentialsProvider is an AWS credentials provider that enables applications to obtain temporary AWS credentials by assuming an IAM role using a web identity token (like OAuth or OIDC tokens). It's particularly important in EKS as it's the underlying mechanism that makes IRSA (IAM Roles for Service Accounts) work.\r\n\r\n \r\n\r\n \r\n\r\nContainerCredentialsProvider is already part of org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider", "output": "S3A: Support WebIdentityTokenFileCredentialsProvider"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Reduce waiting for event completion in AsyncKafkaConsumer.poll()\nDescription: We create—and wait on—{{{}PollEvent{}}} in {{Consumer.poll()}} to ensure we wait for reconciliation and/or auto-commit. However, reconciliation is relatively rare, and auto-commit only happens every _N_ seconds, so the remainder of the time, we should try to avoid sending poll events.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When `org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider` is used in a Hadoop Configuration object, it will attempt to `resolveCredentials()` inside the constructor.\r\n\r\n(lines 165-167)\r\n{code:java}\r\n// and force in a fail-fast check just to keep the stack traces less\r\n// convoluted\r\nresolveCredentials();{code}\r\n\r\nIf this method fails, because the current identity is not able to assume role, the constructor will throw an exception, and fail to close the `stsClient`, `stsProvider` and any other resources that are created in the constructor, leaking threads and other resources.\r\n \r\nIn a long running application, that handles Hadoop S3 file systems, where the user can dynamically change to configured role to assume, and external id, this will lead to eventually the system running out of resources due to the leaked threads created by the AWS SDK clients that are not closed when a wrong role or external id is used.\r\n \r\n\r\nThere are two potential fixes for this problem:\r\n\r\n - Don't attempt to `resolveCredentials()` inside the constructor\r\n\r\n - Wrap the `resolveCredentials()` in the constructor in a try/catch block, that cleans the resources and rethrows the exception in the catch block.", "output": "Resource leak in AssumedRoleCredentialProvider"}
{"instruction": "Answer the question based on the bug.", "input": "Title: kraft Add/RemoveVoterHandlers do not check request's timeout\nDescription: \nQ: Hi [~kevinwu2412] , may I take this one?", "output": "Hi [~brandboat], apologies I meant to assign it to myself, since I'm still not sure about the best way to handle this properly yet."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hello,\r\n\r\nSince Kafka 4.1.0, we're experiencing an issue with Kafka Streams metrics for topics containing dots in their names. (I think the problem is also for simple usage of consumers not only kstream)\r\n\r\nIn FetchMetricsManager, methods like {{recordRecordsFetched()}} now create duplicate sensors for the same topic if they contain dot in their name: \r\n{code:java}\r\nvoid recordRecordsFetched(String topic, int records) { \r\n\r\n\r\n String name = topicRecordsFetchedMetricName(topic); \r\n\r\n\r\n maybeRecordDeprecatedRecordsFetched(name, topic, records);  Map.of(\"topic\", topic)) \r\n\r\n\r\n .withAvg(metricsRegistry.topicRecordsPerRequestAvg) \r\n\r\n\r\n .withMeter(metricsRegistry.topicRecordsConsumedRate, metricsRegistry.topicRecordsConsumedTotal) \r\n\r\n\r\n .build(); \r\n\r\n\r\n recordsFetched.record(records); \r\n\r\n\r\n }  {code}\r\nIt currently record two sensors, one with my original topic name, one time with a topic name with dots replaced by underscore. \r\n\r\n-While we can work around this by reversing the transformation in our case (replacing underscores back to dots in Micrometer filters) or by removing this specific list of metrics, this does not feel like a long-term solution for us.-\r\n\r\nCurrently using spring-boot and micrometer, it's really hard to remove one of those duplicated metrics. \r\n\r\nCould a configuration option be added to disable one of the two sensor to avoid having duplicated metrics?\r\n\r\nThanks in advance", "output": "Metrics from FetchMetricsManager containing a topic tag are duplicated"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support Geography and Geometry in SRS mappings\nDescription: \nQ: And what about this, [~uros-db]?\r\n\r\nI changed my mind - if you have something that I could take, you can ping me ;)", "output": "Sorry, also in progress: https://github.com/apache/spark/pull/52667."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Ingress Operations\nDescription: We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Ingress Related operations on blob Endpoint (Create + Append + Flush filesystem calls)\r\n\r\nThis Jira tracks implementing these tests.\nQ: anmolanmol1234 opened a new pull request, #7424:\nURL: https://github.com/apache/hadoop/pull/7424\n\n   This PR covers 2 changes:-\r\n   \r\n   1. Removed the need for an additional list for storing the committedBlockEntries and replaced it with a string builder which is then cleared out during outputstream close.\r\n   2. Added tests for the negative scenarios identified for the append flush cases.", "output": "hadoop-yetus commented on PR #7424:\nURL: https://github.com/apache/hadoop/pull/7424#issuecomment-2674706460\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  8s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m  0s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 3 new + 11 unchanged - 0 fixed = 14 total (was 11)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 58s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 134m 26s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7424 |\r\n   | JIRA Issue | HADOOP-19444 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 865901031f1d 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6039e0e78db5f7428899066b00da992623adf9e3 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/1/testReport/ |\r\n   | Max. process+thread count | 524 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7424/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)\nDescription: In core-default.xml, the property hadoop.security.crypto.buffer.size is currently documented as “The buffer size used by CryptoInputStream and CryptoOutputStream.” It does not specify the legal value constraints.\r\n{code:java}\r\n  \r\nhadoop.security.crypto.buffer.size  \r\n8192  \r\nThe buffer size used by CryptoInputStream and CryptoOutputStream.  \r\n {code}\r\nThe runtime enforces two hidden constraints that are not documented:\r\n1. Minimum value is 512 bytes. Values below 512 cause IllegalArgumentException at stream construction time.\r\n2. Block-size flooring: The effective buffer size is floored to a multiple of the cipher algorithm’s block size (e.g., 16 bytes for AES/CTR/NoPadding).\r\n\r\nAs a result, users may be surprised that:\r\n1. Setting a value like 4100 results in an actual capacity of 4096.\r\n2. Setting values <512 fails fast with IllegalArgumentException.\r\n\r\n*Expected*\r\ncore-default.xml (and user-facing docs) should explicitly document:\r\n1. Minimum legal value: 512 bytes.\r\n\r\n2. The effec\nQ: please provide a pull request in trunk which updates the file with the relevant information.\r\nhadoop-hdfs-project/hadoop-hdfs/src/site/markdown/TransparentEncryption.md", "output": "zzz0706 opened a new pull request, #7907:\nURL: https://github.com/apache/hadoop/pull/7907\n\n   ### Description of PR\r\n   \r\n   Docs-only change to **TransparentEncryption.md** clarifying the hidden constraints for\r\n   `hadoop.security.crypto.buffer.size`:\r\n   \r\n   - **Minimum value is 512 bytes** (values  4096`, `8195 -> 8192`.\r\n   \r\n   This is a minimal wording update (single-sentence addition) to avoid operator surprise, with no behavior change.\r\n   \r\n   **Target branch:** `trunk` (per contributor guide)\r\n   \r\n   **JIRA:** HADOOP-19667\r\n   \r\n   **Files changed:**\r\n   - `hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/TransparentEncryption.md`\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - Docs-only; built site/module locally to validate formatting:"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Wrap IllegalArgumentException with proper error code for invalid datetime patterns\nDescription: When `TO_TIMESTAMP` is called with an invalid datetime pattern (e.g., `'yyyyMMddHHMIss'` with invalid letter `'I'`), the `IllegalArgumentException` from Java's `DateTimeFormatterBuilder` escapes during constant folding optimization, resulting in a raw exception without proper error codes or `SQLSTATE`.\nQ: I am working ok this", "output": "Issue resolved by pull request 52762\n[https://github.com/apache/spark/pull/52762]"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Improve consumer and producer config files that are shipped with Kafka binary.\r\nThis may include additional important config to consider or simply improved comments.", "output": "Improve producer and consumer config files"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider\nDescription: Externally Reported Enhancement:\r\n\r\n*Current Limitation*\r\nThe current WorkloadIdentityTokenProvider implementation works well for file-based token scenarios, but it's tightly coupled to file system operations and cannot be easily extended for alternative token sources\r\n\r\n{*}Use Case{*}: *Kubernetes TokenRequest API* \r\nIn modern Kubernetes environments, the recommended approach is to use the TokenRequest API to generate short-lived, on-demand service account tokens rather than relying on projected volume mounts.\r\n\r\n*Proposed Enhancement* \r\nI propose modifying WorkloadIdentityTokenProvider to accept a Supplier for token retrieval instead of being hardcoded to file operations:\nQ: Draft PR without test to see if the structure looks okay - [https://github.com/apache/hadoop/pull/7901]\r\n\r\ncc: [~anujmodi]", "output": "Its not showing your name somehow"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: document the encoding of nullable struct\nDescription: In [https://kafka.apache.org/protocol#protocol_types,] we didn't specify the encoding of a struct. In particular, how a nullable struct is represented. We should document that if a struct is nullable, the first byte indicates whether is null and the rest of the bytes are the serialization of each field.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [ReadAheadV2] Negative tests for Read Buffer Manager V2 and dynamic scaling\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Changes in the SDK related to a new AWS SDK Feature, Default integritY Protection {{https://docs.aws.amazon.com/sdkref/latest/guide/feature-dataintegrity.html}} break all intereraction with third party S3 Stores.\r\n\r\nSee {{https://github.com/aws/aws-sdk-java-v2/issues/5801}}\r\n\r\nThere are documented mechanisms to turn this off\r\n* an environment variable\r\n* a change in ~/..aws/config\r\n* a system property\r\n* looks like a new builder option, though the docs don't cover it.\r\n\r\nThat checksum builder option looks like the only viable strategy, but it will need testing. maybe even make it a property which can be enabled/disabled, with tests and docs.\r\n\r\nWhoever wants a 2.30.x feature either gets to do the fixing and testing. For 3.4.2 it'll be an older release.", "output": "S3A: AWS SDK 2.30+ incompatible with third party stores"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Clover breaks on double semicolon\nDescription: Building with {{-Pclover}} fails with\r\n{code}\r\n[INFO] Instrumentation error\r\ncom.atlassian.clover.api.CloverException: /home/jenkins/hadoop/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ;\r\n...\r\nFailed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory\r\n{code}\r\n\r\nIt doesn't seem to like a double semicolon in ITestS3APutIfMatchAndIfNoneMatch.java that was added in HADOOP-19256.\nQ: hadoop-yetus commented on PR #7956:\nURL: https://github.com/apache/hadoop/pull/7956#issuecomment-3287077606\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  15m 13s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 21s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 140m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7956 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d1403c764b02 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9fc528eeb6112dd9b3209b648fb33da720214dff |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/testReport/ |\r\n   | Max. process+thread count | 709 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "cnauroth closed pull request #7956: HADOOP-19685. Fix double semicolon breaking clover\nURL: https://github.com/apache/hadoop/pull/7956"}
{"instruction": "Answer the question based on the bug.", "input": "Title: HADOOP-19467: [ABFS][FnsOverBlob] Fixing Config Name in Documenatation\nDescription: There is a typo in config name added as part of fns-blob documentation.\r\nThis will fix that typo\nQ: anujmodi2021 opened a new pull request, #7420:\nURL: https://github.com/apache/hadoop/pull/7420\n\n   ### Description of PR\r\n   Fixing Config Name in FNS-Blob documentation.\r\n   \r\n   ### How was this patch tested?\r\n   No production code change, no testing needed.", "output": "hadoop-yetus commented on PR #7420:\nURL: https://github.com/apache/hadoop/pull/7420#issuecomment-2673574442\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  71m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7420/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  7s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 109m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7420/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7420 |\r\n   | JIRA Issue | HADOOP-19467 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint |\r\n   | uname | Linux 2635e92ca74b 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 51155a07b5813908a9fec32d53fda2b385bc0960 |\r\n   | Max. process+thread count | 613 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7420/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: upgrade commons-beanutils to 1.11.0\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix hadoop-client-minicluster\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This implements KIP-1147 for kafka-verifiable-producer.sh and kafka-verifiable-consumer.sh.", "output": "Consistency of command-line arguments for verifiable producer/consumer"}
{"instruction": "Answer the question based on the bug.", "input": "Title: asyncCheckpoint.enabled incompatible with transformWithStateInPandas (StatefulProcessor)\nDescription: In structured streaming job, we use transformWithStateInPandas stateful processing and we have com.databricks.sql.streaming.state.RocksDBStateStoreProvider enabled. Everything works as expected.\r\n\r\nBut when we enable:\r\nspark.conf.set(\"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\", \"true\")\r\n\r\nWe get an error: org.apache.spark.SparkUnsupportedOperationException: Synchronous commit is not supported. Use asynchronous commit.\r\n \r\nWe have successfully replicated that in notebook, and also tried the same thing with applyInPandasWithState.\r\n * transformWithStateInPandas - throws Synchronous commit error\r\n * applyInPandasWithState - works as expected\r\n\r\nWe followed this guide: [https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/async-checkpointing] and also used chatGPT for references.\r\n\r\nLooking through the stacktrace, It seems like the issue is where {{TransformWithStateInPandasExec}} uses the sync {{commit()}} instead of the async {{{}commitAsync()\nQ: Looks like your issue is mostly scoped to Databricks. Could you please contact the support in Databricks? This JIRA is about \"Apache\" Spark.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Display connect server execution threads first in thread dump page\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "apache/hadoop:3 is built using Hadoop 3.3.6 binary. https://hub.docker.com/layers/apache/hadoop/3/images/sha256-af361b20bec0dfb13f03279328572ba764926e918c4fe716e197b8be2b08e37f\r\n\r\nNow that Hadoop 3.4 line is getting adopted, it's time to update the tag.", "output": "Update Hadoop docker image tag 3 to 3.4"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Test failures during AWS SDK upgrade\nDescription: 1. The logic to skip vector IO contract tests doesn't work  if the analytics stream is set on a per-bucket basis for the test bucket\r\n2. tests with SSE-C are failing. Test bucket is normally set up to use SSE-KMS, FWIW\r\n\r\n{code}\r\n  \r\n    fs.s3a.bucket.stevel-london.encryption.algorithm\r\n    SSE-KMS\r\n  \r\n\r\n\r\n{code}\r\n\r\nthis only happens when the analytics stream is set for the test bucket fs.s3a.bucket.stevel-london.input.stream.type=analytics; set it globally all is good\r\n\r\n+ various other tests. FInal pr message\r\n{code}\r\nCode changes related to HADOOP-19485.\r\n\r\nAwsSdkWorkarounds no longer needs to cut back on transfer manager logging\r\n(HADOOP-19272) :\r\n- Remove log downgrade and change assertion to expect nothing to be logged.\r\n- remove false positives from log.\r\n\r\nITestS3AEndpointRegion failure:\r\n- Change in state of AwsExecutionAttribute.ENDPOINT_OVERRIDDEN\r\n  attribute requires test tuning to match.\r\n\r\nSome tests against third-party stores fail\r\n- Includes fix for the assumeStoreAwsHosted() logic.\r\n- Documents how to turn off multipart uploads with third-party stores.\r\n{code}", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Apache Client Connection Pool Relook\nDescription: \nQ: bhattmanish98 opened a new pull request, #7817:\nURL: https://github.com/apache/hadoop/pull/7817\n\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19609\r\n   \r\n   **Description**\r\n   The ABFS (Azure Blob File System) driver currently utilizes the default JDK HTTP client to communicate with the Azure Storage backend service, known as XFE. While the JDK client is lightweight, simple, and built into the JDK, it falls short when it comes to providing advanced connection management capabilities. \r\n   By switching to Apache HTTP Client, the ABFS driver aims to gain better control over HTTP connections while enabling enterprise-level features such as configurable keep-alive strategies, idle connection eviction, retries on failure, and better resource usage tracking. \r\n   This is the initial PR where we have optimized the structure of keep alive cache for Apache client. Migration from JDK and Apache client will be done in subsequent PR.", "output": "hadoop-yetus commented on PR #7817:\nURL: https://github.com/apache/hadoop/pull/7817#issuecomment-3095586717\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 3 unchanged - 0 fixed = 7 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 20s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  93m 52s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7817 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b6b106bf7326 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b9e21c77c34a6e66ccf1d38b82b777973ecc38f2 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/1/testReport/ |\r\n   | Max. process+thread count | 556 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7817/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Publish multi-arch hadoop-runner image to GitHub\nDescription: Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image to GitHub Container Registry.  Build for both amd64 and arm64.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ConsumerPerformance#ConsumerPerfRebListener get corrupted value when the number of partitions is increased \nDescription: see https://github.com/apache/kafka/pull/20221#discussion_r2228931829", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: HADOOP-19467: [ABFS][FnsOverBlob] Fixing Config Name in Documenatation\nDescription: There is a typo in config name added as part of fns-blob documentation.\r\nThis will fix that typo\nQ: hadoop-yetus commented on PR #7420:\nURL: https://github.com/apache/hadoop/pull/7420#issuecomment-2673574442\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  71m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7420/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  7s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 109m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7420/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7420 |\r\n   | JIRA Issue | HADOOP-19467 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint |\r\n   | uname | Linux 2635e92ca74b 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 51155a07b5813908a9fec32d53fda2b385bc0960 |\r\n   | Max. process+thread count | 613 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7420/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7420:\nURL: https://github.com/apache/hadoop/pull/7420#issuecomment-2673874059\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 56s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  71m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 33s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 109m 45s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7420/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7420 |\r\n   | JIRA Issue | HADOOP-19467 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint |\r\n   | uname | Linux 5c0ca64ef96a 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8e62a2c3a32dbfc21cd90c2e3163e7278c35dd26 |\r\n   | Max. process+thread count | 700 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7420/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In the blob endpoint, we determine whether the path is a file, or a directory based on the metadata attribute hdi_isfolder. When creating a directory, we set hdi_isfolder to true. Currently, our method for checking if the path is a directory involves a case-sensitive equality check. Consequently, if someone configures a directory with Hdi_isfolder, the driver will not recognize that path as a directory. We need to address this issue because, in the backend, hdi_isfolder and Hdi_isfolder are considered the same metadata attribute. Therefore, the solution involves modifying our equality check to be case-insensitive, ensuring that the driver correctly identifies directories regardless of case variations in the metadata attribute.", "output": "ABFS: Fix Case Sensitivity Issue for hdi_isfolder metadata"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: stream write/close fails badly once FS is closed\nDescription: when closing a process during a large upload, and NPE is triggered in the abort call. This is because the S3 client has already been released.\r\n\r\n{code}\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$abortMultipartUpload$41(S3AFileSystem.java:5337)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.abortMultipartUpload(S3AFileSystem.java:5336)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$abortMultipartUpload$4(WriteOperationHelper.java:392)\r\n{code}\r\n\r\n* close() in small writes also fails, just with a different exception\r\n* and on some large writes, the output str\nQ: {code}\r\nBlock size 16,777,216\r\nWriting data as 8 blocks each of size 16,777,216 bytes\r\nStarting: Opening s3a://stevel-usw2/bw for upload\r\nDuration of Opening s3a://stevel-usw2/bw for upload: 0:00:00.038\r\nWrite block 0 in 0.077 seconds\r\nWrite block 1 in 0.065 seconds\r\nWrite block 2 in 0.065 seconds\r\nWrite block 3 in 1.715 seconds\r\nWrite block 4 in 0.067 seconds\r\nWrite block 5 in 0.066 seconds\r\nWrite block 6 in 0.065 seconds\r\nWrite block 7 in 0.074 seconds\r\n\r\nStarting: upload stream close()\r\n`cc^C^C^C^C^C^C^C2025-05-14 17:32:37,416 [shutdown-hook-0] INFO  s3a.S3AFileSystem (S3AFileSystem.java:processDeleteOnExit(4331)) - Ignoring failure to deleteOnExit for path s3a://stevel-usw2/bw\r\n2025-05-14 17:32:41,846 [s3a-transfer-stevel-usw2-bounded-pool1-t1] WARN  s3a.S3ABlockOutputStream (S3ABlockOutputStream.java:progressChanged(1334)) - Transfer failure of block ByteArrayBlock{index=0, state=Upload, limit=67108864, dataSize=67108864}                                                                              \r\n2025-05-14 17:32:41,852 [s3a-transfer-stevel-usw2-bounded-pool1-t2] WARN  s3a.S3ABlockOutputStream (S3ABlockOutputStream.java:progressChanged(1334)) - Transfer failure of block ByteArrayBlock{index=0, state=Closed, limit=67108864, dataSize=67108864}                                                                              \r\n2025-05-14 17:32:41,856 [main] INFO  impl.LoggingAuditor (LoggingAuditor.java:requestCreated(510)) - Creating a request outside an audit span {multipart_upload_aborted 'bw' size=0, mutating=true}\r\nDuration of upload stream close(): 0:00:22.772\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$abortMultipartUpload$41(S3AFileSystem.java:5337)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.abortMultipartUpload(S3AFileSystem.java:5336)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$abortMultipartUpload$4(WriteOperationHelper.java:392)\r\n        at org.apache.hadoop.fs.store.audit.AuditingFunctions.lambda$withinAuditSpan$1(AuditingFunctions.java:80)\r\n        at org.apache.hadoop.fs.s3a.Invoker.lambda$once$0(Invoker.java:165)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:163)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.abortMultipartUpload(WriteOperationHelper.java:389)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.lambda$abort$4(S3ABlockOutputStream.java:1258)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:494)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:465)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.abort(S3ABlockOutputStream.java:1255)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.waitForAllPartUploads(S3ABlockOutputStream.java:1185)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$1400(S3ABlockOutputStream.java:875)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:534)\r\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\r\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\r\n        at org.apache.hadoop.fs.store.commands.Bandwidth.run(Bandwidth.java:272)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)\r\n        at org.apache.hadoop.fs.store.commands.Bandwidth.exec(Bandwidth.java:470)\r\n        at org.apache.hadoop.fs.store.commands.Bandwidth.main(Bandwidth.java:479)\r\n        at bandwidth.main(bandwidth.java:25)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:333)\r\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:254)\r\n2025-05-14 17:32:41,865 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(241)) - Exiting with status -1: java.lang.NullPointerException\r\n{code}", "output": "Can also seem to hang if close() is called after the fs is shut as the submission failure isn't picked up on\r\n{code}\r\n2025-05-20 20:56:53,212 [JUnit-testOutputClosed] DEBUG s3a.Invoker (DurationInfo.java:(80)) - Starting: upload part request\r\n2025-05-20 20:56:53,212 [JUnit-testOutputClosed] DEBUG impl.RequestFactoryImpl (RequestFactoryImpl.java:newUploadPartRequestBuilder(662)) - Creating part upload request for dNf9u4SxpkPi9UN5XHGVK2BiAzR7JqRFsLwowvY2TtIS0vAo1IE9wgBprGTYYfeGOVMe5G_nbHHtjk8l1K7hcwBelCeWr2fNR3mhSe7._fmKNA2kDThpzX_.aAMqSXPS #1 size 1\r\n2025-05-20 20:56:53,216 [JUnit-testOutputClosed] DEBUG s3a.Invoker (DurationInfo.java:close(101)) - upload part request: duration 0:00.004s\r\n2025-05-20 20:56:53,225 [JUnit-testOutputClosed] ERROR util.BlockingThreadPoolExecutorService (BlockingThreadPoolExecutorService.java:rejectedExecution(141)) - Could not submit task to executor java.util.concurrent.ThreadPoolExecutor@2c12bb5e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\r\n2025-05-20 20:56:53,225 [JUnit-testOutputClosed] DEBUG s3a.S3ABlockOutputStream (S3ABlockOutputStream.java:clearActiveBlock(340)) - Clearing active block\r\n2025-05-20 20:56:53,225 [JUnit-testOutputClosed] DEBUG s3a.S3ABlockOutputStream (S3ABlockOutputStream.java:waitForAllPartUploads(1164)) - Waiting for 1 uploads to complete\r\n{code}"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: S3 Express bucket failure of conditional overwrite of multiparts is 200 + error, not 412\nDescription: The S3 Express response to create conflicts of multipart uploads is not a 412/419, it is 200 + an xml error.\r\n\r\n\r\n{code}\r\n\r\nError \r\n\r\nCode: PreconditionFailed\r\nMessage: At least one of the pre-conditions you specified did not hold\r\nCondition: If-None-Match\r\nRequestId: 01e17cb4f5000197899662aa050968218250cedb\r\nHostId: K1OApth\r\n\r\n{code}\r\n\r\nThis is raised an exception in the SDK, but it isn't mapping to a 419/412, the way we expect, so is mapped to\r\na generic AWSS3IOException.\r\n\r\n{code}\r\n\r\nAWSS3IOException: Completing multipart upload on test/testIfNoneMatchConflictOnMultipartUpload: software.amazon.awssdk.services.s3.model.S3Exception: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 200, Request ID: 0110480881000197899bc814050902966399617b, Extended Request ID: y1E4oDawYBQ7XsAVFA):PreconditionFailed: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 200, Request ID: 0110480881000197899bc814050902966399617b, E\nQ: {code}\r\n\r\norg.apache.hadoop.fs.s3a.AWSS3IOException: Completing multipart upload on test/testIfNoneMatchConflictOnMultipartUpload: software.amazon.awssdk.services.s3.model.S3Exception: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 200, Request ID: 0110480881000197899bc814050902966399617b, Extended Request ID: y1E4oDawYBQ7XsAVFA):PreconditionFailed: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 200, Request ID: 0110480881000197899bc814050902966399617b, Extended Request ID: y1E4oDawYBQ7XsAVFA)\r\n\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:368)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)\r\n\tat org.apache.hadoop.fs.s3a.WriteOperationHelper.finalizeMultipartUpload(WriteOperationHelper.java:318)\r\n\tat org.apache.hadoop.fs.s3a.WriteOperationHelper.completeMPUwithRetries(WriteOperationHelper.java:361)\r\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.lambda$complete$3(S3ABlockOutputStream.java:1227)\r\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:494)\r\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:465)\r\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:1225)\r\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$1500(S3ABlockOutputStream.java:876)\r\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:545)\r\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\r\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\r\n\tat org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.createFileWithFlags(ITestS3APutIfMatchAndIfNoneMatch.java:176)\r\n\tat org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchConflictOnMultipartUpload(ITestS3APutIfMatchAndIfNoneMatch.java:319)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)\r\n\tat java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java)\r\n\tat --- Async.Stack.Trace --- (captured by IntelliJ IDEA debugger)\r\n\tat java.util.concurrent.FutureTask.(FutureTask.java:132)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:122)\r\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)\r\n\tat org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)\r\n\tat org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:413)\r\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\r\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)\r\n\tat com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)\r\n\tat com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)\r\n\tat com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)\r\n\tat com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:231)\r\n\tat com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 200, Request ID: 0110480881000197899bc814050902966399617b, Extended Request ID: y1E4oDawYBQ7XsAVFA)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:155)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:107)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:84)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:42)\r\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler$Crc32ValidationResponseHandler.handle(AwsSyncClientHandler.java:93)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseClientHandler.lambda$successTransformationResponseHandler$7(BaseClientHandler.java:279)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:50)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:38)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:74)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:43)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:79)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:41)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage2.executeRequest(RetryableStage2.java:93)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage2.execute(RetryableStage2.java:56)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage2.execute(RetryableStage2.java:36)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:53)\r\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:35)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:82)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:43)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)\r\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)\r\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.completeMultipartUpload(DefaultS3Client.java:779)\r\n\tat software.amazon.awssdk.services.s3.DelegatingS3Client.lambda$completeMultipartUpload$1(DelegatingS3Client.java:598)\r\n\tat software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67)\r\n\tat software.amazon.awssdk.services.s3.DelegatingS3Client.completeMultipartUpload(DelegatingS3Client.java:598)\r\n\tat org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.completeMultipartUpload(S3AStoreImpl.java:904)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelperCallbacksImpl.completeMultipartUpload(S3AFileSystem.java:1956)\r\n\tat org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$finalizeMultipartUpload$1(WriteOperationHelper.java:324)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)\r\n\t... 54 more\r\n  {code}", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip doctests in pyspark.sql.functions.builtin if pyarrow is not installe\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: LocalDirAllocator still doesn't always recover from directory tree deletion\nDescription: In HADOOP-18636, LocalDirAllocator was modified to recreate missing dirs. But it appears that there are still codepaths which don't do that.\r\n\r\nwhen charrypicking this -please follow up with  HADOOP-19573, to ensure an associated test never fails", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: MirrorMaker2 Offset Replication Issue\nDescription: I am using *Apache Kafka 4.0* with *MirrorMaker 2* to link the primary cluster ({*}clusterA{*}) to the secondary cluster ({*}clusterB{*}).\r\nThe secondary cluster will not have any producers or consumers until a disaster recovery event occurs, at which point all producers and consumers will switch to it.\r\n\r\n*Setup:*\r\n * Dedicated standalone MirrorMaker 2 node\r\n * {{IdentityReplicationPolicy}} (no topic renaming)\r\n * No clients connected to secondary cluster under normal operation\r\n\r\n*MirrorMaker 2 config:*\r\n {{# Cluster aliases\r\nclusters = clusterA, clusterB\r\n\r\n# Bootstrap servers\r\nclusterA.bootstrap.servers = serverA-kafka-1:9092\r\nclusterB.bootstrap.servers = serverB-kafka-1:9092\r\n\r\n# Replication policy\r\nreplication.policy.class=org.apache.kafka.connect.mirror.IdentityReplicationPolicy\r\n\r\n# Offset/Checkpoint sync\r\nemit.checkpoints.enabled=true\r\nemit.checkpoints.interval.seconds=5\r\nsync.group.offsets.enabled=true\r\nsync.group.offsets.interval.seconds=5\r\noffset.lag.max=10\r\nrefresh.topics.\nQ: Hi Greg, \r\nThanks for replying. \r\n\r\nIn a planned flip to DR, is there a way for me to force sync the consumer offsets?\r\n\r\nExample: to delete all the consumers and re-sync the offset?\r\n\r\nThanks\r\n\r\nGeric", "output": "[~geric] I'm not sure exactly what you mean by force-sync. I mentioned in my last message \"you either need to let your consumers reach 0 lag prior to cut-over, or ...\".\r\n\r\nTo be more explicit, If you're planning to flip to the target cluster and want minimum redelivery you should:\r\n1. Stop the source producers\r\n2. Wait for source consumers to reach 0 lag and commit offsets\r\n3. Wait for MM2 to translate the offsets to get 0 lag on the target\r\n4. Alter target ACLs and start target producers\r\n5. Start the target consumers\r\n6. Stop MM2 mirroring\r\n\r\nStep 3 is the synchronization point, because if you perform step (4) early, some of the newly produced data will be dropped (data loss), and if you perform step (5) early, MM2 won't be able to sync offsets into an active group (extra redelivery)."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)\nDescription: The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone.\r\n\r\nAfter the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will take time, and during this period, other jobs are still catching up with the new realm configuration. However, the namenode down disrupts all of them.\r\n\r\n \r\n{code:java}\r\nERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception\r\njava.lang.IllegalArgumentException: Illegal principal name hive/xxxxx@yyyy.COM\r\norg.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM         at org.apache.hadoop.security.User.(User.java:51)\r\n        at org.apache.hadoop.security.User.(User.java:43)\r\n        at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1417)\r\n        at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1401)\r\n        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier.getUser(AbstractDelegationTokenIdentifier.java:80)\r\n        at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.getUser(DelegationTokenId", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Streams groups sometimes describe as NOT_READY when STABLE\nDescription: Streams groups sometimes describe as NOT_READY when STABLE. That is, the group is configured and all topics exist, but when you use LIST_GROUP and STREAMS_GROUP_DESCRIBE, the group will show up as not ready.\r\n\r\nThe root cause seems to be that [https://github.com/apache/kafka/pull/19802] moved the creation of the soft state configured topology from the replay path to the heartbeat. This way, LIST_GROUP and STREAMS_GROUP_DESCRIBE, which show the snapshot of the last committed offset, may not show the configured topology, and therefore assume that the streams group is NOT_READY instead of STABLE.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Handle JDK-8225499 IpAddr.toString() format changes in tests\nDescription: The IP address string format has changed in JDK14\r\n[https://bugs.openjdk.org/browse/JDK-8225499|https://bugs.openjdk.org/browse/JDK-8232369]\r\n\r\n\r\n\r\nHDFS-15685 adjust some tests for it, but not all.\nQ: hadoop-yetus commented on PR #7502:\nURL: https://github.com/apache/hadoop/pull/7502#issuecomment-2722677852\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  18m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 47s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  36m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 31s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 14s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 44s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  15m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 39s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 45s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  38m  4s |  |  hadoop-mapreduce-client-app in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 51s | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs-rbf.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/1/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs-rbf.txt) |  hadoop-hdfs-rbf in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 46s |  |  ASF License check generated no output?  |\r\n   |  |   | 280m 58s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7502 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 97c7a921b1f6 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4ed6e2dc2bba937d153bdc7e9584e60e534183c1 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/1/testReport/ |\r\n   | Max. process+thread count | 565 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-hdfs-project/hadoop-hdfs-rbf U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7502:\nURL: https://github.com/apache/hadoop/pull/7502#issuecomment-2724244749\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 51s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  36m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m  9s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 30s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 39s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  9s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  7s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  15m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 34s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   3m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 14s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   9m 16s |  |  hadoop-mapreduce-client-app in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  47m 58s |  |  hadoop-hdfs-rbf in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 278m 39s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7502 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2014519d0667 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4ed6e2dc2bba937d153bdc7e9584e60e534183c1 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/3/testReport/ |\r\n   | Max. process+thread count | 2766 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-hdfs-project/hadoop-hdfs-rbf U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7502/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Increase `s.k.o.reconciler.foregroundRequestTimeoutSeconds` to `60s`\nDescription: \nQ: Issue resolved by pull request 395\n[https://github.com/apache/spark-kubernetes-operator/pull/395]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The metric assigned-partitions is missing in the new async consumer.\r\n\r\nRegistered by the coordinator in the old consumer here [https://github.com/apache/kafka/blob/dbd2b527d0fa5eda4ec603ee449a47c69ac41852/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L1640]", "output": "Missing assigned-partitions metric in new consumer"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce Geography and Geometry data types to Java API\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52623.", "output": "Issue resolved by pull request 52623\n[https://github.com/apache/spark/pull/52623]"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "While working on KAFKA-19722  I noticed we had no test coverage for the rebalance metrics on the new consumer (this class [https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/metrics/ConsumerRebalanceMetricsManager.java] )\r\n\r\nI introduced the test file and added a test related to KAFKA-19722, but we should extend coverage and test the other rebalance metrics (ex. tests like the ones for the ShareRebalanceMetrics [https://github.com/apache/kafka/blob/trunk/clients/src/test/java/org/apache/kafka/clients/consumer/internals/metrics/ShareRebalanceMetricsManagerTest.java] )", "output": "Add tests for ConsumerRebalanceMetricsManager"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Maven site task fails with Java 17\nDescription: If I try to build site with Java 17, I get an IllegalArgumentException from analyze-report\r\n{code}\r\n$ mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site\r\n...\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report: IllegalArgumentException -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:347)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.i\nQ: hadoop-yetus commented on PR #7936:\nURL: https://github.com/apache/hadoop/pull/7936#issuecomment-3262110832\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   9m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  15m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   6m  7s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  94m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  19m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m  3s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 20s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   8m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   9m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m  5s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 21s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 663m 19s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 840m 24s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapreduce.v2.TestUberAM |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.server.router.subcluster.capacity.TestYarnFederationWithCapacityScheduler |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7936 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 33f1385abb00 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e89a211da8c19aed6f68dbd9ea4418f6e657d615 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/testReport/ |\r\n   | Max. process+thread count | 3913 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7936/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "szetszwo merged PR #7936:\nURL: https://github.com/apache/hadoop/pull/7936"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add `SparkConf.getAllWithPrefix(String, String => K)` API\nDescription: We need to set some config related to S3 for our inner Spark. The implementation of the function show below.\r\n\r\n{code:java}\r\nprivate def setS3Configs(conf: SparkConf): Unit = {\r\n    val S3A_PREFIX = \"spark.fs.s3a\"\r\n    val SPARK_HADOOP_S3A_PREFIX = \"spark.hadoop.fs.s3a\"\r\n    val s3aConf = conf.getAllWithPrefix(S3A_PREFIX)\r\n    s3aConf\r\n      .foreach(\r\n        confPair => {\r\n          val keyWithoutPrefix = confPair._1\r\n          val oldKey = S3A_PREFIX + keyWithoutPrefix\r\n          val newKey = SPARK_HADOOP_S3A_PREFIX + keyWithoutPrefix\r\n          val value = confPair._2\r\n          (newKey, value)\r\n        })\r\n  }\r\n\r\n{code}\r\n\r\nThese code seems redundant and complicated. The reason is getAllWithPrefix only return the suffix part.\nQ: Issue resolved by pull request 52693\n[https://github.com/apache/spark/pull/52693]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Delete Operation\nDescription: We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Ingress Related operations on blob Endpoint (Create + Append + Flush filesystem calls)\r\n\r\nThis Jira tracks implementing these tests.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4's {{org.junit.Assume}} API to JUnit5's {{org.junit.jupiter.api.Assumptions}} API in order to unify the testing framework and improve maintainability.\r\nh4. Scope of changes:\r\n * Replace usages of {{{}Assume.assumeTrue(...){}}}, {{{}Assume.assumeFalse(...){}}}, etc., with the corresponding methods from JUnit5, such as {{{}Assumptions.assumeTrue(...){}}};\r\n\r\n * Ensure the assertion logic remains consistent with the original behavior;\r\n\r\n * Update any outdated import statements referencing JUnit4's {{{}Assume{}}};\r\n\r\n * Verify that all affected unit tests pass correctly under JUnit5.", "output": "S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Initial implement Connect JDBC driver\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Ammonite was upgraded to 3.0.3 so semanticdb-shared should be upgraded correspondingly.\r\n\r\nhttps://mvnrepository.com/artifact/com.lihaoyi/ammonite-interp-3.3.6_3.3.6/3.0.3", "output": "Upgrade semanticdb-shared to 4.13.10"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Reorganize the Python TWS tests:\r\n* moving tws related tests to a new `/streaming` directory\r\n\r\n* further split the Python TWS tests to smaller ones to speed up the CI", "output": "Reorganize Python streaming TWS test"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{{RocksDbStateStore#close}} will need to detect when {{upgrade.from}} is set to a version that doesn't support offsets, and downgrade by porting them out to {{.checkpoint}} and removing the column family.", "output": "Add downgrade support"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Rename Recovery Should Succeed When Marker File Exists with Destination Directory\nDescription: On the blob endpoint, since rename is not a direct operation but a combination of two operations—copy and delete—in the case of directory rename, we first rename all the blobs that have the source prefix and, at the end, rename the source to the destination.\r\n\r\nIn the normal rename flow, renaming is not allowed if the destination already exists. However, in the case of recovery, there is a possibility that some files have already been renamed from the source to the destination. With the recent change ([HADOOP-19474] ABFS: [FnsOverBlob] Listing Optimizations to avoid multiple iteration over list response. - ASF JIRA), where we create a marker if the path is implicit, rename recovery will fail at the end when it tries to rename the source to the destination after renaming all the files.\r\n\r\nTo fix this, while renaming the source to the destination, if we encounter an error indicating that the path already exists, we will suppress the error and mark the rename recovery as successful.\nQ: hadoop-yetus commented on PR #7559:\nURL: https://github.com/apache/hadoop/pull/7559#issuecomment-2766074619\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 39s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m  2s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 133m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7559 |\r\n   | JIRA Issue | HADOOP-19522 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2be4af61ef21 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 71ef4361d61b6a1e8e8bcd29e1d7a913dce4d519 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/testReport/ |\r\n   | Max. process+thread count | 591 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7559:\nURL: https://github.com/apache/hadoop/pull/7559#issuecomment-2768413476\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 137m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7559 |\r\n   | JIRA Issue | HADOOP-19522 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 706e5d959e7a 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e5fbd0193cd0e9bdf083a29a2534c9b952308480 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/2/testReport/ |\r\n   | Max. process+thread count | 526 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7559/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Insert Overwrite Jobs With MagicCommitter Fails On S3 Express Storage\nDescription: Query engines which uses Magic Committer to overwrite a directory would ideally upload the MPUs (not complete) and then delete the contents of the directory before committing the MPU.\r\n\r\n \r\n\r\nFor S3 express storage, The directory purge operation is enabled by default. Refer [here|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L688] for code pointers.\r\n\r\n \r\n\r\nDue to this, the pending MPU uploads are purged and query fails with \r\n\r\n{{NoSuchUpload: The specified multipart upload does not exist. The upload ID might be invalid, or the multipart upload might have been aborted or completed. }}\nQ: people trying to do INSERT OVERWRITE? hmm. \r\n\r\nok, set to default, mark as incompatible, and update the docs with \r\n* a section on this. \r\n* what the error means\r\n\r\npeople should always have auto cleanup enabled anyway, shouldn't they? always always always. in fact, aws cost analyzer should warn if you don't (does it do this?). and given that, shouldn't really matter much. we have CLI tools in terms of \"hadoop s3guard uploads\" to enum/purge it too. \r\n\r\nFWIW one of our qe buckets didn't do the cleanup, I used it for a scale test of MPU cleanup. We could actually have an ILoadTest for this now that createFile() lets you create a zero byte MPU", "output": "Yes - it is always recommended to have S3 bucket life cycle policy to clean up dangling MPUs after a certain threshold period of time."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Drop temporary functions in Pandas UDF tests\nDescription: \nQ: Issue resolved by pull request 52690\n[https://github.com/apache/spark/pull/52690]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove unsupported \"upgrade from\" versions\nDescription: Cf [https://github.com/apache/kafka/pull/20518#discussion_r2338270305]\r\n\r\nWith AK 4.0.0 release, we officially cut support for direct upgrades of Kafka Streams applications from versions 2.3 and older, and only direct upgrades from 2.4 or newer to 4.0+ are supported.\r\n\r\nThus, we should update `UpgradeFromValues.java` and remove older version, and cleanup the backward compatibility code accordingly.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Make DSv2 table resolution aware of cached tables\nDescription: Make DSv2 table resolution aware of cached tables. If the user executes CACHE TABLE t, the subsequent resolution calls must use the cached table metadata, instead of potentially allowing the connectors to refresh and break the cache lookup.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is the first part of HADOOP-19574 which adds the compatibility code, and fixes Daemon, but does not replace native Thread instances.", "output": "Add SubjectInheritingThread to restore pre JDK22 Subject behaviour in Threads"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update byte-buddy to 1.15.11\nDescription: This is required for being able to handle recent Java bytecodes.\r\n\r\nByte-buddy is used by BOTH mockito and maven-shade-plugin.\r\n\r\n1.15.11 is the last byte-buddy version that maven-shade-plugin 3.6.0 works with.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Some config settings are related to each other, or certain configuration constraints haven’t been discussed.\r\n\r\n\r\nSee:\r\n[https://github.com/apache/kafka/pull/18998#discussion_r1988001109]\r\nhttps://github.com/apache/kafka/pull/20318/files#r2465660429", "output": "Adjust quorum-related config lower bounds"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add off-heap mode support for on-heap-only memory consumers\nDescription: There are a few memory consumers that only support on-heap mode. Including:\r\n\r\n# LongToUnsafeRowMap (for long key hash join)\r\n# ExternalSorter (for non-serializable sort-based shuffle)\r\n\r\nIt's ideal to make them support off-heap when Spark is running with off-heap memory mode. For 3rd accelerator plugins that leverage Spark off-heap memory, having the large allocations in vanilla Spark supporting off-heap mode will significantly ease the memory settings for running the mixed query plan that contains both vanilla Spark and offloaded computations.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "On the blob endpoint, since rename is not a direct operation but a combination of two operations—copy and delete—in the case of directory rename, we first rename all the blobs that have the source prefix and, at the end, rename the source to the destination.\r\n\r\nIn the normal rename flow, renaming is not allowed if the destination already exists. However, in the case of recovery, there is a possibility that some files have already been renamed from the source to the destination. With the recent change ([HADOOP-19474] ABFS: [FnsOverBlob] Listing Optimizations to avoid multiple iteration over list response. - ASF JIRA), where we create a marker if the path is implicit, rename recovery will fail at the end when it tries to rename the source to the destination after renaming all the files.\r\n\r\nTo fix this, while renaming the source to the destination, if we encounter an error indicating that the path already exists, we will suppress the error and mark the rename recovery as successful.", "output": "ABFS: [FnsOverBlob] Rename Recovery Should Succeed When Marker File Exists with Destination Directory"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-sls.\nDescription: \nQ: slfan1989 opened a new pull request, #7553:\nURL: https://github.com/apache/hadoop/pull/7553\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19440. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-sls.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   mvn clean test & junit test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7553:\nURL: https://github.com/apache/hadoop/pull/7553#issuecomment-2763299450\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  2s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 13 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 16s | [/results-checkstyle-hadoop-tools_hadoop-sls.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-sls.txt) |  hadoop-tools/hadoop-sls: The patch generated 12 new + 22 unchanged - 0 fixed = 34 total (was 22)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 53s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 57s | [/patch-unit-hadoop-tools_hadoop-sls.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/1/artifact/out/patch-unit-hadoop-tools_hadoop-sls.txt) |  hadoop-sls in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 114m 22s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.sls.TestSLSStreamAMSynth |\r\n   |   | hadoop.yarn.sls.TestReservationSystemInvariants |\r\n   |   | hadoop.yarn.sls.TestSLSDagAMSimulator |\r\n   |   | hadoop.yarn.sls.TestSLSRunner |\r\n   |   | hadoop.yarn.sls.TestSLSGenericSynth |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.sls.nodemanager.TestNMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7553 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 329636455c63 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1b56a9a9ec8fb4d4f93f189faeb78c89eb1cfeee |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/1/testReport/ |\r\n   | Max. process+thread count | 707 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-sls U: hadoop-tools/hadoop-sls |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The project is currently using {{restrict-imports-enforcer-rule}} version {*}2.0.0{*}, which was released in *2021* and is now outdated. Since then, several new versions have been released with important bug fixes, performance improvements, and enhanced compatibility. To maintain a modern and stable build environment, it is necessary to upgrade to version {*}2.6.1{*}.", "output": "Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-distcp.\nDescription: \nQ: slfan1989 opened a new pull request, #7368:\nURL: https://github.com/apache/hadoop/pull/7368\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19431. Upgrade JUnit from 4 to 5 in hadoop-distcp.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Junit Test & mvn clean test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7368:\nURL: https://github.com/apache/hadoop/pull/7368#issuecomment-2644452532\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 32 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-distcp.txt) |  hadoop-tools/hadoop-distcp: The patch generated 85 new + 272 unchanged - 9 fixed = 357 total (was 281)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 36s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  24m 19s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 138m 25s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.TestDistCpSystem |\r\n   |   | hadoop.tools.TestIntegration |\r\n   |   | hadoop.tools.util.TestDistCpUtilsWithCombineMode |\r\n   |   | hadoop.tools.TestCopyListing |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7368 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 710c27ea8366 5.15.0-126-generic #136-Ubuntu SMP Wed Nov 6 10:38:22 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 889e4304ec1131bc7df79f69e31114ce50e50c4b |\r\n   | Default Java | Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/testReport/ |\r\n   | Max. process+thread count | 553 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Turn on Gradle reproducible builds feature\nDescription: *Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923] \r\n\r\n*Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future)\r\n\r\n*Related Gradle issues and links:*\r\n *  [https://github.com/gradle/gradle/issues/34643]\r\n * [https://github.com/gradle/gradle/issues/30871]  \r\n * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives]\r\n * [https://docs.gradle.org/9.1.0/userguide/upgrading_major_version_9.html#reproducible_archives_by_default] \r\n * [https://docs.gradle.org/9.1.0/dsl/org.gradle.api.tasks.bundling.Tar.html#org.gradle.api.tasks.bundling.Tar:preserveFileTimestamps]  \r\n\r\n \r\n\r\n*Definition of done (at the minimum):*\r\n * *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc) : [https://kafka.apache.org/quickstart]\nQ: Hi [~dejan2609] Per the docs reproducible builds is enabled by deafult on gradle 9\r\n\r\n{code:java}\r\nStarting with Gradle 9, archives are reproducible by default.\r\n{code}\r\n\r\nis this code block no longer required? \r\n\r\n\r\n{code:java}\r\n  tasks.withType(AbstractArchiveTask).configureEach {\r\n    reproducibleFileOrder = false\r\n    preserveFileTimestamps = true\r\n    useFileSystemPermissions()\r\n  }\r\n{code}\r\n\r\nSources:\r\n[Gradle Docks|https://docs.gradle.org/current/userguide/working_with_files.html#sec:reproducible_archives]\r\n[Gradle Reproducible Plugin|https://gradlex.org/reproducible-builds/]", "output": "Hi [~naveenthuvana] \r\n\r\nMy suggestion for you is to try to build with *reproducibleFileOrder = true* (this is a default, so it can be left out) and *preserveFileTimestamps = true*"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use Java `Set.of` instead of `Collections.emptySet`\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix TestTextCommand on Windows\nDescription: The following 2 tests are failing in TestTextCommand -\r\n1. testDisplayForNonWritableSequenceFile\r\n2. testDisplayForSequenceFileSmallMultiByteReads\r\n\r\nThese tests create a file and flush a string ending with \"\\n\". However, for verification, the test expects \"\\r\\n\" as the line ending on Windows. Thus, the test fails.\nQ: GauthamBanasandra opened a new pull request, #7679:\nURL: https://github.com/apache/hadoop/pull/7679\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   The following 2 tests are failing in TestTextCommand -\r\n   1. testDisplayForNonWritableSequenceFile\r\n   2. testDisplayForSequenceFileSmallMultiByteReads\r\n   \r\n   These tests create a file and flush a string ending with \"\\n\". However, for verification, the test expects \"\\r\\n\" as the line ending on Windows. Thus, the test fails.\r\n   \r\n   This PR fixes the same by checking for \"\\n\" as the line ending on Windows.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   1. Ran the fixed unit tests locally.\r\n   2. Jenkins CI validation.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7679:\nURL: https://github.com/apache/hadoop/pull/7679#issuecomment-2869010098\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 16s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 16s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  44m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 55s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 21s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 12s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 53s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m  7s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 227m 19s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7679/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7679 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux e93e5cdd2cc7 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a1fdc69daaa1f5f477df15bd17e3c25c517ea058 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7679/1/testReport/ |\r\n   | Max. process+thread count | 3135 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7679/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The Hadoop 3.4.2 version has been released, and we need to update the Hadoop Docker image to version 3.4.2.", "output": "Upgrade hadoop3 docker scripts to 3.4.2"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support logging in driver-side workers\nDescription: \nQ: Issue resolved by pull request 52808\n[https://github.com/apache/spark/pull/52808]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: SecretManager logs at INFO in bin/hadoop calls\nDescription: When I invoke a CLI command, I now get told information about HMAC keys which are\r\n* utterly meaningless to me\r\n* completely unrelated to what I am doing\r\n\r\n{code}\r\n bin/hadoop s3guard bucket-info $BUCKET\r\n\r\n2025-03-25 12:05:44,838 [main] INFO  token.SecretManager (SecretManager.java:(126)) - Selected hash algorithm: HmacSHA1\r\n2025-03-25 12:05:44,838 [main] INFO  token.SecretManager (SecretManager.java:(131)) - Selected hash key length:64\r\n{code}\r\n\r\nLooks like the changes in YARN-11738 have created this\nQ: hadoop-yetus commented on PR #7537:\nURL: https://github.com/apache/hadoop/pull/7537#issuecomment-2753082179\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m  9s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 23s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  14m  0s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 15s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 55s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 53s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 55s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 11s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 36s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 10s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 211m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7537/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7537 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 6c2e714cad3e 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b562ce1790963766339128c1707f51df35a35e0b |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7537/1/testReport/ |\r\n   | Max. process+thread count | 1271 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7537/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "cnauroth closed pull request #7537: HADOOP-19514: SecretManager logs at INFO in bin/hadoop calls\nURL: https://github.com/apache/hadoop/pull/7537"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Dockerfile_windows_10 cannot be build\nDescription: The following command from BUILDING.txt does not work anymore:\r\n{code:java}\r\ndocker build -t hadoop-windows-10-builder -f .\\dev-support\\docker\\Dockerfile_windows_10 .\\dev-support\\docker\\ {code}\r\nSeveral dependencies are missing and vcpkg points to an outdated 7-zip package.\nQ: mgmgwi opened a new pull request, #7417:\nURL: https://github.com/apache/hadoop/pull/7417\n\n   \r\n   \r\n   ### Description of PR\r\n   Fix a few outdated dependencies.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7417:\nURL: https://github.com/apache/hadoop/pull/7417#issuecomment-2671652878\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7417/1/console in case of problems."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Client#call decrease asyncCallCounter incorrectlly when exceptions occur\nDescription: Client#call decrease asyncCallCounter incorrectlly when exceptions occur", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use java.time.Clock instead of org.apache.hadoop.util.Clock\nDescription: Hadoop's {{Clock}} interface was recently moved from {{org.apache.hadoop.yarn.util}} (in hadoop-yarn) to {{org.apache.hadoop.util}} (in hadoop-common) as part of YARN-11765.\r\n\r\nI propose to seize the opportunity of this being targeted done for 3.5.0 to modernize it usage:\r\n# Deprecate {{org.apache.hadoop.util.Clock}}\r\n# Replace all of its usages with {{java.time.Clock}}\r\n# Replace existing usages of its simple implementations, e.g. {{SystemClock}}/{{UTCClock}} with standard {{java.time.Clock}} subclasses, e.g. {{Clock.systemUTC()}}\r\n# Re-implement other implementations, e.g. {{MonotonicClock}}/{{ControllerClock}}, as {{java.time.Clock}} subclasses.\r\n\r\nThe standard {{java.time.Clock}} has a richer API supports modern {{java.time}} classes such as {{Instant}} and {{ZoneId}}, and migration would be straightforward:\r\nJust changing {{org.apache.hadoop.util.Clock.getTime()}} to {{java.time.Clock.millis()}}\nQ: YanivKunda opened a new pull request, #7569:\nURL: https://github.com/apache/hadoop/pull/7569\n\n   ### Description of PR\r\n   \r\n   Closes HADOOP-19525\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran all existing tests.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [V] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7569:\nURL: https://github.com/apache/hadoop/pull/7569#issuecomment-2769358580\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 21s |  |  https://github.com/apache/hadoop/pull/7569 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7569 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7569/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Disk space is not being freed\nDescription: I found that the remoteFetch thread is holding the file, preventing the disk space from being freed.\r\n\r\n!image-2025-09-24-10-38-25-161.png!  \r\n\r\nHowever, I didn't encounter this problem with the earlier 0.0.1-SNAPSHOT build, which used Kafka 3.8.1. The current issue appears in version 1.0.0 together with Kafka 3.9.1.\r\n\r\n!image-2025-09-24-10-38-36-572.png!\r\n\r\nI'm not sure whether this issue is caused by the tiered storage plugin or by Kafka itself. Could someone help take a look?", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade commons-beanutils to 1.11.0 due to CVE-2025-48734.\nDescription: Upgrade commons-beanutils to 1.11.0 due to CVE-2025-48734.\nQ: [~stevel@apache.org] Confirmed, I’ve submitted a PR#7743 to fix it. Could you please review it when you have time?", "output": "JackPGreen commented on PR #7743:\nURL: https://github.com/apache/hadoop/pull/7743#issuecomment-3272237671\n\n   This also addresses [HADOOP-19633](https://issues.apache.org/jira/browse/HADOOP-19633)."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Set charsetEncoder in HadoopArchiveLogs\nDescription: The test for it fails in CI on Centos 7, likely because it has different default charsets than the other CI environments.\r\n{noformat}\r\n[ERROR] org.apache.hadoop.tools.TestHadoopArchiveLogs.testGenerateScript  Time elapsed: 0.036 s  <<< ERROR!\r\njava.lang.IllegalStateException: Mismatched Charset(UTF-8) and CharsetEncoder(US-ASCII)\r\n\tat org.apache.commons.io.output.FileWriterWithEncoding$Builder.get(FileWriterWithEncoding.java:113)\r\n\tat org.apache.hadoop.tools.HadoopArchiveLogs.generateScript(HadoopArchiveLogs.java:512)\r\n\tat org.apache.hadoop.tools.TestHadoopArchiveLogs._testGenerateScript(TestHadoopArchiveLogs.java:280)\r\n\tat org.apache.hadoop.tools.TestHadoopArchiveLogs.testGenerateScript(TestHadoopArchiveLogs.java:248)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.in\nQ: hadoop-yetus commented on PR #7520:\nURL: https://github.com/apache/hadoop/pull/7520#issuecomment-2731985497\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 58s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 34s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  31m 45s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 27s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7520/1/artifact/out/patch-unit-hadoop-tools_hadoop-archive-logs.txt) |  hadoop-archive-logs in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 28s |  |  ASF License check generated no output?  |\r\n   |  |   | 122m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7520/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7520 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 1d5312c73ba2 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 82894b473c62ccf62e98c68411bd40c50743c9a5 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7520/1/testReport/ |\r\n   | Max. process+thread count | 699 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-archive-logs U: hadoop-tools/hadoop-archive-logs |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7520/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on PR #7520:\nURL: https://github.com/apache/hadoop/pull/7520#issuecomment-2735034804\n\n   @stoty Thanks for the contribution! LGTM."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: ITestS3AEndpointRegion fails when using with access points\nDescription: failures are of the form:\r\n\r\n\r\n{{software.amazon.awssdk.core.exception.SdkClientException: Invalid configuration: region from ARN `us-east-1` does not match client region `us-east-2` and UseArnRegion is `false`}}\r\n{{ }}\r\n{{at software.amazon.awssdk.core.exception.SdkClientException$BuilderImpl.build(SdkClientException.java:111)}}\r\n\r\n{{ }}\r\n{{happens because when making the head request in the tests, we do }}\r\n{{HeadBucketRequest.builder().bucket(getFileSystem().getBucket()).build();}}\r\n{{ }}\r\n{{when using access points, bucket is \"arn:aws:s3:us-east-1:xxxx:accesspoint/test-bucket\", so client gets the region from the ARN which does not match the configured region. }}", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Consumer NoOffsetForPartitionException for partitions being revoked\nDescription: Currently, the consumer attempts to update positions for all partitions it owns that don't have a position, even when the partition may be already being revoked. This could lead to NoOffsetForPartitionException for such partitions (if there is no reset strategy defined). There are 2 main issues around this:\r\n * is probably unneeded work to fetch offsets and update positions for partitions being revoked. At that point we're actually not allowing fetching from there anymore\r\n * throwing NoOffsetForPartitionException may lead applications to take action to set positions themselves, which will most probably fail (the partition will most probably not owned by the consumer anymore since it was already being revoked when the NoOffsetForPartitionException was generated). We've seen this on Kafka Streams, that handled NoOffsetForPartitionException by calling seek to set positions.   \r\n\r\nThis task is to review if there are no other implications I may be missing? Then fix to ensure we don't update positions for partitions being revoked (aligned with how we don't allow fetching from them)", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "sendDeferedResponse should also log exception info.", "output": "sendDeferedResponse should also log exception info."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use `DescribeTopicsResult.allTopicNames` instead of the deprecated `all` API\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip tests in Hadoop common that depend on SecurityManager if the JVM does not support it\nDescription: TestExternalCall fails when SecurityManager cannot be set, we need to skip it on thos JVMs.\r\n\r\nTestGridmixSubmission has already been rewritten to use ExitUtil, we just need to remove the leftover SecurityManager calls.\nQ: stoty opened a new pull request, #7567:\nURL: https://github.com/apache/hadoop/pull/7567\n\n   ### Description of PR\r\n   \r\n   Skip tests in Hadoop common that depend on SecurityManager if the JVM does not support it\r\n   Required for recent Java versions.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   On my JDK24 branch, plus CI for JDK8\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7567:\nURL: https://github.com/apache/hadoop/pull/7567#issuecomment-2768959741\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 27s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  22m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 52s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 47s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 21s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 50s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 53s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  25m 22s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  14m 28s |  |  hadoop-gridmix in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 120m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7567/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7567 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 79aac4ca5778 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 94c543b40111d65e1c531cb1b7f4e8e81aaecfb4 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7567/1/testReport/ |\r\n   | Max. process+thread count | 1043 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-distcp hadoop-tools/hadoop-gridmix U: hadoop-tools |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7567/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix sbt inconsistent shading package\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-distcp.\nDescription: \nQ: hadoop-yetus commented on PR #7368:\nURL: https://github.com/apache/hadoop/pull/7368#issuecomment-2644452532\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 32 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-distcp.txt) |  hadoop-tools/hadoop-distcp: The patch generated 85 new + 272 unchanged - 9 fixed = 357 total (was 281)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 36s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  24m 19s | [/patch-unit-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/artifact/out/patch-unit-hadoop-tools_hadoop-distcp.txt) |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 138m 25s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.TestDistCpSystem |\r\n   |   | hadoop.tools.TestIntegration |\r\n   |   | hadoop.tools.util.TestDistCpUtilsWithCombineMode |\r\n   |   | hadoop.tools.TestCopyListing |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7368 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 710c27ea8366 5.15.0-126-generic #136-Ubuntu SMP Wed Nov 6 10:38:22 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 889e4304ec1131bc7df79f69e31114ce50e50c4b |\r\n   | Default Java | Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/testReport/ |\r\n   | Max. process+thread count | 553 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7368:\nURL: https://github.com/apache/hadoop/pull/7368#issuecomment-2644654276\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 32 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 28s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/results-checkstyle-hadoop-tools_hadoop-distcp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-distcp.txt) |  hadoop-tools/hadoop-distcp: The patch generated 20 new + 268 unchanged - 18 fixed = 288 total (was 286)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  26m 11s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 139m 17s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7368 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ffd94fda9ed4 5.15.0-126-generic #136-Ubuntu SMP Wed Nov 6 10:38:22 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 023b1c94bf293e1f608e6041b28d292d8599fa37 |\r\n   | Default Java | Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/2/testReport/ |\r\n   | Max. process+thread count | 674 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-distcp U: hadoop-tools/hadoop-distcp |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7368/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Avoid noisy NPE logs when closing consumer after constructor failures\nDescription: If there's a failure in the kafka consumer constructor, we attempt to close it https://github.com/lianetm/kafka/blob/2329def2ff9ca4f7b9426af159b6fa19a839dc4d/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L540\r\nIn that case, it could be the case that some components may have not been created, so we should consider some null checks to avoid noisy logs about NPE. \r\n\r\nThis noisy logs have been reported with the console share consumer in a similar scenario, so this task is to review and do a similar fix for the Async if needed.\nQ: Hi [~francisgodinho]! Thanks for your interest! There is already someone from the team working on this :S \r\nBut stay on the lookout for new issue that we create, and also maybe check what's already out with minor/trivial complexity (ex. [this filter|https://issues.apache.org/jira/browse/KAFKA-15642?jql=project%20%3D%20KAFKA%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened%2C%20%22Patch%20Available%22)%20AND%20priority%20in%20(Minor%2C%20Trivial)%20AND%20component%20%3D%20clients] for the clients space, but also check other components). Welcome to the community!", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864\nDescription: *CVE-2025-53864:*\r\n\r\nConnect2id Nimbus JOSE + JWT before 10.0.2 allows a remote attacker to cause a denial of service via a deeply nested JSON object supplied in a JWT claim set, because of uncontrolled recursion. NOTE: this is independent of the Gson 2.11.0 issue because the Connect2id product could have checked the JSON object nesting depth, regardless of what limits (if any) were imposed by Gson.\r\n\r\nSeverity: 6.9 (medium)\nQ: duplicate of HADOOP-19632 for which there are already PRs", "output": "rohit-kb commented on PR #7965:\nURL: https://github.com/apache/hadoop/pull/7965#issuecomment-3302275652\n\n   Hi @pjfanning, can we use this patch instead of the original one as I don't see any progress on that? We need this upgrade in the downstream soon. \r\n   \r\n   Also, it seems like the original patch hasn't handled the shading of com.github.stephenc.jcip:jcip-annotations in later nimbus versions. Thanks"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade to com.google.guava 32.1.3-jre to fix CVE-2023-2976 and CVE-2020-8908\nDescription: Upgrade com.google.guava library to 32.1.3-jre to fix:\r\n * [CVE-2023-2976|https://github.com/advisories/GHSA-7g45-4rm6-3mm3]\r\n * [CVE-2020-8908|https://github.com/advisories/GHSA-5mg8-w23w-74h3]\r\n\r\nLink to PR:\r\n[https://github.com/apache/hadoop/pull/7473]", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add `FAIR` schedule examples with Spark (Connect|Thrift) Servers\nDescription: \nQ: Issue resolved by pull request 393\n[https://github.com/apache/spark-kubernetes-operator/pull/393]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Kafka streams with chained emitStrategy(onWindowClose) example does not work\nDescription: Hi, I got this example by using the following prompt in Google:\r\n # kafka streams unit testing with chained \"emitStrategy\"\r\n # Provide an example of testing chained suppress with different grace periods\r\n\r\n[https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d]\r\n\r\nCompiled and ran the example using latest kafka jars only to get\r\n\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s  expected:  but was: \r\nat org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:158)\r\nat org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:139)\r\nat org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:201)\r\nat org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:168)\r\nat org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:694)\r\nat com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopologyTest.java:123)\r\n\r\nIt appears that the test is not able to drive the kafka stream to emit the 2nd event.\r\nCould be a bug in test code/test driver/kafka streams?\r\nThanks in advance\r\nGreg", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is a peer of https://issues.apache.org/jira/browse/KAFKA-19564.\r\n\r\nEssentially, it's not reliable to access the metrics from the share consumer once it has been closed, and this tool does precisely that. As a result, it prints a truncated list of metrics.", "output": "Close ShareConsumer in ShareConsumerPerformance only after metrics displayed"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: AAL - Add support for stream leak detection\nDescription: AAL currently does not support leak detection. \r\n\r\nIt may not require this, as individual streams do not hold only to any connections/resources, the factory does. We should verify if it's required, and if yes, add support.\nQ: base class ObjectInputStream now does this", "output": "...although we have stream leak detection, do you mean extra stuff for the actual factory?\r\n\r\nthe key leak problems we have hit so far are\r\n* libraries/apps forgetting to close() streams.\r\n* code which builds a large list of streams and only calls close() on them at the end of a larger piece of work. (iceberg did this at some point).\r\n\r\nthe stream factory has its lifecycle tied to that of the store, so provided the fs closes that, it will release all its resources. and as the s3 clients are released at the same time, their pools close"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support TIME in the try_make_timestamp function in Python\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Display a detailed error message when table metadata is corrupted\nDescription: Currently, no meaningful error message appears when the table metadata is corrupted due to a mismatch between the partition column names in the table schema and the declared partition columns. To facilitate debugging, a detailed error message should be shown in this situation.\nQ: Issue resolved by pull request 52731\n[https://github.com/apache/spark/pull/52731]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Simplify Jackson deps management by using BOM\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We came across a new behavior from server where ListBlob call can return empty list even after returning a next marker(continuation token) from previous list call.\r\n\r\nThis is to handle that case and do not infer listing to be incomplete.", "output": "ABFS: [FnsOverBlob] Empty Page Issue on Subsequent ListBlob call"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Throw an exception if client tries to override topic replication factor with a value that it < min.insync.replicas\nDescription: The goal of this ticket is to start a KIP with the following proposed changes:\r\n\r\nCurrently, the broker overrides settings upon a client initial request. I.e. there's some logic to consolidate/merge settings from the broker and from the client. During that step, it should be doable to throw an exception if there's an invalid configuration. This ticket is specifically for throwing an exception if the client overrides `replication.factor` with a value that is less than `min.insync.replicas` specified by the broker. This check should happen where the “merging” logic happens, on the broker side.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Set charsetEncoder in HadoopArchiveLogs\nDescription: The test for it fails in CI on Centos 7, likely because it has different default charsets than the other CI environments.\r\n{noformat}\r\n[ERROR] org.apache.hadoop.tools.TestHadoopArchiveLogs.testGenerateScript  Time elapsed: 0.036 s  <<< ERROR!\r\njava.lang.IllegalStateException: Mismatched Charset(UTF-8) and CharsetEncoder(US-ASCII)\r\n\tat org.apache.commons.io.output.FileWriterWithEncoding$Builder.get(FileWriterWithEncoding.java:113)\r\n\tat org.apache.hadoop.tools.HadoopArchiveLogs.generateScript(HadoopArchiveLogs.java:512)\r\n\tat org.apache.hadoop.tools.TestHadoopArchiveLogs._testGenerateScript(TestHadoopArchiveLogs.java:280)\r\n\tat org.apache.hadoop.tools.TestHadoopArchiveLogs.testGenerateScript(TestHadoopArchiveLogs.java:248)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.in\nQ: stoty opened a new pull request, #7520:\nURL: https://github.com/apache/hadoop/pull/7520\n\n   ### Description of PR\r\n   \r\n   Set CharsetEncoder in HadoopArchiveLogs so that it works regardless of the system charset defaults.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   On my JDK23 branch, as the failure does not happen on my system, and the CI for this PR won't run on Centos 7.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7520:\nURL: https://github.com/apache/hadoop/pull/7520#issuecomment-2731985497\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 58s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 34s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  31m 45s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 27s | [/patch-unit-hadoop-tools_hadoop-archive-logs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7520/1/artifact/out/patch-unit-hadoop-tools_hadoop-archive-logs.txt) |  hadoop-archive-logs in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 28s |  |  ASF License check generated no output?  |\r\n   |  |   | 122m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7520/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7520 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 1d5312c73ba2 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 82894b473c62ccf62e98c68411bd40c50743c9a5 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7520/1/testReport/ |\r\n   | Max. process+thread count | 699 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-archive-logs U: hadoop-tools/hadoop-archive-logs |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7520/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Close Consumer in ConsumerPerformance only after metrics displayed\nDescription: In {{ConsumerPerformance}} (used by {{kafka-consumer-perf-test.sh}}), the metrics are shown, but only after the {{Consumer}} has been closed. Because metrics are removed from the {{Metrics}} object on {{Consumer.close()}}, this means that the complete set of metrics is not displayed when the performance tool outputs the metrics.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: JDK8: RawLocalFileSystem calls ByteBuffer.flip\nDescription: running 3.4.2 rc1 on older JDK 8 runtimes can fail\r\n{code}\r\n\r\n\r\n[INFO] -------------------------------------------------------\r\n[INFO] Running org.apache.parquet.hadoop.TestParquetReader\r\nException in thread \"Thread-8\" java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;\r\n        at org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler.completed(RawLocalFileSystem.java:428)\r\n        at org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler.completed(RawLocalFileSystem.java:362)\r\n        at sun.nio.ch.Invoker.invokeUnchecked(Invoker.java:126)\r\n        at sun.nio.ch.SimpleAsynchronousFileChannelImpl$2.run(SimpleAsynchronousFileChannelImpl.java:335)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:750)\r\n{code}\r\n\r\n\r\nthis can be fixed trivially, casting to java.io.Buffer first: ((Byffer)buff).\nQ: steveloughran opened a new pull request, #7725:\nURL: https://github.com/apache/hadoop/pull/7725\n\n   \r\n   Fixes the issues in the readVectored code of raw local and s3a by casting to the superclass first.\r\n   \r\n   But it is used in many other places.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   waiting for yetus\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7725:\nURL: https://github.com/apache/hadoop/pull/7725#issuecomment-2941341314\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 39s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 56s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 17s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  9s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 46s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   5m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 41s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  3s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  4s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 39s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 38s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 39s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  4s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 256m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.50 ServerAPI=1.50 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7725/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7725 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f34bb0af7b0b 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1014e42f534268f1b45bc5a2e7963818fb154864 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7725/1/testReport/ |\r\n   | Max. process+thread count | 3134 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7725/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade commons-lang3 to 3.19.0 \nDescription: \nQ: Issue resolved by pull request 52485\n[https://github.com/apache/spark/pull/52485]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title:  Enable Filter Push-Down for Pandas UDFs with an Immutable Column Hint\nDescription: h3. Problem Description\r\n\r\nPandas UDFs ({{{}mapInPandas{}}}, {{{}applyInPandas{}}}, etc.) are powerful for custom data processing in PySpark. However, they currently act as a black box to the Catalyst Optimizer. This prevents the optimizer from pushing down filters on columns that pass through the UDF unmodified. As a result, filtering operations occur _after_ the expensive UDF execution and associated data shuffling, leading to significant performance degradation.\r\n\r\nThis is especially common in pipelines where transformations are applied to grouped data, and the grouping key itself is not modified within the UDF.\r\n\r\n*Example:*\r\n\r\nConsider the following DataFrame and Pandas UDFs:\r\n{code:java}\r\nimport pandas as pd\r\nfrom typing import Iterator\r\n\r\ndf = spark.createDataFrame(\r\n    [[\"A\", 1], [\"A\", 1], [\"B\", 2]], \r\n    schema=[\"id string\", \"value int\"]\r\n)\r\n\r\n# UDF to modify the 'value' column\r\ndef map_udf(pdfs: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\r\n    for pdf in pdfs:\r\n     \nQ: Hi [~lioritzhak] ,\r\n\r\nI would _love_ to work on this, because I am super excited about query optimization and want to get my feet wet working on Spark. Since this would be my first issue for Spark, I might have questions during the process, though.\r\nI do have another thing to work on first, AND I'll have to setup my dev eivironment for Spark first (which might take some time). All in all, I might take 2 to 3 weeks for setting this up and getting ready, I'd guess (subject to change!).\r\nIf that does not sound too off-putting, feel free to assign this to me. I'd be super excited to do this!\r\n\r\nBest,\r\nJulian", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile\nDescription: \nQ: hadoop-yetus commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223525111\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   0m 21s |  |  Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7900 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223527763\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   0m 21s |  |  Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7900 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/2/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Jira for KIP-1066: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1066%3A+Mechanism+to+cordon+brokers+and+log+directories", "output": "Mechanism to cordon brokers and log directories"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add metrics corresponding to consumer rebalance listener metrics\nDescription: The consumer provides metrics for execution of the consumer rebalance listener:\r\n * Consumer PartitionsLost Latency\r\n\r\n * Consumer PartitionsAssigned Latency\r\n\r\n * Consumer PartitionsRevoked Latency\r\n\r\nIt would be useful for the StreamsRebalanceListener to replicate semilar metrics for TasksLost, TasksAssigned and TasksRevoked.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: hadoop binary distribution to move cloud connectors to hadoop common/lib\nDescription: Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed.\r\n\r\n* filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. \r\n* Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. \r\n\r\nThis adds a lot more stuff into the distribution, so I'm doing the following design\r\n* all hadoop-* modules in common/lib\r\n* minimal dependencies for hadoop-azure and hadoop-gcs (once we get those right!)\r\n* hadoop-aws: everything except bundle.jar\r\n* other connectors: only included with explicit profiles.\r\n\r\nASF releases will support azure out the box, the others once you add the dependencies. And anyone can build their own release with everything\r\n\r\nOne concern here, we make hadoop-cloud-storage artifact incomplete at pulling in things when depended on. We may need a separate module for the distro setup.\r\n\r\nNoticed during thi\nQ: steveloughran opened a new pull request, #7980:\nURL: https://github.com/apache/hadoop/pull/7980\n\n   \r\n   \r\n   * new assembly for hadoop cloud storage\r\n   * hadoop-cloud-storage does the assembly on -Pdist\r\n   * layout stitching to move into share/hadoop/common/lib\r\n   * remove connectors from hadoop-tools-dist\r\n   * cut old jackson version from huawaei cloud dependency -even though it was being upgraded by our own artifacts, it was a complication.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Manual build, review, storediag, hadoop fs commands\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [=] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "steveloughran commented on PR #7980:\nURL: https://github.com/apache/hadoop/pull/7980#issuecomment-3303127263\n\n   \r\n   * This puts the hadoop-azure, hadoop-aws &c binaries into common/lib and so on the classpath everywhere\r\n   * some problem with gcs instantiation during enum (will file later, as while it surfaces here, I think it's unrelated)\r\n   * my local builds end up (today) with some versioned jars as well as the -SNAPSHOT. I think this is from me tainting my maven repo, would like to see what others see\r\n   \r\n   ```\r\n   total 1401704\r\n   -rw-r--r--@ 1 stevel  staff     106151 Sep 17 12:57 aliyun-java-core-0.2.11-beta.jar\r\n   -rw-r--r--@ 1 stevel  staff     194215 Sep 17 12:57 aliyun-java-sdk-core-4.5.10.jar\r\n   -rw-r--r--@ 1 stevel  staff     163698 Sep 17 12:57 aliyun-java-sdk-kms-2.11.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     220800 Sep 17 12:57 aliyun-java-sdk-ram-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     928456 Sep 17 12:57 aliyun-sdk-oss-3.18.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    2470776 Sep 17 12:57 analyticsaccelerator-s3-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      27006 Sep 17 13:11 aopalliance-repackaged-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      20891 Sep 17 13:11 audience-annotations-0.12.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     651391 Sep 17 13:11 avro-1.11.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     113966 Sep 17 12:57 azure-data-lake-store-sdk-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff      10288 Sep 17 12:57 azure-keyvault-core-1.0.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     815331 Sep 17 12:57 azure-storage-7.0.1.jar\r\n   -rw-r--r--@ 1 stevel  staff    8324412 Sep 17 13:11 bcprov-jdk18on-1.78.1.jar\r\n   -rw-r--r--@ 1 stevel  staff  641534749 Sep 17 12:57 bundle-2.29.52.jar\r\n   -rw-r--r--@ 1 stevel  staff     223979 Sep 17 13:11 checker-qual-3.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      75479 Sep 17 13:11 commons-cli-1.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     353793 Sep 17 13:11 commons-codec-1.15.jar\r\n   -rw-r--r--@ 1 stevel  staff     751914 Sep 17 13:11 commons-collections4-4.4.jar\r\n   -rw-r--r--@ 1 stevel  staff    1079377 Sep 17 13:11 commons-compress-1.26.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     657516 Sep 17 13:11 commons-configuration2-2.10.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      24239 Sep 17 13:11 commons-daemon-1.0.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     508826 Sep 17 13:11 commons-io-2.16.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     673587 Sep 17 13:11 commons-lang3-3.17.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      70816 Sep 17 13:11 commons-logging-1.3.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    2213560 Sep 17 13:11 commons-math3-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     316431 Sep 17 13:11 commons-net-3.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     238400 Sep 17 13:11 commons-text-1.10.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    8661164 Sep 17 12:57 cos_api-bundle-5.6.19.jar\r\n   -rw-r--r--@ 1 stevel  staff    2983237 Sep 17 13:11 curator-client-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     336384 Sep 17 13:11 curator-framework-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     315569 Sep 17 13:11 curator-recipes-5.2.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     583996 Sep 17 13:11 dnsjava-3.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     324655 Sep 17 12:57 dom4j-2.1.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     670059 Sep 17 12:57 esdk-obs-java-3.20.4.2.jar\r\n   -rw-r--r--@ 1 stevel  staff       4617 Sep 17 13:11 failureaccess-1.0.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     249277 Sep 17 13:11 gson-2.9.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    3037368 Sep 17 13:11 guava-32.0.1-jre.jar\r\n   -rw-r--r--@ 1 stevel  staff      94013 Sep 17 12:57 hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      14456 Sep 17 13:11 hadoop-annotations-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     114335 Sep 17 13:11 hadoop-auth-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     930516 Sep 17 12:57 hadoop-aws-3.5.0-20250916.124028-686.jar\r\n   -rw-r--r--@ 1 stevel  staff     827349 Sep 17 12:57 hadoop-azure-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      33363 Sep 17 12:57 hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n   -rw-r--r--@ 1 stevel  staff      70007 Sep 17 12:57 hadoop-cos-3.5.0-20250916.124028-683.jar\r\n   -rw-r--r--@ 1 stevel  staff     138447 Sep 17 12:57 hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff     142274 Sep 17 12:57 hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n   -rw-r--r--@ 1 stevel  staff    3519516 Sep 17 13:11 hadoop-shaded-guava-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1952967 Sep 17 13:11 hadoop-shaded-protobuf_3_25-1.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    4019589 Sep 17 12:57 hadoop-tos-3.5.0-20250916.124028-202.jar\r\n   -rw-r--r--@ 1 stevel  staff     200223 Sep 17 13:11 hk2-api-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     203358 Sep 17 13:11 hk2-locator-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     131590 Sep 17 13:11 hk2-utils-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     780321 Sep 17 13:11 httpclient-4.5.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     328593 Sep 17 13:11 httpcore-4.4.13.jar\r\n   -rw-r--r--@ 1 stevel  staff     102220 Sep 17 12:57 ini4j-0.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff      29807 Sep 17 13:11 istack-commons-runtime-3.0.12.jar\r\n   -rw-r--r--@ 1 stevel  staff       9301 Sep 17 13:11 j2objc-annotations-2.8.jar\r\n   -rw-r--r--@ 1 stevel  staff      76636 Sep 17 13:11 jackson-annotations-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     473081 Sep 17 13:11 jackson-core-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff    1617187 Sep 17 13:11 jackson-databind-2.14.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      68453 Sep 17 13:11 jakarta.activation-1.2.2.jar\r\n   -rw-r--r--@ 1 stevel  staff      44399 Sep 17 13:11 jakarta.activation-api-1.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      25058 Sep 17 13:11 jakarta.annotation-api-1.3.5.jar\r\n   -rw-r--r--@ 1 stevel  staff      18140 Sep 17 13:11 jakarta.inject-2.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      82973 Sep 17 13:11 jakarta.servlet-api-4.0.4.jar\r\n   -rw-r--r--@ 1 stevel  staff      53683 Sep 17 13:11 jakarta.servlet.jsp-api-2.3.6.jar\r\n   -rw-r--r--@ 1 stevel  staff      91930 Sep 17 13:11 jakarta.validation-api-2.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     140376 Sep 17 13:11 jakarta.ws.rs-api-2.1.6.jar\r\n   -rw-r--r--@ 1 stevel  staff     115638 Sep 17 13:11 jakarta.xml.bind-api-2.3.3.jar\r\n   -rw-r--r--@ 1 stevel  staff       7771 Sep 17 12:57 java-trace-api-0.2.11-beta.jar\r\n   -rw-r--r--@ 1 stevel  staff      18432 Sep 17 12:57 java-xmlbuilder-1.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     794714 Sep 17 13:11 javassist-3.30.2-GA.jar\r\n   -rw-r--r--@ 1 stevel  staff      95806 Sep 17 13:11 javax.servlet-api-3.1.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1019097 Sep 17 13:11 jaxb-runtime-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff       4722 Sep 17 13:11 jcip-annotations-1.0-1.jar\r\n   -rw-r--r--@ 1 stevel  staff     327806 Sep 17 12:57 jdom2-2.0.6.1.jar\r\n   -rw-r--r--@ 1 stevel  staff     311826 Sep 17 13:11 jersey-client-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff    1267957 Sep 17 13:11 jersey-common-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      32929 Sep 17 13:11 jersey-container-servlet-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      75742 Sep 17 13:11 jersey-container-servlet-core-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      80272 Sep 17 13:11 jersey-hk2-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff     964550 Sep 17 13:11 jersey-server-2.46.jar\r\n   -rw-r--r--@ 1 stevel  staff      90184 Sep 17 12:57 jettison-1.5.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     249911 Sep 17 13:11 jetty-http-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     183011 Sep 17 13:11 jetty-io-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     118496 Sep 17 13:11 jetty-security-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     739348 Sep 17 13:11 jetty-server-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     146064 Sep 17 13:11 jetty-servlet-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     588962 Sep 17 13:11 jetty-util-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff      66643 Sep 17 13:11 jetty-util-ajax-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     140308 Sep 17 13:11 jetty-webapp-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff      68894 Sep 17 13:11 jetty-xml-9.4.57.v20241219.jar\r\n   -rw-r--r--@ 1 stevel  staff     282591 Sep 17 13:11 jsch-0.1.55.jar\r\n   -rw-r--r--@ 1 stevel  staff      19936 Sep 17 13:11 jsr305-3.0.2.jar\r\n   -rw-r--r--@ 1 stevel  staff       4519 Sep 17 13:11 jul-to-slf4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff     223129 Sep 17 13:11 kerb-core-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     115065 Sep 17 13:11 kerb-crypto-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      36361 Sep 17 13:11 kerb-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     100095 Sep 17 13:11 kerby-asn1-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      30190 Sep 17 13:11 kerby-config-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     200581 Sep 17 13:11 kerby-pkix-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff      40787 Sep 17 13:11 kerby-util-2.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff       2199 Sep 17 13:11 listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar\r\n   -rw-r--r--@ 1 stevel  staff     136314 Sep 17 13:11 metrics-core-3.2.4.jar\r\n   -rw-r--r--@ 1 stevel  staff       4554 Sep 17 13:11 netty-all-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     339045 Sep 17 13:11 netty-buffer-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     355199 Sep 17 13:11 netty-codec-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      67192 Sep 17 13:11 netty-codec-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      37789 Sep 17 13:11 netty-codec-haproxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     674362 Sep 17 13:11 netty-codec-http-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     490985 Sep 17 13:11 netty-codec-http2-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      44736 Sep 17 13:11 netty-codec-memcache-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     113699 Sep 17 13:11 netty-codec-mqtt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      46015 Sep 17 13:11 netty-codec-redis-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      21344 Sep 17 13:11 netty-codec-smtp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     121032 Sep 17 13:11 netty-codec-socks-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      34636 Sep 17 13:11 netty-codec-stomp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      19823 Sep 17 13:11 netty-codec-xml-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     719225 Sep 17 13:11 netty-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     580162 Sep 17 13:11 netty-handler-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      25650 Sep 17 13:11 netty-handler-proxy-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      26833 Sep 17 13:11 netty-handler-ssl-ocsp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      37842 Sep 17 13:11 netty-resolver-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     188360 Sep 17 13:11 netty-resolver-dns-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff       9145 Sep 17 13:11 netty-resolver-dns-classes-macos-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      19825 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      19629 Sep 17 13:11 netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff     521428 Sep 17 13:11 netty-transport-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     147621 Sep 17 13:11 netty-transport-classes-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     108558 Sep 17 13:11 netty-transport-classes-kqueue-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      42321 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      36594 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar\r\n   -rw-r--r--@ 1 stevel  staff      40644 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff       6193 Sep 17 13:11 netty-transport-native-epoll-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      25741 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      25170 Sep 17 13:11 netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar\r\n   -rw-r--r--@ 1 stevel  staff      44157 Sep 17 13:11 netty-transport-native-unix-common-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      18241 Sep 17 13:11 netty-transport-rxtx-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      50814 Sep 17 13:11 netty-transport-sctp-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff      32189 Sep 17 13:11 netty-transport-udt-4.1.118.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     779369 Sep 17 13:11 nimbus-jose-jwt-9.37.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     425763 Sep 17 12:57 okhttp-3.14.2.jar\r\n   -rw-r--r--@ 1 stevel  staff      91980 Sep 17 12:57 okio-1.17.2.jar\r\n   -rw-r--r--@ 1 stevel  staff     141734 Sep 17 12:57 opentelemetry-api-1.38.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      47252 Sep 17 12:57 opentelemetry-context-1.38.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      18189 Sep 17 12:57 opentracing-api-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff      10542 Sep 17 12:57 opentracing-noop-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff       7504 Sep 17 12:57 opentracing-util-0.33.0.jar\r\n   -rw-r--r--@ 1 stevel  staff     281989 Sep 17 12:57 org.jacoco.agent-0.8.5-runtime.jar\r\n   -rw-r--r--@ 1 stevel  staff      19479 Sep 17 13:11 osgi-resource-locator-1.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     128414 Sep 17 13:11 re2j-1.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      11369 Sep 17 12:57 reactive-streams-1.0.3.jar\r\n   -rw-r--r--@ 1 stevel  staff     332398 Sep 17 13:11 reload4j-1.2.22.jar\r\n   -rw-r--r--@ 1 stevel  staff      41125 Sep 17 13:11 slf4j-api-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff       9824 Sep 17 13:11 slf4j-reload4j-1.7.36.jar\r\n   -rw-r--r--@ 1 stevel  staff    2112099 Sep 17 13:11 snappy-java-1.1.10.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     195909 Sep 17 13:11 stax2-api-4.2.1.jar\r\n   -rw-r--r--@ 1 stevel  staff      72007 Sep 17 13:11 txw2-2.3.9.jar\r\n   -rw-r--r--@ 1 stevel  staff     443788 Sep 17 12:57 wildfly-openssl-2.1.4.Final.jar\r\n   -rw-r--r--@ 1 stevel  staff     522679 Sep 17 13:11 woodstox-core-5.4.0.jar\r\n   -rw-r--r--@ 1 stevel  staff    1323991 Sep 17 13:11 zookeeper-3.8.4.jar\r\n   -rw-r--r--@ 1 stevel  staff     254932 Sep 17 13:11 zookeeper-jute-3.8.4.jar\r\n   \r\n   ```"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix hadoop-client-minicluster\nDescription: \nQ: I should close this ticket, the issue was fixed by HADOOP-19652", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Limitations of KRAFT Dual Write Mode for Production Support\nDescription: We are currently running Kafka version 3.9.0 and are in the process of migrating to KRaft. As part of the migration, we intend to operate in dual-write mode in production for an initial period to help identify and address any issues.\r\n\r\nAre there any known limitations or risks associated with running in dual-write mode? Would you recommend maintaining this mode for production stability, and are there best practices we should follow?", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implement the ST_Srid expression in SQL\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Prefer to use native Netty transports by default\nDescription: \nQ: Issue resolved by pull request 52736\n[https://github.com/apache/spark/pull/52736]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Unexpected Fatal error InvalidTxnStateException on Kafka Streams (KIP-890)\nDescription: We are seeing cases where a Kafka Streams (KS) thread stalls for ~20 seconds, resulting in a visible gap in the logs. During this stall, the broker correctly aborts the open transaction (triggered by the 10-second transaction timeout). So far, this is expected behavior. However, when the KS thread resumes, instead of receiving the expected InvalidProducerEpochException (which we already handle gracefully as part of transaction abort), the client is instead hit with an InvalidTxnStateException. KS currently treats this as a fatal error, causing the application to fail.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: volcano tos: disable shading when -DskipShade is set on a build\nDescription: hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M artifact, with unknown content inside\r\n{code}\r\n 92K    share/hadoop/common/lib/hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n912K    share/hadoop/common/lib/hadoop-aws-3.5.0-20250916.124028-686.jar\r\n808K    share/hadoop/common/lib/hadoop-azure-3.5.0-20250916.124028-685.jar\r\n 36K    share/hadoop/common/lib/hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n 72K    share/hadoop/common/lib/hadoop-cos-3.5.0-20250916.124028-683.jar\r\n136K    share/hadoop/common/lib/hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n140K    share/hadoop/common/lib/hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n3.8M    share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar\r\n{code}\r\n\r\nOne thing it includes is yet-another mozilla/public-suffix-list.txt. These are a recurrent PITA and I don't want\r\nto find them surfacing again.\r\n{code}\r\n\r\n15. Required Resources\r\n======================\r\n\r\nresource: mozilla/public-suffix-list.txt\r\n       jar:file:/Users\nQ: (note the shading is troubled anyway\r\n{code}\r\n[INFO] Dependency-reduced POM written at: /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/dependency-reduced-pom.xml\r\n[WARNING] httpclient5-5.3.jar, httpcore5-5.2.4.jar, httpcore5-h2-5.2.4.jar define 3 overlapping resources: \r\n[WARNING]   - META-INF/DEPENDENCIES\r\n[WARNING]   - META-INF/LICENSE\r\n[WARNING]   - META-INF/NOTICE\r\n[WARNING] hadoop-tos-3.5.0-SNAPSHOT.jar, httpclient5-5.3.jar, httpcore5-5.2.4.jar, httpcore5-h2-5.2.4.jar, nimbus-jose-jwt-10.4.jar, ve-tos-java-sdk-hadoop-2.8.9.jar define 1 overlapping resource: \r\n[WARNING]   - META-INF/MANIFEST.MF\r\n[WARNING] maven-shade-plugin has detected that some files are\r\n[WARNING] present in two or more JARs. When this happens, only one\r\n[WARNING] single version of the file is copied to the uber jar.\r\n[WARNING] Usually this is not harmful and you can skip these warnings,\r\n[WARNING] otherwise try to manually exclude artifacts based on\r\n[WARNING] mvn dependency:tree -Ddetail=true and the above output.\r\n[WARNING] See https://maven.apache.org/plugins/maven-shade-plugin/\r\n[INFO] Replacing original artifact with shaded artifact.\r\n[INFO] Replacing /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/target/hadoop-tos-3.5.0-SNAPSHOT.jar with /Users/stevel/Projects/hadoop-trunk/hadoop-cloud-storage-project/hadoop-tos/target/hadoop-tos-3.5.0-SNAPSHOT-shaded.jar\r\n[INFO] \r\n[INFO] --- cyclonedx:2.9.1:makeBom (default) @ hadoop-tos ---\r\n{code}", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Kafka streams with chained emitStrategy(onWindowClose) example does not work\nDescription: Hi, I got this example by using the following prompt in Google:\r\n # kafka streams unit testing with chained \"emitStrategy\"\r\n # Provide an example of testing chained suppress with different grace periods\r\n\r\n[https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d]\r\n\r\nCompiled and ran the example using latest kafka jars only to get\r\n\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s  expected:  but was: \r\nat org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:158)\r\nat org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:139)\r\nat org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:201)\r\nat org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:168)\r\nat org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:694)\r\nat com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopo\nQ: Thanks. is this a bug or a feature?\r\nI don't want several 1-minute windows for this test\r\nHow do I fix the test so the end result (finalOutputTopic) contains a single aggregated value?", "output": "It's not a bug, but behavior by design.\r\n\r\nFor this particular test, you would need to send one more even, with ts => 10:08:30, to close the [10:07; 10:08) window; when this window gets closed the result goes into the second window operator, advancing the time there accordingly emitting the result of window [10:00; 10:05)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: AAL - Update to version 1.2.1\nDescription: Version 1.2.1 brings in better memory management, readVectored and SSE-C support.\nQ: ahmarsuhail opened a new pull request, #7807:\nURL: https://github.com/apache/hadoop/pull/7807\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Updates to AAL v1.2.1, which brings in better memory management, as well as support for readVectored and SSE-C. \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   in progress", "output": "ahmarsuhail commented on PR #7807:\nURL: https://github.com/apache/hadoop/pull/7807#issuecomment-3079244390\n\n   @steveloughran @mukund-thakur small PR to update AAL version. We've made a few improvements since 1.0.0, mainly in how memory is managed. So would be great to get 3.4.2 released with 1.2.1, instead of the current 1.0.0 version. \r\n   \r\n   This also brings in support for readVectored, IoStats and SSE-C, which is required by the other PR's we have up."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: BloomMapFile: invalid io.mapfile.bloom.error.rate (≤0 or ≥1) causes NaN/zero vector size and writer construction failure\nDescription: {{BloomMapFile.Writer#initBloomFilter(Configuration)}} computes the Bloom filter vector size as:\r\n{code:java}\r\nint numKeys = conf.getInt(IO_MAPFILE_BLOOM_SIZE_KEY, IO_MAPFILE_BLOOM_SIZE_DEFAULT);\r\nfloat errorRate = conf.getFloat(IO_MAPFILE_BLOOM_ERROR_RATE_KEY, IO_MAPFILE_BLOOM_ERROR_RATE_DEFAULT);\r\nint vectorSize = (int) Math.ceil(\r\n  (double)(-HASH_COUNT * numKeys) /\r\n  Math.log(1.0 - Math.pow(errorRate, 1.0 / HASH_COUNT))\r\n); {code}\r\nWhen {{io.mapfile.bloom.error.rate}} is *≤ 0* or {*}≥ 1{*}:\r\n * {{Math.pow(errorRate, 1/k)}} produces *NaN* (negative base with non-integer exponent) or an invalid value;\r\n * {{Math.log(1 - NaN)}} becomes {*}NaN{*};\r\n * {{Math.ceil(NaN)}} cast to {{int}} yields {*}0{*}, so {{{}vectorSize == 0{}}};\r\n * constructing {{DynamicBloomFilter}} subsequently fails, and {{BloomMapFile.Writer}} construction fails (observed as assertion failure in tests).\r\n\r\nThe code misses input validation for {{io.mapfile.bloom.error.rate}} which should be strictly within {{{}(0, 1){}}}. With invalid values, the math silently degrades to NaN/0 and fails at runtime.\r\n\r\n*Reproduction*\r\n\r\nInjected values: {{io.mapfile.bloom.error.rate = 0,-1}}\r\n\r\nTest: {{org.apache.hadoop.io.TestBloomMapFile#testBloomMapFileConstructors}}\r\n{code:java}\r\n[INFO] Running org.apache.hadoop.io.TestBloomMapFile\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.358 s <<< FAILURE! - in org.apache.hadoop.io.TestBloomMapFile\r\n[ERROR] org.apache.hadoop.io.TestBloomMapFile.test", "output": "Patch Available"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support TIME in the try_make_timestamp function in Python\nDescription: \nQ: Hi [~dekrate], thank you for your help! I already have this one in progress: [https://github.com/apache/spark/pull/52666.]\r\n\r\nBut I can ping you when another task comes up - does that sound good to you?", "output": "Ahh, I did not notice that :D\r\n\r\nI will try to find another tasks, thanks :)"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "After https://issues.apache.org/jira/browse/HADOOP-19425\r\n\r\nmost of the integration tests are getting skipped. All tests need to be fixed with this PR", "output": "ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update Java 24 to 25 in docker images\nDescription: Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25 in the docker images.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Gradle build fails after Swagger patch version update\nDescription: *How to reproduce:* \r\n * checkout trunk (Swagger version: 2.2.25)\r\n * change `swaggerVersion` in `gradle.properties` to some more recent version (something in between of 2.2.27 and 2.2.39)\r\n * execute *./gradlew clean releaseTarGz*\r\n * build fails with an error _*Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String.*_\r\n * see details below\r\n * (!) note: build works for swagger version 2.2.26 (although reporter had some issues once or twice with that particular version, so please keep that on your mind)\r\n\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew clean releaseTarGz \r\n\r\n> Configure project :\r\nStarting build with version 4.2.0-SNAPSHOT (commit id 34581dff) using Gradle 9.1.0, Java 17 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\n\r\n[Incubating] Problems report is available at: file:///home/dejan/kafka/build/reports/problems/problems-report.html\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* Where:\r\nBuild file '/home/dejan/kafka/build.gradle' line: 3728\r\n\r\n* What went wrong:\r\nA problem occurred evaluating root project 'kafka'.\r\n> Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String.\r\n\r\n* Try:\r\n> Run with --stacktrace option to get the stack trace.\r\n> Run with --info or --debug", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When writing records with idempotent producer, the broker can help de-duplicate records. But when records being de-duplicated, there is no log in the broker side. This will confuse the producer because the records sent going to nowhere. Sometimes it's caused by the buggy producer that keeps using the wrong sequence number to cause the duplicated records. We should at least log the duplicated records info instead of pretending nothing happened.", "output": "Duplicated batches should be logged"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Allow to configure custom `ReplicaPlacer` implementation\nDescription: Replica assignment is a complex issue, as it depends on how a kafka cluster is run, maintained, and used. KAFKA-19507 aims to enhance the default assignment policy, and in my opinion, the best approach is to make the system flexible enough to allow users to customize the policy according to their specific needs", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Remove mockito-all 1.10.19 and powermock\nDescription: - The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11.\r\n\r\n - The powermock dependency is specified in the pom below. We should replace it with mockito since it is known to be incompatible with JDK17.\r\n -* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml\nQ: hadoop-yetus commented on PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935#issuecomment-3260267569\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 33s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  41m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  19m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  17m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 10s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   4m 40s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |   0m 15s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 15s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   5m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   4m 41s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 59s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 17s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 18s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 18s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 33s |  |  hadoop-auth in the patch passed.  |\r\n   | -1 :x: |  unit  | 118m 19s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) |  hadoop-yarn-server-resourcemanager in the patch failed.  |\r\n   | -1 :x: |  unit  | 141m 25s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 50s |  |  hadoop-yarn-server-timelineservice-documentstore in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 23s |  |  hadoop-yarn-server-globalpolicygenerator in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m  5s |  |  hadoop-yarn-applications-catalog-webapp in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 20s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt) |  hadoop-yarn-ui in the patch failed.  |\r\n   | +1 :green_heart: |  unit  |   3m 12s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 47s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 541m 38s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7935 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 21c4e19a37e1 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7dadba11fdeaf702d291642f8b2d59c197ec9ecb |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/testReport/ |\r\n   | Max. process+thread count | 1139 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935#issuecomment-3260284208\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  0s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m  4s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  46m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  20m  8s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m 16s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  9s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   0m 38s | [/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 16s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 16s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 15s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 15s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 46s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   0m 39s | [/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   4m 50s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 11s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 15s |  |  hadoop-project has no data from spotbugs  |\r\n   | -1 :x: |  spotbugs  |   0m 34s | [/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | +0 :ok: |  spotbugs  |   0m 17s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 20s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 29s |  |  hadoop-auth in the patch passed.  |\r\n   | -1 :x: |  unit  | 118m 15s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) |  hadoop-yarn-server-resourcemanager in the patch failed.  |\r\n   | -1 :x: |  unit  | 139m 46s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 51s |  |  hadoop-yarn-server-timelineservice-documentstore in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 15s |  |  hadoop-yarn-server-globalpolicygenerator in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 48s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-applications_hadoop-yarn-applications-catalog_hadoop-yarn-applications-catalog-webapp.txt) |  hadoop-yarn-applications-catalog-webapp in the patch failed.  |\r\n   | -1 :x: |  unit  |   0m 21s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt) |  hadoop-yarn-ui in the patch failed.  |\r\n   | +1 :green_heart: |  unit  |   3m  4s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 50s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 517m 52s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7935 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 1742a3955488 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1d5fb3ced9d16f8c847312dfd39d90d69a3f608e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/testReport/ |\r\n   | Max. process+thread count | 1134 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Acked record on new topic not immediately visible to consumer\nDescription: h2. Steps to reproduce\r\n * program uses a single broker (we see the issue with an in-JVM embedded kafka server)\r\n * create a new topic with 1 partition\r\n * produce a record with {{acks=all}}\r\n * await acknowledgement from the broker\r\n * start a consumer (configured to read from beginning of topic)\r\n * spuriously, _the consumer never sees the record_\r\n\r\nThe problem seems to occur more on a very busy server (e.g. a build server running many tests in parallel). We have not seen it on a modern laptop.\r\n\r\nA delay of 10ms after receiving the acknowledgement, and before starting the consumer, makes the record always visible.\r\n\r\nWe observe this problem in ~1 in 6 runs of [zio-kafka|https://github.com/zio/zio-kafka]'s test suite, when run from a GitHub action. Since we run the test-suite for 3 different JVM versions, more than half of the zio-kafka builds fails due to this issue.\r\nh2. Expected behavior\r\n\r\nA record, produced with {{{}acks=all{}}}, of which producing was acknowledged, should be i\nQ: Addition: when the consumer misses the just produced record, it will also not see any record produced thereafter either (I didn't wait until a rebalance happened).", "output": "[~erikvanoosten] Can you provide the code for the consumer application? I am pretty confident that the problem as described in the issue would have been caught by automated tests. I wonder whether the consuming application design is making this occur."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module\nDescription: Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution:\r\n{code:java}\r\njava.lang.IllegalStateException: Failed to set environment variable\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:120)\tat org.apache.hadoop.fs.tosfs.object.ObjectStorageTestBase.setUp(ObjectStorageTestBase.java:59)\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.util.Map java.util.Collections$UnmodifiableMap.m accessible: module java.base does not \"opens java.util\" to unnamed module @69eee410\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)\tat java.base/java.lang.reflect.Accessi\nQ: slfan1989 opened a new pull request, #8029:\nURL: https://github.com/apache/hadoop/pull/8029\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19726. [JDK17] Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   CI\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #8029:\nURL: https://github.com/apache/hadoop/pull/8029#issuecomment-3395542255\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 39s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 23s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  javadoc  |   0m 12s | [/patch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/patch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tos in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  shadedclient  |   1m 41s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 13s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  55m  3s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8029 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux dd7bc96b56b5 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f188198cdec1613ae955ea31795a8cb1ca496139 |\r\n   | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/testReport/ |\r\n   | Max. process+thread count | 576 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: AWS SDK 2.30+ incompatible with third party stores\nDescription: Changes in the SDK related to a new AWS SDK Feature, Default integritY Protection {{https://docs.aws.amazon.com/sdkref/latest/guide/feature-dataintegrity.html}} break all intereraction with third party S3 Stores.\r\n\r\nSee {{https://github.com/aws/aws-sdk-java-v2/issues/5801}}\r\n\r\nThere are documented mechanisms to turn this off\r\n* an environment variable\r\n* a change in ~/..aws/config\r\n* a system property\r\n* looks like a new builder option, though the docs don't cover it.\r\n\r\nThat checksum builder option looks like the only viable strategy, but it will need testing. maybe even make it a property which can be enabled/disabled, with tests and docs.\r\n\r\nWhoever wants a 2.30.x feature either gets to do the fixing and testing. For 3.4.2 it'll be an older release.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{{cacheManager.uncacheTableOrView(spark, Seq(\"testcat\", \"tbl\"), cascade = false)}} does not find the table reference because it is wrapped into a subquery alias.", "output": "Fix uncaching table by name without cascading"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Last redundant offset calculation to take deleted share partitions into account.\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: The default value of io.compression.codec.lzo.buffersize should not be in the core-site-default.xml file.\nDescription: The default value of io.compression.codec.lzo.buffersize should not be in the core-site-default.xml file.\r\n\r\nThe io.compression.codec.lzo.buffersize configuration file is the core configuration file of LZO. The default value is 245 MB. The default value should be controlled by ZLO and should not be contained in the core-site file.\r\n\r\n \r\n\r\n!image-2025-05-12-16-21-43-114.png!\nQ: fix : [https://github.com/apache/hadoop/pull/7708]", "output": "hadoop-yetus commented on PR #7708:\nURL: https://github.com/apache/hadoop/pull/7708#issuecomment-2908003220\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 23s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 53s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 104m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  6s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 57s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 12s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 55s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 43s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 24s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 208m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7708/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7708 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 58d7d0dff95a 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 450e163c5ed90e493a085e4815296f7311f7e321 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7708/1/testReport/ |\r\n   | Max. process+thread count | 3001 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7708/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implement deliveryCompleteCount in writeShareGroupStateRPC\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade SpotBugs Version to Support JDK 17 Compilation\nDescription: The current SpotBugs version used in the project is 4.2.2 (SpotBugs) and 4.2.0 (SpotBugs Maven Plugin), which is not fully compatible with JDK 17 compilation. To ensure proper functionality of the code quality check tool in a JDK 17 environment, SpotBugs needs to be upgraded to the latest version that supports JDK 17.\nQ: slfan1989 commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3397979702\n\n   @cnauroth @szetszwo After upgrading to JDK 17, I found that spotbug could not run properly because the current version does not support JDK 17. To resolve this issue, I upgraded the versions of the two related plugins. The changes have been tested locally, and the results are as expected.", "output": "hadoop-yetus commented on PR #8028:\nURL: https://github.com/apache/hadoop/pull/8028#issuecomment-3398540465\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 58s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  mvnsite  |   9m 55s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 51s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  | 124m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  28m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 17s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 43s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   7m  2s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   9m  7s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 45s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  53m 32s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 853m 28s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 44s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 1109m 31s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.hdfs.tools.TestDFSAdmin |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.server.router.webapp.TestFederationWebApp |\r\n   |   | hadoop.yarn.server.router.subcluster.fair.TestYarnFederationWithFairScheduler |\r\n   |   | hadoop.yarn.server.router.webapp.TestRouterWebServicesREST |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8028 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux da08cd8994df 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3b47bd160a2122ed84df1730e54cbfafa3ab60b8 |\r\n   | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/testReport/ |\r\n   | Max. process+thread count | 3529 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8028/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Introduce a function analogous to bitmap_or_agg, but performing a bitwise AND operation instead of OR.\r\n\r\nSpecifically, the bitmap_and_agg function should output a bitmap that represents the bitwise AND of all bitmaps in the input column. The input column must contain bitmaps generated from bitmap_construct_agg(). \r\n\r\nExample:\r\n{code:java}\r\n>>> from pyspark.sql import functions as sf\r\n>>> df = spark.createDataFrame([(\"30\",),(\"70\",),(\"F0\",)], [\"a\"])\r\n>>> df.select(sf.bitmap_and_agg(sf.to_binary(df.a, sf.lit(\"hex\")))).show()\r\n\r\n+--------------------------------+\r\n|bitmap_and_agg(to_binary(a, hex))|\r\n+--------------------------------+\r\n|            [30 00 00 00 00 0...|\r\n+--------------------------------+{code}", "output": "Add bitmap_and_agg aggregation function"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip tests that require JavaScript engine when it's not available\nDescription: Some features require a JavaScript engine, which has been removed from JDK.\r\n\r\nSkip those tests if JavaScript is not available.\nQ: hadoop-yetus commented on PR #7503:\nURL: https://github.com/apache/hadoop/pull/7503#issuecomment-2722816991\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   7m 38s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  7s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  21m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   9m 25s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 57s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 14s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m  7s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 55s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 21s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m  2s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   8m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  8s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  | 108m  0s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  11m 57s |  |  hadoop-sls in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 255m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7503/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7503 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 47479813e725 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 726153b49ba3288e782af90a5c6bc51ebaea6126 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7503/1/testReport/ |\r\n   | Max. process+thread count | 963 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-tools/hadoop-sls U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7503/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on code in PR #7503:\nURL: https://github.com/apache/hadoop/pull/7503#discussion_r1994515014\n\n\n##########\nhadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/invariants/TestMetricsInvariantChecker.java:\n##########\n@@ -30,8 +30,11 @@\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.Timeout;\n \n+import static org.junit.Assume.assumeNotNull;\n\nReview Comment:\n   If we find that the relevant classes already use org.junit.jupiter, we can use the JUnit 5 syntax."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add message key to org.apache.kafka.clients.producer.RecordMetadata\nDescription: Although the message key is not really {_}metadata{_}, it may be useful to include it in the _org.apache.kafka.clients.producer.RecordMetadata_ class so that metdata can be tied back to a specific message.\r\nWhen using a standard Kafka producer it is easy to tie back the metadata to a specific message by using the callback mechanism.\r\nHowever, when using Kafka streams, the only way to access the metadata and log the details is to register a stream-level _org.apache.kafka.clients.producer.ProducerInterceptor_ instance.\r\nThis mechanism has a drawback in that it is impossible to tie the RecordMetadata instance back to a particular message. \r\nIncluding the message key in the metadata would solve this problem.\nQ: Not sure if adding the key to RecordMetadata would be the right thing to do. As you already pointed out, is not really metadata.\r\n\r\nCan you elaborate on your use case, and what you try to accomplish in more details? It might be better to solve this issue at the KS layer?", "output": "I have a Kafka Streams application that is part of a pipeline of applications which process messages off a series of Kafka topics.\r\nOne input message results in related messages on the different topics. The related messages on the downstream topics use the same message key as the input message.\r\nAs each application in the pipeline processes messages, the logging uses the Kafka message key on the logging context  The message key can thus be used to trace processing across the entire application pipeline via the application logging.\r\n\r\nThe Kafka Streams application has a registered _org.apache.kafka.clients.producer.ProducerInterceptor_ instance which is used to log any exceptions that occur during message publishing, or, on success, to log the partition and offset of the output message{_}.{_}\r\nThe problem is that _org.apache.kafka.clients.producer.ProducerInterceptor.onAcknowledgement(RecordMetadata, Exception)_ is called from the producer thread created by the Kafka clients library and it doesn't have the details of the producer record, so there is no way for my application to put the message key into the logging context for any logging to pick up.\r\nIn hindsight, suggesting adding the key to the metadata was a bad idea.\r\nA better solution would be to add an overloaded variant of the _onAcknowledgement_ method to _org.apache.kafka.clients.producer.ProducerInterceptor_ which takes the _org.apache.kafka.clients.producer.ProducerRecord_ instance as an additional parameter.\r\nFrom browsing through the Kafka clients code, this looks doable, but I'll leave that decision up to you guys.\r\nThanks for your time."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We should store the last used assignment configuration in the group metadata and bump the group epoch if the assignment configuration is changed.", "output": "Store the last used assignment configuration in the group metadata"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add spark.logConf configuration to operator docs\nDescription: \nQ: Issue resolved by pull request 406\n[https://github.com/apache/spark-kubernetes-operator/pull/406]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: UserGroupInformation.java is using a non-support operation in JDK25\nDescription: Hello,\r\n\r\nI'm trying to upgrade my version of ParquetJava and I'm seeing the following error locally\r\n{code:java}\r\n    java.lang.UnsupportedOperationException: getSubject is not supported\r\n\r\n\r\n        at java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\r\n\r\n        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.(FileSystem.java:3852)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.(FileSystem.java:3842)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3630)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541)\r\n\r\n\r\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\r\n\r\n        at org.apache.parquet.hadoop.util.HadoopOutputFile.fromPath(HadoopOutputFi\nQ: Hadoop only supports Java 11.\r\nhttps://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\r\n\r\nThere is work in progress to support Java 17. Java 21 and 25 will be worked on later.\r\nHADOOP-19486", "output": "Appreciate the link, thanks PJ, will keep a watch on that :)"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Migrate ViewFileSystemBaseTest to Junit 5\nDescription: Its children have been converted to Junit 5, but the parent has not.\r\nThis breaks most of the child test classes (at least with the latest JUnit5+Surefire).\r\nThe breakage may not happen with the current old Surefire, but it is more likely that it is just silently ignored.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: InferFiltersFromConstraints rule does not infer filter conditions for complex join expressions\nDescription: The Spark optimizer's {{InferFiltersFromConstraints}} rule is currently unable to infer filter conditions when join constraints involve complex expressions. While it works for simple attribute equalities ({{{}a = b{}}}), it can't infer constraint from anything more complex than that.\r\n\r\n*Example (works as expected):*\r\n{code:sql}\r\nSELECT *\r\nFROM t1\r\nJOIN t2 ON t1.a = t2.b\r\nWHERE t2.b = 1\r\n{code}\r\nIn this case, the optimizer correctly infers the additional constraint {{{}t1.a = 1{}}}.\r\n\r\n*Example (does not work):*\r\n{code:sql}\r\nSELECT *\r\nFROM t1\r\nJOIN right ON t1.a = t2.b + 2\r\nWHERE t2.b = 1\r\n{code}\r\nIn this case, it is clear that {{t1.a = 3}} (since {{b = 1}} and {{{}a = b + 2{}}}), but the optimizer does not infer this constraint.\r\n\r\n*How to Reproduce:*\r\n{code:scala}\r\nspark.sql(\"CREATE TABLE t1(a INT)\")\r\nspark.sql(\"CREATE TABLE t2(b INT)\")\r\n\r\nspark.sql(\"\"\"\r\nSELECT * \r\nFROM t1 \r\nINNER JOIN t2 ON t2.b = t1.a + 2 \r\nWHERE t1.a = 1\r\n\"\"\").explain\r\n{code}\r\n{code:java}\r\n== Physical Plan ==\r\nAdaptiveSparkPlan\r\n+- BroadcastHashJoin [(a#2 + 2)], [b#3], Inner, BuildRight, false\r\n   :- Filter (isnotnull(a#2) AND (a#2 = 1))\r\n   :  +- FileScan spark_catalog.default.t1[a#2]\r\n      +- Filter isnotnull(b#3)\r\n         +- FileScan spark_catalog.default.t2[b#3]\r\n{code}\r\n*Expected Behavior:*\r\nThe optimizer should be able to statically evaluate and infer that {{t2.b = 3}} given the join condition and the filter on {{{}t1.a{}}}.\r\n\r\n*Impact:*\r\nThis limits the optimizer's ability to push down filters a", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consistency of command-line arguments for verifiable producer/consumer\nDescription: This implements KIP-1147 for kafka-verifiable-producer.sh and kafka-verifiable-consumer.sh.\nQ: Go for it.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update non-thirdparty Guava version to  33.4.8-jre\nDescription: Keep in sync with recently upgraded thirdparty Guava\nQ: hadoop-yetus commented on PR #7994:\nURL: https://github.com/apache/hadoop/pull/7994#issuecomment-3326537486\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  31m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |  33m 33s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m  9s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m  9s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m  8s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 10s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  shadedclient  |   1m 22s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 10s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  45m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7994 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 9c1032fe6d33 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bd6bdde979214be1291cda6a340e8e629a848d7a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/testReport/ |\r\n   | Max. process+thread count | 98 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7994/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on PR #7994:\nURL: https://github.com/apache/hadoop/pull/7994#issuecomment-3332389576\n\n   @stoty In my opinion, there is no issue with this PR. I have one question, why not upgrade to 33.5.0-jre?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade jsonschema2pojo due to CVE-2025-3588\nDescription: https://github.com/advisories/GHSA-66rc-vg9f-48m7\r\n\r\nNo fix release is available yet.\r\n\r\nIt looks like we would need a fix release for the older branch (1.0). jsonschema2pojo 1.1 and above needs a version of javax.validation that is incompatible with the version of javax.validation needed by Jersey 2.\nQ: hadoop-yetus commented on PR #7645:\nURL: https://github.com/apache/hadoop/pull/7645#issuecomment-2826220805\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 35s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 20s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  16m  5s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m 21s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 47s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  | 148m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 49s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   5m  7s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7645/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |  15m 39s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  0s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  15m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  20m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |  10m  1s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m  5s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |  50m  0s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 599m 42s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7645/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 839m 22s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndWeight |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModePercentageAndWeight |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsCustomResourceTypes |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.fairscheduler.TestRMWebServicesFairScheduler |\r\n   |   | hadoop.yarn.server.resourcemanager.nodelabels.TestNodeLabelFileReplication |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndWeightVector |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDefaultLabel |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentageAndWeightVector |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps |\r\n   |   | hadoop.yarn.server.resourcemanager.TestClientRMService |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesReservation |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppAttempts |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesSchedulerActivities |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedLegacyQueueCreation |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.fairscheduler.TestRMWebServicesFairSchedulerCustomResourceTypes |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModePercentageAndWeightVector |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentageVector |\r\n   |   | hadoop.yarn.webapp.TestRMWithXFSFilter |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfigWeightModeDQC |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfig |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentage |\r\n   |   | hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.TestFSSchedulerConfigurationStore |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedMode |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedLegacyQueueCreationAbsoluteMode |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppCustomResourceTypes |\r\n   |   | hadoop.yarn.webapp.TestRMWithCSRFFilter |\r\n   |   | hadoop.yarn.server.resourcemanager.metrics.TestCombinedSystemMetricsPublisher |\r\n   |   | hadoop.yarn.server.resourcemanager.TestRMAdminService |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesConfigurationMutation |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerConfigMutation |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesContainers |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesForCSWithPartitions |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedulerMixedModeAbsoluteAndPercentageAndWeight |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServiceAppsNodelabel |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodeLabels |\r\n   |   | hadoop.yarn.server.resourcemanager.metrics.TestSystemMetricsPublisher |\r\n   |   | hadoop.yarn.server.resourcemanager.TestRMHA |\r\n   |   | hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesSchedulerActivitiesWithMultiNodesEnabled |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfigAbsoluteMode |\r\n   |   | hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService |\r\n   |   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySchedDynamicConfigWeightMode |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7645/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7645 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 38da54e35de4 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 63799fc7f155daaedbc47f8baf19bf1717b3e2a7 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7645/1/testReport/ |\r\n   | Max. process+thread count | 4010 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7645/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "pjfanning commented on PR #7645:\nURL: https://github.com/apache/hadoop/pull/7645#issuecomment-2826954897\n\n   ```\r\n    Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.5.0:enforce (depcheck) on project hadoop-yarn-server-resourcemanager: \r\n   [ERROR] Rule 0: org.apache.maven.enforcer.rules.dependency.DependencyConvergence failed with message:\r\n   [ERROR] Failed while enforcing releasability.\r\n   [ERROR] \r\n   [ERROR] Dependency convergence error for jakarta.validation:jakarta.validation-api:jar:2.0.2 paths to dependency are:\r\n   [ERROR] +-org.apache.hadoop:hadoop-yarn-server-resourcemanager:jar:3.5.0-SNAPSHOT\r\n   [ERROR]   +-org.apache.hadoop:hadoop-yarn-common:jar:3.5.0-SNAPSHOT:compile\r\n   [ERROR]     +-org.glassfish.jersey.core:jersey-server:jar:2.46:compile\r\n   [ERROR]       +-jakarta.validation:jakarta.validation-api:jar:2.0.2:compile\r\n   [ERROR] and\r\n   [ERROR] +-org.apache.hadoop:hadoop-yarn-server-resourcemanager:jar:3.5.0-SNAPSHOT\r\n   [ERROR]   +-org.jsonschema2pojo:jsonschema2pojo-core:jar:1.2.2:compile\r\n   [ERROR]     +-jakarta.validation:jakarta.validation-api:jar:3.0.2:compile\r\n   [ERROR] -> [Help 1]\r\n   ```"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FNSOverBlob] Add Distinct String In User Agent to Get Telemetry for FNS-Blob\nDescription: Add a unique identifier in FNS-Blob user agent to get their usage through telemetry", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: S3ABlockOutputStream to never log/reject hflush(): calls\nDescription: Parquet's GH-3204 patch uses hflush() just before close()\r\n\r\nthis is needless and hurts write performance on hdfs.\r\nFor s3A it will trigger a warning long (Syncable is not supported) or an actual failure if\r\nfs.s3a.downgrade.syncable.exceptions is false\r\n\r\nproposed: hflush to log at debug -only log/reject on hsync, which is the real place where semantics cannot be met\nQ: steveloughran opened a new pull request, #7662:\nURL: https://github.com/apache/hadoop/pull/7662\n\n   \r\n   Calls to Syncable.hflush() on S3ABlockOutputStream instances are logged at debug and the statistics counter upgraded\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   * modify existing tests for new behaviour\r\n   * s3a tests ` -Dparallel-tests -DtestsThreadCount=9`\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7662:\nURL: https://github.com/apache/hadoop/pull/7662#issuecomment-2841687000\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 53s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 15s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 33s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 137m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7662/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7662 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f3d94f735602 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 429bbdd723b4106126930d93a2b5f20ce03ccfd6 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7662/1/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7662/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Restrict records acquisition post max inflight records\nDescription: Currently, the check for max inflight records happen prior fetching data for some share partition but the fetched data can be acquired and have records which are past max inflight records. This is evident when some records are released from the inflight records and the next fetch results in data past max inflight records.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: NoSuchElementException (again) in Kafka Streams iterator metrics\nDescription: Since upgrading to Kafka 4.1.0, we again see in our metrics collection:\r\n{code:java}\r\nException thrown from GraphiteReporter#report. Exception was suppressed.\r\njava.util.NoSuchElementException: null\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:1863)\r\n\tat java.base/java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:393)\r\n\tat org.apache.kafka.streams.internals.metrics.OpenIterators.lambda$add$0(OpenIterators.java:57)\r\n\tat org.apache.kafka.common.metrics.KafkaMetric.metricValue(KafkaMetric.java:81) {code}\r\nI think unfortunately the dynamic registration introduced in KAFKA-19398 ([https://github.com/apache/kafka/pull/20022]) might have reverted the fix from [https://github.com/apache/kafka/pull/18771]\r\n\r\nThere is still a race between removing the metric gauge, and the last iterator being removed from the tracking set.\nQ: [~stevenschlansker] – Can you test the fix and verify if it does work as expected, and let us know?", "output": "Thank you! Will do. I left a comment on the PR about another possible race condition."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add -o option in hdfs \"count\" command to show the owner's summarization.\nDescription: Add -o option in hdfs \"count\" command to show the owner's summarization.\nQ: hadoop-yetus commented on PR #7641:\nURL: https://github.com/apache/hadoop/pull/7641#issuecomment-2820056653\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 48s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 57s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   7m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m  8s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  13m 12s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 121m 25s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.cli.TestCLI |\r\n   |   | hadoop.fs.TestFilterFileSystem |\r\n   |   | hadoop.fs.TestHarFileSystem |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7641 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint xmllint |\r\n   | uname | Linux 23b158b27a15 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8c681bba96b970fcea4dbb306517e26bf39d0c39 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/1/testReport/ |\r\n   | Max. process+thread count | 3151 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7641:\nURL: https://github.com/apache/hadoop/pull/7641#issuecomment-2820594969\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 22s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 48s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  2s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 37s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/2/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 2 new + 260 unchanged - 0 fixed = 262 total (was 260)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  13m 16s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 120m 39s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7641 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint xmllint |\r\n   | uname | Linux fc424c8b9059 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d7bab15a4c125653b0296e6d6959b88a7aa90786 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/2/testReport/ |\r\n   | Max. process+thread count | 1262 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Python streaming transform_with_state StateServer does not fully read large state values\nDescription: The TransformWithState StateServer's {{parseProtoMessage}} method uses {{read}} (InputStream/FilterInputStream) which only reads all available data and may not return the full message. We should be using the [readFully DataInputStream API|https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/DataInput.html#readFully(byte%5B%5D)] instead, which will continue fetching until it fills up the provided buffer.\r\n\r\nIn addition to the linked API above, this StackOverflow post also illustrates the difference between the two APIs: [https://stackoverflow.com/a/25900095]\r\n\r\nWithout this change, it is possible for the state server to fail to fully read large proto messages (e.g., those containing a large state value update) and run into a parsing error.\r\n\r\n \r\n\r\nAffected versions identified by the tags on the original PR, it seems to have been present since the state server was introduced: [https://github.com/apache/spark/commit/def42d44405af5df78c3039ac5ad0f8a0469efaa]\r\n\r\n \r\n\r\nIn practice this seems like an uncommon scenario (bug was identified/confirmed with a 512KB string state value update which likely produces a proto message much larger than typical use cases)", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade libopenssl to 3.1.2 on Windows\nDescription: The current libopenssl 3.1.1 isn't available for download from the repo.msys2.org website. Hence, we're upgrading to 3.1.2.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update non-thirdparty Guava version to  33.4.8-jre\nDescription: Keep in sync with recently upgraded thirdparty Guava", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump netty to 4.1.127 due to CVE-2025-58057\nDescription: https://www.cve.org/CVERecord?id=CVE-2025-58057\r\n\r\nfixed in 4.1.125 but no harm upgrading to latest\nQ: pjfanning opened a new pull request, #7969:\nURL: https://github.com/apache/hadoop/pull/7969\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Upgrade netty due to CVE-2025-58057\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "slfan1989 merged PR #7969:\nURL: https://github.com/apache/hadoop/pull/7969"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Migrate FileContextPermissionBase to Junit 5\nDescription: TestFcLocalFsPermission has been recently migrated to Junit 5. However, its superclass FileContextPermissionBase uses Junit 4, which causes the class initializerers (BeforeAll/BeforeClass) not to be called in certain circumstances.\r\n\r\nMigrate FileContextPermissionBase and its children fully to Junit 5.\nQ: [~stoty]  I am responsible for handling the upgrade from Junit4 to Junit5. Currently, some parts of Hadoop-common have not been fully upgraded due to its complex dependencies.", "output": "slfan1989 commented on PR #7630:\nURL: https://github.com/apache/hadoop/pull/7630#issuecomment-2813196807\n\n   @stoty Thank you for your contribution! I will continue to handle the upgrade from JUnit4 to JUnit5. Currently, some classes in the hadoop-common module serve as parent classes for other modules, so the upgrade for this part has not yet started, but I will push forward with it as soon as possible."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Hi,\r\n\r\nI've discovered a bug when accessing COSN using temporary credentials (access key, secret key, and session token).\r\n\r\nIn the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n\r\nFurthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n!image-2025-08-11-10-37-12-451.png|width=1048,height=540! \r\n!image-2025-08-11-10-42-36-375.png!", "output": "cos use token credential will lost token field"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "related to https://issues.apache.org/jira/browse/SPARK-53779", "output": "Implement transform in column API in PySpark"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support logging in driver-side workers\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix `test_in_memory_data_source` in Python 3.14\nDescription: {code}\r\n======================================================================\r\nERROR [0.007s]: test_in_memory_data_source (pyspark.sql.tests.test_python_datasource.PythonDataSourceTests.test_in_memory_data_source)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps\r\n    return cloudpickle.dumps(obj, pickle_protocol)\r\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1537, in dumps\r\n    cp.dump(obj)\r\n    ~~~~~~~^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1303, in dump\r\n    return super().dump(obj)\r\n           ~~~~~~~~~~~~^^^^^\r\nTypeError: cannot pickle '_abc._abc_data' object\r\nwhen serializing dict item '_abc_impl'\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item '__annotate_func__'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing t", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK17] Attempted to build on CentOS Stream 9.\nDescription: We aim to successfully compile Apache Hadoop on CentOS Stream 9 to verify the feasibility of using this platform in future development and deployment environments.\r\n\r\nThis task may involve addressing potential compatibility issues related to GCC and libc versions. Initial attempts will focus on resolving compilation challenges and ensuring the platform meets the necessary requirements for Hadoop builds.", "output": "In Progress"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Enhance error reporting for corrupted view metadata by adding a detailed, user-friendly assertion message when the number of view query column names and the number of columns in the view schema mismatch.", "output": "Add detailed error message for corrupted view metadata with mismatched column counts"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We can't run the e2e tests through ducker-ak on the same machine for now. That is a bit troublesome when I want to run e2e for different PRs. Perhaps we could introduce a \"prefix\" to ducker to isolate the containers.", "output": "Enable to ducker-ak to \"isolate\" the ducker containers"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Based on JIRA *SPARK-49872* and the implementation in [{{JsonProtocol.scala}}|https://github.com/apache/spark/blob/29434ea766b0fc3c3bf6eaadb43a8f931133649e/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L71], the relevant limit was removed in {*}4.0.1{*}. However, in our environment we can still reliably reproduce:\r\n # When generating/processing a very large string:\r\n\r\n{code:java}\r\nCaused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String value length (20040525) exceeds the maximum allowed (20000000, from `StreamReadConstraints.getMaxStringLength()`){code}\r\n # When using {{from_json}} on a _valid_ very large single-line JSON (no missing comma), Jackson throws at around column {*}20,271,838{*}:\r\n\r\n{code:java}\r\nCaused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('1' (code 49)): was expecting comma to separate Object entries\r\n at [Source: UNKNOWN; line: 1, column: 20271838]{code}\r\nI'm sure this is not a formatting issue. If I truncate the JSON to below column {*}20,271,838{*}, it parses successfully.\r\nHere is my parsing code:\r\n\r\n{code:java}\r\nraw_df.withColumn(\"parsed_item\", f.from_json(f.col(\"item\"), my_schema){code}", "output": "Spark 4.0.1 still throws \"String length (20054016) exceeds the maximum length (20000000)\" and \"from_json\" fails on a very large JSON with a jackson_core parse error"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add tree node pattern bits for supported expressions in ParameterizedQuery argument list\nDescription: In this PR I propose that we add tree node pattern bits for supported expressions in ParameterizedQuery argument list to prepare implementation of parameters in single-pass framework.\nQ: Issue resolved by pull request 52611\n[https://github.com/apache/spark/pull/52611]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix SpotBugs warnings introduced after SpotBugs version upgrade.\nDescription: Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged.\r\nWe plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules.", "output": "In Progress"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In case of TEZ *DAGAppMaster* the Hadoop *SecretManager* code can not read yarn config xml file, therefore the SELECTED_ALGORITHM and SELECTED_LENGTH variables in SecretManager can not be set at runtime.\r\nThis can results with the following exception in FIPS environment:\r\n\r\n{code:java}\r\njava.security.InvalidParameterException: Key size for HMAC must be at least 112 bits in approved mode: SHA-1/HMAC\r\n\tat com.safelogic.cryptocomply.fips.core/com.safelogic.cryptocomply.jcajce.provider.BaseKeyGenerator.engineInit(Unknown Source)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:540)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:517)\r\n\tat org.apache.hadoop.security.token.SecretManager.(SecretManager.java:157)\r\n\tat org.apache.hadoop.yarn.security.client.BaseClientToAMTokenSecretManager.(BaseClientToAMTokenSecretManager.java:38)\r\n\tat org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager.(ClientToAMTokenSecretManager.java:46)\r\n\tat org.apache.tez.common.security.TezClientToAMTokenSecretManager.(TezClientToAMTokenSecretManager.java:33)\r\n\tat org.apache.tez.dag.app.DAGAppMaster.serviceInit(DAGAppMaster.java:493)\r\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\r\n\tat org.apache.tez.dag.app.DAGAppMaster$9.run(DAGAppMaster.java:2649)\r\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1910)\r\n\tat org.apache.tez.dag.app.DAGAppMaster.initAndStartAppMaster(DAGAppMaster.java:2646)\r\n\tat org.apache.tez.dag.app.DAGAppMaster.main(DAGAppMaster.java:2440)\r\n{code}\r\n\r\nTo mitigate the problem we should provide some ability for the component to be able to modify the configuration without corresponding config files on class path.", "output": "SecretManager configuration at runtime"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: ITests to run under JUnit5\nDescription: hadoop-aws tests which need to be parameterized on a class level\r\nare configured to do so through the @ParameterizedClass tag.\r\nFilesystem contract test suites in hadoop-common have\r\nalso been parameterized as appropriate.\r\n\r\nThere are custom JUnit tags declared in org.apache.hadoop.test.tags,\r\nwhich add tag strings to test suites/cases declaring them.\r\nThey can be used on the command line and in IDEs to control\r\nwhich tests are/are not executed.\r\n\r\n@FlakyTest \"flaky\"\r\n@LoadTest \"load\"\r\n@RootFilesystemTest \"rootfilesystem\"\r\n@ScaleTest \"scale\"\r\n\r\nFor anyone migrating tests to JUnit 5\r\n* Methods which subclass an existing test case MUST declare the @Test\r\n  tag again -it is no longer inherited.\r\n* All overridden setup/teardown methods MUST be located and\r\n  @BeforeEach/@AfterEach attribute added respectively\r\n* Subclasses of a parameterized test suite MUST redeclare themselves\r\n  as a @ParameterizedClass, and the binding mechanism again.\r\n* Parameterized test suites SHOULD declare a pattern to generate an\r\n  informative parameter value string for logs, IDEs and stack traces, e.g.\r\n  @ParameterizedClass(name=\"performance-{0}\")\r\n* Test suites SHOULD add a org.apache.hadoop.test.tags tag to\r\n  declare what kind of test it is. These tags are inherited, so it\r\n  may be that only shared superclasses of test suites need to be tagged.\r\n  The abstract filesystem contract tests are NOT declared as integration\r\n  tests -implementations MUST do so if they are integration tests.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump netty to 4.1.127 due to CVE-2025-58057\nDescription: https://www.cve.org/CVERecord?id=CVE-2025-58057\r\n\r\nfixed in 4.1.125 but no harm upgrading to latest\nQ: slfan1989 merged PR #7969:\nURL: https://github.com/apache/hadoop/pull/7969", "output": "slfan1989 commented on PR #7969:\nURL: https://github.com/apache/hadoop/pull/7969#issuecomment-3310122835\n\n   @pjfanning Thanks for the contribution! Merged into trunk. The branch-3.4 should also be taken into consideration."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In https://issues.apache.org/jira/browse/KAFKA-19570 we implemented offline migration between groups, that is, the following integration test or system test should be possible:\r\n\r\nTest A:\r\n * Start a streams application with classic protocol, process up to a certain offset and commit the offset and shut down.\r\n\r\n * Start the same streams application with streams protocol (same app ID!).\r\n\r\n * Make sure that the offsets before the one committed in the first run are not reprocessed in the second run.\r\n\r\n \r\n\r\nTest B:\r\n * Start a streams application with streams protocol, process up to a certain offset and commit the offset and shut down.\r\n\r\n * Start the same streams application with classic protocol (same app ID!).\r\n\r\n * Make sure that the offsets before the one committed in the first run are not reprocessed in the second run.\r\n\r\n \r\n\r\nWe have unit tests that make sure that non-empty groups will not be converted. This should be enough.", "output": "Implement integration test for offline migration"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `Selenium` to 4.32.0\nDescription: \nQ: Issue resolved by pull request 52579\n[https://github.com/apache/spark/pull/52579]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Log auto topic creation failures more visibly\nDescription: Hi, I was playing with Share groups on the 4.1.0-rc2 and found it a little opaque to detect a failure to auto create the `__share_group_state` topic due to the replication factor not being satisfiable.\r\n\r\nI start up a cluster like this (taken from the dockerhub instructions):\r\n{code:java}\r\npodman run --rm \\\r\n  -p 9092:9092 \\\r\n  -e KAFKA_NODE_ID=1 \\\r\n  -e KAFKA_PROCESS_ROLES=broker,controller \\\r\n  -e KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \\\r\n  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\r\n  -e KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER \\\r\n  -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \\\r\n  -e KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9093 \\\r\n  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\r\n  -e KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1 \\\r\n  -e KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1 \\\r\n  -e KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0 \\\r\n  -e KAFKA_NUM_PARTITIONS=3 \\\r\n  apache/kafka:4.1.0-rc2{code}\r\nand then run the comma\nQ: [~robyoung] Thanks for this issue. It would be good to improve this area. I think we should also improve the instructions you followed, probably when share groups are enabled by default (AK 4.2, I hope). After all, we have specific configs in the config/server.properties that ship with AK to reflect the single-broker config requirements, and that's appropriate for what you did with Docker too I think.", "output": "Hi [~schofielaj]. Yes the docker images work smoothly when you don't set any of the configuration environment variables, as the default configuration file tunes the replication factors/isrs down to 1. The docs make it pretty clear that if you are using environment variable configuration that you are responsible for configuring {_}everything{_}.\r\n\r\nI guess the examples of environment variable configuration [https://hub.docker.com/r/apache/kafka#overriding-the-default-broker-configuration] could be updated since they already contain the equivalent settings for the transaction state topic. I imagine these examples should represent a working single-node configuration that users can then modify."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Spark Connect Python client does not throw a proper error when creating a dataframe from an empty pandas dataframe\nDescription: Spark Connect Python client does not throw a proper error when creating a dataframe from a pandas dataframe with a index and empty data.\r\n\r\nGenerally, spark connect client throws a client-side error `[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from an empty dataset`. when creating a dataframe without data, for example via\r\n{quote}spark.createDataFrame([]).show()\r\n{quote}\r\nor\r\n{quote}df = pd.DataFrame()\r\nspark.createDataFrame(df).show(){quote}\r\nor\r\n{quote}df = pd.DataFrame(\\{\"a\": []})\r\nspark.createDataFrame(df).show(){quote}\r\nThis does not happen when pandas dataframe has an index but no data, e.g.\r\n{quote}df = pd.DataFrame(index=range(5))\r\nspark.createDataFrame(df).show(){quote}\r\nWhat happens instead is that the dataframe is successfully converted to a LocalRelation on the client, is sent to the server, but the server then throws the following exception: `INTERNAL_ERROR: Input data for LocalRelation does not produce a schema. SQLSTATE: XX000`. XX000 is an internal error sql state \nQ: [~dongjoon] my bad, I'll leave them empty next time, thanks!", "output": "Issue resolved by pull request 52670\n[https://github.com/apache/spark/pull/52670]"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, both {{AbfsClient}} and {{AzureBlobFileSystemStore}} store information about whether the account is FNS or HNS (i.e., whether namespace is enabled). This information should instead be stored at the {{AbfsConfiguration}} level, allowing both the client and the store to retrieve it from there when needed.", "output": "ABFS: AbfsConfiguration should store account type information (HNS or FNS)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip tests that depend on SecurityManager if the JVM does not support it\nDescription: Due to JEP411, depending on the Java version, SecurityManager either has to be explicitly enabled, or is completely disabled.\nQ: hadoop-yetus commented on PR #7507:\nURL: https://github.com/apache/hadoop/pull/7507#issuecomment-2726031007\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 30s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  7s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  31m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 38s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 41s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   6m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 36s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 57s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   9m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   3m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 52s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 52s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 43s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m  8s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/2/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 2 new + 160 unchanged - 2 fixed = 162 total (was 162)  |\r\n   | +1 :green_heart: |  mvnsite  |   6m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   6m  4s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |  11m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 31s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   1m 53s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 111m 34s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n   | -1 :x: |  unit  |  27m 35s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/2/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt) |  hadoop-yarn-server-nodemanager in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 129m 22s |  |  hadoop-mapreduce-client-jobclient in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  26m  3s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  17m  3s |  |  hadoop-gridmix in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 14s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 555m 43s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7507 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a16c748b3d8d 5.15.0-134-generic #145-Ubuntu SMP Wed Feb 12 20:08:39 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ef27d7d0ffcc1186408ffd5ecb652f330022dea2 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/2/testReport/ |\r\n   | Max. process+thread count | 1248 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-gridmix U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7507:\nURL: https://github.com/apache/hadoop/pull/7507#issuecomment-2726033599\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 26s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  32m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 35s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 42s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   6m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 35s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 59s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |  10m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   3m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  4s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  4s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  13m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m  6s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/1/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 2 new + 160 unchanged - 2 fixed = 162 total (was 162)  |\r\n   | +1 :green_heart: |  mvnsite  |   6m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 37s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 57s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |  11m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 20s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   1m 54s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 112m 13s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n   | -1 :x: |  unit  |  27m 43s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt) |  hadoop-yarn-server-nodemanager in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 129m 40s |  |  hadoop-mapreduce-client-jobclient in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  26m 41s |  |  hadoop-distcp in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  16m 57s |  |  hadoop-gridmix in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 15s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 558m  0s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7507 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3ee3531b6efd 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ef27d7d0ffcc1186408ffd5ecb652f330022dea2 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/1/testReport/ |\r\n   | Max. process+thread count | 1245 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-distcp hadoop-tools/hadoop-gridmix U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7507/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob][Tests] Update Test Scripts to Run Tests with Blob Endpoint\nDescription: Following Cobination of test Suites will run as a part of CI after blob endpoint support has been added.\r\n\r\nHNS-OAuth-DFS\r\nHNS-SharedKey-DFS\r\nNonHNS-SharedKey-DFS\r\nAppendBlob-HNS-OAuth-DFS\r\nNonHNS-SharedKey-Blob\r\nNonHNS-OAuth-DFS\r\nNonHNS-OAuth-Blob\r\nAppendBlob-NonHNS-OAuth-Blob\r\nHNS-Oauth-DFS-IngressBlob\r\nNonHNS-Oauth-DFS-IngressBlob", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update Kafka Streams integration tests using IQv2 to new streams protocol\nDescription: Update all integration tests using IQv2 to use both streams and classic group protocol to receive correct endpoint to partition information for all members of the group.\r\n\r\n \r\n\r\nAC: All existing integration tests with IQv2 are passing", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support integrating BeeLine with Connect JDBC driver\nDescription: \nQ: Issue resolved by pull request 52706\n[https://github.com/apache/spark/pull/52706]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Avoid intermediate pandas dataframe creation in df.toPandas\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: NPE Cannot invoke org.apache.kafka.connect.runtime.ConnectMetrics$MetricGroup.close()\nDescription: Several tasks in multiple sink connectors in a long-running connect cluster broke spontaneously (within a couple of hours) with the following stacktrace:\r\n{code:java}\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.kafka.connect.runtime.ConnectMetrics$MetricGroup.close()\" because the return value of \"java.util.concurrent.ConcurrentMap.get(Object)\" is null\r\n    at org.apache.kafka.connect.runtime.Worker$ConnectorStatusMetricsGroup.recordTaskRemoved(Worker.java:2333)\r\n    at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:707)\r\n    at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:568)\r\n    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2009)\r\n    at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$39(DistributedHerder.java:2059)\r\n    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)\r\n    at java.base/java.util.concurrent.ThreadPoolEx\nQ: Clarification: The tasks broke after two connect worker pods were killed and re-created. Only 2 out of 48 tasks broke, so this is not a configuration issue.", "output": "I also encountered this issue. The NPE is due to multiple uninitialized tasks unregistering ConnectorStatus Metrics Group. Created a PR to fix it."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Multiple-partition topic support for TopologyTestDriver\nDescription: The TopologyTestDriver is great for writing fast and reliable tests for Streams applications. However, a shortcoming of the TopologyTestDriver is that it only supports topics with one partition, therefore making it difficult to test that data has been correctly partitioned throughout the entire topology. This is especially relevant for topologies utilizing PAPI, where the user has to manage the partitioning of the data.\r\n\r\nA simple starting point could be to keep the current fully synchronous execution for a single input event, just with a TestInputTopic that supports a “Partitioner”.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: AAL - Update to version 1.2.1\nDescription: Version 1.2.1 brings in better memory management, readVectored and SSE-C support.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][Optimizations] Reduce Network Calls In Create and Mkdir Flow\nDescription: Implementing Create and Mkdir file system APIs for FNS(HNS Disabled) accounts on Blob Endpoint involves a lot of checks and marker file creations to handle implicit explicit cases of paths involved in these APIs.\r\n\r\nThis Jira proposes a few optimizations to reduce the network calls wherever possible and in case where create/mkdir is bound to fail, it should fail faster before doing any post checks,\nQ: anmolanmol1234 closed pull request #7340: HADOOP-19448: [ABFS][FNSOverBlob] Optimizing the current create and mkdir flow\nURL: https://github.com/apache/hadoop/pull/7340", "output": "hadoop-yetus commented on PR #7340:\nURL: https://github.com/apache/hadoop/pull/7340#issuecomment-2636089068\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  7s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   1m  7s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 38s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 39s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  37m  0s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7340/11/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 3 unchanged - 0 fixed = 5 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | -1 :x: |  spotbugs  |   0m 29s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7340/11/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 51s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 127m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7340/11/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7340 |\r\n   | JIRA Issue | HADOOP-19448 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux 0fb4d79ec841 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e3931d30c19c06c1856ad041e95437e0fc76f796 |\r\n   | Default Java | Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7340/11/testReport/ |\r\n   | Max. process+thread count | 719 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7340/11/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "as title", "output": "Move UnifiedLogTest to storage module"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Separate the controller config and admin config when add controller\nDescription: Currently, when adding a controller via CLI, we have to run:\r\n{code:java}\r\n$ bin/kafka-metadata-quorum.sh --command-config config/controller.properties --bootstrap-server localhost:9092 add-controller\r\n\r\nor\r\n\r\n$ bin/kafka-metadata-quorum.sh --command-config config/controller.properties --bootstrap-controller localhost:9093 add-controller{code}\r\nThe controller.properties file is expected to be the controller property file that to be added. But if we want to pass configs to the admin client, what can we do?\r\n{code:java}\r\nbin/kafka-metadata-quorum.sh --help\r\nusage: kafka-metadata-quorum [-h] [--command-config COMMAND_CONFIG] (--bootstrap-server BOOTSTRAP_SERVER | --bootstrap-controller BOOTSTRAP_CONTROLLER)\r\n                             {describe,add-controller,remove-controller} ...This tool describes kraft metadata quorum status.\r\n\r\n...\r\n\r\n --command-config COMMAND_CONFIG\r\n   Property file containing configs to be passed to Admin Client.  For add-controller,  the file is used to specify the controller properties as well.{code}\r\nAs this help output said, the \"--command-config\" can pass configs to admin client, but when add controller, it is also used as controller property file.\r\n\r\nFor example, we want to set the \"client-id\" to the admin client, when doing the add-controller, we have to add one more line in the controller.properties file:\r\n{code:java}\r\nclient.id=test-admin-client{code}\r\nThis is not ideal to ask users to mix the client config into the controller config.\r\n\r\n \r\n\r\n", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [Bug Report] Thread leak in ABFS AbfsClientThrottlingAnalyzer\nDescription: Bug reported by Matt over common-dev discussion.\r\n\r\n> What seems to be the issue is that the timer tasks are cleaned up but\r\n> the timer threads themselves are never actually cleaned up. This will\r\n> eventually lead to an OOM since nothing is collecting these. I was\r\n> able to reproduce this locally in 3.3.6 and 3.4.1 but I believe that\r\n> it would affect any version that relies on autothrottling for ABFS.\r\n>\r\n> I was also able to make a quick fix as well as confirm a workaround --\r\n> the long term fix would be to include `timer.cancel()` and\r\n> `timer.purge()` in a method for AbfsClientThrottlingAnalyzer.java. The\r\n> short term workaround is to disable autothrottling and rely on Azure\r\n> to throttle the connections as needed with the below configuration.\nQ: [~anujmodi] Got everything set up! It looks like I don't have permission to assign this issue to myself.\r\n\r\nPR filed here: \r\n\r\nhttps://github.com/apache/hadoop/pull/7852", "output": "hadoop-yetus commented on PR #7852:\nURL: https://github.com/apache/hadoop/pull/7852#issuecomment-3148687502\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 21s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 23s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 23s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 21s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/blanks-eol.txt) |  The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   0m 23s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  spotbugs  |   0m 22s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  44m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 25s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 160m 49s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7852 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 39b56ca5b682 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fee9861e3ff8d302a76812102c6c0cf38ebf37b5 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/testReport/ |\r\n   | Max. process+thread count | 536 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7852/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Resource leak in AssumedRoleCredentialProvider\nDescription: When `org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider` is used in a Hadoop Configuration object, it will attempt to `resolveCredentials()` inside the constructor.\r\n\r\n(lines 165-167)\r\n{code:java}\r\n// and force in a fail-fast check just to keep the stack traces less\r\n// convoluted\r\nresolveCredentials();{code}\r\n\r\nIf this method fails, because the current identity is not able to assume role, the constructor will throw an exception, and fail to close the `stsClient`, `stsProvider` and any other resources that are created in the constructor, leaking threads and other resources.\r\n \r\nIn a long running application, that handles Hadoop S3 file systems, where the user can dynamically change to configured role to assume, and external id, this will lead to eventually the system running out of resources due to the leaked threads created by the AWS SDK clients that are not closed when a wrong role or external id is used.\r\n \r\n\r\nThere are two potential fixes for this problem:\r\n\r\n - Don't attempt to `resolveCredentials()` inside the constructor\r\n\r\n - Wrap the `resolveCredentials()` in the constructor in a try/catch block, that cleans the resources and rethrows the exception in the catch block.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h1. Problem description\r\n\r\nLocalRelation is a Catalyst logical operator used to represent a dataset of rows inline as part of the LogicalPlan. LocalRelations represent dataframes created directly from Python and Scala objects, e.g., Python and Scala lists, pandas dataframes, csv files loaded in memory, etc.\r\n\r\nIn Spark Connect, local relations are transferred over gRPC using LocalRelation (for relations under 64MB) and CachedLocalRelation (larger relations over 64MB) messages.\r\n\r\nCachedLocalRelations currently have a hard size limit of 2GB, which means that spark users can’t execute queries with local client data, pandas dataframes, csv files of over 2GB.\r\nh1. Design\r\n\r\nIn Spark Connect, the client needs to serialize the local relation before transferring it to the server. It serializes data via an Arrow IPC stream as a single record batch and schema as a json string. It then embeds data and schema as LocalRelation\\{schema,data} proto message.\r\nSmall local relations (under 64MB) are sent directly as part of the ExecutePlanRequest.\r\n\r\n!image-2025-10-15-13-50-04-179.png!\r\n\r\nLarger local relations are first sent to the server via addArtifact and stored in memory or on disk via BlockManager. Then an ExecutePlanRequest is sent containing CachedLocalRelation\\{hash}, where hash is the artifact hash. The server retrieves the cached LocalRelation from the BlockManager via the hash, deserializes it, adds it to the LogicalPlan and then executes it.\r\n\r\n!image-2025-10-15-13-50-44-333.png!\r\n\r\n \r\n\r\nThe server reads the data from the BlockManager as a stream and tries to create proto.LocalRelation via\r\n{quote}proto.Relation\r\n.newBuilder()\r\n.getLocalRelation\r\n.getParserForType\r\n.parseFrom(blockData.toInputStream())\r\n{quote}\r\nThis fails, because java protobuf library has a 2GB limit on deserializing protobuf messages from a string.\r\n{quote}org.sparkproject.connect.com.google.protobuf.InvalidProtocolBufferException) CodedInputStream encountered an embedded string or message which clai", "output": "[CONNECT] Supporting large LocalRelations"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce Geography and Geometry data types to PySpark API\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `FlatBuffers` to v25.9.23\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: add ipv6 support\nDescription: Support IPv6 with a flag to enable/disable dual stack endpoints\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/ipv6-access.html", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove `Java 17` requirement from `deploy.gradle`\nDescription: \nQ: Issue resolved by pull request 381\n[https://github.com/apache/spark-kubernetes-operator/pull/381]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Refactor Nullable Types to Use a Unified Pattern\nDescription: see [https://github.com/apache/kafka/pull/20614#pullrequestreview-3379156676]\r\n\r\nRegarding the implementation of the nullable vs non-nullable types. We use 3 different approaches.\r\n # For bytes, we implement two independent classes BYTES and NULLABLE_BYTES.\r\n # For array, we use one class ArraryOf, which takes a nullable param.\r\n # For schema, we implement NULLABLE_SCHEMA as a subclass of SCHEMA.\r\n\r\nWe need  to pick one approach to implement all nullable types in a consistent way.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Integration test for Streams-related Admin APIs[2/N]\nDescription: Integration tests for Stream Admin related API\r\n\r\nPrevious one: https://issues.apache.org/jira/browse/KAFKA-19550\r\n\r\nThis one adds:\r\n * Integration test for {{Admin#listStreamsGroupOffsets}} API\r\n * Integration test fo {{Admin#deleteStreamsGroupOffsets}} API\r\n * Integration test fo {{Admin#alterStreamsGroupOffsets}} API", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Remove mockito-all 1.10.19 and powermock\nDescription: - The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11.\r\n\r\n - The powermock dependency is specified in the pom below. We should replace it with mockito since it is known to be incompatible with JDK17.\r\n -* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml\nQ: szetszwo opened a new pull request, #7935:\nURL: https://github.com/apache/hadoop/pull/7935\n\n   ### Description of PR\r\n   \r\n   The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   By updating existing tests\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [NA] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [NA] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [NA] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7935:\nURL: https://github.com/apache/hadoop/pull/7935#issuecomment-3260267569\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  23m 29s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 33s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  41m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  19m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  17m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   7m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 24s |  |  branch/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 10s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   4m 40s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |   0m 15s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 15s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 14s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   5m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   4m 41s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 59s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 17s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 18s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 18s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 33s |  |  hadoop-auth in the patch passed.  |\r\n   | -1 :x: |  unit  | 118m 19s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt) |  hadoop-yarn-server-resourcemanager in the patch failed.  |\r\n   | -1 :x: |  unit  | 141m 25s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client.txt) |  hadoop-mapreduce-client in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 50s |  |  hadoop-yarn-server-timelineservice-documentstore in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 23s |  |  hadoop-yarn-server-globalpolicygenerator in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m  5s |  |  hadoop-yarn-applications-catalog-webapp in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 20s | [/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-ui.txt) |  hadoop-yarn-ui in the patch failed.  |\r\n   | +1 :green_heart: |  unit  |   3m 12s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 47s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 541m 38s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.resourcemanager.reservation.TestCapacityOverTimePolicy |\r\n   |   | hadoop.mapreduce.v2.TestUberAM |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7935 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 21c4e19a37e1 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7dadba11fdeaf702d291642f8b2d59c197ec9ecb |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/testReport/ |\r\n   | Max. process+thread count | 1139 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-auth hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-mapreduce-project/hadoop-mapreduce-client hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-globalpolicygenerator hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui hadoop-tools/hadoop-azure U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7935/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [ReadAheadV2] Implement Read Buffer Manager V2 with improved aggressiveness\nDescription: \nQ: anujmodi2021 opened a new pull request, #7832:\nURL: https://github.com/apache/hadoop/pull/7832\n\n   ### Description of PR\r\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19622\r\n   \r\n   Implementing ReadBufferManagerV2 as per the new design document.\r\n   Following capabilities are added to ReadBufferManager:\r\n   1. Configurable minimum and maximum number of prefetch threads.\r\n   2. Configurable minimum and maximum size of cached buffer pool\r\n   3. Dynamically adjusting thread pool size and buffer pool size based on workload requirement and resource utilization but within the limits defined by user.\r\n   4. Mapping prefetched data to file ETag so that multiple streams reading same file can share the cache and save TPS.\r\n   \r\n   For more details on design doc please refer to the design doc attached to parent JIRA: https://issues.apache.org/jira/browse/HADOOP-19596\r\n   \r\n   ### How was this patch tested?\r\n   TBA", "output": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3131423891\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 59s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 7 new + 3 unchanged - 9 fixed = 10 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 48s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 22s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 17s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 26s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  81m 35s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 06ad120d8f97 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 28cb97fde1eda2fa096401499e9ee1dbccbdef2b |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "1. The cluster id is optional, but users are not aware of its purpose\r\n2. the RPC documentation is also missing\r\n3. there are no integration tests", "output": "improve the docs of \"clusterId\" for AddRaftVoterOptions and RemoveRaftVoterOptions"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Restore Subject propagation semantics for Java 22+\nDescription: Java 22 breaks Subject propagation for new Threads (when SecurityManager is not enabled).\r\n\r\nPreviously, the Subject set by Subject.doAs() / Subject.callAs() automatically propagated to any new Threads created (via new Thread(), not Executors).\r\n\r\nWith JDK22, this is no longer the case, new Threads do NOT inherit the Subject.\r\n\r\nAs Hadoop heavily relies on the original behavior, we somehow need to solve this problem.\nQ: stoty opened a new pull request, #7892:\nURL: https://github.com/apache/hadoop/pull/7892\n\n   ### Description of PR\r\n   \r\n   JDK 22 breaks subject propagation into new Threads.\r\n   This patch adds a new HadoopThread class which restores the pre JDK22 semantics, and replaces\r\n   most Thread objects in the code with HadoopThread.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Testing still in progress\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7892:\nURL: https://github.com/apache/hadoop/pull/7892#issuecomment-3213228211\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 21s |  |  https://github.com/apache/hadoop/pull/7892 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7892/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Improve `ConfOptionDocGenerator` to generate a sorted doc by config key\nDescription: \nQ: Issue resolved by pull request 407\n[https://github.com/apache/spark-kubernetes-operator/pull/407]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The current problem is that the view definition in the session catalog captures the analyzed plan that references DataSourceV2Relation and Table. If a connector doesn’t have an internal cache and produces a new Table object after TableCatalog$load, Table referenced in the view will become orphan and there will be no way to refresh it unless that Table instance auto refreshes on each scan (super dangerous).", "output": "Reload DSv2 tables in views created using plans on each access"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implement SparkConnectDatabaseMetaData simple methods\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Fix Case Sensitivity Issue for hdi_isfolder metadata\nDescription: In the blob endpoint, we determine whether the path is a file, or a directory based on the metadata attribute hdi_isfolder. When creating a directory, we set hdi_isfolder to true. Currently, our method for checking if the path is a directory involves a case-sensitive equality check. Consequently, if someone configures a directory with Hdi_isfolder, the driver will not recognize that path as a directory. We need to address this issue because, in the backend, hdi_isfolder and Hdi_isfolder are considered the same metadata attribute. Therefore, the solution involves modifying our equality check to be case-insensitive, ensuring that the driver correctly identifies directories regardless of case variations in the metadata attribute.\nQ: hadoop-yetus commented on PR #7496:\nURL: https://github.com/apache/hadoop/pull/7496#issuecomment-2714703867\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  13m  3s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  37m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  1s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  6s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 133m  6s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7496/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7496 |\r\n   | JIRA Issue | HADOOP-19494 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 48a24b66f245 5.15.0-134-generic #145-Ubuntu SMP Wed Feb 12 20:08:39 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0cbaae8d373e56ef3914349b03c6171e5bfcf38c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7496/1/testReport/ |\r\n   | Max. process+thread count | 699 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7496/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "bhattmanish98 commented on PR #7496:\nURL: https://github.com/apache/hadoop/pull/7496#issuecomment-2716425304\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 790, Failures: 0, Errors: 0, Skipped: 159\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 793, Failures: 0, Errors: 0, Skipped: 113\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 632, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 790, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 2\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 635, Failures: 0, Errors: 0, Skipped: 142\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 629, Failures: 0, Errors: 0, Skipped: 216\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 632, Failures: 0, Errors: 0, Skipped: 143\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 630, Failures: 0, Errors: 0, Skipped: 160\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 664, Failures: 0, Errors: 0, Skipped: 162\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 629, Failures: 0, Errors: 0, Skipped: 214\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 24"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: The normal topic auto-creation paths should use num.partitions and default.replication.factor from the controller properties\nDescription: see https://github.com/apache/kafka/pull/20421#issuecomment-3236341849", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Class Authorizer.authorizeByResourceType under the hood uses HashMap for \r\ncheck if the caller is authorized to perform the given ACL operation on at least one resource of the given type.\r\n\r\nIt check each character in allowPatterns and pass to deny patterns\r\n\r\n{code:java}\r\n// For any literal allowed, if there's no dominant literal and prefix denied, return allow.\r\n        // For any prefix allowed, if there's no dominant prefix denied, return allow.\r\n        for (Map.Entry> entry : allowPatterns.entrySet()) {\r\n            for (String allowStr : entry.getValue()) {\r\n                if (entry.getKey() == PatternType.LITERAL\r\n                        && denyPatterns.get(PatternType.LITERAL).contains(allowStr))\r\n                    continue;\r\n                StringBuilder sb = new StringBuilder();\r\n                boolean hasDominatedDeny = false;\r\n                for (char ch : allowStr.toCharArray()) {\r\n                    sb.append(ch);\r\n                    if (denyPatterns.get(PatternType.PREFIXED).contains(sb.toString())) {\r\n                        hasDominatedDeny = true;\r\n                        break;\r\n                    }\r\n                }\r\n                if (!hasDominatedDeny)\r\n                    return AuthorizationResult.ALLOWED;\r\n            }\r\n        }\r\n{code}\r\n\r\nTo improve performance better use Prefix Tree", "output": "Improve perfomance Authorizer.authorizeByResourceType by using Prefix Trie"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add --enable-native-access=ALL-UNNAMED JVM option\nDescription: We get warnings like \r\n{noformat}\r\nWARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module\r\nWARNING: Restricted methods will be blocked in a future release unless native access is enabled\r\n{noformat}\r\non JDK21+.\r\n\r\nWhile this works now, it's better to add this early, and avoid breakage later.\r\n\r\nThis also cleans up the the console output.\nQ: hadoop-yetus commented on PR #7627:\nURL: https://github.com/apache/hadoop/pull/7627#issuecomment-2812034325\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  29m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |  49m 25s |  |  branch has errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |  38m 25s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 26s | [/patch-unit-hadoop-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7627/1/artifact/out/patch-unit-hadoop-project.txt) |  hadoop-project in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 28s |  |  ASF License check generated no output?  |\r\n   |  |   | 120m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7627/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7627 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 259fca8f8e8d 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / eb3b538b54f0e6f3c2c50d48bc39827bf26f7c8c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7627/1/testReport/ |\r\n   | Max. process+thread count | 98 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7627/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "pan3793 commented on code in PR #7627:\nURL: https://github.com/apache/hadoop/pull/7627#discussion_r2054579431\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -182,6 +182,7 @@\n       --add-opens=java.base/java.util.zip=ALL-UNNAMED\n       --add-opens=java.base/sun.security.util=ALL-UNNAMED\n       --add-opens=java.base/sun.security.x509=ALL-UNNAMED\n+      --enable-native-access=ALL-UNNAMED\n\nReview Comment:\n   please\r\n   > keep this list sync with\r\n   > hadoop-common-project/hadoop-common/src/main/bin/hadoop-functions.sh#hadoop_finalize_jpms_opts"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Use java.time.Clock instead of org.apache.hadoop.util.Clock\nDescription: Hadoop's {{Clock}} interface was recently moved from {{org.apache.hadoop.yarn.util}} (in hadoop-yarn) to {{org.apache.hadoop.util}} (in hadoop-common) as part of YARN-11765.\r\n\r\nI propose to seize the opportunity of this being targeted done for 3.5.0 to modernize it usage:\r\n# Deprecate {{org.apache.hadoop.util.Clock}}\r\n# Replace all of its usages with {{java.time.Clock}}\r\n# Replace existing usages of its simple implementations, e.g. {{SystemClock}}/{{UTCClock}} with standard {{java.time.Clock}} subclasses, e.g. {{Clock.systemUTC()}}\r\n# Re-implement other implementations, e.g. {{MonotonicClock}}/{{ControllerClock}}, as {{java.time.Clock}} subclasses.\r\n\r\nThe standard {{java.time.Clock}} has a richer API supports modern {{java.time}} classes such as {{Instant}} and {{ZoneId}}, and migration would be straightforward:\r\nJust changing {{org.apache.hadoop.util.Clock.getTime()}} to {{java.time.Clock.millis()}}", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Release Hadoop Third-Party 1.4.0\nDescription: Make a new hadoop-thirdparty release to go with the 3.4.2 release", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add -o option in hdfs \"count\" command to show the owner's summarization.\nDescription: Add -o option in hdfs \"count\" command to show the owner's summarization.\nQ: fuchaohong opened a new pull request, #7641:\nURL: https://github.com/apache/hadoop/pull/7641\n\n   Add -o option in hdfs \"count\" command to show the owner's summarization.\r\n   `hdfs dfs -count -o hdfs -v /tmp/`\r\n   ![Uploading HADOOP-19549.output.png…]()", "output": "hadoop-yetus commented on PR #7641:\nURL: https://github.com/apache/hadoop/pull/7641#issuecomment-2820056653\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 48s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 57s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   7m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m  8s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  13m 12s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 121m 25s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.cli.TestCLI |\r\n   |   | hadoop.fs.TestFilterFileSystem |\r\n   |   | hadoop.fs.TestHarFileSystem |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7641 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets markdownlint xmllint |\r\n   | uname | Linux 23b158b27a15 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8c681bba96b970fcea4dbb306517e26bf39d0c39 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/1/testReport/ |\r\n   | Max. process+thread count | 3151 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7641/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support `spark.kubernetes.executor.useDriverPodIP`\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] WASB to ABFS Migration Config Support Script\nDescription: The legacy WASB driver has been deprecated and is no longer recommended for use. To support customer onboard for migration from WASB to ABFS driver, we've introduced a script to help with the configuration changes required for the same.\r\n\r\nThe script requires the configuration file (in XML format) used for WASB and would generate configuration file required for ABFS driver respectively.\nQ: manika137 opened a new pull request, #7564:\nURL: https://github.com/apache/hadoop/pull/7564\n\n   ### Description of PR\r\n   The legacy WASB driver has been deprecated and is no longer recommended for use. To support customer onboard for migration from WASB to ABFS driver, we've introduced a script to help with the configuration changes required for the same.\r\n   \r\n   The script requires the configuration file (in XML format) used for WASB and would generate configuration file required for ABFS driver respectively. \r\n   \r\n   JIRA ticket: https://issues.apache.org/jira/browse/HADOOP-19518\r\n   \r\n   ## How was this patch tested?\r\n   No production code change, no testing needed.", "output": "hadoop-yetus commented on PR #7564:\nURL: https://github.com/apache/hadoop/pull/7564#issuecomment-2768581697\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  1s |  |  Shelldocs was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7564/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 33s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 37s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7564/1/artifact/out/results-asflicense.txt) |  The patch generated 2 ASF License warnings.  |\r\n   |  |   | 122m 36s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7564/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7564 |\r\n   | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets shellcheck shelldocs xmllint markdownlint |\r\n   | uname | Linux 88dd9936b8a7 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3a1a2ebc5a14860824ad9ce035668d08fc1565cc |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7564/1/testReport/ |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7564/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Keep coverage data when running pip tests\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update GitHub Actions workflow to use the latest versions of actions\nDescription: The current GitHub Actions workflows in the Hadoop repository are using outdated versions of GitHub Actions, such as {{{}actions/checkout@v3{}}}. To ensure better security, performance, and compatibility, we should update them to the latest stable versions.\nQ: hadoop-yetus commented on PR #7454:\nURL: https://github.com/apache/hadoop/pull/7454#issuecomment-2695116996\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  13m 39s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  yamllint  |   0m  0s |  |  yamllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  37m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  86m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7454/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7454 |\r\n   | JIRA Issue | HADOOP-19477 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets yamllint |\r\n   | uname | Linux 2481122392a4 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7334f1f9a04d312c0755f5e30215c2b167bafa7f |\r\n   | Max. process+thread count | 610 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7454/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "Bcoderx6 commented on PR #7454:\nURL: https://github.com/apache/hadoop/pull/7454#issuecomment-2711348039\n\n   @cnauroth Thanks for reviewing! The updates follow different strategies based on stability and compatibility. Some actions are pinned to specific versions to avoid unexpected issues, while others are updated to the latest major version after checking their changelogs.\r\n   \r\n   Feel free to tag another reviewer happy to discuss if anything needs tweaking!"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analytics-Accelerator: Move AAL to use Java sync client\nDescription: Java sync client is giving the best performance for our use case, especially for readVectored() where a large number of requests can be made concurrently.\nQ: Sync. \r\n\r\nIt's using async currently. But the readVectored + AAL use case is not ideal for the async client. As we already have our own thread pool, and each thread is responsible for making a single S3 request, and start reading data from that input stream immediately to fill the internal buffers.. \r\n\r\nWith the async client, this means you need to join() immediately, and when at a higher concurrency things get stuck in the Netty thread pool and the AsyncResponseTransformer.toBlockingInputStream() of\r\n\r\ns3AsyncClient\r\n.getObject(builder.build(), AsyncResponseTransformer.toBlockingInputStream()).\r\n \r\nS3Async client works well (I think) when you have high concurrency but don't need to join on the data immediately, so the netty io pool is sufficient to satisfy those requests.", "output": "internal benchmarking has been showing a 4-5% improvement with the Sync client consistently."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [kms] Negative hadoop.security.kms.client.encrypted.key.cache.expiry causes KMSClientProvider init failure with generic IllegalArgumentException; \nDescription: When the client-side config hadoop.security.kms.client.encrypted.key.cache.expiry is set to a negative value in core-site.xml, any tool that initializes KMSClientProvider (e.g., KeyShell) fails immediately with:\r\n\r\n{code:java}\r\njava.lang.IllegalArgumentException: expiry must be > 0\r\n    at org.apache.hadoop.crypto.key.kms.ValueQueue.(ValueQueue.java:xxx)\r\n    at org.apache.hadoop.crypto.key.kms.KMSClientProvider.(KMSClientProvider.java:xxx)\r\n    ...\r\n\r\n{code}\r\n\r\nThis is a controlled failure (JVM doesn’t crash), but the error message does not mention which property and what value triggered it. Users typically see a stack trace without a clear remediation hint.\r\n\r\n*Expected behavior*\r\n\r\nFail fast with a clear configuration error that names the property and value, e.g.:\r\nInvalid configuration: hadoop.security.kms.client.encrypted.key.cache.expiry = -1 (must be > 0 ms)\r\n\r\n*Steps to Reproduce*\r\n1. In the client core-site.xml, set:\r\n\r\n{code:xml}\r\n\r\n  hadoop.security.kms.client.encrypted.key.cache.expiry\r\n  -1\r\n\r\n{code}\r\n2. Ensure the conf is active (echo $HADOOP_CONF_DIR points to this dir).\r\n3. Run:\r\n\r\n{code:java}\r\n./bin/hadoop key list -provider kms://http@localhost:9600/kms -metadata\r\n{code}", "output": "Patch Available"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In HADOOP-18636, LocalDirAllocator was modified to recreate missing dirs. But it appears that there are still codepaths which don't do that.\r\n\r\nwhen charrypicking this -please follow up with  HADOOP-19573, to ensure an associated test never fails", "output": "LocalDirAllocator still doesn't always recover from directory tree deletion"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add `StatefulSet`-based SparkApp example\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SHOW TBLPROPERTIES AS JSON\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Potential Long.MAX_VALUE overflow in sessionExpirationTimeNanos calculation in SaslServerAuthenticator \nDescription: There is a potential risk of Long.MAX_VALUE overflow in the sessionExpirationTimeNanos calculation within the SaslServerAuthenticator class.\r\nLocation:\r\n !image-2025-08-01-10-12-04-784.png! \r\nThe calculation sessionExpirationTimeNanos = authenticationEndNanos + 1000 * 1000 * retvalSessionLifetimeMs can potentially overflow when:\r\nretvalSessionLifetimeMs is very large \r\nauthenticationEndNanos is already a large value\r\nThe multiplication 1000 * 1000 * retvalSessionLifetimeMs exceeds Long.MAX_VALUE - authenticationEndNanos\nQ: OK i will close this one", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`\nDescription: HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found.\r\n\r\nI think the cause is:\r\n * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration`\r\n * However, code paths to load AWS credential provider call `S3AUtils.getInstanceFromReflection`, which uses the classloader that loaded the S3AUtils class. That's likely to be the built-in application classloader, which won't be able to load classes in a Spark job JAR.\r\n\r\nAnd the fix seems small:\r\n * Change `S3AUtils.getInstanceFromReflection` to load classes using the `Configuration`'s classloader. Luckily we already have the Configuration in this method.\nQ: I haven't contributed to Hadoop or other Apache projects before, but this approachable for a first contribution. I'll open a PR.", "output": "brandonvin opened a new pull request, #8048:\nURL: https://github.com/apache/hadoop/pull/8048\n\n   …lassloader\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Follow-up to [HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) and [HADOOP-19733](https://issues.apache.org/jira/browse/HADOOP-19733) before it.\r\n   \r\n   With `fs.s3a.classloader.isolation` set to `false` in a Spark application, it was still impossible to load a credentials provider class from the Spark application jar.\r\n   \r\n   `fs.s3a.classloader.isolation` works by saving a reference to the intended classloader in the `Configuration`.\r\n   \r\n   However, loading credentials providers goes through\r\n   `S3AUtils#getInstanceFromReflection`, which always used the classloader that loaded `S3AUtils`.\r\n   \r\n   With this patch, credentials providers will be loaded using the `Configuration`'s classloader.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Unit tests in `org.apache.hadoop.fs.s3a.ITestS3AFileSystemIsolatedClassloader`.\r\n   \r\n   Manual testing in a Spark application.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: High number of Threads Launched when Calling fs.getFileStatus() via proxyUser after Kerberos authentication.\nDescription: We have observed an issue where very large number of threads are being launched when performing concurrent {{fs.getFileStatus(path) operations}} as proxyUser.\r\n\r\nAlthough this issue was observed in our hive services, we were able to isolate and replicate this issue without hive by writing a sample standalone program which first logs in via a principal and keytab and then creates a proxy user and fires concurrent {{fs.getFileStatus(path)}} for a few mins. Eventually when the concurrency increases it tries to create more threads than max available threads(ulimit range) and the process eventually slows down.\r\n{code:java}\r\nUserGroupInformation proxyUserUGI = UserGroupInformation.createProxyUser(\r\n\"hive\", UserGroupInformation.getLoginUser());{code}\r\nIn this particular case, when launching 30 concurrent threads calling , the max number of threads launched by the PID are 6066.\r\n \r\n{code:java}\r\nEvery 1.0s: ps -eo nlwp,pid,args --sort -nlwp | head                                                                       Wed Feb 19 06:12:47 2025\r\nNLWP     PID COMMAND\r\n6066  700718 /usr/lib/jvm/java-17-openjdk/bin/java -cp ./test.jar:/usr/hadoop/*:/usr/hadoop/lib/*:/usr/hadoop-hdfs/* org.apache.hadoop.hive.common.HDFSFileStatusExample hdfs://namenode:8020 principal keytab_location 30 true\r\n\r\n\r\n{code}\r\n \r\n \r\n \r\n \r\nBut the same behaviour is not observed when the same calls are made using the current userUGI instead of proxyUser.\r\n{code:java}\r\nUserGroupInformation currentUserUgi = UserGroupInf", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-sls.\nDescription: \nQ: hadoop-yetus commented on PR #7553:\nURL: https://github.com/apache/hadoop/pull/7553#issuecomment-2763299450\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  2s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 13 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 16s | [/results-checkstyle-hadoop-tools_hadoop-sls.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-sls.txt) |  hadoop-tools/hadoop-sls: The patch generated 12 new + 22 unchanged - 0 fixed = 34 total (was 22)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 53s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 57s | [/patch-unit-hadoop-tools_hadoop-sls.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/1/artifact/out/patch-unit-hadoop-tools_hadoop-sls.txt) |  hadoop-sls in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 114m 22s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.sls.TestSLSStreamAMSynth |\r\n   |   | hadoop.yarn.sls.TestReservationSystemInvariants |\r\n   |   | hadoop.yarn.sls.TestSLSDagAMSimulator |\r\n   |   | hadoop.yarn.sls.TestSLSRunner |\r\n   |   | hadoop.yarn.sls.TestSLSGenericSynth |\r\n   |   | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.sls.nodemanager.TestNMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7553 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 329636455c63 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1b56a9a9ec8fb4d4f93f189faeb78c89eb1cfeee |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/1/testReport/ |\r\n   | Max. process+thread count | 707 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-sls U: hadoop-tools/hadoop-sls |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7553:\nURL: https://github.com/apache/hadoop/pull/7553#issuecomment-2768125985\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 56s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 13 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 15s | [/results-checkstyle-hadoop-tools_hadoop-sls.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-sls.txt) |  hadoop-tools/hadoop-sls: The patch generated 17 new + 22 unchanged - 0 fixed = 39 total (was 22)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |  11m 58s | [/patch-unit-hadoop-tools_hadoop-sls.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/2/artifact/out/patch-unit-hadoop-tools_hadoop-sls.txt) |  hadoop-sls in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 126m 12s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.sls.appmaster.TestAMSimulator |\r\n   |   | hadoop.yarn.sls.nodemanager.TestNMSimulator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7553 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 94479cf357d9 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d7b386a7187783f38997dedea35cfa4f93e9c147 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/2/testReport/ |\r\n   | Max. process+thread count | 607 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-sls U: hadoop-tools/hadoop-sls |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7553/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Listing APIs on both DFS and Blob Endpoints return response as part of response body and has to be read from an Socket Input Stream.\r\nAny network error occuring while reading the stream should be retried.\r\n\r\nToday, this parsing happens in client and such errors are not retried.\r\nThis change fixes this behavior", "output": "ABFS: [FnsOverBlob] Streaming List Path Result Should Happen Inside Retry Loop"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Migrate from Java to Jakarta - Upgrade to Jersey 3.x, Jetty 12.x\nDescription: This Jira is to track the migration of Javax to Jakarta in Hadoop and identify all the transitive dependencies which need to be migrated as well. Currently, I see the following major upgrades required:\r\n # Jersey: 2.46 to 3.x\r\n # Jetty: 9.x to 12.x\nQ: [~palsai] I already created HADOOP-19395 earlier this year,\r\nbut as I've mentioned, it needs to wait for Java 17 as the baseline.", "output": "Hi [~kunda], thank you very much for your reply. \r\n\r\nI see that a lot of the JDK17 work has been completed and the CI pipeline setup work for trunk is in progress. HADOOP-17177\r\n\r\nThank you [~slfan1989] for driving the JDK17 efforts.\r\n\r\nMeanwhile, I came across this mail thread for jetty upgrade in Hbase: [https://lists.apache.org/thread/bkrfm705kqd3bqzyvo7jv46t6p64x2n5] \r\n\r\nSince jetty-9.x line is currently EOCS, we need to bump jetty to 12.x (jetty 11 line is EOCS as well).\r\n\r\n*Proposal 1:* \r\n\r\n2 Phase Approach:\r\n # Bump jetty 9.x to 12.x EE8 which supports the javax namespace, once this change is done and tested, we work on moving to jakarta completely.\r\n # Bump jetty from 12.x EE8 (javax namespace) to 12.x EE10 (jakarta namespace), bump jersey to 3.x and other required library upgrades for moving completely to the jakarta namespace.\r\n\r\n*Proposal 2:* \r\n\r\nDirectly move to jakarta namespace. (bump jetty to 12.x EE10 and jersey to 3.x and other required library upgrades, code changes and testing).\r\n\r\nCan you please provide your inputs on the same? \r\n\r\ncc: [~kunda], [~slfan1989], [~stevel@apache.org], [~brahmareddy] \r\n\r\nThank you."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: MirrorCheckpointTask causes consumers on target cluster to rewind offsets or skip messages due to erroneous offset commits\nDescription: h2. *Description*\r\n\r\nThe MirrorCheckpointTask in Mirror Maker 2 commits offsets for active consumer groups on the target cluster, causing consumers to rewind their offsets or skip messages. Typically brokers prevent committing offsets for consumer groups that are not in `EMPTY` state by throwing {{{}UnknownMemberIdException{}}}. In addition, {{MirrorCheckpointTask}} has logic in place to prevent committing offsets older than target for {{EMPTY}} consumer groups. However, due to a bug in {{MirrorCheckpointTask}} code, this prevention check is not enforced and it attempts to commit offsets for {{STABLE}} consumers. These calls can go through if consumers were momentarily disconnected moving the group state to {{{}EMPTY{}}}. This results in consumers' offsets getting reset to older values. If the offset is not available on the target broker (due to retention), the consumers can get reset to \"{{{}earliest\"{}}} or \"{{{}latest\"{}}}, thus reading duplicates or skipping messages. \r\nh2. *Bug location*\r\n\r\n1. In [MirrorCheckpointTask|#L305], we only update the latest target cluster offsets ({{{}idleConsumerGroupsOffset{}}})  if target consumer group state is {{{}EMPTY{}}}.\r\n\r\n2. When {{syncGroupOffset}} is called, we check if the target consumer group is present in  \r\n\r\n{{{}idleConsumerGroupsOffset{}}}. The consumer group won't be present as it's an active group. We assume that this is a new group and start syncing consumer group offsets to target. These calls fail with {_}{{{{{}Unable ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump Maven 3.9.10\nDescription: \nQ: pan3793 opened a new pull request, #7760:\nURL: https://github.com/apache/hadoop/pull/7760\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Maven prior 3.8 is EOL, bump Maven from 3.6.3 to latest 3.9.10\r\n   \r\n   https://maven.apache.org/docs/3.9.10/release-notes.html\r\n   \r\n   > Maven 3.9.10 now has a far better support if you want to run your builds on Java 24.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   CI.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "pan3793 commented on PR #7760:\nURL: https://github.com/apache/hadoop/pull/7760#issuecomment-3050892025\n\n   #7782 gets merged, rebase this PR"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: UserGroupInformation.java is using a non-support operation in JDK25\nDescription: Hello,\r\n\r\nI'm trying to upgrade my version of ParquetJava and I'm seeing the following error locally\r\n{code:java}\r\n    java.lang.UnsupportedOperationException: getSubject is not supported\r\n\r\n\r\n        at java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\r\n\r\n        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.(FileSystem.java:3852)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.(FileSystem.java:3842)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3630)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\r\n\r\n\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541)\r\n\r\n\r\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\r\n\r\n        at org.apache.parquet.hadoop.util.HadoopOutputFile.fromPath(HadoopOutputFile.java:58)\r\n\r\n\r\n        at com.amazon.networkvalidator.parquet.ParquetWriter.write(ParquetWriter.kt:75)\r\n\r\n\r\n        at com.amazon.networkvalidator.parquet.ParquetWriterTest$test \r\nwriting to parquet$1.invokeSuspend(ParquetWriterTest.kt:88)\r\n\r\n\r\n        at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt)\r\n\r\n\r\n        at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt)\r\n\r\n\r\n   ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: The default value of io.compression.codec.lzo.buffersize should not be in the core-site-default.xml file.\nDescription: The default value of io.compression.codec.lzo.buffersize should not be in the core-site-default.xml file.\r\n\r\nThe io.compression.codec.lzo.buffersize configuration file is the core configuration file of LZO. The default value is 245 MB. The default value should be controlled by ZLO and should not be contained in the core-site file.\r\n\r\n \r\n\r\n!image-2025-05-12-16-21-43-114.png!\nQ: hadoop-yetus commented on PR #7708:\nURL: https://github.com/apache/hadoop/pull/7708#issuecomment-2908003220\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 23s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  15m 53s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 56s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  | 104m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  6s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 57s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 12s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 55s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 43s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  15m 24s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  6s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 208m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7708/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7708 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 58d7d0dff95a 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 450e163c5ed90e493a085e4815296f7311f7e321 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7708/1/testReport/ |\r\n   | Max. process+thread count | 3001 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7708/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "github-actions[bot] commented on PR #7708:\nURL: https://github.com/apache/hadoop/pull/7708#issuecomment-3336331729\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Listing Optimizations to avoid multiple iteration over list response.\nDescription: On blob endpoint, there are a couple of handling that is needed to be done on client side.\r\nThis involves:\r\n # Parsing of xml response and converting them to VersionedFileStatus list\r\n # Removing duplicate entries for non-empty explicit directories coming due to presence of the marker files\r\n # Trigerring Rename recovery on the previously failed rename indicated by the presence of pending json file.\r\n\r\nCurrently all three are done in a separate iteration over whole list. This is to pbring all those things to a common place so that single iteration over list reposne can handle all three.\nQ: hadoop-yetus commented on PR #7421:\nURL: https://github.com/apache/hadoop/pull/7421#issuecomment-2695226615\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  10m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 57s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  35m 58s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 18 new + 12 unchanged - 0 fixed = 30 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 25s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 2 new + 10 unchanged - 0 fixed = 12 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 generated 2 new + 10 unchanged - 0 fixed = 12 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 50s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 135m  4s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7421 |\r\n   | JIRA Issue | HADOOP-19474 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b3d24f968b2c 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b37c3d2bdb5f8771b1dbad0a179be307cf3444b1 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/testReport/ |\r\n   | Max. process+thread count | 761 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7421:\nURL: https://github.com/apache/hadoop/pull/7421#issuecomment-2697120282\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 59s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  40m 11s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  1s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 137m 10s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7421 |\r\n   | JIRA Issue | HADOOP-19474 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux 8621c9c4dac7 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3edab0f5b5ab5d0def2b79d9ac394d4c72001e8e |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/5/testReport/ |\r\n   | Max. process+thread count | 524 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7421/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix flaky RestoreIntegrationTest#shouldInvokeUserDefinedGlobalStateRestoreListener\nDescription: org.opentest4j.AssertionFailedError: Condition not met within timeout 60000. Timed out waiting for active restoring task ==> expected:  but was: \r\n\tat app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n\tat app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n\tat app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)\r\n\tat app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)\r\n\tat app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214)\r\n\tat app//org.apache.kafka.test.TestUtils.lambda$waitForCondition$4(TestUtils.java:447)\r\n\tat app//org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:495)\r\n\tat app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:444)\r\n\tat app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:428)\r\n\tat app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:418)\r\n\tat app//org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitForActiveRestoringTask(IntegrationTestUtils.java:611)\r\n\tat app//org.apache.kafka.streams.integration.RestoreIntegrationTest.shouldInvokeUserDefinedGlobalStateRestoreListener(RestoreIntegrationTest.java:689)\r\n\tat java.base@17.0.16/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat java.base@17.0.16/java.util.Optional.ifPresent(Optional.java:178)\r\n\tat java.base@17.0.16/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.ja", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Streaming List Path Result Should Happen Inside Retry Loop\nDescription: Listing APIs on both DFS and Blob Endpoints return response as part of response body and has to be read from an Socket Input Stream.\r\nAny network error occuring while reading the stream should be retried.\r\n\r\nToday, this parsing happens in client and such errors are not retried.\r\nThis change fixes this behavior\nQ: anujmodi2021 opened a new pull request, #7582:\nURL: https://github.com/apache/hadoop/pull/7582\n\n   ### Description of PR\r\n   Listing APIs on both DFS and Blob Endpoints return response as part of response body and has to be read from an Socket Input Stream.\r\n   Any network error occuring while reading the stream should be retried.\r\n   \r\n   Today, this parsing happens in client and such errors are not retried.\r\n   This change fixes this behavior\r\n   \r\n   ### How was this patch tested?\r\n   Added a few more tests and test suite ran for validation", "output": "hadoop-yetus commented on PR #7582:\nURL: https://github.com/apache/hadoop/pull/7582#issuecomment-2780668739\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m  0s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  40m 25s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7582/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 40s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 43s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 153m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7582/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7582 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d4259183eced 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e3f272aaa285701ddea68e9fca0364940811e166 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7582/1/testReport/ |\r\n   | Max. process+thread count | 588 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7582/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support getSQLKeywords for SparkConnectDatabaseMetaData\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Jira for KIP-1217\r\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-1217%3A+Include+push+interval+in+ClientTelemetryReceiver+context", "output": "Include push interval in ClientTelemetryReceiver context"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "On FNS-Blob, List Blobs API is known to return duplicate entries for the non-empty explicit directories. One entry corresponds to the directory itself and another entry corresponding to the marker blob that driver internally creates and maintains to mark that path as a directory. We already know about this behaviour and it was handled to remove such duplicate entries from the set of entries that were returned as part current list iterations.\r\n\r\nDue to possible partition split if such duplicate entries happen to be returned in separate iteration, there is no handling on this and caller might get back the result with duplicate entries as happening in this case. The logic to remove duplicate was designed before the realization of partition split came.\r\n\r\nThis PR fixes this bug", "output": "ABFS: [FnsOverBlob] Remove Duplicates from Blob Endpoint Listing Across Iterations"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Handle batch alignment when share partition is at capacity\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Move AutoTopicCreationManager and AutoTopicCreationManagerTest to server module.", "output": "Move AutoTopicCreationManager to server module"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analytics-Accelerator: AAL stream factory not being closed\nDescription: When the Factory service is stopped, we're currently missing the code to close the factory. My miss, this got lost in the move to the new factory code.\nQ: ahmarsuhail commented on PR #7616:\nURL: https://github.com/apache/hadoop/pull/7616#issuecomment-2807036987\n\n   @steveloughran @mukund-thakur  small PR to close AAL stream factory on service stop, missed that in the factory refactor. \r\n   \r\n   Was debating if that stasitic should be a generic `OBJECT_INPUT_STREAM_FACTORY_CLOSED` and be incremented in the serviceStop of `AbstractObjectInputStreamFactory`. But wasn't sure. the other stream factories don't need this because their factories don't hold any state, AAL does. let me know what you think.", "output": "hadoop-yetus commented on PR #7616:\nURL: https://github.com/apache/hadoop/pull/7616#issuecomment-2807382529\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 11s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  19m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 36s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   9m  0s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  6s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 21s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   0m 18s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  compile  |   8m 43s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   8m 43s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   7m  0s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | -1 :x: |  javac  |   7m  0s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 57s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   0m 27s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   1m  2s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 54s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  spotbugs  |   0m 25s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m  6s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  12m 20s |  |  hadoop-common in the patch passed.  |\r\n   | -1 :x: |  unit  |   0m 25s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 33s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 140m 35s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7616 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux efc1cd66614a 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a570aa18ab907a5f2ed9ed98e4440edb09ef4040 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/testReport/ |\r\n   | Max. process+thread count | 3149 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7616/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We are currently running Kafka version 3.9.0 and are in the process of migrating to KRaft. As part of the migration, we intend to operate in dual-write mode in production for an initial period to help identify and address any issues.\r\n\r\nAre there any known limitations or risks associated with running in dual-write mode? Would you recommend maintaining this mode for production stability, and are there best practices we should follow?", "output": "Limitations of KRAFT Dual Write Mode for Production Support"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Disk space is not being freed\nDescription: I found that the remoteFetch thread is holding the file, preventing the disk space from being freed.\r\n\r\n!image-2025-09-24-10-38-25-161.png!  \r\n\r\nHowever, I didn't encounter this problem with the earlier 0.0.1-SNAPSHOT build, which used Kafka 3.8.1. The current issue appears in version 1.0.0 together with Kafka 3.9.1.\r\n\r\n!image-2025-09-24-10-38-36-572.png!\r\n\r\nI'm not sure whether this issue is caused by the tiered storage plugin or by Kafka itself. Could someone help take a look?\nQ: Could someone please take a look at this issue? It is causing disk space to be freed extremely slowly.", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Adding support for new authentication type: user bound SAS", "output": "ABFS: Support for new auth type: User-bound SAS"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add documentation comparing behavioral differences between Spark Connect and Spark Classic\nDescription: While https://spark.apache.org/docs/latest/spark-connect-overview.html introduces the architecture of Spark Connect, we should add a guide that helps users migrating from Spark Classic understand and avoid unexpected behaviors caused by deferred schema analysis and name resolution.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use empty schema when altering a view which is not Hive compatible\nDescription: Spark attempts to save views in a Hive-compatible format and only sets the schema to empty if the save operation fails.\r\n\r\nHowever, due to certain Hive compatibility issues, the save operation may succeed while subsequent read operations fail. This issue arises after the change introduced in SPARK-46934, which removed the {{verifyColumnDataType}} check during the {{ALTER TABLE}} operation.\nQ: I am working on a Fix, Can I assign to myself?", "output": "Issue resolved by pull request 52730\n[https://github.com/apache/spark/pull/52730]"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In client.java before HADOOP-18324 , rpc connection class used newCachedThreadPool, which reuses previously constructed threads if they are available else create new. \r\n\r\nIn the HADOOP-18324, This has been modified to create new single thread every time and threads will not be reused for other tasks once it completes its execution.\r\n\r\nBecause of this change, there is overhead in the total number threads created over the lifespan of a client jvm, for example a Hive Metastore. This overhead leads to slight performance latency in our applications.", "output": "Performance overhead in Client.java due to Increased threads creation"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update non-thirdparty Guava version to 32.0.1\nDescription: Guava has been already updated to 32.0.1 in hadoop-thirdparty.\r\nHowever, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners.\r\n\r\nSync the non-thirdparty Guava version to the thirdparty one.\nQ: stoty opened a new pull request, #7940:\nURL: https://github.com/apache/hadoop/pull/7940\n\n   same as the current thirdparty Guava version\r\n   \r\n   ### Description of PR\r\n   \r\n   Guava has been already updated to 32.0.1 in hadoop-thirdparty.\r\n   However, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners.\r\n   \r\n   Sync the non-thirdparty Guava version to the thirdparty one.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Test suite in CI (on this PR)\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7940:\nURL: https://github.com/apache/hadoop/pull/7940#issuecomment-3264930118\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 57s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  87m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 17s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 134m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7940 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux fe8aa1f6455e 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8794a7e5b6cf955551d93f3a15effbd83d5174b7 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/testReport/ |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7940/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analytics-Accelerator: Move AAL to use Java sync client\nDescription: Java sync client is giving the best performance for our use case, especially for readVectored() where a large number of requests can be made concurrently.\nQ: sync or async?", "output": "Sync. \r\n\r\nIt's using async currently. But the readVectored + AAL use case is not ideal for the async client. As we already have our own thread pool, and each thread is responsible for making a single S3 request, and start reading data from that input stream immediately to fill the internal buffers.. \r\n\r\nWith the async client, this means you need to join() immediately, and when at a higher concurrency things get stuck in the Netty thread pool and the AsyncResponseTransformer.toBlockingInputStream() of\r\n\r\ns3AsyncClient\r\n.getObject(builder.build(), AsyncResponseTransformer.toBlockingInputStream()).\r\n \r\nS3Async client works well (I think) when you have high concurrency but don't need to join on the data immediately, so the netty io pool is sufficient to satisfy those requests."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove Google Analytics from Hadoop Website\nDescription: Hi Hadoop Team,\r\n\r\nThe ASF {_}*Privacy Policy*{_}[1][2] does not permit the use of _*Google Analytics*_ on any ASF websites.\r\n\r\nIt looks like _*Google Analytics*_ was removed from the Hadoop site on *13th Nov 2024* in [Commit 86c0957|https://github.com/apache/hadoop-site/commit/86c09579bff7bf26e86475939d106e900a7da94d] and re-introduced on *20th Feb 2025* in [Commit 27f835e|https://github.com/apache/hadoop-site/commit/27f835ef0369a6bc95a12131af75c696cafb4b4c].\r\n\r\nI'm not sure how this has happened, but I see the following:\r\n* [layouts/partials/footer.html|https://github.com/apache/hadoop-site/blob/asf-site/layouts/partials/footer.html] references *__internal/google_analytics.html_*\r\n* I can't find *__internal/google_analytics.html_*\r\n* It looks like the safest action is to remove the reference to *google_analytics.html* in the *footer.html*\r\n\r\nI have created [PR #67|https://github.com/apache/hadoop-site/pull/67] to remove the reference in the footer, although I have not understood fully how the content for your website is generated and would appreciate if you could check for the correct resolution to this issue.\r\n\r\nThanks\r\n\r\nNiall\r\n\r\n[1] [https://privacy.apache.org/policies/website-policy.html]\r\n[2] [https://privacy.apache.org/faq/committers.html#can-i-use-google-analytics]\r\n\r\nh1. Matomo\r\n\r\nThe ASF hosts its own _*Matomo*_ instance to provide projects with analytics and you can request a tracking id for your project by sending a mail to *privacy AT apache.org.*\r\n * [https://p", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "StreamsGroupDescribe does not include a topology if the topology is not configured.\r\n\r\n \r\n\r\nThe problem is that it relies on `ConfiguredTopology` which may not be present or empty. Instead, we should use the unconfigured topology when the configured topology is not present", "output": "StreamsGroupDescribe result is missing topology when topology not configured"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When Hadoop is requested to perform operations against ADLS Gen2 storage, *AbfsRestOperation* attempts to obtain an access token from Microsoft. Underneath the hood, it uses a simple *java.net.HttpURLConnection* HTTP client.\r\n\r\nOccasionally, environments may run into network intermittent issues, including DNS-related {*}UnknownHostException{*}. Technically, the HTTP client throws *IOException* whose cause is {*}UnknownHostException{*}. *AzureADAuthenticator* in its turn catches {*}IOException{*}, sets *httperror = -1* and then checks whether the error is recoverable and can be retried. However, it's neither an instance of {*}MalformedURLException{*}, nor an instance of {*}FileNotFoundException{*}, nor a recoverable status code ({*}= 500 && != 501 && != 505{*}), hence a retry never occurs which is sensitive for our project causing problems with state recovery.\r\n\r\nThe final exception stack trace on the client side looks as follows (Apache Spark application, tenant ID is redacted):\r\n{code:java}\r\nJob aborted due to stage failure: Task 14 in stage 384.0 failed 4 times, most recent failure: Lost task 14.3 in stage 384.0 TID 3087 10.244.91.7 executor 29 : Status code: -1 error code: null error message: Auth failure: HTTP Error -1; url='https://login.microsoftonline.com/$TENANT_ID/oauth2/v2.0/token' AzureADAuthenticator.getTokenCall threw java.net.UnknownHostException: login.microsoftonline.com\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation AbfsRestOperation.java:321\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute AbfsRestOperation.java:263\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0 AbfsRestOperation.java:235\r\nat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation IOStatisticsBinding.java:494\r\nat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation IOStatisticsBinding.java:465\r\nat org.apache.hadoop.fs.azurebfs.serv", "output": "[ABFS] AzureADAuthenticator should be able to retry on UnknownHostException"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: add openFile() option to pass down footer length\nDescription: Add option, \"fs.option.openfile.footer.length\" to declare length of the footer in parquet/orc files.\r\n\r\nThis will tell prefetch/cache code how much to prefetch and keep.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [ABFS] Fix logging in FSDataInputStream buffersize as that is not used and confusing the customer\nDescription: Fix debug logs here. \r\n\r\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java#L350\nQ: cc [~anujmodi] [~manika137]", "output": "Thanks for reporting this [~mthakur] \r\nWe will plan for this work as soon as possible."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support `AUTO` Netty IO Mode\nDescription: \nQ: Issue resolved by pull request 52724\n[https://github.com/apache/spark/pull/52724]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Disallow create too many partitions in ZK mode\nDescription: Also when running in Zookeeper mode Kafka can go OOM if an attempt is made to create a topic with too many partitions, or to extend a topic with too many partitions.\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-17870\r\nhttps://issues.apache.org/jira/browse/KAFKA-19673\r\n\r\nI think it is worth to port also the constrain introduded in KRaft mode to the 3.9 version of Kafka.\nQ: https://github.com/apache/kafka/pull/20487", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update PR template to ask about AI contribution; other AI hardening\nDescription: Add a section at the bottom of the PR template to ask which AI tooling was used, if any, \r\n\r\n--- \r\n## AI\r\n\r\nIf an AI tool was used: \r\n[ ] The PR includes the phrase \"Generated by  where tool is  the AI tool used\r\n[ ] My use of AI contributions follows the ASF legal policy\r\nhttps://www.apache.org/legal/generative-tooling.html", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: kafka-consumer-groups.sh --describe --all-groups command times out on large clusters with many consumer groups\nDescription: Description:\r\nWhen running the following command in a Kafka cluster with a large number of consumer groups (over 380) and topics (over 500), the kafka-consumer-groups.sh --describe --all-groups operation consistently times out and fails to return results.\r\n\r\nCommand used:\r\n\r\n./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups\r\nObserved behavior:\r\nThe command fails with a TimeoutException, and no consumer group information is returned. The following stack trace is observed:\r\n\r\njava.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeConsumerGroups, deadlineMs=1753170317381, tries=1, nextAllowedTryMs=1753170317482) timed out at 1753170317382 after 1 attempt(s)\r\n    at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\r\n    at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\r\n    ...\r\nCaused by: org.apache.kafka.common.errors.Tim\nQ: Thank you for looking into this.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Thread.wait(0) unintentionally called under rare conditions in ExecuteGrpcResponseSender\nDescription: A bug in ExecuteGrpcResponseSender causes RPC streams to hang indefinitely when the configured deadline passes. The bug was introduced in [[PR|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253]|https://github.com/apache/spark/pull/49003/files#diff-d4629281431427e41afd6d3db6630bcfdbfdbf77ba74cf7e48a988c1b66c13f1L244-L253] during migration from System.currentTimeMillis() to System.nanoTime(), where an integer division error converts sub-millisecond timeout values to 0, triggering Java's wait(0) behavior (infinite wait).\r\nh2. Root Cause\r\nexecutionObserver.responseLock.wait(timeoutNs / NANOS_PER_MILLIS)  // ← BUG\r\n{*}The Problem{*}: When deadlineTimeNs < System.nanoTime() (deadline has passed):\r\n # Math.max(1, negative_value) clamps to 1 nanosecond\r\n\r\n # Math.min(progressInterval_ns, 1) remains 1 nanosecond\r\n\r\n # Integer division: 1 / 1,000,000 = 0 milliseconds\r\n\r\n # wait(0) in Java means *wait indefinitely until notified*\r\n\r\n # No notification arrives (execution already completed), thread hangs forever\r\n\r\nWhile one the loop conditions guards against deadlineTimeNs < System.nanoTime(), it isn’t sufficient as the deadline can elapse while inside the loop (the time is freshly fetched in the latter timeout calculation). The probability of occurence can exacerbated by GC pauses\r\nh2. Conditions Required for Bug to Trigger\r\n\r\nThe bug manifests when *all* of the following conditions are met:\r\n # *Reattachable ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [Bug Report] Thread leak in ABFS AbfsClientThrottlingAnalyzer\nDescription: Bug reported by Matt over common-dev discussion.\r\n\r\n> What seems to be the issue is that the timer tasks are cleaned up but\r\n> the timer threads themselves are never actually cleaned up. This will\r\n> eventually lead to an OOM since nothing is collecting these. I was\r\n> able to reproduce this locally in 3.3.6 and 3.4.1 but I believe that\r\n> it would affect any version that relies on autothrottling for ABFS.\r\n>\r\n> I was also able to make a quick fix as well as confirm a workaround --\r\n> the long term fix would be to include `timer.cancel()` and\r\n> `timer.purge()` in a method for AbfsClientThrottlingAnalyzer.java. The\r\n> short term workaround is to disable autothrottling and rely on Azure\r\n> to throttle the connections as needed with the below configuration.\nQ: mattkduran opened a new pull request, #7852:\nURL: https://github.com/apache/hadoop/pull/7852\n\n   \r\n   \r\n   ### Description of PR\r\n   The ABFS driver's auto-throttling feature (`fs.azure.enable.autothrottling=true`) creates Timer threads in AbfsClientThrottlingAnalyzer that are never properly cleaned up, leading to a memory leak that eventually causes OutOfMemoryError in long-running applications like Hive Metastore.\r\n   \r\n   #### Impact:\r\n   - Thread count grows indefinitely (observed >100,000 timer threads)\r\n   - Affects any long-running service that creates multiple ABFS filesystem instances\r\n   \r\n   #### Root Cause:\r\n   AbfsClientThrottlingAnalyzer creates Timer objects in its constructor but provides no mechanism to cancel them. When AbfsClient instances are closed, the associated timer threads continue running indefinitely.\r\n   \r\n   #### Solution\r\n   Implement proper resource cleanup by making the throttling components implement Closeable and ensuring timers are cancelled when ABFS clients are closed.\r\n   \r\n   #### Changes Made\r\n   1. AbfsClientThrottlingAnalyzer.java\r\n   \r\n   - Added: implements Closeable\r\n   - Added: close() method that calls timer.cancel() and timer.purge()\r\n   - Purpose: Ensures timer threads are properly terminated when analyzer is no longer needed\r\n   \r\n   2. AbfsThrottlingIntercept.java (Interface)\r\n   \r\n   - Added: extends Closeable\r\n   - Added: close() method signature\r\n   - Purpose: Establishes cleanup contract for all throttling intercept implementations\r\n   \r\n   3. AbfsClientThrottlingIntercept.java\r\n   \r\n   - Added: close() method that closes both readThrottler and writeThrottler\r\n   - Purpose: Coordinates cleanup of both read and write throttling analyzers\r\n   \r\n   4. AbfsNoOpThrottlingIntercept.java\r\n   \r\n   - Added: No-op close() method\r\n   - Purpose: Satisfies interface contract for no-op implementation\r\n   \r\n   5. AbfsClient.java\r\n   \r\n   - Added: IOUtils.cleanupWithLogger(LOG, intercept) in existing close() method\r\n   - Purpose: Integrates throttling cleanup into existing client resource management\r\n   \r\n   https://github.com/mattkduran/ABFSleaktest\r\n   https://www.mail-archive.com/common-dev@hadoop.apache.org/msg43483.html\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   #### Standalone Validation Tool\r\n   This fix was validated using a standalone reproduction and testing tool that directly exercises the ABFS auto-throttling components outside of a full Hadoop deployment.\r\n   Repository: [ABFSLeakTest](https://github.com/mattkduran/ABFSleaktest)\r\n   #### Testing Scope\r\n   \r\n   - Problem reproduction confirmed - demonstrates the timer thread leak\r\n   - Fix validation confirmed - proves close() method resolves the leak\r\n   - Resource cleanup verified - shows proper timer cancellation\r\n   - Limited integration testing - standalone tool, not full Hadoop test suite\r\n   \r\n   #### Test Results\r\n   Leak Reproduction Evidence\r\n   ```\r\n   # Without fix: Timer threads accumulate over filesystem creation cycles\r\n   Cycle    Total Threads    ABFS Timer Threads    Status\r\n   1        50->52          0->2                   LEAK DETECTED\r\n   50       150->152        98->100               LEAK GROWING  \r\n   200      250->252        398->400              LEAK CONFIRMED\r\n   \r\n   Final Analysis: 400 leaked timer threads named \"abfs-timer-client-throttling-analyzer-*\"\r\n   Memory Impact: ~90MB additional heap usage\r\n   \r\n   # Direct analyzer testing:\r\n   🔴 Without close(): +3 timer threads (LEAKED)\r\n   ✅ With close():    +0 timer threads (NO LEAK)\r\n   \r\n   ```\r\n   \r\n   #### Test Environment\r\n   \r\n   - Java Version: OpenJDK 11.0.x\r\n   - Hadoop Version: 3.3.6/3.4.1 (both affected)\r\n   - Test Duration: 200 filesystem creation/destruction cycles\r\n   - Thread Monitoring: JMX ThreadMXBean \r\n   # Fix effectiveness: 100% - no threads leaked when close() called\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ X ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "[~anujmodi] Got everything set up! It looks like I don't have permission to assign this issue to myself.\r\n\r\nPR filed here: \r\n\r\nhttps://github.com/apache/hadoop/pull/7852"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix flaky RemoteLogManagerTest#testCopyQuota\nDescription: see https://github.com/apache/kafka/actions/runs/16651849478/job/47166854957?pr=20269\nQ: In case it helps, saw this one today in a PR [here|https://github.com/apache/kafka/actions/runs/17769042747/job/50500642991?pr=20542], passes locally but times out every one and then on CI it seems, on:\r\n{code:java}\r\n// Verify the copy operation completes within the timeout, since it does not need to wait for quota availability\r\nassertTimeoutPreemptively(Duration.ofMillis(100), () -> task.copyLogSegmentsToRemote(mockLog));{code}", "output": "Thanks, [~lianetm]  Didn't yet have a time to take a look on this one\r\nbtw. if we're talking about flaky tests, could you please check this one? It was already waiting for like 3 months :)\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-19299?jql=resolution%20%3D%20Unresolved%20AND%20assignee%20%3D%20currentUser()%20AND%20project%20%3D%2012311720\r\n\r\n[https://github.com/apache/kafka/pull/19927]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade ASM to 9.9\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Streams | Fix order of arguments to assertEquals in unit test\nDescription: Part OF [KAFKA-19097|https://issues.apache.org/jira/browse/KAFKA-19097]\r\n\r\nThis sub-task is intended to fix the order of arguments passed in assertions in test cases within the streams package.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: processValues() must be declared as value-changing operation\nDescription: When adding `KStreams#processValues()` we missed to declare the operation as \"value changing\". This can lead to an \"incorrectly\" built topology.\r\n\r\nThe main problem is, that `processValues()` is the replacement of `transformValues()` which we removed with AK 4.0.0 release. Thus, if users rewrite existing programs from `transformValues()` to the new `processValues()` (what will be required when upgrading to 4.x release), they might observe this change as a regression.\r\n\r\nThe impact of the changed topology is, that local state is effectively lost, and must be restored from the changelog topic, resulting in downtime after an upgrade.\r\n\r\nNote: the bug does only surface, if topology optimization is used, in particular the \"merge repartition topics\" rewrite.\nQ: Do not break backward compatibility, we decided to not enable this fix by default in AK 4.0.1 and 4.1.1 releases, and will do some follow up work for AK 4.2.0 for allow us to enable the fix by default there.", "output": "mjsax opened a new pull request, #721:\nURL: https://github.com/apache/kafka-site/pull/721\n\n   (no comment)"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "`num.replica.fetchers` parameter does not enforce a lower bound, so both `num.replica.fetchers=0` or `num.replica.fetchers=-1` are valid when starting a broker. However, setting `num.replica.fetchers=-1` will result in only one fetcher being created, while setting `num.replica.fetchers=0` will cause a `java.lang.ArithmeticException: / by zero` during replication startup", "output": "add a lower bound to num.replica.fetchers"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [Java 25] few tests are failling (class: clients/org.apache.kafka.common.network.SslTransportLayerTest)\nDescription: {panel:title=Notes:|borderStyle=dashed|borderColor=#cccccc|titleBGColor=#f7d6c1|bgColor=#ffffce}\r\n * (!) this ticket blocks KAFKA-19664 *_Support building with Java 25 (LTS release)_*\r\n * (on) looks similar to: KAFKA-15117 *_SslTransportLayerTest.testValidEndpointIdentificationCN fails with Java 20 & 21_*{panel}\r\n*Reproducer:*\r\n * use this branch: [https://github.com/dejan2609/kafka/tree/KAFKA-19664] (i.e. this PR: [https://github.com/apache/kafka/pull/20561])\r\n * execute: *./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -i* and change Java versions\r\n * test results:\r\n ** {*}Java 17{*}: (/)\r\n ** J{*}ava 24{*}: (/)\r\n ** {*}Java 25{*}: 159 tests completed, *8 failed* (x)\r\n\r\n*Test results on Github CI:*\r\n!Screenshot from 2025-10-07 19-59-32.png!\r\n\r\n*Test results locally:*\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ git log -3 --oneline \r\na37616a1eb (HEAD -> KAFKA-19664, origin/KAFKA-19664) KAFKA-19664 Support building with Java 25 (LTS rele\nQ: You got that right [~gnarula] :) !\r\n\r\nThank you for the tip (y)", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: MergeScalarSubqueries code cleanup\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Native docker image authentication fails with SASL PLAIN\nDescription: I'm trying to use the native docker image for SASL PLAIN authentication.\r\n\r\nThe server starts okay but when I connect a client it emits an exception:\r\n\r\n\r\n \r\n{code:java}\r\n[2025-08-06 23:20:47,302] WARN [SocketServer listenerType=BROKER, nodeId=1] Unexpected error from /192.168.178.96 (channelId=192.168.178.96:9092-192.168.178.96:42552-1-1); closing connection (org.apache.kafka.common.network.Selector) java.lang.UnsupportedOperationException: Unable to find suitable Subject#doAs or Subject#callAs implementation at org.apache.kafka.common.internals.UnsupportedStrategy.createException(UnsupportedStrategy.java:40) ~[?:?] at org.apache.kafka.common.internals.UnsupportedStrategy.callAs(UnsupportedStrategy.java:58) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.lambda$callAs$1(CompositeStrategy.java:104) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.performAction(CompositeStrategy.java:78) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.callAs(CompositeStrategy.java:104) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.createSaslServer(SaslServerAuthenticator.java:208) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.handleKafkaRequest(SaslServerAuthenticator.java:533) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.authenticate(SaslServerAuthenticator.java:281) ~[?:?] at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:1", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add integration tests similar to {{ShareGroupDescribeRequestTest}} and {{ConsumerGroupDescribeRequestTest for StreamsGroupDescribeRequest}}", "output": "Add RPC-level integration tests for StreamsGroupDescribe"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: acknowledge Guava license on LimitInputStream\nDescription: When ASF projects copy 3rd party code into their code bases, they are meant to:\r\n* check the orginal license is Category A - https://www.apache.org/legal/resolved.html\r\n* keep the original source code headers\r\n* add something to their LICENSE that mentions the source file and what license is on it\r\n* if the 3rd party project is Apache licensed but has a NOTICE file, we need to copy its contents to our NOTICE\r\n* these requirements are only negated if the original code is submitted to the ASF project by the code's copyright holder (the individual or company that wrote the original code).\r\n\r\n* Hadoop copy https://github.com/apache/hadoop/blob/c357e435fd691b4c184b82325bef6d7c65e5f32b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LimitInputStream.java#L39\r\n* Original code has a Guava copyright that we probably need to keep https://github.com/google/guava/blob/master/guava/src/com/google/common/io/ByteStreams.java", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This UnregisterBrokerOptions extends from the UpdateFeaturesOptions, which is obviously wrong.\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/UnregisterBrokerOptions.java#L23]\r\n\r\n \r\n\r\nAnd that's why in KafkaAdminClientTest.java, we set timeout via `options.timeoutMs = 10;`. We should set it via timeoutMs(int) method gracefully like other tests.", "output": "Wrong generic type for UnregisterBrokerOptions"}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade gson due to security fixes\nDescription: not sure why https://github.com/google/gson/pull/2588 didn't attract a CVE\r\n\r\nlinked to https://issues.apache.org/jira/browse/HADOOP-19632 - nimbus-jose-jwt uses gson\nQ: hadoop-yetus commented on PR #7833:\nURL: https://github.com/apache/hadoop/pull/7833#issuecomment-3134038336\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 23s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  33m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m 49s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |  22m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   9m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   7m 49s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  51m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 37s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |  30m 48s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |  15m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  13m 51s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  13m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  19m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   9m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   8m 22s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  56m  3s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 339m 51s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 16s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 629m  8s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.federation.policies.amrmproxy.TestLocalityMulticastAMRMProxyPolicy |\r\n   |   | hadoop.crypto.key.kms.server.TestKMSAudit |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7833 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux 3d08bdf50c29 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 04292f85b2cde016fe4e4c12bfe4c8cd1c040dd4 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/testReport/ |\r\n   | Max. process+thread count | 3066 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7833/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This issue is caused with changes from https://issues.apache.org/jira/browse/KAFKA-14915\r\n\r\nBroker heap memory gets filled up and throws OOM error when remote reads are triggered for multiple partitions within a FETCH request. \r\n\r\nSteps to reproduce: \r\n\r\n1. Start a one node broker and configure LocalTieredStorage as remote storage. \r\n2. Create a topic with 5 partitions. \r\n3. Produce message and ensure that few segments are uploaded to remote.\r\n4. Start a consumer to read from those 5 partitions. Seek the offset to beginning for 4 partitions and to end for 1 partition. This is to simulate that the FETCH request read from both remote-log and local-log.\r\n5. The broker crashes with the OOM error.\r\n6. The DelayedRemoteFetch / RemoteLogReadResult references are being held by the purgatory, so the broker crashes.\r\n\r\ncc [~showuon] [~satish.duggana]", "output": "Parallel remote reads causes memory leak in broker"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile\nDescription: \nQ: pan3793 opened a new pull request, #7900:\nURL: https://github.com/apache/hadoop/pull/7900\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Rocky Linux is supposed to be a drop-in replacement for the discontinued CentOS. See more details at https://rockylinux.org/about\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   ```\r\n   $ ./start-build-env.sh rockylinux_8\r\n   ...\r\n   \r\n    _   _           _                    ______\r\n   | | | |         | |                   |  _  \\\r\n   | |_| | __ _  __| | ___   ___  _ __   | | | |_____   __\r\n   |  _  |/ _` |/ _` |/ _ \\ / _ \\| '_ \\  | | | / _ \\ \\ / /\r\n   | | | | (_| | (_| | (_) | (_) | |_) | | |/ /  __/\\ V /\r\n   \\_| |_/\\__,_|\\__,_|\\___/ \\___/| .__/  |___/ \\___| \\_(_)\r\n                                 | |\r\n                                 |_|\r\n   \r\n   This is the standard Hadoop Developer build environment.\r\n   This has all the right tools installed required to build\r\n   Hadoop from source.\r\n   \r\n   [chengpan@4af99dc981b9 hadoop]$\r\n   ```\r\n   \r\n   ```\r\n   $ mvn clean install -DskipTests -Pnative -Pyarn-ui -DskipShade\r\n   ...\r\n   [INFO] Reactor Summary for Apache Hadoop Main 3.5.0-SNAPSHOT:\r\n   [INFO]\r\n   [INFO] Apache Hadoop Main ................................. SUCCESS [  0.675 s]\r\n   [INFO] Apache Hadoop Build Tools .......................... SUCCESS [  1.518 s]\r\n   [INFO] Apache Hadoop Project POM .......................... SUCCESS [  0.651 s]\r\n   [INFO] Apache Hadoop Annotations .......................... SUCCESS [  0.686 s]\r\n   [INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  0.072 s]\r\n   [INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.077 s]\r\n   [INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  1.476 s]\r\n   [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  0.316 s]\r\n   [INFO] Apache Hadoop Auth ................................. SUCCESS [  2.002 s]\r\n   [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  0.535 s]\r\n   [INFO] Apache Hadoop Common ............................... SUCCESS [ 18.943 s]\r\n   [INFO] Apache Hadoop NFS .................................. SUCCESS [  1.071 s]\r\n   [INFO] Apache Hadoop KMS .................................. SUCCESS [  1.085 s]\r\n   [INFO] Apache Hadoop Registry ............................. SUCCESS [  1.282 s]\r\n   [INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.046 s]\r\n   [INFO] Apache Hadoop HDFS Client .......................... SUCCESS [ 10.259 s]\r\n   [INFO] Apache Hadoop HDFS ................................. SUCCESS [ 17.829 s]\r\n   [INFO] Apache Hadoop HDFS Native Client ................... SUCCESS [02:26 min]\r\n   [INFO] Apache Hadoop HttpFS ............................... SUCCESS [  1.772 s]\r\n   [INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  1.020 s]\r\n   [INFO] Apache Hadoop YARN ................................. SUCCESS [  0.041 s]\r\n   [INFO] Apache Hadoop YARN API ............................. SUCCESS [  5.855 s]\r\n   [INFO] Apache Hadoop YARN Common .......................... SUCCESS [  4.910 s]\r\n   [INFO] Apache Hadoop YARN Server .......................... SUCCESS [  0.038 s]\r\n   [INFO] Apache Hadoop YARN Server Common ................... SUCCESS [  3.994 s]\r\n   [INFO] Apache Hadoop YARN ApplicationHistoryService ....... SUCCESS [  1.619 s]\r\n   [INFO] Apache Hadoop YARN Timeline Service ................ SUCCESS [  1.207 s]\r\n   [INFO] Apache Hadoop YARN Web Proxy ....................... SUCCESS [  1.058 s]\r\n   [INFO] Apache Hadoop YARN ResourceManager ................. SUCCESS [ 10.150 s]\r\n   [INFO] Apache Hadoop YARN NodeManager ..................... SUCCESS [ 28.435 s]\r\n   [INFO] Apache Hadoop YARN Server Tests .................... SUCCESS [  1.134 s]\r\n   [INFO] Apache Hadoop YARN Client .......................... SUCCESS [  1.959 s]\r\n   [INFO] Apache Hadoop MapReduce Client ..................... SUCCESS [  0.616 s]\r\n   [INFO] Apache Hadoop MapReduce Core ....................... SUCCESS [  4.440 s]\r\n   [INFO] Apache Hadoop MapReduce Common ..................... SUCCESS [  2.062 s]\r\n   [INFO] Apache Hadoop MapReduce Shuffle .................... SUCCESS [  1.414 s]\r\n   [INFO] Apache Hadoop MapReduce App ........................ SUCCESS [  2.901 s]\r\n   [INFO] Apache Hadoop MapReduce HistoryServer .............. SUCCESS [  1.887 s]\r\n   [INFO] Apache Hadoop MapReduce JobClient .................. SUCCESS [  3.319 s]\r\n   [INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [  1.954 s]\r\n   [INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  1.113 s]\r\n   [INFO] Apache Hadoop Federation Balance ................... SUCCESS [  1.234 s]\r\n   [INFO] Apache Hadoop HDFS-RBF ............................. SUCCESS [  5.733 s]\r\n   [INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop YARN SharedCacheManager .............. SUCCESS [  0.886 s]\r\n   [INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SUCCESS [  0.866 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Backend ... SUCCESS [  0.035 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Common .... SUCCESS [  1.291 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Client .... SUCCESS [  1.709 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Servers ... SUCCESS [  0.036 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Server 2.5  SUCCESS [  1.677 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase tests ..... SUCCESS [  1.493 s]\r\n   [INFO] Apache Hadoop YARN Router .......................... SUCCESS [  1.912 s]\r\n   [INFO] Apache Hadoop YARN TimelineService DocumentStore ... SUCCESS [  1.018 s]\r\n   [INFO] Apache Hadoop YARN GlobalPolicyGenerator ........... SUCCESS [  1.175 s]\r\n   [INFO] Apache Hadoop YARN Applications .................... SUCCESS [  0.036 s]\r\n   [INFO] Apache Hadoop YARN DistributedShell ................ SUCCESS [  1.166 s]\r\n   [INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SUCCESS [  0.689 s]\r\n   [INFO] Apache Hadoop YARN Services ........................ SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop YARN Services Core ................... SUCCESS [  2.393 s]\r\n   [INFO] Apache Hadoop YARN Services API .................... SUCCESS [  1.552 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog ............. SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog Webapp ...... SUCCESS [ 10.674 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog Docker Image  SUCCESS [  0.046 s]\r\n   [INFO] Apache Hadoop YARN Application MaWo ................ SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop YARN Application MaWo Core ........... SUCCESS [  0.781 s]\r\n   [INFO] Apache Hadoop YARN Site ............................ SUCCESS [  0.036 s]\r\n   [INFO] Apache Hadoop YARN Registry ........................ SUCCESS [  0.439 s]\r\n   [INFO] Apache Hadoop YARN UI .............................. SUCCESS [ 51.592 s]\r\n   [INFO] Apache Hadoop YARN CSI ............................. SUCCESS [  2.776 s]\r\n   [INFO] Apache Hadoop YARN Project ......................... SUCCESS [  1.155 s]\r\n   [INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SUCCESS [  0.854 s]\r\n   [INFO] Apache Hadoop MapReduce NativeTask ................. SUCCESS [ 19.795 s]\r\n   [INFO] Apache Hadoop MapReduce Uploader ................... SUCCESS [  0.777 s]\r\n   [INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  1.427 s]\r\n   [INFO] Apache Hadoop MapReduce ............................ SUCCESS [  1.079 s]\r\n   [INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [  1.533 s]\r\n   [INFO] Apache Hadoop Client Aggregator .................... SUCCESS [  0.642 s]\r\n   [INFO] Apache Hadoop Dynamometer Workload Simulator ....... SUCCESS [  1.110 s]\r\n   [INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SUCCESS [  2.098 s]\r\n   [INFO] Apache Hadoop Dynamometer Block Listing Generator .. SUCCESS [  0.994 s]\r\n   [INFO] Apache Hadoop Dynamometer Dist ..................... SUCCESS [  1.063 s]\r\n   [INFO] Apache Hadoop Dynamometer .......................... SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop Archives ............................. SUCCESS [  0.908 s]\r\n   [INFO] Apache Hadoop Archive Logs ......................... SUCCESS [  1.125 s]\r\n   [INFO] Apache Hadoop Rumen ................................ SUCCESS [  1.466 s]\r\n   [INFO] Apache Hadoop Gridmix .............................. SUCCESS [  1.450 s]\r\n   [INFO] Apache Hadoop Data Join ............................ SUCCESS [  1.002 s]\r\n   [INFO] Apache Hadoop Extras ............................... SUCCESS [  0.995 s]\r\n   [INFO] Apache Hadoop Pipes ................................ SUCCESS [  3.333 s]\r\n   [INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [  4.599 s]\r\n   [INFO] Apache Hadoop Kafka Library support ................ SUCCESS [  0.636 s]\r\n   [INFO] Apache Hadoop Azure support ........................ SUCCESS [  4.082 s]\r\n   [INFO] Apache Hadoop Aliyun OSS support ................... SUCCESS [  0.878 s]\r\n   [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  1.312 s]\r\n   [INFO] Apache Hadoop Resource Estimator Service ........... SUCCESS [  0.868 s]\r\n   [INFO] Apache Hadoop Azure Data Lake support .............. SUCCESS [  0.844 s]\r\n   [INFO] Apache Hadoop Image Generation Tool ................ SUCCESS [  0.893 s]\r\n   [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [  0.624 s]\r\n   [INFO] Apache Hadoop OpenStack support .................... SUCCESS [  0.042 s]\r\n   [INFO] Apache Hadoop Common Benchmark ..................... SUCCESS [  9.870 s]\r\n   [INFO] Apache Hadoop Compatibility Benchmark .............. SUCCESS [  0.598 s]\r\n   [INFO] Apache Hadoop Tools ................................ SUCCESS [  0.032 s]\r\n   [INFO] Apache Hadoop Client API ........................... SUCCESS [  0.851 s]\r\n   [INFO] Apache Hadoop Client Runtime ....................... SUCCESS [  0.639 s]\r\n   [INFO] Apache Hadoop Client Packaging Invariants .......... SUCCESS [  0.373 s]\r\n   [INFO] Apache Hadoop Client Test Minicluster .............. SUCCESS [  0.690 s]\r\n   [INFO] Apache Hadoop Client Packaging Invariants for Test . SUCCESS [  0.171 s]\r\n   [INFO] Apache Hadoop Client Packaging Integration Tests ... SUCCESS [  0.639 s]\r\n   [INFO] Apache Hadoop Distribution ......................... SUCCESS [  0.447 s]\r\n   [INFO] Apache Hadoop Client Modules ....................... SUCCESS [  0.032 s]\r\n   [INFO] Apache Hadoop Tencent COS Support .................. SUCCESS [  0.640 s]\r\n   [INFO] Apache Hadoop OBS support .......................... SUCCESS [  1.068 s]\r\n   [INFO] Apache Hadoop Volcano Engine Services support ...... SUCCESS [  1.601 s]\r\n   [INFO] Apache Hadoop Cloud Storage ........................ SUCCESS [  0.401 s]\r\n   [INFO] Apache Hadoop Cloud Storage Project ................ SUCCESS [  0.033 s]\r\n   [INFO] ------------------------------------------------------------------------\r\n   [INFO] BUILD SUCCESS\r\n   [INFO] ------------------------------------------------------------------------\r\n   [INFO] Total time:  07:35 min\r\n   [INFO] Finished at: 2025-08-26T10:05:06Z\r\n   [INFO] ------------------------------------------------------------------------\r\n   [chengpan@4af99dc981b9 hadoop]$\r\n   ```\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7900:\nURL: https://github.com/apache/hadoop/pull/7900#issuecomment-3223525111\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   0m 21s |  |  Dockerfile '/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-7900/rockylinux-8/src/dev-support/docker/Dockerfile_rockylinux_8' not found.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7900 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7900/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix RBAC to allow `Spark` driver to create `StatefulSet`\nDescription: \nQ: Issue resolved by pull request 389\n[https://github.com/apache/spark-kubernetes-operator/pull/389]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix flaky RestoreIntegrationTest#shouldInvokeUserDefinedGlobalStateRestoreListener\nDescription: org.opentest4j.AssertionFailedError: Condition not met within timeout 60000. Timed out waiting for active restoring task ==> expected:  but was: \r\n\tat app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n\tat app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n\tat app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)\r\n\tat app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)\r\n\tat app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214)\r\n\tat app//org.apache.kafka.test.TestUtils.lambda$waitForCondition$4(TestUtils.java:447)\r\n\tat app//org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:495)\r\n\tat app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:444)\r\n\tat app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:428)\r\n\tat app//org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:418)\r\n\tat app//org.a\nQ: We recently merged [https://github.com/apache/kafka/pull/20347] – w/o looking into details, I assume this change might be the root cause.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Remove JUnit4 Dependency\nDescription: Due to the extensive JUnit4 dependencies in the Hadoop modules, we will attempt to remove JUnit4 dependencies on a module-by-module basis.\nQ: slfan1989 opened a new pull request, #7798:\nURL: https://github.com/apache/hadoop/pull/7798\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19617. [JDK17] Remove JUnit4 Dependency(YARN).\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Junit Test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "slfan1989 opened a new pull request, #7799:\nURL: https://github.com/apache/hadoop/pull/7799\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19617 - [JDK17] Remove JUnit4 Dependency(MapReduce).\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Junit Test.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add addition E2E tests for RTM\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864\nDescription: *CVE-2025-53864:*\r\n\r\nConnect2id Nimbus JOSE + JWT before 10.0.2 allows a remote attacker to cause a denial of service via a deeply nested JSON object supplied in a JWT claim set, because of uncontrolled recursion. NOTE: this is independent of the Gson 2.11.0 issue because the Connect2id product could have checked the JSON object nesting depth, regardless of what limits (if any) were imposed by Gson.\r\n\r\nSeverity: 6.9 (medium)", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add dual-stack/IPv6 Support to HttpServer2\nDescription: To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp class to bind to an IPv6 address. WebApp uses the HttpServer2, and adding the IPv6 connector to this class makes the solution more elegant.\r\n\r\nTo enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host.\r\nWhen the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6.\r\nWhen java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors.\r\nTo disable IPv4, you need to configure the OS at the system level.\nQ: ferdelyi opened a new pull request, #7979:\nURL: https://github.com/apache/hadoop/pull/7979\n\n   To enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host. When the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6. When java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors. To disable IPv4, you need to configure the OS at the system level.\r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7979:\nURL: https://github.com/apache/hadoop/pull/7979#issuecomment-3303626071\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 55s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  1s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 40s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 14s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 1 new + 68 unchanged - 0 fixed = 69 total (was 68)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 52s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 44s |  |  the patch passed  |\r\n   | -1 :x: |  shadedclient  |  42m 42s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 33s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  5s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 251m 12s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7979 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 69abc2152550 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 05144e7770fd55d4ed400b5ee8d71ea02d4d37b6 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/testReport/ |\r\n   | Max. process+thread count | 3098 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7979/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The file permission denial error message in Linux systems end with \"(Permission denied)\" particularly.\r\nHowever, an error message in the same scenario on Windows ends with \"(Access is denied)\" error.\r\n\r\nThis results in a bug in *org.apache.hadoop.fs.ChecksumFileSystem.ChecksumFSInputChecker* and also leads to a unit test failure *org.apache.hadoop.fs.TestFsShellCopy#testPutSrcFileNoPerm*.\r\n\r\nThus, we need to make the appropriate check in accordance with the platform.", "output": "Fix file permission errors as per the platform"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Release Spark Kubernetes Operator 0.7.0\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update PR template to ask about AI contribution; other AI hardening\nDescription: Add a section at the bottom of the PR template to ask which AI tooling was used, if any, \r\n\r\n--- \r\n## AI\r\n\r\nIf an AI tool was used: \r\n[ ] The PR includes the phrase \"Generated by  where tool is  the AI tool used\r\n[ ] My use of AI contributions follows the ASF legal policy\r\nhttps://www.apache.org/legal/generative-tooling.html\nQ: and someone with the permissions to edit apache/hadoop should exclude auth-keys.xml files.", "output": "+ add template copilot instructions for the project\r\n\r\nhttps://docs.github.com/en/copilot/how-tos/custom-instructions/adding-repository-custom-instructions-for-github-copilot :\r\n\r\n* we use maven for builds\r\n* junit5 for tests\r\n* require tests with all new code"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `sbt-jupiter-interface` to 0.17.0\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Reported by [~enigma25] :\r\nIn [this code snippet](https://github.com/Azure/azure-data-lake-store-java/blob/26eca936da60286aa70b0dd7823ff5e627987bc4/src/main/java/com/microsoft/azure/datalake/store/oauth2/AzureADAuthenticator.java#L252-L293) from AzureADAuthenticator, the `conn` is being returned as null due to a (possibly) corrupted connection between Azure Data Lake Store Client and AAD while fetching AAD token. The code snippet linked, runs into an NPE. I wanted to know from the maintainers and community if this bug/symptom has been seen by them earlier and what might be the best way to handle this? Secondly, I wanted to know the right place for the fix too - whether it should be the application code or the SDK code itself should handle such NPEs and fail more gracefully?\r\nOpen to thoughts and comments.\r\nCheers,\r\nNikhil\r\n \r\n \r\n```\r\njava.util.concurrent.ExecutionException: java.lang.NullPointerException: Cannot invoke \"java.io.InputStream.read(byte[], int, int)\" because \"inStream\" is null\r\nat org.apache.kafka.connect.util.ConvertingFutureCallback.result(ConvertingFutureCallback.java:135)\r\nat org.apache.kafka.connect.util.ConvertingFutureCallback.get(ConvertingFutureCallback.java:122)\r\nat org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource.validateConfigs(ConnectorPluginsResource.java:129)\r\nat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\nat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\r\nat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatche", "output": "[ABFS]: Throw HTTPException when AAD token fetch fails "}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Mark KIP-932 interfaces as stable for GA release\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ExplodeBase.eval Iterate directly on input\nDescription: It was noticed that `ExplodeBase.eval` returns an IterableOnce[InternalRow].  The current implementation creates a pre-allocated array, populates the array appropriately, and returns the Array.  This works as the is an implicit conversion from Array to IterableOnce.\r\n\r\nHowever Allocating and populating an array does not seem to provide benefits over exposing an iterator over the input data type.\r\n\r\nA proposed PR removes the creation and population of the array and instead returns a IterableOnce object that iterates over the underlying input.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "During release we now merge the RC tag into the release branch. This means the release branch has a real release number instead of -SNAPSHOT.\r\n\r\nThe Streams upgrade system tests expect -SNAPSHOT and thus fail to run.\r\n\r\nWe should either only merge RC tags once the vote pass or change the tests to use  instead of -SNAPSHOT.\r\n\r\nSee:\r\n- https://lists.apache.org/thread/v6873zlvp5bl9qf5zsr3g904nxdynr74\r\n- https://lists.apache.org/thread/y3rh1nnxqz6tc4brnyfbpbrx2gy655y6", "output": "Issues running Streams system tests on release candidates"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add Connect JDBC module\nDescription: \nQ: Issue resolved by pull request 52619\n[https://github.com/apache/spark/pull/52619]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In `ServiceStateModel`, `enterState` can set `state` to a state that is different from `expectedState`, which causes an exception in `ensureCurrentState`.", "output": "ServiceStateException thrown due to unexpected state"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Parallel remote reads causes memory leak in broker\nDescription: This issue is caused with changes from https://issues.apache.org/jira/browse/KAFKA-14915\r\n\r\nBroker heap memory gets filled up and throws OOM error when remote reads are triggered for multiple partitions within a FETCH request. \r\n\r\nSteps to reproduce: \r\n\r\n1. Start a one node broker and configure LocalTieredStorage as remote storage. \r\n2. Create a topic with 5 partitions. \r\n3. Produce message and ensure that few segments are uploaded to remote.\r\n4. Start a consumer to read from those 5 partitions. Seek the offset to beginning for 4 partitions and to end for 1 partition. This is to simulate that the FETCH request read from both remote-log and local-log.\r\n5. The broker crashes with the OOM error.\r\n6. The DelayedRemoteFetch / RemoteLogReadResult references are being held by the purgatory, so the broker crashes.\r\n\r\ncc [~showuon] [~satish.duggana]", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `sbt-jupiter-interface` to 0.17.0\nDescription: \nQ: Issue resolved by pull request 52562\n[https://github.com/apache/spark/pull/52562]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix DSv2 in PushVariantIntoScan by adding SupportsPushDownVariants\nDescription: This goes to add DSv2 support to the optimization rule PushVariantIntoScan. The PushVariantIntoScan rule only supports DSv1 Parquet (ParquetFileFormat) source. It limits the effectiveness of variant type usage on DSv2.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In JUnit 4, {{org.junit.internal.AssumptionViolatedException}} is used to indicate assumption failure and skip the test. However, {{AssumptionViolatedException}} is an implementation in JUnit 4, and in JUnit 5, we can use {{TestAbortedException}} to replace {{{}AssumptionViolatedException{}}}.\r\n\r\n{{TestAbortedException}} is used to indicate that a test has been aborted, and it can be used to replace {{{}AssumptionViolatedException{}}}. However, it is not directly related to assumption failure and is more commonly used in situations where the test needs to be aborted during execution.", "output": "Replace AssumptionViolatedException with TestAbortedException"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Restore Subject propagation semantics for Java 22+\nDescription: Java 22 breaks Subject propagation for new Threads (when SecurityManager is not enabled).\r\n\r\nPreviously, the Subject set by Subject.doAs() / Subject.callAs() automatically propagated to any new Threads created (via new Thread(), not Executors).\r\n\r\nWith JDK22, this is no longer the case, new Threads do NOT inherit the Subject.\r\n\r\nAs Hadoop heavily relies on the original behavior, we somehow need to solve this problem.\nQ: hadoop-yetus commented on PR #7892:\nURL: https://github.com/apache/hadoop/pull/7892#issuecomment-3213228211\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 21s |  |  https://github.com/apache/hadoop/pull/7892 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7892/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7892:\nURL: https://github.com/apache/hadoop/pull/7892#issuecomment-3213242905\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 22s |  |  https://github.com/apache/hadoop/pull/7892 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7892/2/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Keep coverage data when running pip tests\nDescription: \nQ: Issue resolved by pull request 51552\n[https://github.com/apache/spark/pull/51552]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Set Curator Connection Timeout\nDescription: Curator 5.2.0 has a default \"connection timeout\" of 15s:\r\n\r\n[https://github.com/apache/curator/blob/apache-curator-5.2.0/curator-framework/src/main/java/org/apache/curator/framework/CuratorFrameworkFactory.java#L63]\r\n\r\nAnd it will throw a warning if the Zookeeper session timeout is less than the Curator connection timeout:\r\n\r\n[https://github.com/apache/curator/blob/apache-curator-5.2.0/curator-client/src/main/java/org/apache/curator/CuratorZookeeperClient.java#L117-L120]\r\n\r\nThe Hadoop default for ZK timeout is set to 10s:\r\n\r\n[https://github.com/apache/hadoop/blob/0dd9bf8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeys.java#L414-L416]\r\n\r\nWhich means without setting the \"connection timeout\" a default installation will keep warning about the connection timeout for Curator being lower than the requested session timeout. This sets both timeout values to override the Curator default.\r\n\r\nAnother option is to change the default Hadoop ZK timeout f\nQ: slfan1989 commented on PR #7426:\nURL: https://github.com/apache/hadoop/pull/7426#issuecomment-2675879198\n\n   @xyu Thanks for the contribution! LGTM.", "output": "Hexiaoqiao commented on PR #7426:\nURL: https://github.com/apache/hadoop/pull/7426#issuecomment-2684198599\n\n   Thanks @xyu . How about add another single configure item about connection timeout here? Actually for zookeeper, the connection timeout is calculated by `sessionTimeout` rather than equal to `sessionTimeout`."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Spark aggregation is incorrect (floating point error)\nDescription: {code:java}\r\nList data = Arrays.asList(\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95)\r\n);\r\n \r\nStructType schema = DataTypes.createStructType(new StructField[] {\r\nDataTypes.createStructField(\"timestamp\", DataTypes.StringType, false),\r\nDataTypes.createStructField(\"id\", DataTypes.StringType, false),\r\nDataTypes.createStructField(\"value\", DataTypes.DoubleType, false)\r\n});\r\n \r\nDataset df = spark.createDataFrame(data, schema);\r\n \r\n// Show the input data\r\nSystem.out.println(\"Input data:\");\r\ndf.show();\r\n \r\n// Perform the aggregation\r\nDataset result = df.groupBy(\"id\")\r\n.agg(\r\navg(\"value\").as(METADATA_COL_METRICVALUE),\r\nsum(\"value\").as(METADATA_COL_SUM_VALUE)\r\n);\r\n \r\n// Show the results\r\nSystem.out.println(\"Aggregation results:\");\r\nresult.show();\r\n \r\n// Collect the results\r\nList results = result.collectAsList();\r\n \r\n// Print the results\r\nSystem.out.println(\"Number of results: \" + results.size());\r\nfor (Row row : results) {\r\nSystem.out.println(\"Metric value: \" + row.getDouble(row.fieldIndex(M", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add missing metrics for document tiered storage\r\n * \r\nkafka.log.remote:type=RemoteLogManager,name=RemoteLogReaderFetchRateAndTimeMs：Introduced in [KIP-1018|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1018%3A+Introduce+max+remote+fetch+timeout+config+for+DelayedRemoteFetch+requests]\r\n * \r\nkafka.server:type=DelayedRemoteListOffsetsMetrics,name=ExpiresPerSec,topic=([-.\\w]+),partition=([0-9]+)：Introduced in [KIP-1075|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1075%3A+Introduce+delayed+remote+list+offsets+purgatory+to+make+LIST_OFFSETS+async]", "output": "Add missing metrics for document tiered storage"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Wrap IllegalArgumentException with proper error code for invalid datetime patterns\nDescription: When `TO_TIMESTAMP` is called with an invalid datetime pattern (e.g., `'yyyyMMddHHMIss'` with invalid letter `'I'`), the `IllegalArgumentException` from Java's `DateTimeFormatterBuilder` escapes during constant folding optimization, resulting in a raw exception without proper error codes or `SQLSTATE`.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove command-line arguments deprecated to implement KIP-1147\nDescription: Several arguments were replaced in [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1147%3A+Improve+consistency+of+command-line+arguments], initially deprecating in 4.2. The deprecated arguments will be removed in 5.0.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Enhance DSV2 partition filtering using catalyst expression\nDescription: Currently, Spark converts Catalyst Expression to either Filter or Predicate and pushes it to DSV2 via SupportsPushdownFilters and SupportsPushdownV2Filters API's.\r\n\r\nHowever, some Spark filters may not convert cleanly.  For example, trim(part_col) = 'a'.  There are cases where DSV2 can return the exact partition value(s) to spark for its InputPartition, and Spark can use the original catalyst expression for filtering.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "KIP-1102 has introduced a new error code to instruct clients to rebootstrap. This error has been introduced to help proxies to instruct the clients to reboostrap when brokers have changed.\r\n\r\nHowever, the current implementation in group consumers leave the connections to the group coordinator intact.\r\n\r\nAs a result, when doing a cluster failover:\r\n - the clients receive REBOOTSTRAP_REQUIRED\r\n - the broker connections are reboostrapped, connections to the primary cluster are closed and connections to the secondary clusters are opened\r\n\r\n - the consumers are still connected to the group coordinator of the primary cluster\r\n\r\n==> the consumers are:\r\n - fetching data from the secondary cluster\r\n - sending hearbeats to the primary cluster\r\n\r\nThe consumers are in an inconsistent state", "output": "REBOOTSTRAP_REQUIRED leave connections to group coordinator untouched"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix incorrect link from current3 of hadoop-site\nDescription: Fix hadoop site incorrect link which reported by https://lists.apache.org/thread/ozvkh0x7qdr8d3vr0tbm5g7jx1jprbct.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).", "output": "Make HadoopArchives support human-friendly units about blocksize and partsize."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Readme.md has a complete command line example for building Hadoop on Windows.\r\nHowever, that one doesn't work in the docker image, because that doesn't have full Visual Studio install, and misses the expected programs.\r\n\r\nAdd to README.md that the *-Dskip.platformToolsetDetection* maven option is needed when building from the docker image.", "output": "Document skip.platformToolsetDetection option in BUILDING.txt"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Build multi-arch hadoop image\nDescription: Build {{apache/hadoop}} Docker image for both amd64 and arm64.\nQ: adoroszlai opened a new pull request, #8023:\nURL: https://github.com/apache/hadoop/pull/8023\n\n   ## What changes were proposed in this pull request?\r\n   \r\n   - Update `Dockerfile` (on branch `docker-hadoop-3.4.2-lean`) to support building for `arm64`, too.\r\n       - Use `ghcr.io/apache/hadoop-runner:jdk11-u2204` as base, because `apache/hadoop-runner:latest` only has `amd64` image available.\r\n       - Use `TARGETPLATFORM` to decide which tarball to use.\r\n       - Create args for version and flavor, replacing URL.\r\n   - Update the `build-hadoop-image` workflow to create multi-arch images.\r\n   - Add build-arg `BASE_URL` to allow using mirrors (for faster local build).\r\n   - Replace deprecated `ENV HADOOP_CONF_DIR ` syntax.\r\n   \r\n   https://issues.apache.org/jira/browse/HADOOP-19723\r\n   \r\n   ## How was this patch tested?\r\n   \r\n   Workflow [run](https://github.com/adoroszlai/hadoop/actions/runs/18377713437) in my fork created multi-arch [image](https://github.com/adoroszlai/hadoop/pkgs/container/hadoop/539671710?tag=HADOOP-19723).\r\n   \r\n   ```\r\n   #8 0.060 Building for linux/amd64\r\n   ...\r\n   #8 0.060 + export HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2-lean.tar.gz\r\n   ...\r\n   \r\n   #10 0.076 Building for linux/arm64\r\n   ...\r\n   #10 0.077 + export HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2-aarch64-lean.tar.gz\r\n   ```\r\n   \r\n   Tested on both amd64 and arm64 platforms.\r\n   \r\n   ```\r\n   $ docker run -it --rm ghcr.io/adoroszlai/hadoop:HADOOP-19723 bash -c \"uname -a; hadoop version\"\r\n   Linux cdb5cdd5ace9 6.8.0-65-generic #68~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 15 18:06:34 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n   Hadoop 3.4.2\r\n   Source code repository https://github.com/apache/hadoop.git -r 84e8b89ee2ebe6923691205b9e171badde7a495c\r\n   Compiled by ahmarsu on 2025-08-20T10:30Z\r\n   Compiled on platform linux-x86_64\r\n   Compiled with protoc 3.23.4\r\n   From source with checksum fa94c67d4b4be021b9e9515c9b0f7b6\r\n   This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.4.2.jar\r\n   ```\r\n   \r\n   ```\r\n   $ docker run -it --rm ghcr.io/adoroszlai/hadoop:HADOOP-19723 bash -c \"uname -a; hadoop version\"\r\n   Linux 9a1237ba8fbc 6.10.14-linuxkit #1 SMP Thu Oct 24 19:28:55 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux\r\n   Hadoop 3.4.2\r\n   Source code repository https://github.com/apache/hadoop.git -r e1c0dee881820a4d834ec4a4d2c70d0d953bb933\r\n   Compiled by ahmar on 2025-08-07T15:32Z\r\n   Compiled on platform linux-aarch_64\r\n   Compiled with protoc 3.23.4\r\n   From source with checksum fa94c67d4b4be021b9e9515c9b0f7b6\r\n   This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.4.2.jar\r\n   ```", "output": "slfan1989 commented on PR #8023:\nURL: https://github.com/apache/hadoop/pull/8023#issuecomment-3385915162\n\n   LGTM."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support Spatial Reference System mapping in PySpark\nDescription: \nQ: Issue resolved by pull request 52799\n[https://github.com/apache/spark/pull/52799]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Surefire upgrade leads to increased report output, can cause Jenkins OOM\nDescription: The Surefire upgrade to 3.3+ added enableOutErrElements which defaults to true and includes system-out and system-err in the xml artifacts. This significantly increases their size: ~45KB to ~1MB in many cases. In my test environment, that leads to\r\n{code}\r\nRecording test results\r\nERROR: Step ‘Publish JUnit test result report’ aborted due to exception: \r\njava.lang.OutOfMemoryError: Java heap space\r\n{code}\r\n\r\nCapturing stdout seems useful when doing ad-hoc testing or smaller test runs. I'd propose adding a profile to set enableOutErrElements=false for CI environments where that extra output can cause problems.\nQ: MikaelSmith opened a new pull request, #7998:\nURL: https://github.com/apache/hadoop/pull/7998\n\n   ### Description of PR\r\n   \r\n   Adds the `quiet-surefire` profile to set enableOutErrElements=false for maven-surefire-plugin. This restores the behavior prior to Surefire 3.3 that stdout/stderr are not included in the TEST-..xml file for passing tests. The newer default behavior results in much larger TEST-*.xml files that can be a problem for CI tools processing them.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran `mvn clean test -Dtest=TestHttpServer` and `mvn clean test -Dtest=TestHttpServer -Pquiet-surefire` and compared size and contents of hadoop-common-project/hadoop-common/target/surefire-reports/TEST-org.apache.hadoop.http.TestHttpServer.xml.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7998:\nURL: https://github.com/apache/hadoop/pull/7998#issuecomment-3336586704\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  56m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  98m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 17s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 163m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7998 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 6c02dd609cd9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 895391e2c52eadd071997eacaaf7bb2f2af8be30 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [pb-upgrade] RouterAdminProtocolTranslatorPB use ShadedProtobufHelper\nDescription: RouterAdminProtocolTranslatorPB should also use ShadedProtobufHelper.\nQ: hfutatzhanghb opened a new pull request, #7407:\nURL: https://github.com/apache/hadoop/pull/7407\n\n   ### Description of PR\r\n   RouterAdminProtocolTranslatorPB should use ShadedProtobufHelper.", "output": "hadoop-yetus commented on PR #7407:\nURL: https://github.com/apache/hadoop/pull/7407#issuecomment-2670667030\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m  1s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  42m  3s |  |  hadoop-hdfs-rbf in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 184m 59s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7407/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7407 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 51178602b2e3 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 037618232be0184eb90aa942991e58b2c6d854dd |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7407/1/testReport/ |\r\n   | Max. process+thread count | 3372 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-rbf U: hadoop-hdfs-project/hadoop-hdfs-rbf |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7407/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove all static classes in Field except TaggedFieldsSection\nDescription: All static classes in Field except TaggedFieldsSection are not really being used. We should remove them.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add RPC-level integration tests for StreamsGroupDescribe\nDescription: Add integration tests similar to {{ShareGroupDescribeRequestTest}} and {{ConsumerGroupDescribeRequestTest for StreamsGroupDescribeRequest}}", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Spark uses FrequentItemsSketch of Apache DataSketches in the approx_top_k function, which does not consider NULL values by itself ([https://github.com/apache/datasketches-java/blob/main/src/main/java/org/apache/datasketches/frequencies/FrequentItemsSketch.java#L587).] However, NULL value could be meaningful in some use cases and users might want to include NULL in the approx_top_k output. Therefore, this ticket aims to add a nullCounter associated with the FrequentItemsSketch to count for NULL in the approx_top_k aggregation.", "output": "Let approx_top_k handle NULLs"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix Maven download link\nDescription: The Jenkins CI for Windows Nightly build is failing since it's unable to download Apache Maven -\r\n\r\n{code}\r\n00:32:18  Step 14/78 : RUN powershell Invoke-WebRequest -URI https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.zip -OutFile $Env:TEMP\\apache-maven-3.8.8-bin.zip\r\n00:32:18   ---> Running in a47cc5638ee5\r\n00:32:22  \u001b[91mInvoke-WebRequest : \r\n00:32:22  \u001b[0m\u001b[91m\r\n00:32:22  \u001b[0m\u001b[91m404 Not Found\r\n00:32:22  \u001b[0m\u001b[91m\r\n00:32:22  \u001b[0m\u001b[91mNot Found\r\n00:32:22  \u001b[0m\u001b[91mThe requested URL was not found on this server.\r\n00:32:22  \u001b[0m\u001b[91m\r\n00:32:22  \u001b[0m\u001b[91mAt line:1 char:1\r\n00:32:22  + Invoke-WebRequest -URI https://downloads.apache.org/maven/maven-3/3.8 ...\r\n00:32:22  \u001b[0m\u001b[91m+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n00:32:22  \u001b[0m\u001b[91m    + CategoryInfo          : InvalidOperation: (System.Net.HttpWebRequest:Htt \r\n00:32:22  \u001b[0m\u001b[91m   pWebRequest) [Invoke-WebRequest], WebException\r\n00:32:22  \u001b[0m\u001b[91m    + FullyQualifiedErrorId : WebCmdletWebResponseException,Microsoft.PowerShe \r\n00:32:22     ll.Commands.InvokeWebRequestCommand\r\n00:32:38  \u001b[0mThe command 'cmd /S /C powershell Invoke-WebRequest -URI https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.zip -OutFile $Env:TEMP\\apache-maven-3.8.8-bin.zip' returned a non-zero code: 1\r\n[Pipeline] }\r\n[Pipeline] // withCredentials\r\n{code}\r\n\r\nWe need to update the URL to https://archive.apache.org/dist/maven/maven-3/3.8.8/binaries/apache-maven-", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support `SPARK_VERSION` placeholder in container image names\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25 in the docker images.", "output": "Update Java 24 to 25 in docker images"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Re-enable SmokeTestDriverIntegrationTest for processing threads\nDescription: The test fails occasionally with a race condition like this:\r\n{code:java}\r\n 2_0 RUNNING StreamTask(active): org.apache.kafka.streams.errors.StreamsException: java.util.ConcurrentModificationExceptionorg.apache.kafka.streams.errors.StreamsException: java.util.ConcurrentModificationException at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:977) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:894)Caused by: java.util.ConcurrentModificationException at java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1597) at java.base/java.util.HashMap$KeyIterator.next(HashMap.java:1620) at org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer.resume(ClassicKafkaConsumer.java:979) at org.apache.kafka.clients.consumer.KafkaConsumer.resume(KafkaConsumer.java:1509) at org.apache.kafka.streams.processor.internals.StreamTask.resumePollingForPartitionsWithAvailableSpace(StreamTask.java:623) at org.apache.kafka.streams.processor.internals.TaskManager.resumePollingForPartitionsWithAvailableSpace(TaskManager.java:1866) at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithProcessingThreads(StreamThread.java:1355) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:932) {code}\r\nh4. *Since the feature is not actively developed by anybody, we disable this test. This ticket is to re-enable the test when the race condition is fixed.*", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-extras.\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "After upgrading our Kafka brokers to version {*}4.1.0{*}, the broker fails to start due to a misconfigured topic-level {{segment.bytes}} setting. The new version enforces a *minimum value of 1MB (1048576 bytes)* for this configuration, and any value below this threshold causes the broker to terminate during startup.\r\n*Error Details:*\r\n**\r\n \r\n{code:java}\r\n[2025-09-12 14:39:51,285] ERROR Encountered fatal fault: Error starting LogManager (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler) org.apache.kafka.common.config.ConfigException: Invalid value 75000 for configuration segment.bytes: Value must be at least 1048576 at org.apache.kafka.common.config.ConfigDef$Range.ensureValid(ConfigDef.java:989) ~[kafka-clients-4.1.0.jar:?]\r\n \r\n{code}\r\nIn our setup, some topics were previously configured with a lower segment.bytes value (e.g., 75000), which was allowed in earlier Kafka versions but is now invalid.\r\n\r\nAs a result Kafka broker cannot start, leading to downtime and unavailability.No snapshot file exists yet, so the {{kafka-metadata-shell}} tool cannot be used to patch the config offline.\r\nWe would appreciate your guidance on the following:\r\n * Are there any supported methods from Kafka 4.1.0 to override or bypass this validation at startup to recover without losing data?\r\n\r\n * If not, is there a documented approach to fix such configuration issues when snapshots are not yet available?", "output": "Kafka Broker Fails to Start After Upgrade to 4.1.0 Due to Invalid segment.bytes Configuration"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update the command usage of NNThroughputBenchmark by adding the \"-blockSize\" option.\nDescription: In HDFS-15652, make block size from NNThroughputBenchmark configurable. Benchmarking.md should also be updated.\nQ: hadoop-yetus commented on PR #7711:\nURL: https://github.com/apache/hadoop/pull/7711#issuecomment-2909065569\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  73m 14s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7711/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7711 |\r\n   | JIRA Issue | HADOOP-19579 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint |\r\n   | uname | Linux 4bf23d90b0e0 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 309232f8b4f786afb79a08f6620e5a09fb38cc3f |\r\n   | Max. process+thread count | 676 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7711/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "github-actions[bot] commented on PR #7711:\nURL: https://github.com/apache/hadoop/pull/7711#issuecomment-3272751907\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Enhance the documentation for producer headers\nDescription: 1. `Header#key` never returns null\r\n2. `Header#value` may return null\r\n3. the order of  `Iterable headers` passed to a `ProducerRecord` is preserved and will match the order of eaders in the corresponding `ConsumerRecord`\nQ: I'm working on this, thanks :)", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Debian 10 to 11 in build env Dockerfile\nDescription: Debian 10 EOL, and the apt repo is unavailable\r\n{code:bash}\r\ndocker run --rm -it debian:10 bash\r\nroot@bc2a4c509cb3:/# apt update\r\nIgn:1 http://deb.debian.org/debian buster InRelease\r\nIgn:2 http://deb.debian.org/debian-security buster/updates InRelease\r\nIgn:3 http://deb.debian.org/debian buster-updates InRelease\r\nErr:4 http://deb.debian.org/debian buster Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:5 http://deb.debian.org/debian-security buster/updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:6 http://deb.debian.org/debian buster-updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nReading package lists... Done\r\nE: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian-security buster/updates \nQ: pan3793 opened a new pull request, #7898:\nURL: https://github.com/apache/hadoop/pull/7898\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Debian 10 EOL, and the apt repo is unavailable, this PR upgrades it to Debian 11\r\n   \r\n   ```\r\n   docker run --rm -it debian:10 bash\r\n   root@bc2a4c509cb3:/# apt update\r\n   Ign:1 http://deb.debian.org/debian buster InRelease\r\n   Ign:2 http://deb.debian.org/debian-security buster/updates InRelease\r\n   Ign:3 http://deb.debian.org/debian buster-updates InRelease\r\n   Err:4 http://deb.debian.org/debian buster Release\r\n     404  Not Found [IP: 151.101.90.132 80]\r\n   Err:5 http://deb.debian.org/debian-security buster/updates Release\r\n     404  Not Found [IP: 151.101.90.132 80]\r\n   Err:6 http://deb.debian.org/debian buster-updates Release\r\n     404  Not Found [IP: 151.101.90.132 80]\r\n   Reading package lists... Done\r\n   E: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file.\r\n   N: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\n   N: See apt-secure(8) manpage for repository creation and user configuration details.\r\n   E: The repository 'http://deb.debian.org/debian-security buster/updates Release' does not have a Release file.\r\n   N: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\n   N: See apt-secure(8) manpage for repository creation and user configuration details.\r\n   E: The repository 'http://deb.debian.org/debian buster-updates Release' does not have a Release file.\r\n   N: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\n   N: See apt-secure(8) manpage for repository creation and user configuration details.\r\n   root@bc2a4c509cb3:/#\r\n   ```\r\n   \r\n   [Debian 11 will be EOL after 31 Aug 2026](https://endoflife.date/debian). I didn't switch it to Debian 12 or 13 because the new version does not have `openjdk-11-jdk` in the apt repo.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   ```\r\n   $ ./start-build-env.sh debian_11\r\n   ...\r\n    _   _           _                    ______\r\n   | | | |         | |                   |  _  \\\r\n   | |_| | __ _  __| | ___   ___  _ __   | | | |_____   __\r\n   |  _  |/ _` |/ _` |/ _ \\ / _ \\| '_ \\  | | | / _ \\ \\ / /\r\n   | | | | (_| | (_| | (_) | (_) | |_) | | |/ /  __/\\ V /\r\n   \\_| |_/\\__,_|\\__,_|\\___/ \\___/| .__/  |___/ \\___| \\_(_)\r\n                                 | |\r\n                                 |_|\r\n   \r\n   This is the standard Hadoop Developer build environment.\r\n   This has all the right tools installed required to build\r\n   Hadoop from source.\r\n   \r\n   chengpan@c85a4426ba52:~/hadoop$\r\n   ```\r\n   \r\n   ```\r\n   $ mvn clean install -DskipTests -Pnative -Pyarn-ui -DskipShade\r\n   ...\r\n   [INFO] Reactor Summary for Apache Hadoop Main 3.5.0-SNAPSHOT:\r\n   [INFO]\r\n   [INFO] Apache Hadoop Main ................................. SUCCESS [  0.662 s]\r\n   [INFO] Apache Hadoop Build Tools .......................... SUCCESS [  1.010 s]\r\n   [INFO] Apache Hadoop Project POM .......................... SUCCESS [  0.592 s]\r\n   [INFO] Apache Hadoop Annotations .......................... SUCCESS [  0.618 s]\r\n   [INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  0.069 s]\r\n   [INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.109 s]\r\n   [INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  1.754 s]\r\n   [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  0.416 s]\r\n   [INFO] Apache Hadoop Auth ................................. SUCCESS [  2.437 s]\r\n   [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  0.581 s]\r\n   [INFO] Apache Hadoop Common ............................... SUCCESS [ 22.868 s]\r\n   [INFO] Apache Hadoop NFS .................................. SUCCESS [  1.273 s]\r\n   [INFO] Apache Hadoop KMS .................................. SUCCESS [  1.393 s]\r\n   [INFO] Apache Hadoop Registry ............................. SUCCESS [  1.533 s]\r\n   [INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.040 s]\r\n   [INFO] Apache Hadoop HDFS Client .......................... SUCCESS [ 11.693 s]\r\n   [INFO] Apache Hadoop HDFS ................................. SUCCESS [ 22.103 s]\r\n   [INFO] Apache Hadoop HDFS Native Client ................... SUCCESS [02:13 min]\r\n   [INFO] Apache Hadoop HttpFS ............................... SUCCESS [  2.320 s]\r\n   [INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  1.312 s]\r\n   [INFO] Apache Hadoop YARN ................................. SUCCESS [  0.042 s]\r\n   [INFO] Apache Hadoop YARN API ............................. SUCCESS [  7.877 s]\r\n   [INFO] Apache Hadoop YARN Common .......................... SUCCESS [  6.436 s]\r\n   [INFO] Apache Hadoop YARN Server .......................... SUCCESS [  0.038 s]\r\n   [INFO] Apache Hadoop YARN Server Common ................... SUCCESS [  5.022 s]\r\n   [INFO] Apache Hadoop YARN ApplicationHistoryService ....... SUCCESS [  2.078 s]\r\n   [INFO] Apache Hadoop YARN Timeline Service ................ SUCCESS [  1.541 s]\r\n   [INFO] Apache Hadoop YARN Web Proxy ....................... SUCCESS [  1.331 s]\r\n   [INFO] Apache Hadoop YARN ResourceManager ................. SUCCESS [ 13.285 s]\r\n   [INFO] Apache Hadoop YARN NodeManager ..................... SUCCESS [ 24.058 s]\r\n   [INFO] Apache Hadoop YARN Server Tests .................... SUCCESS [  1.467 s]\r\n   [INFO] Apache Hadoop YARN Client .......................... SUCCESS [  2.682 s]\r\n   [INFO] Apache Hadoop MapReduce Client ..................... SUCCESS [  0.682 s]\r\n   [INFO] Apache Hadoop MapReduce Core ....................... SUCCESS [  5.570 s]\r\n   [INFO] Apache Hadoop MapReduce Common ..................... SUCCESS [  2.672 s]\r\n   [INFO] Apache Hadoop MapReduce Shuffle .................... SUCCESS [  1.767 s]\r\n   [INFO] Apache Hadoop MapReduce App ........................ SUCCESS [  3.697 s]\r\n   [INFO] Apache Hadoop MapReduce HistoryServer .............. SUCCESS [  2.380 s]\r\n   [INFO] Apache Hadoop MapReduce JobClient .................. SUCCESS [  4.119 s]\r\n   [INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [  2.581 s]\r\n   [INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  1.206 s]\r\n   [INFO] Apache Hadoop Federation Balance ................... SUCCESS [  1.630 s]\r\n   [INFO] Apache Hadoop HDFS-RBF ............................. SUCCESS [ 15.224 s]\r\n   [INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.035 s]\r\n   [INFO] Apache Hadoop YARN SharedCacheManager .............. SUCCESS [  1.195 s]\r\n   [INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SUCCESS [  1.104 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Backend ... SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Common .... SUCCESS [  1.532 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Client .... SUCCESS [  4.403 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Servers ... SUCCESS [  0.039 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase Server 2.5  SUCCESS [  1.835 s]\r\n   [INFO] Apache Hadoop YARN TimelineService HBase tests ..... SUCCESS [  1.916 s]\r\n   [INFO] Apache Hadoop YARN Router .......................... SUCCESS [  2.665 s]\r\n   [INFO] Apache Hadoop YARN TimelineService DocumentStore ... SUCCESS [  1.284 s]\r\n   [INFO] Apache Hadoop YARN GlobalPolicyGenerator ........... SUCCESS [  1.474 s]\r\n   [INFO] Apache Hadoop YARN Applications .................... SUCCESS [  0.033 s]\r\n   [INFO] Apache Hadoop YARN DistributedShell ................ SUCCESS [  1.413 s]\r\n   [INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SUCCESS [  0.925 s]\r\n   [INFO] Apache Hadoop YARN Services ........................ SUCCESS [  0.033 s]\r\n   [INFO] Apache Hadoop YARN Services Core ................... SUCCESS [  3.138 s]\r\n   [INFO] Apache Hadoop YARN Services API .................... SUCCESS [  1.914 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog ............. SUCCESS [  0.038 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog Webapp ...... SUCCESS [ 11.624 s]\r\n   [INFO] Apache Hadoop YARN Application Catalog Docker Image  SUCCESS [  0.048 s]\r\n   [INFO] Apache Hadoop YARN Application MaWo ................ SUCCESS [  0.035 s]\r\n   [INFO] Apache Hadoop YARN Application MaWo Core ........... SUCCESS [  0.991 s]\r\n   [INFO] Apache Hadoop YARN Site ............................ SUCCESS [  0.075 s]\r\n   [INFO] Apache Hadoop YARN Registry ........................ SUCCESS [  0.458 s]\r\n   [INFO] Apache Hadoop YARN UI .............................. SUCCESS [02:07 min]\r\n   [INFO] Apache Hadoop YARN CSI ............................. SUCCESS [  3.403 s]\r\n   [INFO] Apache Hadoop YARN Project ......................... SUCCESS [  1.283 s]\r\n   [INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SUCCESS [  1.046 s]\r\n   [INFO] Apache Hadoop MapReduce NativeTask ................. SUCCESS [ 18.413 s]\r\n   [INFO] Apache Hadoop MapReduce Uploader ................... SUCCESS [  1.040 s]\r\n   [INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  1.912 s]\r\n   [INFO] Apache Hadoop MapReduce ............................ SUCCESS [  1.140 s]\r\n   [INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [  2.034 s]\r\n   [INFO] Apache Hadoop Client Aggregator .................... SUCCESS [  0.728 s]\r\n   [INFO] Apache Hadoop Dynamometer Workload Simulator ....... SUCCESS [  1.363 s]\r\n   [INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SUCCESS [  1.878 s]\r\n   [INFO] Apache Hadoop Dynamometer Block Listing Generator .. SUCCESS [  1.241 s]\r\n   [INFO] Apache Hadoop Dynamometer Dist ..................... SUCCESS [  1.161 s]\r\n   [INFO] Apache Hadoop Dynamometer .......................... SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop Archives ............................. SUCCESS [  1.310 s]\r\n   [INFO] Apache Hadoop Archive Logs ......................... SUCCESS [  1.536 s]\r\n   [INFO] Apache Hadoop Rumen ................................ SUCCESS [  1.914 s]\r\n   [INFO] Apache Hadoop Gridmix .............................. SUCCESS [  1.920 s]\r\n   [INFO] Apache Hadoop Data Join ............................ SUCCESS [  1.347 s]\r\n   [INFO] Apache Hadoop Extras ............................... SUCCESS [  1.377 s]\r\n   [INFO] Apache Hadoop Pipes ................................ SUCCESS [  2.679 s]\r\n   [INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [  7.690 s]\r\n   [INFO] Apache Hadoop Kafka Library support ................ SUCCESS [  0.901 s]\r\n   [INFO] Apache Hadoop Azure support ........................ SUCCESS [  5.553 s]\r\n   [INFO] Apache Hadoop Aliyun OSS support ................... SUCCESS [  1.183 s]\r\n   [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  1.679 s]\r\n   [INFO] Apache Hadoop Resource Estimator Service ........... SUCCESS [  1.213 s]\r\n   [INFO] Apache Hadoop Azure Data Lake support .............. SUCCESS [  1.156 s]\r\n   [INFO] Apache Hadoop Image Generation Tool ................ SUCCESS [  1.256 s]\r\n   [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [  0.704 s]\r\n   [INFO] Apache Hadoop OpenStack support .................... SUCCESS [  0.047 s]\r\n   [INFO] Apache Hadoop Common Benchmark ..................... SUCCESS [  9.812 s]\r\n   [INFO] Apache Hadoop Compatibility Benchmark .............. SUCCESS [  0.853 s]\r\n   [INFO] Apache Hadoop Tools ................................ SUCCESS [  0.032 s]\r\n   [INFO] Apache Hadoop Client API ........................... SUCCESS [  0.911 s]\r\n   [INFO] Apache Hadoop Client Runtime ....................... SUCCESS [  0.763 s]\r\n   [INFO] Apache Hadoop Client Packaging Invariants .......... SUCCESS [  0.367 s]\r\n   [INFO] Apache Hadoop Client Test Minicluster .............. SUCCESS [  0.756 s]\r\n   [INFO] Apache Hadoop Client Packaging Invariants for Test . SUCCESS [  0.174 s]\r\n   [INFO] Apache Hadoop Client Packaging Integration Tests ... SUCCESS [  0.778 s]\r\n   [INFO] Apache Hadoop Distribution ......................... SUCCESS [  0.493 s]\r\n   [INFO] Apache Hadoop Client Modules ....................... SUCCESS [  0.034 s]\r\n   [INFO] Apache Hadoop Tencent COS Support .................. SUCCESS [ 56.113 s]\r\n   [INFO] Apache Hadoop OBS support .......................... SUCCESS [  1.335 s]\r\n   [INFO] Apache Hadoop Volcano Engine Services support ...... SUCCESS [  2.005 s]\r\n   [INFO] Apache Hadoop Cloud Storage ........................ SUCCESS [  0.444 s]\r\n   [INFO] Apache Hadoop Cloud Storage Project ................ SUCCESS [  0.032 s]\r\n   [INFO] ------------------------------------------------------------------------\r\n   [INFO] BUILD SUCCESS\r\n   [INFO] ------------------------------------------------------------------------\r\n   [INFO] Total time:  10:25 min\r\n   [INFO] Finished at: 2025-08-26T05:03:57Z\r\n   [INFO] ------------------------------------------------------------------------\r\n   ```\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "pan3793 commented on code in PR #7898:\nURL: https://github.com/apache/hadoop/pull/7898#discussion_r2299802155\n\n\n##########\nstart-build-env.sh:\n##########\n@@ -93,7 +93,7 @@ RUN userdel -r \\$(getent passwd ${USER_ID} | cut -d: -f1) 2>/dev/null || :\n RUN groupadd --non-unique -g ${GROUP_ID} ${USER_NAME}\n RUN useradd -g ${GROUP_ID} -u ${USER_ID} -k /root -m ${USER_NAME} -d \"${DOCKER_HOME_DIR}\"\n RUN echo \"${USER_NAME} ALL=NOPASSWD: ALL\" > \"/etc/sudoers.d/hadoop-build-${USER_ID}\"\n-ENV HOME \"${DOCKER_HOME_DIR}\"\n+ENV HOME=\"${DOCKER_HOME_DIR}\"\n\nReview Comment:\n   update because\r\n   ```\r\n    1 warning found (use docker --debug to expand):\r\n    - LegacyKeyValueFormat: \"ENV key=value\" should be used instead of legacy \"ENV key value\" format (line 7)\r\n   ```"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-federation-balance.\nDescription: \nQ: hadoop-yetus commented on PR #7580:\nURL: https://github.com/apache/hadoop/pull/7580#issuecomment-2777570888\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 35s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 28s |  |  hadoop-federation-balance in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 129m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7580/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7580 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 35bad4acad66 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d2f80c01d181fc13adc0212ebd77d27e514ae13c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7580/1/testReport/ |\r\n   | Max. process+thread count | 686 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-federation-balance U: hadoop-tools/hadoop-federation-balance |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7580/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7580:\nURL: https://github.com/apache/hadoop/pull/7580#issuecomment-2782105310\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m  3s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 32s |  |  hadoop-federation-balance in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 128m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7580/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7580 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3c24184d36b5 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 549a4b9510f2ae4759760d62a8bfe05646657a20 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7580/2/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-federation-balance U: hadoop-tools/hadoop-federation-balance |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7580/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade Kafka repo to use JUnit6\nDescription: As JUnit6 was released [https://docs.junit.org/6.0.0/release-notes], it would be great to upgrade repo with such version. Currently {*}5.13.1{*}.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: DESCRIBE AS JSON <col>\nDescription: Support DESCRIBE AS JSON", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Rename Operation\nDescription: We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Rename File and Rename directory operations on blob Endpoint\r\n\r\nThis Jira tracks implementing these tests.\nQ: hadoop-yetus commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2657734081\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 9 new + 2 unchanged - 0 fixed = 11 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 58s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 134m 13s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7386 |\r\n   | JIRA Issue | HADOOP-19445 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f1c6768f1b54 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0bfe00728dc19c14378d0a2aa58842330a1b5f4a |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/testReport/ |\r\n   | Max. process+thread count | 604 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2669092678\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 9 new + 7 unchanged - 0 fixed = 16 total (was 7)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  75m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7386 |\r\n   | JIRA Issue | HADOOP-19445 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3e831a046d9d 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 35b31f95f614c22e990cddad3450bf1f7d448fda |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/2/testReport/ |\r\n   | Max. process+thread count | 738 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Provide clean upgrade path from transformValues to processValues\nDescription: This ticket is a follow up to https://issues.apache.org/jira/browse/KAFKA-19668\r\n\r\nWith K19668, we introduced some \"hot fix\", that is actually disabled by default for backward compatibility reasons. A cleaner fix (which required a KIP, and thus cannot go into a bug-fix release), would be to add a new overload of `processValues()` which has the fix enabled, and deprecate the existing `processValues()` methods.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Make ResolvedCollation evaluable\nDescription: In this PR I propose to make ResolvedCollation evaluable. By making ResolvedCollation evaluable, it can now pass the canEvaluateWithinJoin check, allowing the optimizer to use efficient hash joins for queries with inline COLLATE in join conditions (before this change we would fallback to BroadcastNestedLoopJoin).", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Similar to `Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` resolved in SPARK-54058, `Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite` also seems flaky.\r\n\r\nhttps://github.com/micheal-o/spark/actions/runs/18895266750/job/53931238163\r\n\r\n{code}\r\n[info] - Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) report available offset which is less than end offset during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-42, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info] \tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)\r\n[info] \tat org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.waitUntilMetadataIsPropagated(KafkaTestUtils.scala:613)\r\n[info] \tat org", "output": "Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches`"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-federation-balance.\nDescription: \nQ: slfan1989 opened a new pull request, #7580:\nURL: https://github.com/apache/hadoop/pull/7580\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19434. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-federation-balance.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7580:\nURL: https://github.com/apache/hadoop/pull/7580#issuecomment-2777570888\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 35s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 28s |  |  hadoop-federation-balance in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 129m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7580/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7580 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 35bad4acad66 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d2f80c01d181fc13adc0212ebd77d27e514ae13c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7580/1/testReport/ |\r\n   | Max. process+thread count | 686 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-federation-balance U: hadoop-tools/hadoop-federation-balance |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7580/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When we try to do assertDataFrameEqual, on two dataframes that have not the same amount of rows, the output gets cascading from the difference till the end. Why this is happening:\r\n\r\n[https://github.com/apache/spark/blob/067969ff946712eeabf47040415f25000837cd87/python/pyspark/testing/utils.py#L1036]\r\n{code:java}\r\n    def assert_rows_equal(\r\n        rows1: List[Row], rows2: List[Row], maxErrors: int = None, showOnlyDiff: bool = False\r\n    ):\r\n        __tracebackhide__ = True\r\n        zipped = list(zip_longest(rows1, rows2))\r\n        diff_rows_cnt = 0\r\n        diff_rows = []\r\n        has_diff_rows = False        \r\n        rows_str1 = \"\"\r\n        rows_str2 = \"\"        \r\n        \r\n        # count different rows\r\n        for r1, r2 in zipped:\r\n            if not compare_rows(r1, r2):\r\n                diff_rows_cnt += 1\r\n                has_diff_rows = True\r\n                if includeDiffRows:\r\n                    diff_rows.append((r1, r2))\r\n                rows_str1 += str(r1) + \"\\n\"\r\n                rows_str2 += str(r2) + \"\\n\"\r\n                if maxErrors is not None and diff_rows_cnt >= maxErrors:\r\n                    break\r\n            elif not showOnlyDiff:\r\n                rows_str1 += str(r1) + \"\\n\"\r\n                rows_str2 += str(r2) + \"\\n\"        \r\n        generated_diff = _context_diff(\r\n            actual=rows_str1.splitlines(), expected=rows_str2.splitlines(), n=len(zipped)\r\n        )        \r\n        if has_diff_rows:\r\n            error_msg = \"Results do not match: \"\r\n            percent_diff = (diff_rows_cnt / len(zipped)) * 100\r\n            error_msg += \"( %.5f %% )\" % percent_diff\r\n            error_msg += \"\\n\" + \"\\n\".join(generated_diff)\r\n            data = diff_rows if includeDiffRows else None\r\n            raise PySparkAssertionError(\r\n                errorClass=\"DIFFERENT_ROWS\", messageParameters={\"error_msg\": error_msg}, data=data\r\n            ){code}\r\nThe problem lies in the way that we zip the lines\r\n{code:java}\r\nzipped = list(zip_longest(rows1, ", "output": "AssertDataframeEqual carries rows when showing differences"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Upgrade to a recent version of 2.33.x or later while off the critical path of things.\r\n\r\n\r\nHADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.", "output": "Upgrade AWS SDK to 2.35.4"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Exclude some files (like .gitkeep) from Rat checks ?\nDescription: *Note:* presumably I need a buy-in from [~chia7712] for this ticket.\r\n\r\n{*}Blocks this tickets{*}: KAFKA-19174, KAFKA-19591 (these two are created separately buy could/should be resolved at once). \r\n\r\n{*}Rationale{*}: Gradle upgrade related PR and Jira ticket are blocked due to a fact that Rat check breaks the build even for such small stuff such as .gitkeep placeholder files.\r\n\r\nSee here for more details: [https://github.com/apache/kafka/pull/19513#issuecomment-3213821218] \r\n{code:java}\r\nRun python .github/scripts/rat.py\r\nFound 1 Rat reports\r\nRead env GITHUB_WORKSPACE: /home/runner/work/kafka/kafka\r\nParsing Rat report file: build/rat/rat-report.xml\r\n6840 approved licenses\r\n1 unapproved licenses\r\nFiles with unapproved licenses:\r\nNotice: File with unapproved license: distribution/.gitkeep\r\nError: Process completed with exit code 1.\r\n{code}\nQ: Solved, it was quite easy one-liner :D\r\n\r\nPatch is submitted here: https://github.com/apache/kafka/pull/19513 (but I will probably demote status back into draft (distribution .tgz file paths must be changed).", "output": "(Will be) implemented via KAFKA-19174 (and hence resolving)."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade\nDescription: After https://issues.apache.org/jira/browse/HADOOP-19425\r\n\r\nmost of the integration tests are getting skipped. All tests need to be fixed with this PR\nQ: Copilot commented on code in PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#discussion_r2272827582\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAppend.java:\n##########\n@@ -608,12 +608,12 @@ public void testCreateExplicitDirectoryOverDfsAppendOverBlob()\n    **/\n   @Test\n   public void testRecreateAppendAndFlush() throws IOException {\n+      assumeThat(isAppendBlobEnabled()).as(\"Not valid for APPEND BLOB\").isFalse();\n+      assumeThat(getIngressServiceType()).isEqualTo(AbfsServiceType.BLOB);\n\nReview Comment:\n   This assumption is placed after the assertThrows() call begins, but should be before it. The assumption should be checked before setting up the exception assertion to ensure the test conditions are met first.\n   ```suggestion\n         assumeThat(getIngressServiceType()).isEqualTo(AbfsServiceType.BLOB);\n   ```", "output": "hadoop-yetus commented on PR #7868:\nURL: https://github.com/apache/hadoop/pull/7868#issuecomment-3183429913\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m  4s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 9 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 51s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 16s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 140m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7868 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 51d70a663e18 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 64f15bfbb96b28d201067683e6e619496d0683da |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/1/testReport/ |\r\n   | Max. process+thread count | 719 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7868/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Include push interval in ClientTelemetryReceiver context\nDescription: Jira for KIP-1217\r\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-1217%3A+Include+push+interval+in+ClientTelemetryReceiver+context", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add detailed error message for corrupted view metadata with mismatched column counts\nDescription: Enhance error reporting for corrupted view metadata by adding a detailed, user-friendly assertion message when the number of view query column names and the number of columns in the view schema mismatch.\nQ: Issue resolved by pull request 52732\n[https://github.com/apache/spark/pull/52732]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump guava to  33.4.8-jre due to EOL\nDescription: We can use the latest 33.4.8-jre version as the current one is quite old.\nQ: ahh, the later versions of guava exclude a dependency on checkerframework. So any references in our code (there are four) fail. Which means we have to manually add it if we want a drop in replacement. PITA.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: The records appended to the log are illegal because of an incorrect base offset during TestLinearWriteSpeed\nDescription: {code:java}\r\nException in thread \"main\" org.apache.kafka.common.InvalidRecordException: The baseOffset of the record batch in the append to kafka-test-0 should be 0, but it is 9\r\n{code}\r\n\r\nWe could simplify reset the offset or create the new records in each write", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [CONNECT] Supporting large LocalRelations\nDescription: h1. Problem description\r\n\r\nLocalRelation is a Catalyst logical operator used to represent a dataset of rows inline as part of the LogicalPlan. LocalRelations represent dataframes created directly from Python and Scala objects, e.g., Python and Scala lists, pandas dataframes, csv files loaded in memory, etc.\r\n\r\nIn Spark Connect, local relations are transferred over gRPC using LocalRelation (for relations under 64MB) and CachedLocalRelation (larger relations over 64MB) messages.\r\n\r\nCachedLocalRelations currently have a hard size limit of 2GB, which means that spark users can’t execute queries with local client data, pandas dataframes, csv files of over 2GB.\r\nh1. Design\r\n\r\nIn Spark Connect, the client needs to serialize the local relation before transferring it to the server. It serializes data via an Arrow IPC stream as a single record batch and schema as a json string. It then embeds data and schema as LocalRelation\\{schema,data} proto message.\r\nSmall local relations (under 64MB) are se\nQ: Issue resolved by pull request 52613\n[https://github.com/apache/spark/pull/52613]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "IllegalArgumentException is thrown when AvroOptions option requiring boolean is given to be not boolean. Should classify the error.", "output": "Classify errors for AvroOptions boolean casting failure"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove unused commons-collections 3.x\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Publish Apache Spark 4.1.0-preview3 to docker registry\nDescription: \nQ: This is resolved via https://github.com/apache/spark-docker/pull/97", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support TIME in the try_make_timestamp function in Python\nDescription: \nQ: Hi, can I take this?\r\n\r\nBut as far as I can see, there is a few try_make_timestamp functions. Which one should I modify?", "output": "Hi [~dekrate], thank you for your help! I already have this one in progress: [https://github.com/apache/spark/pull/52666.]\r\n\r\nBut I can ping you when another task comes up - does that sound good to you?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implementation of ShareConsumer.acquisitionLockTimeoutMs() method\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Simplify Jackson deps management by using BOM\nDescription: \nQ: Issue resolved by pull request 52668\n[https://github.com/apache/spark/pull/52668]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: region resolution within AWS infra always goes to us-east-2\nDescription: I think this is new to the V2 SDK, or at least our region logic there.\r\nWhen you try to connect to a bucket without specifying the region, then even if you're running in the same region of the store a HEAD request is still made to US Central. This adds latency, makes us-central a SPoF and if the VM/container has network rules which blocks such access, we actually timeout and eventually fail.\r\n\r\nWhile Machine configurations should ideally have the fs.s3a.endpoint.region setting configured, that information is actually provided as IAM metadata. Therefore it would be possible \r\n\r\nThis is actually included in the default region chain according to the SDK docs \"If running in EC2, check the EC2 metadata service for the region\", so maybe this isn't being picked up because\r\n\r\n# cross region access is being checked for first.\r\n# the region chain we are setting off doesn't check the EC2 metadata service.\r\n\r\nThe SDK region chain does do the right thing within AWS infra. How do we restore that while still supporting remote deployments?", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager\nDescription: We have observed this across multiple workload runs that when we start reading data from input stream. The first read which came to input stream has to be read synchronously even if we trigger prefetch request for that particular offset. Most of the times we end up doing extra work of checking if the prefetch is trigerred, removing prefetch from the pending queue and go ahead to do a direct remote read in workload thread itself.\r\n\r\nTo avoid all this overhead, we will always bypass read ahead for the very first read of each input stream and trigger read aheads for second read onwards.\nQ: Thanks for the feedback steve. We will definitely incorporate that.\r\nI will hold onto this PR and will make this change with the Read Policy suggested by user taken into consideration.\r\n\r\nWill work diligently on all the read policies and have reads happening in way optimal for each one of them.", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This is the first step in the parent task: To clean up all the integration tests related to state-updater flag\r\n\r\nSee [https://github.com/apache/kafka/pull/20392#issuecomment-3240659815] for more details", "output": "Clean up integration tests related to state-updater"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Release Spark Connect Swift Client 0.6.0\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consistency of command-line arguments for console producer/consumer\nDescription: This implements KIP-1147 for kafka-console-producer, kafka-console-consumer and kafka-console-share-consumer.\nQ: -Hi [~schofielaj], I saw you didin't assign anyone to this jira, does that mean I can help with this one?-", "output": "Hi [~isding_l] , I already had a volunteer in mind for this one. Feel free to take any of the unassigned others."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Release hadoop-thirdparty 1.4.0\nDescription: Build and Release Hadoop-Thirdparty 1.4.0", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Document newly added K8s configurations\nDescription: \nQ: Issue resolved by pull request 52618\n[https://github.com/apache/spark/pull/52618]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We need to set some config related to S3 for our inner Spark. The implementation of the function show below.\r\n\r\n{code:java}\r\nprivate def setS3Configs(conf: SparkConf): Unit = {\r\n    val S3A_PREFIX = \"spark.fs.s3a\"\r\n    val SPARK_HADOOP_S3A_PREFIX = \"spark.hadoop.fs.s3a\"\r\n    val s3aConf = conf.getAllWithPrefix(S3A_PREFIX)\r\n    s3aConf\r\n      .foreach(\r\n        confPair => {\r\n          val keyWithoutPrefix = confPair._1\r\n          val oldKey = S3A_PREFIX + keyWithoutPrefix\r\n          val newKey = SPARK_HADOOP_S3A_PREFIX + keyWithoutPrefix\r\n          val value = confPair._2\r\n          (newKey, value)\r\n        })\r\n  }\r\n\r\n{code}\r\n\r\nThese code seems redundant and complicated. The reason is getAllWithPrefix only return the suffix part.", "output": "Add `SparkConf.getAllWithPrefix(String, String => K)` API"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix `initialize` to add `CREATE` option additionally in `DriverRunner`\nDescription: When submitting jobs to standalone cluster using restful api, we get errors like:\r\n\r\n```\r\n25/10/30 16:02:59 INFO DriverRunner: Killing driver process!\r\n25/10/30 16:02:59 INFO CommandUtils: Redirection to /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stdout closed: Stream closed\r\n25/10/30 16:02:59 WARN Worker: Driver driver-20251030160259-0020 failed with unrecoverable exception: java.nio.file.NoSuchFileException: /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stderr\r\n\r\n```\r\n\r\nWe need to fix the usage of `Files.writeString` in DriverRunner", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Updating Documentations of Hadoop Drivers for Azure\nDescription: Fixing some typos, details and adding links for better readability in the documentation files for ABFS driver.\nQ: hadoop-yetus commented on PR #7540:\nURL: https://github.com/apache/hadoop/pull/7540#issuecomment-2753466292\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  0s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  73m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7540/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 116m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7540/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7540 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint |\r\n   | uname | Linux 19618c012176 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b0bf7a91e6078fbc1a9b2a5c3d2d5d61286d260b |\r\n   | Max. process+thread count | 553 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7540/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7540:\nURL: https://github.com/apache/hadoop/pull/7540#issuecomment-2761236317\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  markdownlint  |   0m  1s |  |  markdownlint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  37m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  75m 18s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 51s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 115m 41s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7540/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7540 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets markdownlint |\r\n   | uname | Linux dd927838e185 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 83a2f77d23246455e8ade61f896fe37d5626416a |\r\n   | Max. process+thread count | 700 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7540/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add acks dimension to BrokerTopicMetrics for produce requests\nDescription: h3. *Title*\r\n\r\nAdd {{acks}} dimension to {{BrokerTopicMetrics}} to measure per-acks produce performance impact\r\n----\r\nh3. *Summary*\r\n\r\nCurrently, Kafka’s {{BrokerTopicMetrics}} only tracks produce metrics such as {{{}BytesInPerSec{}}}, {{{}MessagesInPerSec{}}}, and {{ProduceRequestsPerSec}} at the topic level. However, these metrics do not distinguish between different producer acknowledgment ({{{}acks{}}}) configurations ({{{}acks=0{}}}, {{{}acks=1{}}}, {{{}acks=-1{}}}).\r\nIn high-throughput environments, different {{acks}} levels have significantly different impacts on broker CPU, I/O, and network utilization.\r\nThis proposal introduces an additional {{acks}} label to existing topic-level metrics, enabling more granular visibility into broker performance under various producer reliability modes.\r\n----\r\nh3. *Motivation*\r\n\r\nThe current aggregated produce metrics make it difficult to assess the performance and stability implications of different {{acks}} settings on brokers.\r\nFor example, asynchronous ({{{}acks=0{}}}) and fully acknowledged ({{{}acks=-1{}}}) produces can have very different effects on disk I/O, request queues, and replication latency, but these effects are hidden in current metrics.\r\n\r\nBy introducing an {{acks}} dimension, operators and performance engineers can:\r\n * Quantify the resource cost of different producer acknowledgment strategies.\r\n\r\n * Analyze how {{acks}} configuration affects cluster throughput, replication load, and latency.\r\n\r\n * Perform fine-gra", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add configuration validation option to Kafka server startup\nDescription: *Problem Statement:* Currently, Kafka administrators have no way to validate server configuration files without actually starting the Kafka broker. This leads to:\r\n * Wasted time during deployments when configuration errors are discovered only at startup\r\n * Potential service disruptions in production environments\r\n * Difficulty in CI/CD pipelines to validate Kafka configurations before deployment\r\n * No quick way to test configuration changes without full broker startup overhead\r\n * *Critical cluster stability issues during rolling restarts* - misconfigured brokers can cause:\r\n ** Partition leadership imbalances\r\n ** Replication factor violations\r\n ** Network connectivity issues between brokers\r\n ** Data consistency problems\r\n ** Cascading failures across the cluster when multiple brokers restart with incompatible configurations\r\n\r\n\r\n*Proposed Solution:* Add a {*}--check-config{*}{{{}{}}} command-line option to the Kafka server startup script that would:\r\n\r\n \r\n * Parse and validate the server configuration file\r\n * Check for common configuration errors and inconsistencies\r\n * Validate property values and ranges\r\n * *Detect configuration incompatibilities that could affect cluster operations*\r\n * Support property overrides for testing different configurations\r\n * Exit with appropriate status codes (0 for valid config, non-zero for errors)\r\n * Provide clear error messages for invalid configurations\r\n\r\n*Usage Example:*\r\n{code:java}\r\n# Validate default server.properties\r\nkafka-s", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Pin robotframework version\nDescription: {{hadoop-runner}} installs {{robotframework}} without version definition.  Re-building the image for unrelated changes may unexpectedly upgrade {{robotframework}}, too.\nQ: smengcl merged PR #8025:\nURL: https://github.com/apache/hadoop/pull/8025", "output": "adoroszlai commented on PR #8025:\nURL: https://github.com/apache/hadoop/pull/8025#issuecomment-3395090403\n\n   Thanks @slfan1989, @smengcl for the review."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have seen a situation in which the ShareFetch responses from the broker appeared logically incorrect. The consumer should be more defensive, and at least log the problem.", "output": "Handle situations where broker responses appear logically incorrect"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Support IPv6 with a flag to enable/disable dual stack endpoints\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/ipv6-access.html", "output": "S3A: add ipv6 support"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analytics-Accelerator: AAL stream factory not being closed\nDescription: When the Factory service is stopped, we're currently missing the code to close the factory. My miss, this got lost in the move to the new factory code.\nQ: ahmarsuhail opened a new pull request, #7616:\nURL: https://github.com/apache/hadoop/pull/7616\n\n   ### Description of PR\r\n   \r\n   Current integration code wasn't closing the AAL stream factory on service stop. \r\n   \r\n   This PR adds that logic in and a statistic to ensure close gets called. \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran `ITestS3AAnalyticsAcceleratorStreamReading`, will run the whole test suite sooon.", "output": "ahmarsuhail commented on PR #7616:\nURL: https://github.com/apache/hadoop/pull/7616#issuecomment-2807036987\n\n   @steveloughran @mukund-thakur  small PR to close AAL stream factory on service stop, missed that in the factory refactor. \r\n   \r\n   Was debating if that stasitic should be a generic `OBJECT_INPUT_STREAM_FACTORY_CLOSED` and be incremented in the serviceStop of `AbstractObjectInputStreamFactory`. But wasn't sure. the other stream factories don't need this because their factories don't hold any state, AAL does. let me know what you think."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Bump Maven 3.9.10\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Bump guava to  33.4.8-jre due to EOL\nDescription: We can use the latest 33.4.8-jre version as the current one is quite old.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Analytics accelerator for S3 to be enabled by default\nDescription: Make \"analytics\" the default input stream in S3A. \r\n\r\nGoals\r\n* Parquet performance through applications running queries over the data (spark etc)\r\n* Performance for other formats good as/better than today. Examples: avro manifests in iceberg, ORC in hive/spark\r\n* Performance for other uses as good as today (whole-file/sequential reads of parquet data in distcp etc)\r\n* better resilience to bad uses (incomplete reads not retaining http streams, buffer allocations on long-retained data)\r\n* efficient on applications like Impala, which caches parquet footers itself, and uses unbuffer() to discard all stream-side resources. Maybe just throw alway all state on unbuffer() and stop trying to be sophisticated, or support some new openFile flag which can be used to disable footer parsing\nQ: hadoop-yetus commented on PR #7776:\nURL: https://github.com/apache/hadoop/pull/7776#issuecomment-3027248675\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  2s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 11s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 13s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 13s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-aws in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-aws in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 12s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) |  hadoop-tools/hadoop-aws: The patch generated 18 new + 7 unchanged - 0 fixed = 25 total (was 7)  |\r\n   | -1 :x: |  mvnsite  |   0m 15s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 12s | [/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 9 new + 0 unchanged - 0 fixed = 9 total (was 0)  |\r\n   | -1 :x: |  spotbugs  |   0m 12s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 15s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  75m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7776 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2e0d17a78cf4 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 198da21540a93c15c514e3fc013ebb843caf56c9 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/testReport/ |\r\n   | Max. process+thread count | 557 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7776:\nURL: https://github.com/apache/hadoop/pull/7776#issuecomment-3027514574\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 23s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 6 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 44s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  20m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 40s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 42s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 59s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  5s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m  1s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 32s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 24s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m  2s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 55s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/2/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 18 new + 7 unchanged - 0 fixed = 25 total (was 7)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  1s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 59s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 28s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 26s |  |  hadoop-project in the patch passed.  |\r\n   | -1 :x: |  unit  |   2m 58s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/2/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 126m  1s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.s3a.impl.streams.TestStreamFactories |\r\n   |   | hadoop.fs.s3a.TestS3AUnbuffer |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7776 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux d304096c3ebc 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 051f6c4c35bf399b5a66a78763e60f22b2091bad |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/2/testReport/ |\r\n   | Max. process+thread count | 547 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Global stream thread ignores all exceptions\nDescription: {{globalStreamThread}} in {{KafkaStreams}} class ignores all exceptions and fails to apply the user-provided {{StreamsUncaughtExceptionHandler}} in:\r\n{code:java}\r\npublic void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler userStreamsUncaughtExceptionHandler)\r\n{code}\r\nThis can lead to streams being stuck in faulty state after an exception is thrown during their initialization phase (e.g. failure to load the RocksDB native library). The exception isn't logged, so debugging such problem is difficult.\r\n\r\nFrom my understanding of the {{b62d8b97}} commit message, the following code is unnecessary as the {{globalStreamThread}} can't be replaced and the issue description from KAFKA-12699 doesn't apply to it. Removing it should help.\r\n{code:java}\r\nif (globalStreamThread != null) {\r\n    globalStreamThread.setUncaughtExceptionHandler((t, e) -> { }\r\n    );\r\n}\r\n{code}", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The test fails occasionally with a race condition like this:\r\n{code:java}\r\n 2_0 RUNNING StreamTask(active): org.apache.kafka.streams.errors.StreamsException: java.util.ConcurrentModificationExceptionorg.apache.kafka.streams.errors.StreamsException: java.util.ConcurrentModificationException at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:977) at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:894)Caused by: java.util.ConcurrentModificationException at java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1597) at java.base/java.util.HashMap$KeyIterator.next(HashMap.java:1620) at org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer.resume(ClassicKafkaConsumer.java:979) at org.apache.kafka.clients.consumer.KafkaConsumer.resume(KafkaConsumer.java:1509) at org.apache.kafka.streams.processor.internals.StreamTask.resumePollingForPartitionsWithAvailableSpace(StreamTask.java:623) at org.apache.kafka.streams.processor.internals.TaskManager.resumePollingForPartitionsWithAvailableSpace(TaskManager.java:1866) at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithProcessingThreads(StreamThread.java:1355) at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:932) {code}\r\nh4. *Since the feature is not actively developed by anybody, we disable this test. This ticket is to re-enable the test when the race condition is fixed.*", "output": "Re-enable SmokeTestDriverIntegrationTest for processing threads"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module\nDescription: Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution:\r\n{code:java}\r\njava.lang.IllegalStateException: Failed to set environment variable\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:120)\tat org.apache.hadoop.fs.tosfs.object.ObjectStorageTestBase.setUp(ObjectStorageTestBase.java:59)\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.util.Map java.util.Collections$UnmodifiableMap.m accessible: module java.base does not \"opens java.util\" to unnamed module @69eee410\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)\tat java.base/java.lang.reflect.Accessi\nQ: hadoop-yetus commented on PR #8029:\nURL: https://github.com/apache/hadoop/pull/8029#issuecomment-3395542255\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 39s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 23s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  javadoc  |   0m 12s | [/patch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/patch-javadoc-hadoop-cloud-storage-project_hadoop-tos-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tos in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  shadedclient  |   1m 41s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 13s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  55m  3s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8029 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux dd7bc96b56b5 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f188198cdec1613ae955ea31795a8cb1ca496139 |\r\n   | Default Java | Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/testReport/ |\r\n   | Max. process+thread count | 576 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #8029:\nURL: https://github.com/apache/hadoop/pull/8029#issuecomment-3411382684\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 55s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 26s | [/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/artifact/out/patch-unit-hadoop-cloud-storage-project_hadoop-tos.txt) |  hadoop-tos in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  58m 21s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.tosfs.object.TestObjectMultiRangeInputStream |\r\n   |   | hadoop.fs.tosfs.object.TestObjectRangeInputStream |\r\n   |   | hadoop.fs.tosfs.object.tos.auth.TestEnvironmentCredentialsProvider |\r\n   |   | hadoop.fs.tosfs.object.tos.auth.TestDefaultCredentialsProviderChain |\r\n   |   | hadoop.fs.tosfs.object.TestObjectOutputStream |\r\n   |   | hadoop.fs.tosfs.commit.TestMagicOutputStream |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8029 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 3f25ded462da 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2bd00ae481cc8a6aeb977fef2700e684236375a4 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/testReport/ |\r\n   | Max. process+thread count | 616 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-tos U: hadoop-cloud-storage-project/hadoop-tos |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8029/2/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Rename File and Rename directory operations on blob Endpoint\r\n\r\nThis Jira tracks implementing these tests.", "output": "ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Rename Operation"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The preservedEntries count always be 0", "output": "Fix preservedEntries count in CopyCommitter when preserving directory attributes"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently the rules in suppressions.xml are a bit messy and need to be cleaned up.", "output": "Cleanup suppressions.xml"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix PlanID preservation in ResolveSQLFunctions with UDFs\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support row position for SparkConnectResultSet\nDescription: \nQ: Issue resolved in https://github.com/apache/spark/pull/52756", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix Flaky Test:`OracleJoinPushdownIntegrationSuite`\nDescription: Recently, `OracleJoinPushdownIntegrationSuite` frequently fails.\r\n\r\nhttps://github.com/beliefer/spark/actions/runs/18904457292/job/53959322233#logs\r\n{code:java}\r\n[info] org.apache.spark.sql.jdbc.v2.join.OracleJoinPushdownIntegrationSuite *** ABORTED *** (10 minutes, 15 seconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 599 times over 10.012648813883333 minutes. Last failure message: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info]   https://docs.oracle.com/error-help/db/ora-12541/. (DockerJDBCIntegrationSuite.scala:214)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventu\nQ: Issue resolved by pull request 52780\n[https://github.com/apache/spark/pull/52780]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix TestThrottledInputStream when bandwidth is equal to throttle limit\nDescription: Some tests in TestThrottledInputStream intermittently fail when the measured bandwidth is equal to the one set for throttling.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade to wildfly version with support for openssl 3\nDescription: Wildfly 2.1.4\r\n* doesn't work with openssl 3 (that symbol change...why did they do that?)\r\n\r\n\r\nwe need a version with \r\nhttps://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d\r\n\r\n2.2.5.Final does the openssl 3 support.\nQ: steveloughran opened a new pull request, #8019:\nURL: https://github.com/apache/hadoop/pull/8019\n\n   \r\n   ### How was this patch tested?\r\n   \r\n   Going to see if it works on a mac...\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "steveloughran commented on PR #8019:\nURL: https://github.com/apache/hadoop/pull/8019#issuecomment-3381330281\n\n   the test which is parameterized on ssl (and storediag when a store is forced to OpenSSL)\r\n   ```\r\n   [ERROR] org.apache.hadoop.fs.contract.s3a.ITestS3AContractSeek.testReadFullyZeroBytebufferPastEOF"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: pull out new configuration load/probes under S3AStore\nDescription: S3AFS i's too full of switches; it is why initialize() is so big, and there are lots and lots of fields to record the values.\r\n\r\nMany of these need to go down into S3AStoreImpl, but replicating the same design just pushes the mess down.\r\n\r\nProposed: a child service StoreConfigurationService, which reads in the config during serviceInit(), and set the state in there, from where it can be probed.\r\n\r\nI am initially doing this purely for new configuration flags for the conditional write feature\r\n\r\n* moving other flags in there would be separate work\r\n* new boolean config options should go in here\r\n* we also need to think about integers, units of scale and durations.\r\n\r\nIdeally, we should  the annotations and reflection code from ABFS and\r\norg.apache.hadoop.fs.azurebfs.contracts.annotations.ConfigurationValidationAnnotations,\r\n\r\n* copy it into common\r\n* adding duration and size attributes\r\n* move reflection code from AbfsConfiguration#AbfsConfiguration into there too,\r\n* use in s3a\r\n* migrate abfs to the moved code", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Make HadoopArchives support human-friendly units about blocksize and partsize.\nDescription: You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).\nQ: hadoop-yetus commented on PR #7611:\nURL: https://github.com/apache/hadoop/pull/7611#issuecomment-2801557527\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 26s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-archives.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-archives.txt) |  hadoop-tools/hadoop-archives: The patch generated 2 new + 87 unchanged - 0 fixed = 89 total (was 87)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   1m 36s | [/patch-unit-hadoop-tools_hadoop-archives.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/1/artifact/out/patch-unit-hadoop-tools_hadoop-archives.txt) |  hadoop-archives in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 131m 39s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.TestHadoopArchives |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7611 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 73fa39924ad9 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d8951df897280187f60aac9cf80b2a3230e27aab |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/1/testReport/ |\r\n   | Max. process+thread count | 707 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-archives U: hadoop-tools/hadoop-archives |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7611:\nURL: https://github.com/apache/hadoop/pull/7611#issuecomment-2803747649\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 36s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-archives.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-archives.txt) |  hadoop-tools/hadoop-archives: The patch generated 2 new + 87 unchanged - 0 fixed = 89 total (was 87)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   1m 34s | [/patch-unit-hadoop-tools_hadoop-archives.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/2/artifact/out/patch-unit-hadoop-tools_hadoop-archives.txt) |  hadoop-archives in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m 41s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.TestHadoopArchives |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7611 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ba5c39589df0 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6558b9859437c77c56a2e89849cdcea28171f63f |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/2/testReport/ |\r\n   | Max. process+thread count | 561 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-archives U: hadoop-tools/hadoop-archives |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7611/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file — it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set -u}}{*}.\r\n\r\nThis issue is {*}just an enhancement to properly log the error details{*}.", "output": "Unbound Error Thrown if some variables are not set for SASL/SSL configuration"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Support DESCRIBE AS JSON", "output": "DESCRIBE AS JSON <col>"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK25] Bump ByteBuddy 1.17.6 and ASM 9.8 to support Java 25 bytecode\nDescription: \nQ: pan3793 opened a new pull request, #7879:\nURL: https://github.com/apache/hadoop/pull/7879\n\n   …tecode\r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   ```\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "slfan1989 commented on code in PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#discussion_r2281285636\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -146,7 +146,7 @@\n     4.1.118.Final\n     1.1.10.4\n     1.7.1\n-    1.15.11\n+    1.17.6\n\nReview Comment:\n   Should the LICENSE-binary be updated?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The default value of num.recovery.threads.per.data.dir is now 2 according to KIP-1030. We should update config files which are still setting 1.", "output": "Update num.recovery.threads.per.data.dir configs"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Introduce computations for inFlightTerminalRecords\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/pull/20221#discussion_r2228931829", "output": "ConsumerPerformance#ConsumerPerfRebListener get corrupted value when the number of partitions is increased "}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Consider bumping 3rd party Github Actions\nDescription: In https://github.com/apache/kafka/commit/e1b7699975e787a78f5e9bca540b4aa19f0d419d we bumped the versions of many Github Actions but we did not bump the following:\r\n- gradle/actions/setup-gradle\r\n- aquasecurity/trivy-action\r\n- docker/setup-qemu-action\r\n- docker/setup-buildx-action\r\n- docker/login-action", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/pull/20438#discussion_r2319682177", "output": "Unify the command-line parser"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve handling of failed push telemetry request\nDescription: When a failure occurs with a push telemetry request, any exception is treated as fatal, causing telemetry to be turned off.  We can enhance this error handling to check if the exception is a transient one with expected recovery, and not turn off telemetry in those cases.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: NPE in `Processor#init()` accessing state store\nDescription: As reported on the dev mailing list, we introduced a regression bug via https://issues.apache.org/jira/browse/KAFKA-13722 in 4.1 branch. We did revert the commit ([https://github.com/apache/kafka/commit/f13a22af0b3a48a4ca1bf2ece5b58f31e3b26b7d]) for 4.1 release, and want to fix-forward for 4.2 release.\r\n\r\nStacktrace:\r\n{code:java}\r\n15:29:05 ERROR [STREAMS] KafkaStreams - stream-client [app1] Encountered the following exception during processing and the registered exception handler opted to SHUTDOWN_CLIENT. The streams client is going to shut down now.\r\norg.apache.kafka.streams.errors.StreamsException: failed to initialize processor random-value-processor\r\n        at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:132) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:141) ~[kafka-streams-4.2.0-SNAPSHOT.jar:?]\r\n        at org.apache.kafka.streams.processor.internals.Strea\nQ: Hi @mjsax, I’d like to work on this issue and submit a patch, can you please assign it to me?", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Have a way to set config to admin client only in TopicBasedRemoteLogMetadataManager\nDescription: Context:\r\nhttps://github.com/apache/kafka/pull/20306#discussion_r2262298636", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "I found that SparkSQL partition overwrite is not an atomic operation. When SparkSQL application rewrites existing table partition, [delete the matching partition]([https://github.com/apache/spark/blob/24a6abf34d253162055c8b9bd0030bf9a2ca75b1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala#L136]), then run the job.\r\n\r\nDuring task execution, the Hive partition is not deleted, but the data in the filesystem is deleted. If you read data while the job is running, you will read empty data. If the job is interrupted, the data will be lost.\r\n\r\nNote: Only for spark.sql.hive.convertMetastoreOrc or spark.sql.hive.convertMetastoreParquet is true. It is not problem for hive serde.", "output": "SparkSQL partition overwrite is not an atomic operation."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Consistency of command-line arguments for kafka-producer-perf-test.sh\nDescription: This implements KIP-1147 for kafka-producer-perf-test.sh.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "https://github.com/apache/spark/pull/50765#discussion_r2357607758", "output": "Construct FileStatus from the executor side directly"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Admin#describeReplicaLogDirs does not handle error correctly\nDescription: Admin#describeReplicaLogDirs has following issues:\r\n\r\n1. Admin#describeReplicaLogDirs does not propagate the CLUSTER_AUTHORIZATION_FAILED\r\n\r\n2. the single directory-level error is propagated to other directories result\r\n\r\n{code:java}\r\n                        if (logDirInfo.error() != null)\r\n                            handleFailure(new IllegalStateException(\r\n                                \"The error \" + logDirInfo.error().getClass().getName() + \" for log directory \" + logDir + \" in the response from broker \" + brokerId + \" is illegal\"));\r\n{code}", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "*APPROX_PERCENTILE_ESTIMATE(state [, k] ])* : Returns the approximate percentile value(s) from a previously accumulated sketch state.", "output": "Introduce APPROX_PERCENTILE_ESTIMATE"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update build instructions for Windows\nDescription: We recently upgraded vcpkg to install Boost 1.86. We need to update the documentation as well.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In v4.2.0, we are able to auto join a controller with the configuration `controller.quorum.auto.join.enable=true` set ([KIP-853|https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes#KIP853:KRaftControllerMembershipChanges-Controllerautojoining](KAFKA-19078)). This is a good improvement for controller addition, but it has a UX issue, which is that when a controller is removed via removeVoterRequest, it will be added immediately due to `controller.quorum.auto.join.enable=true`. In the KIP, we also mention you have to stop the controller before removing the controller:\r\n\r\n \r\n{noformat}\r\ncontroller.quorum.auto.join.enable:\r\n\r\nControls whether a KRaft controller should automatically join the cluster \r\nmetadata partition for its cluster id. If the configuration is set to \r\ntrue the controller must be stopped before removing the controller with kafka-metadata-quorum remove-controller.{noformat}\r\n \r\n\r\nThis is not a user friendly behavior in my opinion. And it will cause many confusion to users and thought there is something wrong in the controller removal. Furthermore, in the kubernetes environment which is controlled by the operator, it is not the cloud native way to shutdown a node, do some operation, then start it up. \r\n\r\n \r\n\r\nSo, I propose we can improve it by \"the removed controller will not be auto joined before this controller restarted\". That is:\r\n1. Once the controller is removed from voters set, it won't be auto joined even if `controller.quorum.auto.join.enable=true`\r\n\r\n2. The controller can be manually join the voters in this state\r\n\r\n3. The controller node will be auto join the voters set after node restarted.\r\n\r\n \r\n\r\nSo basically, the semantics is not changed, it just add some unexpected remove/add loop. Thoughts?", "output": "KRaft voter auto join will add a removed voter immediately"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Enable messageTemplate propagation to SparkThrowable \nDescription: The goal is to add a new *default* method, getDefaultMessageTemplate, to the public SparkThrowable interface. This gives clients a consistent, machine-readable *default* template for error rendering, while leaving them free to localize or otherwise transform the message.\nQ: Issue resolved by pull request 52559\n[https://github.com/apache/spark/pull/52559]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Develocity Gradle plugin version can't be upgraded (CI build fails with error: \"build scan failed to be published\")\nDescription: *Prologue:* this issue was discovered while working on Gradle upgrade (from 8 to 9); related JIRA tickets:\r\n * KAFKA-19174\r\n * KAFKA-19654\r\n\r\n*Environment:*\r\n * Develocity instance (version 2025.2.4): [https://develocity.apache.org/scans?search.rootProjectNames=kafka]\r\n * Gradle version 8.14.3, GitHub Actions for Gradle builds: 4.3.0\r\n * Gradle version 9.0.0, GitHub Actions for Gradle builds: 4.4.3\r\n * note: Github Actions for Gradle is using wrapper Gradle version (definition is here: .github/actions/setup-gradle/action.yml)\r\n\r\n*Scenario:*\r\n * upgrade {{com.gradle.develocity}} Gradle plugin version from a current version (3.19) to any recent version (3.19.1 and/or more recent)\r\n * GitHub Action build breaks (with this error: \"build scan failed to be published\") for both Gradle 8.14.3 and Gradle 9.0.0\r\n * (!) note for commit that contains Gradle 9.0.0: Github Actions build shows wrong Gradle version (8.14.3)\r\n\r\n*Test cases:*\r\n * test case [1]\r\n ** baseline: Gradle 8.14.3 /com.gradle.develocity 3.19\r\n ** git commit: last main branch (trunk) commit\r\n ** build result -->> everything works fine (/) (no surprises here, it's a trunk after all)\r\n * test case [2]\r\n ** Gradle version 8.14.3 / com.gradle.develocity 3.19.2\r\n ** git commit: [https://github.com/apache/kafka/pull/20450/commits/d930867e8c2b4dd3f79f032e7b17328cf0ea97ef]\r\n ** Github actions error: (x) [https://github.com/apache/kafka/actions/runs/17714050963]\r\n * test case [3]\r\n ** Gradle version 9.0.0 / com.gradle.develocity", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove `SPARK_JENKINS` and related logics from `dev/run-tests.py`\nDescription: \nQ: Issue resolved by pull request 52548\n[https://github.com/apache/spark/pull/52548]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support Geography and Geometry in SRS mappings\nDescription: \nQ: Sorry, also in progress: https://github.com/apache/spark/pull/52667.", "output": "But don't worry - there will be others soon, I can ping you then [~dekrate]!"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Implement transform in column API in PySpark\nDescription: related to https://issues.apache.org/jira/browse/SPARK-53779\nQ: Issue resolved by pull request 52593\n[https://github.com/apache/spark/pull/52593]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see [https://github.com/apache/kafka/pull/20614#pullrequestreview-3379156676]\r\n\r\nRegarding the implementation of the nullable vs non-nullable types. We use 3 different approaches.\r\n # For bytes, we implement two independent classes BYTES and NULLABLE_BYTES.\r\n # For array, we use one class ArraryOf, which takes a nullable param.\r\n # For schema, we implement NULLABLE_SCHEMA as a subclass of SCHEMA.\r\n\r\nWe need  to pick one approach to implement all nullable types in a consistent way.", "output": "Refactor Nullable Types to Use a Unified Pattern"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade sbt-pom-reader from 2.4.0 to 2.5.0\nDescription: \nQ: Issue resolved by pull request 52580\n[https://github.com/apache/spark/pull/52580]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "while working on HADOOP-19554 I added a per-bucket setting for fs.s3a.buffer.dir\r\n\r\nafter this, ITestS3AConfiguration.testDirectoryAllocatorDefval() would fail in a test run of the entire class, but not if run alone.\r\n\r\nCauses\r\n* dir allocator map of config key to allocator is static; previous uses tainted outcome\r\n* per-bucket settings were't being overridden. This is complicated by the fact that \"unset\" isn't a setting, therefore can't be forced in. Instead some whitespace needs to be set.", "output": "S3A: ITestS3AConfiguration.testDirectoryAllocatorDefval() failing"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-dynamometer-workload.\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{{`DESCRIBE AS JSON`}} is a Spark SQL syntax added in Spark 4.0.0 that returns table metadata as structured JSON.\r\nSee SPARK-50541 for more information.\r\n\r\nThis umbrella JIRA tracks ongoing work to provide structured JSON output for additional Spark SQL metadata inspection commands.", "output": "Structured JSON output for Spark SQL metadata commands"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Build and Release Hadoop-Thirdparty 1.4.0", "output": "Release hadoop-thirdparty 1.4.0"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Include cipher feature for HttpServer2 and SSLFactory\nDescription: Currently, we have a feature to exclude weak ciphers from *HttpServer2* and *SSLFactory* using the *ssl.server.exclude.cipher.list property*. \r\nWith this feature, we can also define an inclusion list of ciphers using the *ssl.server.include.cipher.list property*. \r\nIf the inclusion list is populated, any cipher not present in the list will not be allowed. \r\nIf a cipher is present in both the exclusion and inclusion lists, it will be excluded.\r\nNote that SSLFactory does not support regex-based cipher patterns, unlike HttpServer2.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The goal of this work item is to enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes.", "output": "ABFS: Enhance performance of ABFS driver for write-heavy workloads"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Deadlock in multipart upload\nDescription: Reproduced while testing system resilience and turning S3 network off (introduced a network partition to the list of IP addresses S3 uses) - but given it's seemingly timers related stack traces, I'd guess it could happen any time?\r\n\r\n\r\n{code:java}\r\nFound one Java-level deadlock:\r\n=============================\r\n\"sdk-ScheduledExecutor-2-3\":\r\n  waiting to lock monitor 0x00007f5c880a8630 (object 0x0000000315523c78, a java.lang.Object),\r\n  which is held by \"sdk-ScheduledExecutor-2-4\"\r\n\"sdk-ScheduledExecutor-2-4\":\r\n  waiting to lock monitor 0x00007f5c7c016700 (object 0x0000000327800000, a org.apache.hadoop.fs.s3a.S3ABlockOutputStream),\r\n  which is held by \"io-compute-blocker-15\"\r\n\"io-compute-blocker-15\":\r\n  waiting to lock monitor 0x00007f5c642ae900 (object 0x00000003af0001d8, a java.lang.Object),\r\n  which is held by \"sdk-ScheduledExecutor-2-3\"\r\nJava stack information for the threads listed above:\r\n===================================================\r\n\"sdk-ScheduledExecutor-2-3\":\r\n        at java.lang.Thread.interrupt(java.base@21/Thread.java:1717)\r\n        - waiting to lock  (a java.lang.Object)\r\n        at software.amazon.awssdk.core.internal.http.timers.SyncTimeoutTask.run(SyncTimeoutTask.java:60)\r\n        - locked  (a java.lang.Object)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(java.base@21/Executors.java:572)\r\n        at java.util.concurrent.FutureTask.run(java.base@21/FutureTask.java:317)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$Schedul", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Unbound Error Thrown if some variables are not set for SASL/SSL configuration\nDescription: I was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file — it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set -u}}{*}.\r\n\r\nThis issue is {*}just an enhancement to properly log the error details{*}.\nQ: Hi [~scienmanas] , I’d like to take this issue and work on improving the error handling for missing SASL/SSL environment variables in the Docker setup.  \r\nCould you please assign this to me?", "output": "Hi [~crw31] , I guess I don't have access to assign the issues."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, we display the following error message when a path contains a double slash without a preceding authority or bucket name. \r\n{code:java}\r\njava.lang.IllegalArgumentException: Wrong FS: s3://test_file.json.gz, expected: s3://test_bucket/ at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:824) at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:683){code}\r\nThe path used in the above case is s3://test_bucket{*}//{*}test_file.json.gz. \r\n\r\nIt is expected that Hadoop will treat the path name preceding the double slash (//) as a bucket or authority because, according to HADOOP-8087, a relative reference that begins with two slash characters is termed a network-path reference.\r\n\r\nCreated this Jira to evaluate printing the full path in the error message or to revise the error message when the path contains a double slash without a preceding authority or bucket name for easier debugging.", "output": "Improve error message when the path contains double slash without a preceding authority or bucket name"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [ABFS] Rename/Create path idempotency client-level resolution\nDescription: CreatePath and RenamePath APIs are idempotent as subsequent retries on same resource don’t change the server state. However, when client experiences connection break on the CreatePath and the RenamePath APIs, client cannot make sense if the request is accepted by the server or not. \r\n\r\nOn connection failure, the client retries the request. The server might return 404 (sourceNotFound) in case of RenamePath API and 409 (pathAlreadyExists) in case of CreatePath (overwrite=false) API. Now the client doesn’t have a path forward. Reason being, in case of CreatePath, client doesn’t know if the path was created on the original request or the path was already there for some other request, in case of RenamePath, client doesn’t know if the source was removed because of the original-try or it was not there on the first place.\nQ: bhattmanish98 opened a new pull request, #7364:\nURL: https://github.com/apache/hadoop/pull/7364\n\n   Description\r\n   -----------------------------------------------------------------------------------------------------------------------------------------------\r\n   CreatePath and RenamePath APIs are idempotent as subsequent retries on same resource don’t change the server state[[1]](bookmark://rfcIdempotency). However, when client experiences connection break on the CreatePath and the RenamePath APIs, client cannot make sense if the request is accepted by the server or not. \r\n   \r\n   On connection failure, the client retries the request. The server might return 404 (sourceNotFound) in case of RenamePath API and 409 (pathAlreadyExists) in case of CreatePath (overwrite=false) API. Now the client doesn’t have a path forward. Reason being, in case of CreatePath, client doesn’t know if the path was created on the original request or the path was already there for some other request, in case of RenamePath, client doesn’t know if the source was removed because of the original-try or it was not there on the first place.\r\n   \r\n   Proposed Solution\r\n   ---------------------------------------------------------------------------------------------------------------------------------------------\r\n   Driver will send addition header \"x-ms-client-transaction-id\" which will store by the server. In case first call fails because of time out and retry happens and server throw source not found (in case of rename) and path already exist (in case of create call). Driver will do list call on the path and check whether the \"x-ms-client-transaction-id\" returned by server same as what driver has at its end. In such case driver will return success to the caller.", "output": "hadoop-yetus commented on PR #7364:\nURL: https://github.com/apache/hadoop/pull/7364#issuecomment-2640770705\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 42s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 131m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7364 |\r\n   | JIRA Issue | HADOOP-19450 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7d4c7cf69c8d 5.15.0-125-generic #135-Ubuntu SMP Fri Sep 27 13:53:58 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e2eba214503db812a4f2f7e19f6e1417ba227929 |\r\n   | Default Java | Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.25+9-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_432-8u432-ga~us1-0ubuntu2~20.04-ga |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/1/testReport/ |\r\n   | Max. process+thread count | 541 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7364/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Migrate ViewFileSystemBaseTest to Junit 5\nDescription: Its children have been converted to Junit 5, but the parent has not.\r\nThis breaks most of the child test classes (at least with the latest JUnit5+Surefire).\r\nThe breakage may not happen with the current old Surefire, but it is more likely that it is just silently ignored.\nQ: hadoop-yetus commented on PR #7646:\nURL: https://github.com/apache/hadoop/pull/7646#issuecomment-2827031446\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 19s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 25s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 48s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 56s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   7m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m 12s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/2/artifact/out/blanks-eol.txt) |  The patch has 4 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 37s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/2/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 33 new + 26 unchanged - 33 fixed = 59 total (was 59)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 54s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 15s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  12m 59s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 120m 39s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7646 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5e8d2a84d7fc 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0e69dd3cb3a71ac8531bf6be81c5b8ab15066b90 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/2/testReport/ |\r\n   | Max. process+thread count | 2635 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7646:\nURL: https://github.com/apache/hadoop/pull/7646#issuecomment-2827091314\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  10m 31s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 44s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 51s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  28m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  10m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  10m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 56s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   8m 56s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/1/artifact/out/blanks-eol.txt) |  The patch has 4 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 37s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 33 new + 26 unchanged - 33 fixed = 59 total (was 59)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  27m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  13m  1s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 56s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7646 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 56915caffafa 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0e69dd3cb3a71ac8531bf6be81c5b8ab15066b90 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/1/testReport/ |\r\n   | Max. process+thread count | 1954 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7646/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: acknowledge Guava license on LimitInputStream\nDescription: When ASF projects copy 3rd party code into their code bases, they are meant to:\r\n* check the orginal license is Category A - https://www.apache.org/legal/resolved.html\r\n* keep the original source code headers\r\n* add something to their LICENSE that mentions the source file and what license is on it\r\n* if the 3rd party project is Apache licensed but has a NOTICE file, we need to copy its contents to our NOTICE\r\n* these requirements are only negated if the original code is submitted to the ASF project by the code's copyright holder (the individual or company that wrote the original code).\r\n\r\n* Hadoop copy https://github.com/apache/hadoop/blob/c357e435fd691b4c184b82325bef6d7c65e5f32b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LimitInputStream.java#L39\r\n* Original code has a Guava copyright that we probably need to keep https://github.com/google/guava/blob/master/guava/src/com/google/common/io/ByteStreams.java\nQ: pjfanning opened a new pull request, #7821:\nURL: https://github.com/apache/hadoop/pull/7821\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   See https://issues.apache.org/jira/browse/HADOOP-19634\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "steveloughran merged PR #7821:\nURL: https://github.com/apache/hadoop/pull/7821"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update Daemon to restore pre JDK22 Subject behaviour in Threads\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add -o option in hdfs \"count\" command to show the owner's summarization.", "output": "Add -o option in hdfs \"count\" command to show the owner's summarization."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Kafka streams with chained emitStrategy(onWindowClose) example does not work\nDescription: Hi, I got this example by using the following prompt in Google:\r\n # kafka streams unit testing with chained \"emitStrategy\"\r\n # Provide an example of testing chained suppress with different grace periods\r\n\r\n[https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d]\r\n\r\nCompiled and ran the example using latest kafka jars only to get\r\n\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s  expected:  but was: \r\nat org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:158)\r\nat org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:139)\r\nat org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:201)\r\nat org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:168)\r\nat org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:694)\r\nat com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopo\nQ: The record you use to close the \"second window\" (ie, `inputTopic.pipeInput(\"D\", \"value6\", secondWindowCloseTime);`) will be processed by the first window, and will get \"stuck there\". It does open a new 1-minute window [10:07; 10:08) which is still open, and thus no result is emitted. Hence, the time for the second windowed-aggregation does not advance to `10:07` and it's own window [10:00; 10:05) does not close yet.", "output": "Thanks. is this a bug or a feature?\r\nI don't want several 1-minute windows for this test\r\nHow do I fix the test so the end result (finalOutputTopic) contains a single aggregated value?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use `4.1.0-preview3` instead of `RC1`\nDescription: \nQ: Issue resolved by pull request 258\n[https://github.com/apache/spark-connect-swift/pull/258]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: ITestS3AConfiguration.testDirectoryAllocatorDefval() failing\nDescription: while working on HADOOP-19554 I added a per-bucket setting for fs.s3a.buffer.dir\r\n\r\nafter this, ITestS3AConfiguration.testDirectoryAllocatorDefval() would fail in a test run of the entire class, but not if run alone.\r\n\r\nCauses\r\n* dir allocator map of config key to allocator is static; previous uses tainted outcome\r\n* per-bucket settings were't being overridden. This is complicated by the fact that \"unset\" isn't a setting, therefore can't be forced in. Instead some whitespace needs to be set.\nQ: {code}\r\n[ERROR] org.apache.hadoop.fs.s3a.ITestS3AConfiguration.testDirectoryAllocatorDefval  Time elapsed: 0.029 s  <<< ERROR!\r\njava.io.IOException: fs.s3a.buffer.dir not configured\r\n        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:319)\r\n        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:425)\r\n        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:171)\r\n        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:152)\r\n        at org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.createTemporaryFileForWriting(S3AStoreImpl.java:933)\r\n        at org.apache.hadoop.fs.s3a.ITestS3AConfiguration.createTemporaryFileForWriting(ITestS3AConfiguration.java:502)\r\n        at org.apache.hadoop.fs.s3a.ITestS3AConfiguration.testDirectoryAllocatorDefval(ITestS3AConfiguration.java:491)\r\n{code}", "output": "steveloughran opened a new pull request, #7699:\nURL: https://github.com/apache/hadoop/pull/7699\n\n   \r\n   \r\n   * trim the buffer dir string before the probe (more robust code anyway)\r\n   * tests to set this, and remove any instantiated context mappers\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   in the ide until eventually the stack trace went away\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/pull/20345#issuecomment-3201413774", "output": "Implement cleanup mechanism for obsolete Remote Log Metadata"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support AQE in stateless streaming workloads\nDescription: We have been disabling AQE for streaming workloads since stateful operators do not work with AQE. But applying AQE in stateless workloads is still beneficial and we have been blocking the case too aggressively.\r\n\r\nWe have been observed streaming workloads which can enjoy the benefits of AQE e.g. stream-static join with large static table, MERGE INTO in ForeachBatch sink, etc. To accelerate the workloads, we would want to support AQE in stateless streaming workloads.\nQ: Issue resolved by pull request 52642\n[https://github.com/apache/spark/pull/52642]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Upgrade AWS V2 SDK to 2.29.52\nDescription: Upgrade to 2.29.52 -the last version compatible with third party stores until there are fixes in the AWS SDK or workarounds added in the S3A connector\r\n\r\nThis SDK update doesn't need to come with some changes to disable some new features (default integrity protections),\r\nand to apply critical changes related to the SDK\r\n\r\nDefault integrity protection came with 2.30, and is on unless disabled.\r\nhttps://github.com/aws/aws-sdk-java-v2/issues/5801\r\n\r\nAs well as being incompatible with third party stores, it has also affected S3 multiregion Access Points: https://github.com/aws/aws-sdk-java-v2/issues/5878\r\n\r\nThis has broken most interaction with third party stores, hence fixes in Iceberg https://github.com/apache/iceberg/pull/12264 and Trinio https://github.com/trinodb/trino/pull/24954\r\n\r\nThere's also [AWS v2.30 SDK InputStream behavior changes #5859](AWS v2.30 SDK InputStream behavior changes).\r\nIt looks like our code is safer from that, but it did require code review.\r\n\r\nSDK 2.30.19 seems\nQ: steveloughran commented on PR #7479:\nURL: https://github.com/apache/hadoop/pull/7479#issuecomment-2706147948\n\n   testing in progress; also writing a new, expanded and very strict doc on qualifying a release, based on the experience of recent upgrades.", "output": "hadoop-yetus commented on PR #7479:\nURL: https://github.com/apache/hadoop/pull/7479#issuecomment-2706148652\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 16s |  |  https://github.com/apache/hadoop/pull/7479 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7479 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7479/1/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "https://github.com/apache/kafka/pull/20466#discussion_r2379695582", "output": "Depreacte broker-level config group.coordinator.rebalance.protocols"}
{"instruction": "Answer the question based on the bug.", "input": "Title: LdapAuthenticationHandler supports configuring multiple ldapUrls.\nDescription: Currently, LdapAuthenticationHandler supports only a single ldap server url, this can obviously cause issues if the ldap instance goes down. This JIRA attempts to improve this by allowing users to list multiple ldap server urls, and performing a failover if we detect any issues.\nQ: github-actions[bot] commented on PR #7772:\nURL: https://github.com/apache/hadoop/pull/7772#issuecomment-3383635403\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution.", "output": "github-actions[bot] closed pull request #7772: HADOOP-19598. LdapAuthenticationHandler supports configuring multiple ldapUrls.\nURL: https://github.com/apache/hadoop/pull/7772"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Streams groups sometimes describe as NOT_READY when STABLE\nDescription: Streams groups sometimes describe as NOT_READY when STABLE. That is, the group is configured and all topics exist, but when you use LIST_GROUP and STREAMS_GROUP_DESCRIBE, the group will show up as not ready.\r\n\r\nThe root cause seems to be that [https://github.com/apache/kafka/pull/19802] moved the creation of the soft state configured topology from the replay path to the heartbeat. This way, LIST_GROUP and STREAMS_GROUP_DESCRIBE, which show the snapshot of the last committed offset, may not show the configured topology, and therefore assume that the streams group is NOT_READY instead of STABLE.\nQ: Hi [~lucasbru] , if you haven't start to handle this issue yet, maybe I could help. :)", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support building with Java 25 (LTS release)\nDescription: *Depends upon/blocked by:*\r\n * Kafka ticket: KAFKA-19174 Gradle version upgrade 8 -->> 9 (review is under way)\r\n * Spotbugs next version:\r\n ** [https://github.com/spotbugs/spotbugs/issues/3564]\r\n ** [https://github.com/spotbugs/spotbugs/issues/3569]\r\n ** https://issues.apache.org/jira/browse/BCEL-377 \r\n ** [https://github.com/spotbugs/spotbugs/pull/3712]\r\n ** [https://github.com/spotbugs/spotbugs/discussions/3380] \r\n\r\n*Related links:*\r\n * JDK 25 release date: September 16th 2025:\r\n ** [https://mail.openjdk.org/pipermail/announce/2025-September/000360.html]\r\n ** [https://openjdk.org/projects/jdk/25]\r\n * Gradle 9.1 will support Java 25: [https://docs.gradle.org/9.1.0/release-notes.html#support-for-java-25]\r\n * Scala 2.13.17 is also announced:\r\n ** [https://docs.scala-lang.org/overviews/jdk-compatibility/overview.html#jdk-25-compatibility-notes]\r\n ** [https://contributors.scala-lang.org/t/scala-2-13-17-release-planning/6994/10]\r\n ** [https://github.com/scala/scala/milestone/109]\r\n * other related links:\r\n ** [https://github.com/adoptium/temurin/issues/96]\r\n ** [https://launchpad.net/ubuntu/+source/openjdk-25]\r\n ** [https://github.com/actions/setup-java/issues/899]\r\n\r\n*Related Github PR:* [https://github.com/apache/kafka/pull/20295] _*MINOR: Run CI with Java 24*_\r\n\r\nFYI [~ijuma] [~chia7712]", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Kafka Broker Fails to Start After Upgrade to 4.1.0 Due to Invalid segment.bytes Configuration\nDescription: After upgrading our Kafka brokers to version {*}4.1.0{*}, the broker fails to start due to a misconfigured topic-level {{segment.bytes}} setting. The new version enforces a *minimum value of 1MB (1048576 bytes)* for this configuration, and any value below this threshold causes the broker to terminate during startup.\r\n*Error Details:*\r\n**\r\n \r\n{code:java}\r\n[2025-09-12 14:39:51,285] ERROR Encountered fatal fault: Error starting LogManager (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler) org.apache.kafka.common.config.ConfigException: Invalid value 75000 for configuration segment.bytes: Value must be at least 1048576 at org.apache.kafka.common.config.ConfigDef$Range.ensureValid(ConfigDef.java:989) ~[kafka-clients-4.1.0.jar:?]\r\n \r\n{code}\r\nIn our setup, some topics were previously configured with a lower segment.bytes value (e.g., 75000), which was allowed in earlier Kafka versions but is now invalid.\r\n\r\nAs a result Kafka broker cannot start, leading to downtime and unavailability.No snapshot file exists yet, so the {{kafka-metadata-shell}} tool cannot be used to patch the config offline.\r\nWe would appreciate your guidance on the following:\r\n * Are there any supported methods from Kafka 4.1.0 to override or bypass this validation at startup to recover without losing data?\r\n\r\n * If not, is there a documented approach to fix such configuration issues when snapshots are not yet available?", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "As a follow-on to the improvements introduced in [KIP-1091|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1091%3A+Improved+Kafka+Streams+operator+metrics] it would be useful to add the application-id as a tag to the `client-state` metric. This allows Kafka Streams developers and operators to connect metrics containing a `thread-id` (which embeds the `application-id`) across separate deployments of Kafka Streams instances, which are members of the same logical application.\r\n\r\nKIP-1221 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1221%3A+Add+application-id+tag+to+Kafka+Streams+state+metric]", "output": "Add application-id as a tag to the ClientState JMX metric"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `docker-java` to 3.6.0\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Define Proto for Sinks\nDescription: \nQ: Issue resolved by pull request 52553\n[https://github.com/apache/spark/pull/52553]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: upgrade to jackson 2.18\nDescription: follow up to HADOOP-19259\nQ: pjfanning opened a new pull request, #7623:\nURL: https://github.com/apache/hadoop/pull/7623\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   HADOOP-19544\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7623:\nURL: https://github.com/apache/hadoop/pull/7623#issuecomment-2811310081\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 37s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 49s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  19m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 31s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |  10m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 30s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  3s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  30m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 29s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   2m 22s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |   8m 44s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 46s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |  11m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shellcheck  |   0m  0s |  |  No new issues.  |\r\n   | +1 :green_heart: |  javadoc  |   5m 25s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  2s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  shadedclient  |  15m 14s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 544m 27s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/1/artifact/out/patch-unit-root.txt) |  root in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  0s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 676m 39s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServices |\r\n   |   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerMultiNodes |\r\n   |   | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |\r\n   |   | hadoop.yarn.server.timeline.webapp.TestTimelineWebServices |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClientPlacementConstraints |\r\n   |   | hadoop.yarn.client.api.impl.TestOpportunisticContainerAllocationE2E |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMProxy |\r\n   |   | hadoop.yarn.client.api.impl.TestAMRMClient |\r\n   |   | hadoop.yarn.client.api.impl.TestYarnClient |\r\n   |   | hadoop.yarn.client.api.impl.TestNMClient |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7623 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint shellcheck shelldocs |\r\n   | uname | Linux 1106262bb69e 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 41ab79fb7df5086d2cd8dddcfa19f501a036a28e |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/1/testReport/ |\r\n   | Max. process+thread count | 3279 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7623/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 shellcheck=0.7.0 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Fix WASB ABFS compatibility issues\nDescription: Fix WASB ABFS compatibility issues. Fix issues such as:-\r\n # BlockId computation to be consistent across clients for PutBlock and PutBlockList\r\n # Restrict url encoding of certain json metadata during setXAttr calls.\r\n # Maintain the md5 hash of whole block to validate data integrity during flush.\nQ: hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3031894608\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 12 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 47s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 16 unchanged - 0 fixed = 20 total (was 16)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 31s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 4 new + 10 unchanged - 0 fixed = 14 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 4 new + 10 unchanged - 0 fixed = 14 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 46s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 34s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2e8591fd6bca 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6b138c098a45024cbab113550c7130911a21df41 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/testReport/ |\r\n   | Max. process+thread count | 528 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3031953572\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  25m 18s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  1s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 23s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 15 unchanged - 0 fixed = 19 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 4 new + 10 unchanged - 0 fixed = 14 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 4 new + 10 unchanged - 0 fixed = 14 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 163m 59s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 15ce74eb5c22 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8292192be43adff87a99987e83f64455e9d42207 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation\nDescription: This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-V platforms supporting the Zbc extension (CLMUL instructions) in bulk_crc32_riscv.c.\r\nKey changes:\r\n * Implements optimized CRC32 and CRC32C routines using CLMUL instructions for zlib and Castagnoli polynomials.\r\n * Automatically switches to hardware acceleration when Zbc is available, otherwise falls back to generic table-based software implementation.\r\n * Maintains compatibility with platforms lacking Zbc support.\r\n\r\nThis optimization improves CRC performance on RISC-V CPUs with Zbc extension.\nQ: hadoop-yetus commented on PR #7896:\nURL: https://github.com/apache/hadoop/pull/7896#issuecomment-3222853817\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  27m 44s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m  7s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 45s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 105m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  16m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 47s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 12s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  24m 53s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 47s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 223m 28s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7896 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 4727479d09e6 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 58baa6eb96c75dd288767f650918073c5c094a6f |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/testReport/ |\r\n   | Max. process+thread count | 3133 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/2/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7896:\nURL: https://github.com/apache/hadoop/pull/7896#issuecomment-3222859121\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  29m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 41s |  |  trunk passed  |\r\n   | -1 :x: |  mvnsite  |   1m 45s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 107m 55s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   1m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |  16m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |  16m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |  16m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   1m 43s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  24m 53s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 54s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 227m 14s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7896 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux 262a84b27cca 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 58baa6eb96c75dd288767f650918073c5c094a6f |\r\n   | Default Java | Red Hat, Inc.-1.8.0_312-b07 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/testReport/ |\r\n   | Max. process+thread count | 3133 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7896/1/console |\r\n   | versions | git=2.27.0 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Exclude junit 4 transitive dependency\nDescription: HADOOP-19617 removed direct junit 4 dependency.  However, junit 4 is still pulled transitively by other dependencies.\nQ: {code}\r\n[INFO] --------------------------------\r\n[INFO] Building Apache Hadoop HttpFS 3.5.0-SNAPSHOT                    [19/117]\r\n[INFO]   from hadoop-hdfs-project/hadoop-hdfs-httpfs/pom.xml\r\n...\r\n[INFO] +- com.googlecode.json-simple:json-simple:jar:1.1.1:compile\r\n[INFO] |  \\- junit:junit:jar:4.10:compile\r\n{code}", "output": "{code}\r\n[INFO] -----------\r\n[INFO] Building Apache Hadoop YARN Application Catalog Webapp 3.5.0-SNAPSHOT [63/117]\r\n[INFO]   from hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml\r\n...\r\n[INFO] +- org.apache.solr:solr-test-framework:jar:8.11.2:test\r\n[INFO] |  +- org.apache.lucene:lucene-test-framework:jar:8.11.2:test\r\n[INFO] |  +- com.carrotsearch.randomizedtesting:junit4-ant:jar:2.7.2:test\r\n[INFO] |  +- com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.7.2:test\r\n[INFO] |  +- io.opentracing:opentracing-mock:jar:0.33.0:test\r\n[INFO] |  +- junit:junit:jar:4.13.1:test\r\n{code}"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Migrate FileContextPermissionBase to Junit 5\nDescription: TestFcLocalFsPermission has been recently migrated to Junit 5. However, its superclass FileContextPermissionBase uses Junit 4, which causes the class initializerers (BeforeAll/BeforeClass) not to be called in certain circumstances.\r\n\r\nMigrate FileContextPermissionBase and its children fully to Junit 5.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "TestExternalCall fails when SecurityManager cannot be set, we need to skip it on thos JVMs.\r\n\r\nTestGridmixSubmission has already been rewritten to use ExitUtil, we just need to remove the leftover SecurityManager calls.", "output": "Skip tests in Hadoop common that depend on SecurityManager if the JVM does not support it"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce type encoders for Geography and Geometry types\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52813.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: JVM GC Metrics supports fine-grained metrics\nDescription: See [https://bugs.openjdk.org/browse/JDK-8307059] , The Generational ZGC separate the Cycles and Pauses as Minor and Major,like this:\r\n * Old ZGC: \"ZGC Cycles\", \"ZGC Pauses\"\r\n * Generational ZGC: \"ZGC Minor Cycles\", \"ZGC Minor Pauses\", \"ZGC Major Cycles\", \"ZGC Major Pauses\"\r\n\r\nlet us separate it same and give 2 new metric about these, may be better...\r\n\r\n-----------------------------------------------------------------------------------------\r\n\r\nImpl to support get minor/major GC time/count from Hadoop Jmx\nQ: test on my cluster, see HADOOP-19461.jpg", "output": "hadoop-yetus commented on PR #7406:\nURL: https://github.com/apache/hadoop/pull/7406#issuecomment-2668953274\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 10s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 21s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/branch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/branch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/branch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in trunk failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/buildtool-branch-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/buildtool-branch-checkstyle-hadoop-common-project_hadoop-common.txt) |  The patch fails to run checkstyle in hadoop-common  |\r\n   | -1 :x: |  mvnsite  |   1m 26s | [/branch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/branch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-common in trunk failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/branch-javadoc-hadoop-common-project_hadoop-common-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/branch-javadoc-hadoop-common-project_hadoop-common-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-common in trunk failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | -1 :x: |  spotbugs  |   0m 14s | [/branch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/branch-spotbugs-hadoop-common-project_hadoop-common.txt) |  hadoop-common in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   3m 28s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m  9s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 48s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 48s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 19s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | -1 :x: |  javac  |   0m 19s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/buildtool-patch-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/buildtool-patch-checkstyle-hadoop-common-project_hadoop-common.txt) |  The patch fails to run checkstyle in hadoop-common  |\r\n   | -1 :x: |  mvnsite  |   0m 56s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  spotbugs  |   0m  9s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/patch-spotbugs-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 11s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  42m 18s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7406 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c94bbf019c14 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 404fcfc6451abb610d136775c3ee14146b4f7ffe |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7406/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Read Buffer Manager V2 should not be allowed untill implemented\nDescription: Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used.\r\nThis is to stop any user explicitly enabling the config to enable RBMV2.\nQ: hadoop-yetus commented on PR #8002:\nURL: https://github.com/apache/hadoop/pull/8002#issuecomment-3346210665\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   9m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  30m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 20s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  90m  2s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8002 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7ed8c85f9284 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bc356262478fb82c786dc3bee7c312b2b1a29634 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/testReport/ |\r\n   | Max. process+thread count | 566 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "anujmodi2021 merged PR #8002:\nURL: https://github.com/apache/hadoop/pull/8002"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add `cluster-preview.yaml` and `spark-connect-server-preview.yaml`\nDescription: \nQ: Issue resolved by pull request 392\n[https://github.com/apache/spark-kubernetes-operator/pull/392]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add bswap support for RISC-V\nDescription: *Problem*\r\n\r\nWhile building Hadoop native code on the riscv64 architecture, the build fails due to missing support for bswap operations. The standard implementation relies on compiler intrinsics or platform-specific assembly which are not defined for RISC-V targets.\r\n\r\nThis results in compilation errors such as:\r\n\r\n* Error: unrecognized opcode `bswap a4'\r\n* Error: unrecognized opcode `bswap a3'\r\n\r\n*Resolution*\r\n\r\nAdd bswap support for RISC-V.\nQ: leiwen2025 opened a new pull request, #7809:\nURL: https://github.com/apache/hadoop/pull/7809\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   While building Hadoop native code on the riscv64 architecture, the build fails due to missing support for bswap operations. The standard implementation relies on compiler intrinsics or platform-specific assembly which are not defined for RISC-V targets.\r\n   \r\n   This results in compilation errors such as:\r\n   ```\r\n   Error: unrecognized opcode `bswap a4'\r\n   Error: unrecognized opcode `bswap a3'\r\n   ```\r\n   Add bswap support for RISC-V can solve the problem.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   mvn package -Pdist,native -DskipTests -Dtar\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7809:\nURL: https://github.com/apache/hadoop/pull/7809#issuecomment-3082749075\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  45m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  80m 13s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7809/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   1m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  | 122m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  cc  |   0m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  golang  |   0m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javac  |   0m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 50s |  |  hadoop-mapreduce-client-nativetask in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 214m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7809/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7809 |\r\n   | Optional Tests | dupname asflicense compile cc mvnsite javac unit codespell detsecrets golang |\r\n   | uname | Linux bb3bde6d0dbd 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3c622c7607941be6914942289a9ea1e4643d2bee |\r\n   | Default Java | Red Hat, Inc.-1.8.0_412-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7809/1/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7809/1/console |\r\n   | versions | git=2.9.5 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update to ApacheDS 2.0.0.AM27 and ldap-api 2.1.7\nDescription: ApacheDS 2.0.0AM26 (or at least its test integration) is incompatible with JDK21+.\r\n\r\nUpdate to 2.0.0.AM27 and the corresponding ldap-api version.\r\n\r\nThis also requires migrating some dependent tests from Junit 4 to Junit 5.\r\n\r\n{noformat}\r\n[ERROR] org.apache.hadoop.security.authentication.server.TestMultiSchemeAuthenticationHandler -- Time elapsed: 1.426 s <<< ERROR!\r\njava.lang.NoSuchMethodError: 'void sun.security.x509.X509CertInfo.set(java.lang.String, java.lang.Object)'\r\n        at org.apache.directory.server.core.security.CertificateUtil.setInfo(CertificateUtil.java:96)\r\n        at org.apache.directory.server.core.security.CertificateUtil.generateSelfSignedCertificate(CertificateUtil.java:194)\r\n        at org.apache.directory.server.core.security.CertificateUtil.createTempKeyStore(CertificateUtil.java:337)\r\n        at org.apache.directory.server.factory.ServerAnnotationProcessor.instantiateLdapServer(ServerAnnotationProcessor.java:158)\r\n        at org.apache.directory.server.factor\nQ: stoty opened a new pull request, #7628:\nURL: https://github.com/apache/hadoop/pull/7628\n\n   ### Description of PR\r\n   \r\n   Update to ApacheDS 2.0.0.AM27 and ldap-api 2.1.5\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran hadoop-auth test suite on my JDK24 branch\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7628:\nURL: https://github.com/apache/hadoop/pull/7628#issuecomment-2812365688\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m 16s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  36m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 45s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 16s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +0 :ok: |  spotbugs  |   0m 40s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 56s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 56s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  15m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m  3s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  15m  3s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 16s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +0 :ok: |  spotbugs  |   0m 38s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  37m 22s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 34s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m  5s |  |  hadoop-auth in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  2s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 209m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7628/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7628 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux 33f8fa3aa330 5.15.0-134-generic #145-Ubuntu SMP Wed Feb 12 20:08:39 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 67f932e041415206999492760101b4d1429a3211 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7628/1/testReport/ |\r\n   | Max. process+thread count | 700 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-auth U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7628/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side\nDescription: Support create and rename idempotency on FNS Blob from client side\nQ: hadoop-yetus commented on PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#issuecomment-3234745585\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 22s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 59s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7914 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5ba8302b1e20 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3ef89e1b92e564468a24cb8d200567b30f283b10 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7914/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "anujmodi2021 commented on code in PR #7914:\nURL: https://github.com/apache/hadoop/pull/7914#discussion_r2309688341\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java:\n##########\n@@ -2236,6 +2238,98 @@ public void testFailureInGetPathStatusDuringCreateRecovery() throws Exception {\n     }\n   }\n \n+  /**\n+   * Test to simulate a successful create operation followed by a connection reset\n+   * on the response, triggering a retry.\n+   *\n+   * This test verifies that the create operation is retried in the event of a\n+   * connection reset during the response phase. The test creates a mock\n+   * AzureBlobFileSystem and its associated components to simulate the create\n+   * operation and the connection reset. It then verifies that the create\n+   * operation is retried once before succeeding.\n+   *\n+   * @throws Exception if an error occurs during the test execution.\n+   */\n+  @Test\n+  public void testCreateIdempotencyForNonHnsBlob() throws Exception {\n+    assumeThat(isAppendBlobEnabled()).as(\"Not valid for APPEND BLOB\").isFalse();\n+    // Create a spy of AzureBlobFileSystem\n+    try (AzureBlobFileSystem fs = Mockito.spy(\n+        (AzureBlobFileSystem) FileSystem.newInstance(getRawConfiguration()))) {\n+      assumeHnsDisabled();\n+      // Create a spy of AzureBlobFileSystemStore\n+      AzureBlobFileSystemStore store = Mockito.spy(fs.getAbfsStore());\n+      assumeBlobServiceType();\n\nReview Comment:\n   We can move all assume before any other statement\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -1702,6 +1705,85 @@ public void testRenamePathRetryIdempotency() throws Exception {\n     }\n   }\n \n+  /**\n+   * Test to simulate a successful copy blob operation followed by a connection reset\n+   * on the response, triggering a retry.\n+   *\n+   * This test verifies that the copy blob operation is retried in the event of a\n+   * connection reset during the response phase. The test creates a mock\n+   * AzureBlobFileSystem and its associated components to simulate the copy blob\n+   * operation and the connection reset. It then verifies that the create\n+   * operation is retried once before succeeding.\n+   *\n+   * @throws Exception if an error occurs during the test execution.\n+   */\n+  @Test\n+  public void testRenameIdempotencyForNonHnsBlob() throws Exception {\n+    assumeThat(isAppendBlobEnabled()).as(\"Not valid for APPEND BLOB\").isFalse();\n+    // Create a spy of AzureBlobFileSystem\n+    try (AzureBlobFileSystem fs = Mockito.spy(\n+        (AzureBlobFileSystem) FileSystem.newInstance(getRawConfiguration()))) {\n+      assumeHnsDisabled();\n\nReview Comment:\n   Same here, move all assume to first few lines\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -509,9 +509,30 @@ public AbfsRestOperation createPath(final String path,\n       final TracingContext tracingContext) throws AzureBlobFileSystemException {\n     AbfsRestOperation op;\n     if (isFileCreation) {\n-      // Create a file with the specified parameters\n-      op = createFile(path, overwrite, permissions, isAppendBlob, eTag,\n-          contextEncryptionAdapter, tracingContext);\n+      AbfsRestOperation statusOp = null;\n+      try {\n+        // Check if the file already exists by calling GetPathStatus\n+        statusOp = getPathStatus(path, false, tracingContext, null);\n\nReview Comment:\n   In case of override true, flow might come here with already a Head call done on path. \r\n   Can we avoid this head call in that case?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h3. Problem Description\r\n\r\nPandas UDFs ({{{}mapInPandas{}}}, {{{}applyInPandas{}}}, etc.) are powerful for custom data processing in PySpark. However, they currently act as a black box to the Catalyst Optimizer. This prevents the optimizer from pushing down filters on columns that pass through the UDF unmodified. As a result, filtering operations occur _after_ the expensive UDF execution and associated data shuffling, leading to significant performance degradation.\r\n\r\nThis is especially common in pipelines where transformations are applied to grouped data, and the grouping key itself is not modified within the UDF.\r\n\r\n*Example:*\r\n\r\nConsider the following DataFrame and Pandas UDFs:\r\n{code:java}\r\nimport pandas as pd\r\nfrom typing import Iterator\r\n\r\ndf = spark.createDataFrame(\r\n    [[\"A\", 1], [\"A\", 1], [\"B\", 2]], \r\n    schema=[\"id string\", \"value int\"]\r\n)\r\n\r\n# UDF to modify the 'value' column\r\ndef map_udf(pdfs: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\r\n    for pdf in pdfs:\r\n        pdf[\"value\"] = pdf[\"value\"] + 1\r\n        yield pdf\r\n\r\n# UDF to aggregate data by 'id'\r\ndef agg_udf(pdf: pd.DataFrame) -> pd.DataFrame:\r\n    return pdf.groupby(\"id\").agg(count=(\"value\", \"count\"))\r\n\r\n# Apply the UDFs\r\nmodified_df = (\r\n    df\r\n    .mapInPandas(map_udf, schema=\"id string,value int\")\r\n    .groupby(\"id\")\r\n    .applyInPandas(agg_udf, schema=\"id string,count int\")\r\n)\r\n\r\n# Filter the result\r\nmodified_df.where(\"id == 'A'\").explain() {code}\r\nIn this example, the {{id}} column is never modified by either UDF. However, the filter on {{id}} is applied only after all transformations are complete.\r\n\r\n*Current Physical Plan:*\r\n\r\nThe physical plan shows the {{Filter}} operation at the very top, processing data that has already been scanned, shuffled, and processed by both Pandas UDFs.\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- Filter (isnotnull(id#20) AND (id#20 = A))\r\n   +- FlatMapGroupsInPandas [id#13], agg_udf(id#13, value#14)#19, [id#2", "output": " Enable Filter Push-Down for Pandas UDFs with an Immutable Column Hint"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, no meaningful error message appears when the table metadata is corrupted due to a mismatch between the partition column names in the table schema and the declared partition columns. To facilitate debugging, a detailed error message should be shown in this situation.", "output": "Display a detailed error message when table metadata is corrupted"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "like tool kafka-broker-api-versions.sh, we can get all RPC version from broker.\r\n\r\nIt should also support for controller.\r\n\r\nKIP: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1220%3A+kafka-broker-api-versions+tool+support+controllers]", "output": "kafka-broker-api tool should support to get controller api version"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Use `DescribeTopicsResult.allTopicNames` instead of the deprecated `all` API\nDescription: \nQ: Issue resolved by pull request 52597\n[https://github.com/apache/spark/pull/52597]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The TopologyTestDriver is great for writing fast and reliable tests for Streams applications. However, a shortcoming of the TopologyTestDriver is that it only supports topics with one partition, therefore making it difficult to test that data has been correctly partitioned throughout the entire topology. This is especially relevant for topologies utilizing PAPI, where the user has to manage the partitioning of the data.\r\n\r\nA simple starting point could be to keep the current fully synchronous execution for a single input event, just with a TestInputTopic that supports a “Partitioner”.", "output": "Multiple-partition topic support for TopologyTestDriver"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [java.time] hadoop-common\nDescription: \nQ: hadoop-yetus commented on PR #7556:\nURL: https://github.com/apache/hadoop/pull/7556#issuecomment-2764944470\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m  3s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 15 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  8s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  35m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 39s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  14m 51s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   5m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m  7s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m  4s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |  10m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 56s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   3m 35s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |  10m  1s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  javac  |  10m  1s | [/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   9m  4s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | -1 :x: |  javac  |   9m  4s | [/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  root in the patch failed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m 22s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 17 new + 436 unchanged - 7 fixed = 453 total (was 443)  |\r\n   | +1 :green_heart: |  mvnsite  |   4m 24s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 55s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  javadoc  |   3m 44s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  spotbugs  |   2m 26s | [/new-spotbugs-hadoop-common-project_hadoop-common.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/new-spotbugs-hadoop-common-project_hadoop-common.html) |  hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | -1 :x: |  shadedclient  |  11m  3s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  14m 27s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 34s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   5m 40s |  |  hadoop-yarn-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   8m 40s |  |  hadoop-mapreduce-client-core in the patch passed.  |\r\n   | -1 :x: |  unit  |   8m 44s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt) |  hadoop-mapreduce-client-app in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 270m 57s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-common-project/hadoop-common |\r\n   |  |  The class name org.apache.hadoop.util.Clock shadows the simple name of the superclass java.time.Clock  At Clock.java:the simple name of the superclass java.time.Clock  At Clock.java:[lines 32-41] |\r\n   | Failed junit tests | hadoop.mapreduce.v2.app.rm.TestRMContainerAllocator |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7556 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d056f058380f 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 670b19020dc64bc90f7f6e6accaf75f5c54c0055 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/testReport/ |\r\n   | Max. process+thread count | 1325 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7556:\nURL: https://github.com/apache/hadoop/pull/7556#issuecomment-2766575990\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 16 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 51s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  36m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 18s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  14m 59s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   6m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 49s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 46s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |  11m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   4m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  16m 41s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  16m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 45s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |  14m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m 40s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/2/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 15 new + 473 unchanged - 8 fixed = 488 total (was 481)  |\r\n   | +1 :green_heart: |  mvnsite  |   6m 37s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   1m 12s | [/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/2/artifact/out/results-javadoc-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-common-project_hadoop-common-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  javadoc  |   5m 48s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  spotbugs  |   2m 42s | [/new-spotbugs-hadoop-common-project_hadoop-common.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/2/artifact/out/new-spotbugs-hadoop-common-project_hadoop-common.html) |  hadoop-common-project/hadoop-common generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  39m  7s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  14m 44s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   1m 48s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   5m 57s |  |  hadoop-yarn-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   8m 57s |  |  hadoop-mapreduce-client-core in the patch passed.  |\r\n   | -1 :x: |  unit  |   8m 56s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/2/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-app.txt) |  hadoop-mapreduce-client-app in the patch passed.  |\r\n   | -1 :x: |  unit  |  83m 31s | [/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/2/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt) |  hadoop-mapreduce-client-jobclient in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m  8s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 392m 13s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-common-project/hadoop-common |\r\n   |  |  The class name org.apache.hadoop.util.Clock shadows the simple name of the superclass java.time.Clock  At Clock.java:the simple name of the superclass java.time.Clock  At Clock.java:[lines 37-72] |\r\n   | Failed junit tests | hadoop.mapreduce.v2.app.rm.TestRMContainerAllocator |\r\n   |   | hadoop.mapred.TestClusterMRNotification |\r\n   |   | hadoop.mapreduce.v2.TestNonExistentJob |\r\n   |   | hadoop.ipc.TestMRCJCSocketFactory |\r\n   |   | hadoop.mapred.TestReduceFetch |\r\n   |   | hadoop.mapred.TestNetworkedJob |\r\n   |   | hadoop.mapreduce.lib.output.TestJobOutputCommitter |\r\n   |   | hadoop.mapred.TestMiniMRClasspath |\r\n   |   | hadoop.mapreduce.v2.TestMRJobsWithProfiler |\r\n   |   | hadoop.mapred.TestClusterMapReduceTestCase |\r\n   |   | hadoop.mapreduce.TestChild |\r\n   |   | hadoop.mapreduce.security.ssl.TestEncryptedShuffle |\r\n   |   | hadoop.mapred.TestMiniMRBringup |\r\n   |   | hadoop.mapreduce.v2.TestMiniMRProxyUser |\r\n   |   | hadoop.mapreduce.TestMRJobClient |\r\n   |   | hadoop.mapred.TestReduceFetchFromPartialMem |\r\n   |   | hadoop.mapred.TestMRTimelineEventHandling |\r\n   |   | hadoop.mapred.TestLazyOutput |\r\n   |   | hadoop.mapred.TestMerge |\r\n   |   | hadoop.mapreduce.v2.TestMRAMWithNonNormalizedCapabilities |\r\n   |   | hadoop.mapreduce.security.TestJHSSecurity |\r\n   |   | hadoop.mapred.TestJobCounters |\r\n   |   | hadoop.mapreduce.v2.TestMRJobsWithHistoryService |\r\n   |   | hadoop.mapred.TestJobName |\r\n   |   | hadoop.mapred.TestSpecialCharactersInOutputPath |\r\n   |   | hadoop.conf.TestNoDefaultsJobConf |\r\n   |   | hadoop.mapreduce.TestMapReduceLazyOutput |\r\n   |   | hadoop.mapreduce.TestLargeSort |\r\n   |   | hadoop.mapred.TestJobSysDirWithDFS |\r\n   |   | hadoop.mapred.TestMROpportunisticMaps |\r\n   |   | hadoop.mapred.TestMiniMRWithDFSWithDistinctUsers |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7556 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 685a1e869203 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 95d182520019d759fca8fe372e73c20ca5334432 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/2/testReport/ |\r\n   | Max. process+thread count | 2136 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7556/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A : Add Support For Multi-Region S3 Access Points\nDescription: [Amazon S3 Multi-Region Access Points|https://aws.amazon.com/s3/features/multi-region-access-points/] provide a global endpoint for applications that need to access data stored in S3 buckets located in multiple AWS regions. This feature simplifies how applications access data distributed across different geographic locations, providing a single endpoint that automatically routes requests to the bucket with the lowest latency.\r\n\r\nThe current implementation in S3A only allows single region access point and the goal is to expand the scope/implementation to include multi-region-access points as well.\nQ: Multi-Region Access Points supports copy operations using Multi-Region Access Points only as a destination when using the Multi-Region Access Point ARN (Refer : [https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiRegionAccessPointRestrictions.html)]\r\n\r\n \r\n\r\nWe need to skip tests which does copy/rename when MRAP Arn is used. Currently due to Junit 5 changes in hadoop-aws module , S3A ITests are failing which is blocking the development on this feature.\r\n\r\n \r\n\r\ncc: [~stevel@apache.org]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade semanticdb-shared to 4.13.10\nDescription: Ammonite was upgraded to 3.0.3 so semanticdb-shared should be upgraded correspondingly.\r\n\r\nhttps://mvnrepository.com/artifact/com.lihaoyi/ammonite-interp-3.3.6_3.3.6/3.0.3", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: KRaft servers don't handle the cluser-level configs in starting\nDescription: see https://github.com/apache/kafka/pull/18949/files#r2296809389", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Switch direct ctors to IoHandlerFactories for EventLoopGroups\nDescription: \nQ: Issue resolved by pull request 52719\n[https://github.com/apache/spark/pull/52719]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "S3A fails to initialize when S3 bucket namespace is having dot followed by a number. \r\n\r\n{*}Specific Problem{*}: URI parsing fails when S3 bucket names contain a dot followed by a number (like {{{}bucket-v1.1-us-east-1{}}}). Java's\r\nURI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null.\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\nhadoop dfs -ls s3a://bucket-v1.1-us-east-1/\r\n\r\nWARNING: Use of this script to execute dfs is deprecated.\r\nWARNING: Attempting to execute replacement \"hdfs dfs\" instead.\r\n\r\n2025-09-08 06:13:06,670 WARN fs.FileSystem: Failed to initialize filesystem s3://bucket-v1.1-us-east-1/: java.lang.IllegalArgumentException: bucket is null/empty\r\n-ls: bucket is null/empty{code}\r\n \r\n\r\n{*}Please Note{*}: Although there has been discussion on not allowing S3 buckets with such a namespace ([https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/]) , Amazon S3 still allows you to create a bucket with such a namespace.", "output": "Fix S3A failing to initialize S3 buckets having namespace with dot followed by number"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update Boost to 1.86.0\nDescription: The currentlly used Boost 1.72.0 does not compile with recent C++ compilers, which prevents compiling on recent systems (at least without replacing the C++ compiler)\r\n\r\nhdfs-native does not compile with 1.87.0, but does with 1.86.0.\nQ: hadoop-yetus commented on PR #7444:\nURL: https://github.com/apache/hadoop/pull/7444#issuecomment-2692416011\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  43m 56s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m  3s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  37m 21s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |  13m  1s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/branch-compile-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  mvnsite  |   5m  7s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 103m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |  35m 38s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  compile  |   2m 48s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  cc  |   2m 48s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javac  |   2m 48s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   4m 47s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 58s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 533m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 767m 29s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.TestHadoopArchiveLogs |\r\n   |   | hadoop.util.TestNativeCodeLoader |\r\n   |   | hadoop.crypto.TestCryptoCodec |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7444 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets mvnsite unit shellcheck shelldocs compile cc javac |\r\n   | uname | Linux a978e95f2477 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4efbd1479550371e00cbfef28812b25ba2144a3a |\r\n   | Default Java | Red Hat, Inc.-1.8.0_412-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/testReport/ |\r\n   | Max. process+thread count | 3137 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/console |\r\n   | versions | git=2.9.5 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "steveloughran commented on PR #7444:\nURL: https://github.com/apache/hadoop/pull/7444#issuecomment-2701028963\n\n   Afraid you'll have to work out why the build doesn't build.\r\n   \r\n   Other than that though, I'm happy"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support atomic rename() for S3 express\nDescription: S3 Express now supports atomic renames: [https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-s3-express-one-zone-atomic-renaming-objects-api/]\r\n\r\n \r\n\r\nRename API is available as of SDK version 2.31.66.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: KIP-1224: Adaptive append.linger.ms for the group coordinator and share coordinator\nDescription: Ticket for KIP-1224.\r\n\r\n* Add a new allowed value for {{group.coordinator.append.linger.ms}} and {{share.coordinator.append.linger.ms}} of -1.\r\n* When {{append.linger.ms}} is set to -1, use the flush strategy outlined in the KIP.\r\n* Add batch-linger-time metrics.\r\n* Add batch-flush-time metrics.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Call acknowledgement commit callback at end of waiting calls\nDescription: The acknowledgement commit callback in the share consumer gets called on the application thread at the start of the poll, commitSync and commitAsync methods. Specifically in the peculiar case of using the callback together with commitSync, the acknowledgement callback for the committed records is called at the start of the next eligible call, even though the information is already known at the end of the commitSync's execution. The results are correct already, but the timing could be improved in some situations.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SparkSQL partition overwrite is not an atomic operation.\nDescription: I found that SparkSQL partition overwrite is not an atomic operation. When SparkSQL application rewrites existing table partition, [delete the matching partition]([https://github.com/apache/spark/blob/24a6abf34d253162055c8b9bd0030bf9a2ca75b1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala#L136]), then run the job.\r\n\r\nDuring task execution, the Hive partition is not deleted, but the data in the filesystem is deleted. If you read data while the job is running, you will read empty data. If the job is interrupted, the data will be lost.\r\n\r\nNote: Only for spark.sql.hive.convertMetastoreOrc or spark.sql.hive.convertMetastoreParquet is true. It is not problem for hive serde.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, the following validation checks are performed twice for each partition placement:\r\n # {{throwInvalidReplicationFactorIfNonPositive()}}\r\n\r\n # {{throwInvalidReplicationFactorIfTooFewBrokers()}}\r\n\r\n # {{throwInvalidReplicationFactorIfZero()}}\r\n\r\nThese checks are already performed in the public {{place()}} method before the partition placement loop begins. Since the cluster state and replication factor don't change during the placement operation, these checks only need to be performed once.\r\n\r\n \r\n\r\nThis redundant validation could be removed from the inner loop to improve performance, especially when placing many partitions.", "output": "Remove Redundant Validation in StripedReplicaPlacer"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add a Kafka client parameter to limit number of messages fetched\nDescription: h3. Description\r\n\r\nCurrently, Kafka fetch requests only support limiting the total size of messages fetched ({{{}fetch.max.bytes{}}}) and the size per partition ({{{}max.partition.fetch.bytes{}}}). However, there is no way to limit the *number of messages* fetched per request—neither globally nor on a per-partition basis.\r\n\r\nWhile Kafka was originally designed as a high-throughput distributed messaging platform and has traditionally focused more on throughput than individual message control, its role has since evolved. Kafka is now not only a leading message queue but also a core component in modern {*}data pipelines and stream processing frameworks{*}.\r\n\r\nIn these newer use cases, especially for downstream services and streaming applications, *rate-limiting by message count* is a common requirement.\r\n\r\nCurrently, the workaround is for clients to {*}fetch a batch of messages, manually truncate them based on count, and then adjust offsets manually{*}, which is inefficient, error-prone, \nQ: Hello, this requires a KIP, are you going to create one?", "output": "To Federico Valeri: Thanks for the feedback. Yes, I will start drafting a KIP for this proposal."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add message key to org.apache.kafka.clients.producer.RecordMetadata\nDescription: Although the message key is not really {_}metadata{_}, it may be useful to include it in the _org.apache.kafka.clients.producer.RecordMetadata_ class so that metdata can be tied back to a specific message.\r\nWhen using a standard Kafka producer it is easy to tie back the metadata to a specific message by using the callback mechanism.\r\nHowever, when using Kafka streams, the only way to access the metadata and log the details is to register a stream-level _org.apache.kafka.clients.producer.ProducerInterceptor_ instance.\r\nThis mechanism has a drawback in that it is impossible to tie the RecordMetadata instance back to a particular message. \r\nIncluding the message key in the metadata would solve this problem.\nQ: I have a Kafka Streams application that is part of a pipeline of applications which process messages off a series of Kafka topics.\r\nOne input message results in related messages on the different topics. The related messages on the downstream topics use the same message key as the input message.\r\nAs each application in the pipeline processes messages, the logging uses the Kafka message key on the logging context  The message key can thus be used to trace processing across the entire application pipeline via the application logging.\r\n\r\nThe Kafka Streams application has a registered _org.apache.kafka.clients.producer.ProducerInterceptor_ instance which is used to log any exceptions that occur during message publishing, or, on success, to log the partition and offset of the output message{_}.{_}\r\nThe problem is that _org.apache.kafka.clients.producer.ProducerInterceptor.onAcknowledgement(RecordMetadata, Exception)_ is called from the producer thread created by the Kafka clients library and it doesn't have the details of the producer record, so there is no way for my application to put the message key into the logging context for any logging to pick up.\r\nIn hindsight, suggesting adding the key to the metadata was a bad idea.\r\nA better solution would be to add an overloaded variant of the _onAcknowledgement_ method to _org.apache.kafka.clients.producer.ProducerInterceptor_ which takes the _org.apache.kafka.clients.producer.ProducerRecord_ instance as an additional parameter.\r\nFrom browsing through the Kafka clients code, this looks doable, but I'll leave that decision up to you guys.\r\nThanks for your time.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add golden files for Analyzer edge cases\nDescription: New golden files for edge-cases discovered during the Analyzer support and development.\nQ: Issue resolved by pull request 52734\n[https://github.com/apache/spark/pull/52734]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move UnifiedLogTest to storage module\nDescription: as title", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove deprecated metrics in Rename AssignmentsManager and RemoteStorageThreadPool\nDescription: Please see the KIP: https://cwiki.apache.org/confluence/x/6oqMEw", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support integrating BeeLine with Connect JDBC driver\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add `Example` section in `operators.md`\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "While https://spark.apache.org/docs/latest/spark-connect-overview.html introduces the architecture of Spark Connect, we should add a guide that helps users migrating from Spark Classic understand and avoid unexpected behaviors caused by deferred schema analysis and name resolution.", "output": "Add documentation comparing behavioral differences between Spark Connect and Spark Classic"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consistency of command-line arguments for remaining CLI tools\nDescription: This implements KIP-1147 for kafka-cluster.sh, kafka-leader-election.sh and kafka-streams-application-reset.sh.\nQ: Sure.", "output": "Add the PR link here, since it is not shown in Jira.\r\nhttps://github.com/apache/kafka/pull/20431"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/pull/20658", "output": "Remove the `gradlew` workaround for running the old `gradle-wrapper.jar`"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Analytics accelerator for S3 to be enabled by default\nDescription: Make \"analytics\" the default input stream in S3A. \r\n\r\nGoals\r\n* Parquet performance through applications running queries over the data (spark etc)\r\n* Performance for other formats good as/better than today. Examples: avro manifests in iceberg, ORC in hive/spark\r\n* Performance for other uses as good as today (whole-file/sequential reads of parquet data in distcp etc)\r\n* better resilience to bad uses (incomplete reads not retaining http streams, buffer allocations on long-retained data)\r\n* efficient on applications like Impala, which caches parquet footers itself, and uses unbuffer() to discard all stream-side resources. Maybe just throw alway all state on unbuffer() and stop trying to be sophisticated, or support some new openFile flag which can be used to disable footer parsing", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Kafka Streams join after unmaterialized transformValues on KTable with extra store fails\nDescription: I believe for this to occur you need\r\n # transformValues on a KTable, followed by a KTable join or leftJoin\r\n # The transformValues is not materialized (no store name given)\r\n # The transformValues accesses at least one extra store\r\n\r\nTested on 3.6.1 and 3.9.1\r\n\r\nExample code:\r\n{code:java}\r\n@Component\r\nclass TestCase {\r\n    private static final StoreBuilder> TRANSFORMER_STORE =\r\n          Stores.timestampedKeyValueStoreBuilder(\r\n                Stores.persistentTimestampedKeyValueStore(\"transformer-store\"),\r\n                Serdes.String(),\r\n                Serdes.String()\r\n          );\r\n\r\n    private final StreamsBuilder streamsBuilder;\r\n\r\n    TestCase(StreamsBuilder streamsBuilder) {\r\n       this.streamsBuilder = streamsBuilder;\r\n    }\r\n\r\n    @PostConstruct\r\n    void configure() {\r\n       streamsBuilder.addStateStore(TRANSFORMER_STORE);\r\n\r\n       var aggregateTable = streamsBuilder\r\n             .stream(\"input\", Consumed.with(Serdes.String(), Serdes.String()).withName(\"input-to-stream\"))\r\n             .toTable(Named.as(\"to-table\"), MaterializedAs.keyValue(\"aggregate-store\",\r\n                   Serdes.String(), Serdes.String()))\r\n             .transformValues(MyTransformer::new,\r\n                   Materialized.with(Serdes.String(), Serdes.String()),\r\n                   Named.as(\"my-transformer\"), TRANSFORMER_STORE.name());\r\n\r\n       aggregateTable\r\n             .join(aggregateTable,\r\n                   (value, _) -> value,\r\n                   Named.as(\"after-transformer\"),\r", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update Java 24 to 25 in docker images\nDescription: Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25 in the docker images.\nQ: The ubuntu packages have been released.", "output": "stoty opened a new pull request, #7991:\nURL: https://github.com/apache/hadoop/pull/7991\n\n   ### Description of PR\r\n   \r\n   Update Java 24 to 25 in docker images\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Built ubuntu_20 and ubuntu_24 x64 images, and ran java 25 in them.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Read Policy set in openFileOptions should be considered for enabling various optimizations\nDescription: AbfsInputStream should take in account the Read Policy set by user with Open File Options. Based on the read policy set, appropriate optimizations should be enabled and kicked in.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Support Fixed SAS token at container level\nDescription: The ABFS driver currently lacks support for multiple SAS tokens for the same storage account across different containers.\r\n\r\nWe are now introducing this support.\r\n\r\nTo use fixed SAS token at container level the configuration to be used is:\r\n{quote}fs.azure.sas.fixed.token..\r\n{quote}\nQ: manika137 opened a new pull request, #7461:\nURL: https://github.com/apache/hadoop/pull/7461\n\n   ### Description of PR\r\n   The ABFS driver currently lacks support for multiple SAS tokens for the same storage account across different containers.\r\n   Introducing this support through this PR.\r\n   \r\n   To use fixed SAS token at container level the configuration to be used is:\r\n   `fs.azure.sas.fixed.token..`\r\n   \r\n   ### How was this patch tested?\r\n   Adding the test results in the comments below.", "output": "hadoop-yetus commented on PR #7461:\nURL: https://github.com/apache/hadoop/pull/7461#issuecomment-2696603323\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 6 new + 4 unchanged - 0 fixed = 10 total (was 4)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 48s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 47s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 140m 41s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7461 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 1292e9229efb 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8c4413e89ff27f0cfd1f426be65e92c5c49f0b02 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/testReport/ |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7461/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix flaky DescribeStreamsGroupTest testDescribeStreamsGroupWithShortTimeout()\nDescription: According to [Develocity|https://develocity.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&tests.container=org.apache.kafka.tools.streams.DescribeStreamsGroupTest&tests.sortField=FLAKY], {{testDescribeStreamsGroupWithShortTimeout}} is very flaky.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix uncaching table by name without cascading\nDescription: {{cacheManager.uncacheTableOrView(spark, Seq(\"testcat\", \"tbl\"), cascade = false)}} does not find the table reference because it is wrapped into a subquery alias.\nQ: Issue resolved by pull request 52712\n[https://github.com/apache/spark/pull/52712]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK17] Set Up CI Support JDK17 & JDK21\nDescription: Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to upgrade the project’s Java compatibility. This pipeline will serve as the foundation for testing and validating future JDK 17 support and ensuring smooth integration with existing CI/CD processes.\nQ: slfan1989 opened a new pull request, #7831:\nURL: https://github.com/apache/hadoop/pull/7831\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19638. [JDK17] Set Up CI Support JDK17.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "slfan1989 commented on PR #7831:\nURL: https://github.com/apache/hadoop/pull/7831#issuecomment-3121008019\n\n   @GauthamBanasandra I’m working on adding a new pipeline in Trunk to support JDK17. While reviewing the history, I noticed you've done in-depth work on this area, particularly around the upgrade to `jenkins.sh`\r\n   \r\n   As part of this change, I reviewed the implementation in HADOOP-16888(#2012) to better understand how JDK11 support was introduced. From what I can tell, it seems that enabling JDK17 unit test support only requires configuring the appropriate JDK17-related variables in `jenkins.sh`, similar to how JDK11 was handled. Could you please confirm if that's sufficient? Let me know if there are any additional compatibility steps or considerations I should be aware of.\r\n   \r\n   Thank you very much!"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add missing metrics for document tiered storage\nDescription: Add missing metrics for document tiered storage\r\n * \r\nkafka.log.remote:type=RemoteLogManager,name=RemoteLogReaderFetchRateAndTimeMs：Introduced in [KIP-1018|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1018%3A+Introduce+max+remote+fetch+timeout+config+for+DelayedRemoteFetch+requests]\r\n * \r\nkafka.server:type=DelayedRemoteListOffsetsMetrics,name=ExpiresPerSec,topic=([-.\\w]+),partition=([0-9]+)：Introduced in [KIP-1075|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1075%3A+Introduce+delayed+remote+list+offsets+purgatory+to+make+LIST_OFFSETS+async]", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: document the encoding of nullable struct\nDescription: In [https://kafka.apache.org/protocol#protocol_types,] we didn't specify the encoding of a struct. In particular, how a nullable struct is represented. We should document that if a struct is nullable, the first byte indicates whether is null and the rest of the bytes are the serialization of each field.\nQ: [~isding_l] : Thanks for your interest. Feel free to take it.", "output": "[~isding_l]: Any progress on this? Thanks."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Include cipher feature for HttpServer2 and SSLFactory\nDescription: Currently, we have a feature to exclude weak ciphers from *HttpServer2* and *SSLFactory* using the *ssl.server.exclude.cipher.list property*. \r\nWith this feature, we can also define an inclusion list of ciphers using the *ssl.server.include.cipher.list property*. \r\nIf the inclusion list is populated, any cipher not present in the list will not be allowed. \r\nIf a cipher is present in both the exclusion and inclusion lists, it will be excluded.\r\nNote that SSLFactory does not support regex-based cipher patterns, unlike HttpServer2.\nQ: K0K0V0K opened a new pull request, #7629:\nURL: https://github.com/apache/hadoop/pull/7629\n\n   ### Description of PR\r\n   \r\n   Currently, we have a feature to exclude weak ciphers from HttpServer2 and SSLFactory using the ssl.server.exclude.cipher.list property. With this feature, we can also define an inclusion list of ciphers using the ssl.server.include.cipher.list property. If the inclusion list is populated, any cipher not present in the list will not be allowed. If a cipher is present in both the exclusion and inclusion lists, it will be excluded. Note that SSLFactory does not support regex-based cipher patterns, unlike HttpServer2.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   I created a cluster (java8) and set ssl-server.xml like\r\n   \r\n   ```\r\n     \r\n       ssl.server.include.cipher.list\r\n       TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\r\n       true\r\n     \r\n   ```\r\n   \r\n   then run the following command successfully\r\n   \r\n   `openssl s_client -connect ccycloud-1.bkosztolnik.root.comops.site:8090 -cipher ECDHE-RSA-AES256-GCM-SHA384`\r\n   \r\n   Than modify the config\r\n   \r\n   TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_EMPTY_RENEGOTIATION_INFO_SCSV -> ssl command still works\r\n   TLS_EMPTY_RENEGOTIATION_INFO_SCSV -> ssl command fails to handshake", "output": "hadoop-yetus commented on PR #7629:\nURL: https://github.com/apache/hadoop/pull/7629#issuecomment-2813004052\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 48s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  19m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 25s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 13s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 44s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 20s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  1s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 32s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   7m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 58s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7629/1/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 1 new + 87 unchanged - 0 fixed = 88 total (was 87)  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m  7s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   4m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  13m 10s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   4m  8s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   4m 59s |  |  hadoop-yarn-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 153m 41s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7629/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7629 |\r\n   | Optional Tests | dupname asflicense mvnsite unit codespell detsecrets compile javac javadoc mvninstall shadedclient spotbugs checkstyle |\r\n   | uname | Linux bc4595305705 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 75e187206e997f125bda2b0fecfd87216094616e |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7629/1/testReport/ |\r\n   | Max. process+thread count | 1262 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7629/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "MapOutputTrackerMaster.shufflestatuses can be mistakenly cleaned by Shuffle Cleanup feature, leading to SparkException (crashing the SparkContext) by the subsequent access to that already removed shuffle metadata. A real case (limited to local cluster currently) is the ongoing subquery could access the shuffle metadata which has been already cleanedup after the main query completes. See the detailed discussion at: [https://github.com/apache/spark/pull/52213#discussion_r2415632474].", "output": "MapOutputTrackerMaster.shufflestatuses is mistakenly cleaned by Shuffle cleanup"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix observations on Spark Connect with plan cache\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The `isFenced` value is only valid for broker nodes. It is always `false` for quorum controller nodes.", "output": "enhance the documentation for `Node#isFanced`"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implement offline migration\nDescription: Offline migration essentially preserves offsets and nothing else. So effectively write tombstones for classic group type when a streams heartbeat is sent to with the group ID of an empty classic group, and write tombstones for the streams group type when a classic consumer attempts to join with a group ID of an empty streams group.\r\n\r\nAC:\r\n * Implementation on the broker-side", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When reviewing https://github.com/apache/spark/pull/52729, I found that KafkaOffsetReaderConsumer and KafkaOffsetReaderAdmin have the exactly same getOffsetRangesFromResolvedOffsets method. The method is actually long so seems good to extract them to common one.", "output": "Extract common methods to KafkaOffsetReaderBase"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title:  Enable Filter Push-Down for Pandas UDFs with an Immutable Column Hint\nDescription: h3. Problem Description\r\n\r\nPandas UDFs ({{{}mapInPandas{}}}, {{{}applyInPandas{}}}, etc.) are powerful for custom data processing in PySpark. However, they currently act as a black box to the Catalyst Optimizer. This prevents the optimizer from pushing down filters on columns that pass through the UDF unmodified. As a result, filtering operations occur _after_ the expensive UDF execution and associated data shuffling, leading to significant performance degradation.\r\n\r\nThis is especially common in pipelines where transformations are applied to grouped data, and the grouping key itself is not modified within the UDF.\r\n\r\n*Example:*\r\n\r\nConsider the following DataFrame and Pandas UDFs:\r\n{code:java}\r\nimport pandas as pd\r\nfrom typing import Iterator\r\n\r\ndf = spark.createDataFrame(\r\n    [[\"A\", 1], [\"A\", 1], [\"B\", 2]], \r\n    schema=[\"id string\", \"value int\"]\r\n)\r\n\r\n# UDF to modify the 'value' column\r\ndef map_udf(pdfs: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\r\n    for pdf in pdfs:\r\n        pdf[\"value\"] = pdf[\"value\"] + 1\r\n        yield pdf\r\n\r\n# UDF to aggregate data by 'id'\r\ndef agg_udf(pdf: pd.DataFrame) -> pd.DataFrame:\r\n    return pdf.groupby(\"id\").agg(count=(\"value\", \"count\"))\r\n\r\n# Apply the UDFs\r\nmodified_df = (\r\n    df\r\n    .mapInPandas(map_udf, schema=\"id string,value int\")\r\n    .groupby(\"id\")\r\n    .applyInPandas(agg_udf, schema=\"id string,count int\")\r\n)\r\n\r\n# Filter the result\r\nmodified_df.where(\"id == 'A'\").explain() {code}\r\nIn this example, the {{id}} column is never m", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Potential Long.MAX_VALUE overflow in sessionExpirationTimeNanos calculation in SaslServerAuthenticator \nDescription: There is a potential risk of Long.MAX_VALUE overflow in the sessionExpirationTimeNanos calculation within the SaslServerAuthenticator class.\r\nLocation:\r\n !image-2025-08-01-10-12-04-784.png! \r\nThe calculation sessionExpirationTimeNanos = authenticationEndNanos + 1000 * 1000 * retvalSessionLifetimeMs can potentially overflow when:\r\nretvalSessionLifetimeMs is very large \r\nauthenticationEndNanos is already a large value\r\nThe multiplication 1000 * 1000 * retvalSessionLifetimeMs exceeds Long.MAX_VALUE - authenticationEndNanos\nQ: Duplicated with KAFKA-14604", "output": "OK i will close this one"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The following scenarios should be included.\r\n\r\n1. non-existed partitions\r\n2. undetermined offset\r\n3. normal case", "output": "add integration tests for Consumer#currentLag"}
{"instruction": "Answer the question based on the bug.", "input": "Title: cos use token credential will lost token field\nDescription: Hi,\r\n\r\nI've discovered a bug when accessing COSN using temporary credentials (access key, secret key, and session token).\r\n\r\nIn the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n\r\nFurthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n!image-2025-08-11-10-37-12-451.png|width=1048,height=540! \r\n!image-2025-08-11-10-42-36-375.png!\nQ: leosanqing closed pull request #7866: HADOOP-19648. [hotfix] Cos use token credential will lose token field\nURL: https://github.com/apache/hadoop/pull/7866", "output": "leosanqing opened a new pull request, #7867:\nURL: https://github.com/apache/hadoop/pull/7867\n\n   \r\n   ### Description of PR\r\n   \r\n   In the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n   Furthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [✔] Does the title or this PR starts with the corresponding JIRA issue id"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix S3A failing to initialize S3 buckets having namespace with dot followed by number\nDescription: S3A fails to initialize when S3 bucket namespace is having dot followed by a number. \r\n\r\n{*}Specific Problem{*}: URI parsing fails when S3 bucket names contain a dot followed by a number (like {{{}bucket-v1.1-us-east-1{}}}). Java's\r\nURI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null.\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\nhadoop dfs -ls s3a://bucket-v1.1-us-east-1/\r\n\r\nWARNING: Use of this script to execute dfs is deprecated.\r\nWARNING: Attempting to execute replacement \"hdfs dfs\" instead.\r\n\r\n2025-09-08 06:13:06,670 WARN fs.FileSystem: Failed to initialize filesystem s3://bucket-v1.1-us-east-1/: java.lang.IllegalArgumentException: bucket is null/empty\r\n-ls: bucket is null/empty{code}\r\n \r\n\r\n{*}Please Note{*}: Although there has been discussion on not allowing S3 buckets with such a namespace ([https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/]) , Amazon S3 still allows you to create a bucket with such a namespace.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip testRenameFileBeingAppended on Windows\nDescription: The AbstractContractAppendTest#testRenameFileBeingAppended() tries to rename the file while the handle is open. This isn't allowed on Windows and thus the call to rename fails.\r\n\r\nThus, we need to skip this test on Windows.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: KIP-1170: Unify cluster metadata bootstrapping\nDescription: Implementation for https://cwiki.apache.org/confluence/display/KAFKA/KIP-1170%3A+Unify+cluster+metadata+bootstrapping", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade to wildfly version with support for openssl 3\nDescription: Wildfly 2.1.4\r\n* doesn't work with openssl 3 (that symbol change...why did they do that?)\r\n\r\n\r\nwe need a version with \r\nhttps://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d\r\n\r\n2.2.5.Final does the openssl 3 support.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: KRaft servers don't handle the cluser-level configs in starting\nDescription: see https://github.com/apache/kafka/pull/18949/files#r2296809389\nQ: trunk: https://github.com/apache/kafka/commit/c797f85de481ad3e6840518665d00e1f9c72111f\r\n4.1: https://github.com/apache/kafka/commit/8de88db65abdbc19581098d9ac2b5bd336997779", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Restore Subject propagation semantics for Java 22+\nDescription: Java 22 breaks Subject propagation for new Threads (when SecurityManager is not enabled).\r\n\r\nPreviously, the Subject set by Subject.doAs() / Subject.callAs() automatically propagated to any new Threads created (via new Thread(), not Executors).\r\n\r\nWith JDK22, this is no longer the case, new Threads do NOT inherit the Subject.\r\n\r\nAs Hadoop heavily relies on the original behavior, we somehow need to solve this problem.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This implements KIP-1147 for kafka-producer-perf-test.sh.", "output": "Consistency of command-line arguments for kafka-producer-perf-test.sh"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Native KQueue Transport support on BSD/MacOS\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "https://www.cve.org/CVERecord?id=CVE-2025-53864", "output": "upgrade nimbus-jose-jwt due to CVE-2025-53864"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade Spark to `4.1.0-preview3`\nDescription: \nQ: Issue resolved by pull request 409\n[https://github.com/apache/spark-kubernetes-operator/pull/409]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `gRPC Swift NIO Transport` to 2.2.0\nDescription: \nQ: Issue resolved by pull request 253\n[https://github.com/apache/spark-connect-swift/pull/253]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove workaround for protoc on M1 mac and unused property build.platform\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "DESCRIBE QUERY command only needs the schema of the query, so we only need to do analysis. Therefore, we can skip the unnecessary execution portion for the command.", "output": "Skip query execution for DESCRIBE QUERY"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [ABFS] Rename/Create path idempotency client-level resolution\nDescription: CreatePath and RenamePath APIs are idempotent as subsequent retries on same resource don’t change the server state. However, when client experiences connection break on the CreatePath and the RenamePath APIs, client cannot make sense if the request is accepted by the server or not. \r\n\r\nOn connection failure, the client retries the request. The server might return 404 (sourceNotFound) in case of RenamePath API and 409 (pathAlreadyExists) in case of CreatePath (overwrite=false) API. Now the client doesn’t have a path forward. Reason being, in case of CreatePath, client doesn’t know if the path was created on the original request or the path was already there for some other request, in case of RenamePath, client doesn’t know if the source was removed because of the original-try or it was not there on the first place.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade nimbusds to 10.0.2\nDescription: Includes fix for CVE-2025-53864\nQ: Sure [~fanningpj]", "output": "AnanyaSingh2121 opened a new pull request, #7836:\nURL: https://github.com/apache/hadoop/pull/7836\n\n   (no comment)"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "LongHashedRelation now only supports on-heap mode. The task is for adding off-heap mode support for the relation.", "output": "Add off-heap mode support for LongHashedRelation"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Spark Connect Python client does not throw a proper error when creating a dataframe from a pandas dataframe with a index and empty data.\r\n\r\nGenerally, spark connect client throws a client-side error `[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from an empty dataset`. when creating a dataframe without data, for example via\r\n{quote}spark.createDataFrame([]).show()\r\n{quote}\r\nor\r\n{quote}df = pd.DataFrame()\r\nspark.createDataFrame(df).show(){quote}\r\nor\r\n{quote}df = pd.DataFrame(\\{\"a\": []})\r\nspark.createDataFrame(df).show(){quote}\r\nThis does not happen when pandas dataframe has an index but no data, e.g.\r\n{quote}df = pd.DataFrame(index=range(5))\r\nspark.createDataFrame(df).show(){quote}\r\nWhat happens instead is that the dataframe is successfully converted to a LocalRelation on the client, is sent to the server, but the server then throws the following exception: `INTERNAL_ERROR: Input data for LocalRelation does not produce a schema. SQLSTATE: XX000`. XX000 is an internal error sql state and the error is not actionable enough for the user.\r\nThis should be fixed.", "output": "Spark Connect Python client does not throw a proper error when creating a dataframe from an empty pandas dataframe"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: testIfMatchOverwriteWithOutdatedEtag() fails when not using SSE-KMS\nDescription: ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteWithOutdatedEtag() fails when no encryption method is set. \r\n \r\nThis is because it does  \r\ncreateFileWithFlags(fs, path, SMALL_FILE_BYTES, true, null);\r\n \r\nand then to overwrite the file, also does\r\n \r\ncreateFileWithFlags(fs, path, SMALL_FILE_BYTES, true, null);\r\n \r\nWhen no encryption is used, the eTAG is the md5 of the object, and so will always be the same, and won't result in the 412 conditional write failure. \r\n \r\nTest passes when using SSE-KMS, as when using encryption, eTag is no longer the md5 of the object content, and changes on every write. \r\n \r\n \r\nFix is simple enough, change the object content on the second write.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Managed Identity Token Provider is not implemented in the hadoop-azure jar. Now, if one wants to use either User Assigned Managed Identity or System Assigned Managed Identity in Azure, this will throw an error because it's not implemented yet.", "output": "Managed Identity Token Provider is not implemented"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Native profile fails to build on SLES 15\nDescription: Hadoop build fails to find pthreads on SLES 15 builds while linking rpc. It looks like it checks for SunRPC library via rpc/rpc.h, but instead finds tirpc and sets it up incorrectly.\r\n\r\n{code}\r\nPerforming C SOURCE FILE Test CMAKE_HAVE_LIBC_PTHREAD failed with the following output:\r\nChange Dir: /grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/usr/bin/gmake cmTC_4ddc0/fast && /usr/bin/gmake  -f CMakeFiles/cmTC_4ddc0.dir/build.make CMakeFiles/cmTC_4ddc0.dir/build\r\ngmake[1]: Entering directory '/grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeTmp'\r\nBuilding C object CMakeFiles/cmTC_4ddc0.dir/src.c.o\r\n/usr/bin/gcc-8 -DCMAKE_HAVE_LIBC_PTHREAD   -o CMakeFiles/cmTC_4ddc0.dir/src.c.o -c /grid/0/jenkins/workspace/workspace/CDH-parallel-sles15/SOURCES/hadoop/hadoop-tools/hadoop-pipes/target/native/CMakeFiles/CMakeTmp/src.c\r\nLinking C executable cmTC_4ddc0\r\n/grid/0/jenkins/tools/cmake/3.19.3/bin/cmake -E cmake_link_script CMakeFiles/cmTC_4ddc0.dir/link.txt --verbose=1\r\n/usr/bin/gcc-8 -rdynamic CMakeFiles/cmTC_4ddc0.dir/src.c.o -o cmTC_4ddc0\r\n/usr/lib64/gcc/x86_64-suse-linux/8/../../../../x86_64-suse-linux/bin/ld: CMakeFiles/cmTC_4ddc0.dir/src.c.o: in function `main':\r\nsrc.c:(.text+0x2d): undefined reference to `pthread_create'\r\n/usr/lib64/gcc/x86_64-suse-linux/8/../../../../x86_64-suse-linux/bin/ld: src.c:", "output": "Patch Available"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip `test_collect_time` test if pandas or pyarrow are unavailable\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: upgrade to jackson 2.18\nDescription: follow up to HADOOP-19259", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consistency of command-line arguments for console producer/consumer\nDescription: This implements KIP-1147 for kafka-console-producer, kafka-console-consumer and kafka-console-share-consumer.\nQ: Hi [~schofielaj], if you're not working on this, may I take it? Thanks.", "output": "-Hi [~schofielaj], I saw you didin't assign anyone to this jira, does that mean I can help with this one?-"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Dockerfile_windows_10 cannot be build\nDescription: The following command from BUILDING.txt does not work anymore:\r\n{code:java}\r\ndocker build -t hadoop-windows-10-builder -f .\\dev-support\\docker\\Dockerfile_windows_10 .\\dev-support\\docker\\ {code}\r\nSeveral dependencies are missing and vcpkg points to an outdated 7-zip package.\nQ: hadoop-yetus commented on PR #7417:\nURL: https://github.com/apache/hadoop/pull/7417#issuecomment-2671652878\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7417/1/console in case of problems.", "output": "hadoop-yetus commented on PR #7417:\nURL: https://github.com/apache/hadoop/pull/7417#issuecomment-2671878618\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  1s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 32s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  78m 24s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7417/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7417 |\r\n   | Optional Tests | dupname asflicense |\r\n   | uname | Linux 3963ac263972 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9a47c0d240ae450fa2decebb7996cd09f7a21907 |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7417/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In business cases Kafka Streams users frequently need *per-key Time-To-Live (TTL)* behavior for state stores such as keyValueStore or kTables , typically to model cache-like or deduplication scenarios.\r\n\r\nToday, achieving this requires *manual handling* using:\r\n * Custom timestamp tracking per key,\r\n\r\n * Punctuators to periodically scan and remove expired entries, and\r\n\r\n * Manual emission of tombstones to maintain changelog consistency.\r\n\r\nThese workarounds are:\r\n * {*}Inconsistent across applications{*}, and\r\n\r\n * {*}Operationally costly{*}, as each developer must reimplement the same logic.\r\n\r\n*Proposal* is to introduce a *built-in TTL mechanism* for Kafka Streams state stores, allowing automatic expiration of records after a configured duration.\r\n\r\nIntroduction to new Api's like : \r\n\r\nStoreBuilder withTTL(Duration ttl);\r\nMaterialized withTtl(Duration ttl);\r\n\r\nWhen configured:\r\n * Each record’s timestamp (from event-time or processing-time) is tracked.\r\n\r\n * Expired keys are automatically evicted by a background task (via ProcessorContext.Schedule()).\r\n\r\n * Corresponding tombstones are flushed to changelog.\r\n\r\nThis feature can provide a *TTL abstraction* that simplifies common use cases as:\r\n * Maintaining cache-like state (e.g., last-seen values with limited lifespan)\r\n\r\n * Automatically purging inactive or stale keys without manual cleanup.\r\n\r\nPoints of Risk and Benifits i considered it can bring : \r\n * Consistency as automatic changelog tombstones will preserve correctness across rebalances and restores.\r\n\r\n * Will help to avoid boilerplate punctuator code for manual expiration.\r\n\r\n * TTL is optional and opt-in; existing stores remain unaffected so backward compatibility would be maintaoined.\r\n\r\nExample to StateStore/ kTable inferface : \r\n\r\nKTable sessions = builder\r\n    .table(\"sessions\", Materialized.>as(\"session-store\")\r\n        .withTtl(Duration.ofHours(1))\r\n        .withValueSerde(userSessionSerde));\r\n\r\nHere, session entries older than 1 hour will be auto", "output": "Add built-in TTL (Time-to-Live) support for Kafka Streams State Stores"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Expose consumer CorruptRecordException\nDescription: As part of our analysis around KAFKA-19430, we realized it's not possible to handle a {{CorruptRecordException}} in Streams because it's not exposed to the client; instead, a generic {{KafkaException}} ** is thrown\r\n\r\nThe proposed solution is to expose {{CorruptRecordException}} with information about affected TopicPartition and offset we're trying read from\r\n\r\nKIP: https://cwiki.apache.org/confluence/x/NQrxFg", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Optimize MetadataShell tool find command print warning message intead of throwing exception\nDescription: When using the {{find}} command, it currently lists all directories and files. However, if it cannot find a child node, it throws an exception and prevents the subsequent directories from being printed. I think we should change this behavior to print an error message instead of throwing an exception, which would be more user-friendly.\r\n\r\nIn addition, since {{zkMigrationState}} is not used in Kafka 4.X, we should remove this child node.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Performance degradation for PySpark apis at scale as compared to Scala apis\nDescription: Customers love PySpark and the flexibility of using several python libraries as part of our workflows. I've a unique scenario where this specific usecase has multiple tables with around 10k columns and some of those columns have array datatype that when exploded, contain ~1k columns each.\r\n\r\n*Issues that we are facing:*\r\n * Frequent driver OOM depending on the use case and how many columns are involved in the logic and how many array type columns are exploded. There is frequent GC, slowing down the workflows.\r\n * We tried equivalent scala apis and the performance as well latency seemed a lot better (no OOM and significantly less GC overheads).\r\n\r\n*Here is what we understand so far from thread and memory dumps:*\r\n * driver ends up having open references for every pyspark object created in the python vm because of py4j bridge-based communication implementation for pyspark apis. and the garbage keeps accumulating on driver ultimately leading to OOM\r\n * even if we delete python references in pyspark code (for example:  del df_dummy1) and run \"gc.collect()\" specifically, we are not able to ease the memory pressure. Python gc or python triggered gc via py4j bridge in the driver doesn't seem to be that good.\r\n\r\nThis is not a typical workload but we have multiple such usecases and we are debating if it's worth changing existing workflows to scala just like that (existing DEs are more comfortable with PySpark, there is cost of migration as well that we will have to convince our manage", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: SecretManager configuration at runtime\nDescription: In case of TEZ *DAGAppMaster* the Hadoop *SecretManager* code can not read yarn config xml file, therefore the SELECTED_ALGORITHM and SELECTED_LENGTH variables in SecretManager can not be set at runtime.\r\nThis can results with the following exception in FIPS environment:\r\n\r\n{code:java}\r\njava.security.InvalidParameterException: Key size for HMAC must be at least 112 bits in approved mode: SHA-1/HMAC\r\n\tat com.safelogic.cryptocomply.fips.core/com.safelogic.cryptocomply.jcajce.provider.BaseKeyGenerator.engineInit(Unknown Source)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:540)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:517)\r\n\tat org.apache.hadoop.security.token.SecretManager.(SecretManager.java:157)\r\n\tat org.apache.hadoop.yarn.security.client.BaseClientToAMTokenSecretManager.(BaseClientToAMTokenSecretManager.java:38)\r\n\tat org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager.(ClientToAMTokenSecretManager.java:46)\r\n\tat org.apache.tez.common.security.TezClientToAMTokenSecretManager.(TezClientToAMTokenSecretManager.java:33)\r\n\tat org.apache.tez.dag.app.DAGAppMaster.serviceInit(DAGAppMaster.java:493)\r\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\r\n\tat org.apache.tez.dag.app.DAGAppMaster$9.run(DAGAppMaster.java:2649)\r\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\n\tat org.apache.hadoop.security.UserGroupInform", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support building with Java 25 (LTS release)\nDescription: *Depends upon/blocked by:*\r\n * Kafka ticket: KAFKA-19174 Gradle version upgrade 8 -->> 9 (review is under way)\r\n * Spotbugs next version:\r\n ** [https://github.com/spotbugs/spotbugs/issues/3564]\r\n ** [https://github.com/spotbugs/spotbugs/issues/3569]\r\n ** https://issues.apache.org/jira/browse/BCEL-377 \r\n ** [https://github.com/spotbugs/spotbugs/pull/3712]\r\n ** [https://github.com/spotbugs/spotbugs/discussions/3380] \r\n\r\n*Related links:*\r\n * JDK 25 release date: September 16th 2025:\r\n ** [https://mail.openjdk.org/pipermail/announce/2025-September/000360.html]\r\n ** [https://openjdk.org/projects/jdk/25]\r\n * Gradle 9.1 will support Java 25: [https://docs.gradle.org/9.1.0/release-notes.html#support-for-java-25]\r\n * Scala 2.13.17 is also announced:\r\n ** [https://docs.scala-lang.org/overviews/jdk-compatibility/overview.html#jdk-25-compatibility-notes]\r\n ** [https://contributors.scala-lang.org/t/scala-2-13-17-release-planning/6994/10]\r\n ** [https://github.com/scala/scala/milestone/109]\r\n * other\nQ: *Small update:*\r\n * Gradle version upgrade (8.14.3 -->> 9.1.0) will hopefully end up in trunk in the next few days\r\n * It seems that Apache *{{commons-bcel}}* will release Java 25 compatible version soon enough - this is important for us because SpotBugs Java 25 compatible versions will follow immediately", "output": "*Update:*\r\n* A few of the tests are failing with Java 25 (x)\r\n* ticket is created here: KAFKA-19769 (i)"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "* Currently, the endpoint for using S3 accesspoint is resolved for S3 and S3 on outposts: https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/ArnResource.java#L29-L30. However, for s3express, the endpoint is \"s3express-..amazonaws.com\". (https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-az-networking.html). This ticket adds support for s3express endpoint.\r\n\r\n* Additionally, for access objects using s3express in the `S3AFileSystem`, we need to parse the access point ARN for an S3express access point into the access point name, since ARNs aren't supported on S3express other than in IAM policies. (https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-directory-buckets-restrictions-limitations-naming-rules.html). This ticket also addresses this change.\r\n\r\n* This ticket also updates the SDK version used by the changes to `2.31.12`, which is the version (at minimum) needed to use s3express access points. It does not do the SDK upgrade, but only the version of the dependency.", "output": "S3A: Support for S3express Access Points"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ListOffsetsHandler should kick out the unsupported timestamp type when receiving an UnsupportedVersionException\nDescription: see https://github.com/apache/kafka/pull/20530#discussion_r2345531638", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Surefire upgrade leads to increased report output, can cause Jenkins OOM\nDescription: The Surefire upgrade to 3.3+ added enableOutErrElements which defaults to true and includes system-out and system-err in the xml artifacts. This significantly increases their size: ~45KB to ~1MB in many cases. In my test environment, that leads to\r\n{code}\r\nRecording test results\r\nERROR: Step ‘Publish JUnit test result report’ aborted due to exception: \r\njava.lang.OutOfMemoryError: Java heap space\r\n{code}\r\n\r\nCapturing stdout seems useful when doing ad-hoc testing or smaller test runs. I'd propose adding a profile to set enableOutErrElements=false for CI environments where that extra output can cause problems.\nQ: hadoop-yetus commented on PR #7998:\nURL: https://github.com/apache/hadoop/pull/7998#issuecomment-3336586704\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  56m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  98m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 17s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 163m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7998 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 6c02dd609cd9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 895391e2c52eadd071997eacaaf7bb2f2af8be30 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7998/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "steveloughran merged PR #7998:\nURL: https://github.com/apache/hadoop/pull/7998"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce Catalyst server-side geospatial execution classes\nDescription: \nQ: Work in progress: https://github.com/apache/spark/pull/52737.", "output": "Issue resolved by pull request 52737\n[https://github.com/apache/spark/pull/52737]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip `test_in_memory_data_source` in Python 3.14\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support getColumns for SparkConnectDatabaseMetaData\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Kinesis tests are broken\nDescription: Running Kinesis test with {{ENABLE_KINESIS_TEST=1}} fails with {{java.lang.NoClassDefFoundError}}:\r\n\r\n{code:java}\r\nENABLE_KINESIS_TESTS=1 ./build/sbt -Pkinesis-asl\r\n...\r\nUsing endpoint URL https://kinesis.us-west-2.amazonaws.com for creating Kinesis streams for tests.\r\n[info] WithoutAggregationKinesisBackedBlockRDDSuite:\r\n[info] org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite *** ABORTED *** (1 second, 131 milliseconds)\r\n[info]   java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/PropertyNamingStrategy$PascalCaseStrategy\r\n[info]   at com.amazonaws.services.kinesis.AmazonKinesisClient.(AmazonKinesisClient.java:86)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient$lzycompute(KinesisTestUtils.scala:59)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.kinesisClient(KinesisTestUtils.scala:58)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.describeStream(KinesisTestUtils.scala:169)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.findNonExistentStreamName(KinesisTestUtils.scala:182)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisTestUtils.createStream(KinesisTestUtils.scala:85)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisBackedBlockRDDTests.$anonfun$beforeAll$1(KinesisBackedBlockRDDSuite.scala:45)\r\n[info]   at org.apache.spark.streaming.kinesis.KinesisFunSuite.runIfTestsEnabled(KinesisFunSuite.scala:41)\r\n[info]   at org.apache.spark.streaming", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: S3ABlockOutputStream to never log/reject hflush(): calls\nDescription: Parquet's GH-3204 patch uses hflush() just before close()\r\n\r\nthis is needless and hurts write performance on hdfs.\r\nFor s3A it will trigger a warning long (Syncable is not supported) or an actual failure if\r\nfs.s3a.downgrade.syncable.exceptions is false\r\n\r\nproposed: hflush to log at debug -only log/reject on hsync, which is the real place where semantics cannot be met\nQ: hadoop-yetus commented on PR #7662:\nURL: https://github.com/apache/hadoop/pull/7662#issuecomment-2841687000\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 53s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 15s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 33s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 137m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7662/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7662 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f3d94f735602 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 429bbdd723b4106126930d93a2b5f20ce03ccfd6 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7662/1/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7662/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "ahmarsuhail commented on code in PR #7662:\nURL: https://github.com/apache/hadoop/pull/7662#discussion_r2068635417\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java:\n##########\n@@ -829,7 +829,8 @@ public boolean hasCapability(String capability) {\n   @Override\n   public void hflush() throws IOException {\n     statistics.hflushInvoked();\n-    handleSyncableInvocation();\n+    // do not reject these, but downgrade to a no-oop\n+    LOG.debug(\"Hflush invoked\");\n\nReview Comment:\n   @steveloughran is parquet the only reader calling hflush? think this changes behaviour for everyone.. is this something we need to care about?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Protect `ExecutorPodsAllocator.numOutstandingPods` as `protected val`\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade com.huaweicloud:esdk-obs-java for CVE-2023-3635\nDescription: The {{com.huaweicloud:esdk-obs-java}} dependency , used exclusively by the {{hadoop-huaweicloud}} uses {{com.squareup.okio:okio:1.17.2}} which has [CVE-2023-3635|https://nvd.nist.gov/vuln/detail/cve-2023-3635].\r\nUpgrading it will use a newer fixed version of {{okio}}, which will mitigate the vulnerability.\nQ: hadoop-yetus commented on PR #7707:\nURL: https://github.com/apache/hadoop/pull/7707#issuecomment-2907769746\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 12s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  71m 56s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 24s | [/patch-mvninstall-hadoop-cloud-storage-project_hadoop-huaweicloud.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7707/1/artifact/out/patch-mvninstall-hadoop-cloud-storage-project_hadoop-huaweicloud.txt) |  hadoop-huaweicloud in the patch failed.  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 24s |  |  hadoop-huaweicloud in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 132m  1s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7707/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7707 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 75c9a3d9b31f 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c3f593b8a1667b38b8c9074b4324a14ba0cd584f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7707/1/testReport/ |\r\n   | Max. process+thread count | 557 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-cloud-storage-project/hadoop-huaweicloud U: hadoop-cloud-storage-project/hadoop-huaweicloud |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7707/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on code in PR #7707:\nURL: https://github.com/apache/hadoop/pull/7707#discussion_r2106551943\n\n\n##########\nhadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml:\n##########\n@@ -29,7 +29,7 @@\n   \n     UTF-8\n     true\n-    3.20.4.2\n+    3.25.4\n\nReview Comment:\n   https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7707/1/artifact/out/patch-mvninstall-hadoop-cloud-storage-project_hadoop-huaweicloud.txt\r\n   \r\n   [ERROR] Dependency convergence error for com.squareup.okio:okio:jar:3.6.0 paths to dependency are:\r\n   [ERROR] +-org.apache.hadoop:hadoop-huaweicloud:jar:3.5.0-SNAPSHOT\r\n   [ERROR]   +-com.huaweicloud:esdk-obs-java:jar:3.25.4:compile\r\n   [ERROR]     +-com.squareup.okhttp3:okhttp:jar:4.12.0:compile\r\n   [ERROR]       +-com.squareup.okio:okio:jar:3.6.0:compile\r\n   [ERROR] and\r\n   [ERROR] +-org.apache.hadoop:hadoop-huaweicloud:jar:3.5.0-SNAPSHOT\r\n   [ERROR]   +-com.huaweicloud:esdk-obs-java:jar:3.25.4:compile\r\n   [ERROR]     +-com.squareup.okio:okio:jar:3.8.0:compile\r\n   \r\n   We need to resolve this compilation error."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Close ShareConsumer in ShareConsumerPerformance only after metrics displayed\nDescription: This is a peer of https://issues.apache.org/jira/browse/KAFKA-19564.\r\n\r\nEssentially, it's not reliable to access the metrics from the share consumer once it has been closed, and this tool does precisely that. As a result, it prints a truncated list of metrics.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Stuck __consumer_offsets partition (kafka streams app)\nDescription: h3. Problem Summary\r\n\r\nMy Kafka Streams application cannot move its {{state_store}} from {{STARTING}} to {{{}RUNNING{}}}.\r\n\r\nI'm using a *Strimzi Kafka cluster* with:\r\n * 3 *controller nodes*\r\n\r\n * 4 *broker nodes*\r\n\r\nh3. Observations\r\nh4. Partition {{__consumer_offsets-35}} is {*}stuck{*}.\r\n\r\nFrom AKHQ, partition details:\r\n * *Broker 10* is the *leader* of {{__consumer_offsets-35}}\r\n\r\n * There are *no interesting logs* on broker 10\r\n\r\n * However, logs are *spamming every 10ms* from broker 11 (a {*}replica{*}):\r\n\r\n2025-08-11 04:05:50 INFO  [TxnMarkerSenderThread-11] TransactionMarkerRequestCompletionHandler:66 \r\n[Transaction Marker Request Completion Handler 10]: Sending irm_r_sbm2_web-backend-web-1-6cad35c7-2be9-4ed2-9849-9c059cc8c409-4's transaction marker for partition __consumer_offsets-35 has failed with error org.apache.kafka.common.errors.NotLeaderOrFollowerException, retrying with current coordinator epoch 38\r\nh4. Brokers 20 and 21 — neither leaders nor replicas — also spamming the same error:\r\n\r\n*Broker 20:*\r\n2025-08-11 04:39:45 INFO  [TxnMarkerSenderThread-20] TransactionMarkerRequestCompletionHandler:66 \r\nSending irm_r_sbm2_web-backend-web-1-6cad35c7-2be9-4ed2-9849-9c059cc8c409-3's transaction marker for partition __consumer_offsets-35 has failed with error org.apache.kafka.common.errors.NotLeaderOrFollowerException, retrying with current coordinator epoch 54\r\n \r\n*Broker 21:*\r\n2025-08-11 04:39:58 INFO  [TxnMarkerSenderThread-21] TransactionMarkerRequestCompletionHa", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "AAL currently does not support leak detection. \r\n\r\nIt may not require this, as individual streams do not hold only to any connections/resources, the factory does. We should verify if it's required, and if yes, add support.", "output": "S3A: AAL - Add support for stream leak detection"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Follow up for KIP-1217\nDescription: [KIP-1217|https://cwiki.apache.org/confluence/x/6QnxFg] deprecates the ClientTelemetryReceiver and ClientTelemetry interfaces. They should be removed in Kafka 5.0.0.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The metavariable names in the usage messages for the verifiable producer, consumer and share consumer are a bit of a mess which makes the usage messages hard to read. It is trivial to improve and only affects the usage messages.\r\n\r\nFor example:\r\n\r\nusage: verifiable-share-consumer\r\n\r\n       [-h] --topic TOPIC --group-id GROUP_ID [--max-messages MAX-MESSAGES] [--verbose]\r\n\r\n       [--acknowledgement-mode ACKNOWLEDGEMENTMODE] [--offset-reset-strategy OFFSETRESETSTRATEGY]\r\n\r\n       [--command-config CONFIG_FILE] --bootstrap-server HOST1:PORT1[,HOST2:PORT2[...]]\r\n\r\nwould be better as:\r\n\r\nusage: verifiable-share-consumer\r\n\r\n       [-h] --topic TOPIC --group-id GROUP-ID [--max-messages MAX-MESSAGES] [--verbose]\r\n\r\n       [--acknowledgement-mode ACKNOWLEDGEMENT-MODE] [--offset-reset-strategy OFFSET-RESET-STRATEGY]\r\n\r\n       [--command-config CONFIG_FILE] --bootstrap-server HOST1:PORT1[,HOST2:PORT2[...]]", "output": "Improve metavariable names for verifiable producer/consumer"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "https://github.com/apache/kafka/blob/trunk/tools/src/main/java/org/apache/kafka/tools/reassign/ReassignPartitionsCommand.java#L572\r\nhttps://github.com/apache/kafka/blob/trunk/tools/src/main/java/org/apache/kafka/tools/reassign/ReassignPartitionsCommand.java#L931\r\n\r\nSince we always pass `Map.of`, the log directories are always treated as \"any\"", "output": "the current assignments shown by ReassignPartitionsCommand should include the log directories"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Query engines which uses Magic Committer to overwrite a directory would ideally upload the MPUs (not complete) and then delete the contents of the directory before committing the MPU.\r\n\r\n \r\n\r\nFor S3 express storage, The directory purge operation is enabled by default. Refer [here|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L688] for code pointers.\r\n\r\n \r\n\r\nDue to this, the pending MPU uploads are purged and query fails with \r\n\r\n{{NoSuchUpload: The specified multipart upload does not exist. The upload ID might be invalid, or the multipart upload might have been aborted or completed. }}", "output": "Insert Overwrite Jobs With MagicCommitter Fails On S3 Express Storage"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add spark.logConf configuration to operator docs\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()\nDescription: We have evidence that `IOStatisticsSupport.snapshotIOStatistics()` can hang, specifically on the statistics collected by an S3AInputStream, whose statistics are merged in to the FS stats in close();\r\n\r\n{code}\r\njdk.internal.misc.Unsafe.park(Native Method)\r\njava.util.concurrent.locks.LockSupport.park(LockSupport.java:341)\r\njava.util.concurrent.ForkJoinTask.awaitDone(ForkJoinTask.java:468)\r\njava.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:687)\r\njava.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:927)\r\njava.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\njava.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)\r\norg.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap.entrySet(EvaluatingStatisticsMap.java:166)\r\njava.util.Collections$UnmodifiableMap.entrySet(Collections.java:1529)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.copyMap(IOStatisticsBinding.java:172)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBindin\nQ: steveloughran commented on PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006#issuecomment-3357467095\n\n   tested s3 london args ` -Dparallel-tests -DtestsThreadCount=8 -Dscale`\r\n   ```\r\n   [ERROR] Failures: \r\n   [ERROR]   ITestS3APrefetchingInputStream.testReadLargeFileFully:130 [Maxiumum named action_executor_acquired.max] \r\n   Expecting:                                                                                                                                                                                                                                             \r\n                                                                                                                                                                                                                                                      \r\n   to be greater than:                                                                                                                                                                                                                                    \r\n               \r\n   ```", "output": "hadoop-yetus commented on PR #8006:\nURL: https://github.com/apache/hadoop/pull/8006#issuecomment-3358241968\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  55m 44s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |  12m 48s | [/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/branch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |  10m 53s | [/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/branch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in trunk failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  7s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 58s |  |  the patch passed  |\r\n   | -1 :x: |  compile  |  12m 27s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |  12m 27s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |  10m 39s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |  10m 39s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 42s | [/results-checkstyle-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/results-checkstyle-hadoop-common-project_hadoop-common.txt) |  hadoop-common-project/hadoop-common: The patch generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 57s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 15s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 223m 40s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8006 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 48973a6e0048 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 40e7c252a5e39fb8e483afe5f900412bf17cd4a3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/testReport/ |\r\n   | Max. process+thread count | 3134 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8006/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob] Remove Duplicates from Blob Endpoint Listing Across Iterations\nDescription: On FNS-Blob, List Blobs API is known to return duplicate entries for the non-empty explicit directories. One entry corresponds to the directory itself and another entry corresponding to the marker blob that driver internally creates and maintains to mark that path as a directory. We already know about this behaviour and it was handled to remove such duplicate entries from the set of entries that were returned as part current list iterations.\r\n\r\nDue to possible partition split if such duplicate entries happen to be returned in separate iteration, there is no handling on this and caller might get back the result with duplicate entries as happening in this case. The logic to remove duplicate was designed before the realization of partition split came.\r\n\r\nThis PR fixes this bug\nQ: anmolanmol1234 commented on code in PR #7614:\nURL: https://github.com/apache/hadoop/pull/7614#discussion_r2044225591\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -1299,6 +1311,29 @@ public String listStatus(final Path path, final String startFrom,\n     return continuation;\n   }\n \n+  private void filterDuplicateEntriesForBlobClient(\n\nReview Comment:\n   add javadocs", "output": "anmolanmol1234 commented on code in PR #7614:\nURL: https://github.com/apache/hadoop/pull/7614#discussion_r2044245375\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemListStatus.java:\n##########\n@@ -532,6 +532,28 @@ public void testEmptyContinuationToken() throws Exception {\n         .describedAs(\"Listing Size Not as expected\").hasSize(1);\n   }\n \n+  @Test\n+  public void testDuplicateEntriesAcrossListBlobIterations() throws Exception {\n\nReview Comment:\n   You can add one more test where there are more than one files in the directory and max list result is 1 and multiple list status calls and verify the overall file statuses don't have any duplicate entries"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A Analytics-Accelerator: Update LICENSE-binary\nDescription: update LICENSE-binary to include AAL dependency", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We should ban all uses of spark.sql except for select statements.", "output": "Prevent spark.sql session state mutation within Python pipeline files"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Set `--sun-misc-unsafe-memory-access=allow` for `JavaExec` Gradle tasks\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Memory leak using range scans\nDescription: We introduced a regression bug in AK 4.1.0 release, when range scans are use. We are leaking some metrics/sensors.\r\n\r\nThanks to [~eduwerc] and [~hermankj] for reporting this issue.\r\n\r\nAnybody who is using iterator, ie, any type of all/rang/prefix scan etc is affected. Note that certain DSL operators like session-windows, sliding-windows, stream-stream joins, and FK-join all use range scans internally, so the number of affected users is expected to be quite high.\nQ: Yes, the upgrade-guide would be the right place. I would also add a note to the top-level upgrade note in addition: [https://github.com/apache/kafka/blob/trunk/docs/upgrade.html#L194]", "output": "Done! Made a PR for this - [#20639|https://github.com/apache/kafka/pull/20639]"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade scala-xml to 2.4.0\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: error stack traces printed on analytics stream factory close\nDescription: When you close an s3a filesystem there is a lot of ERROR level stack traces about a CancellationException -despite that being exactly what is wanted.\r\n\r\nCore of it comes from netty.\r\n{code}\r\n        Suppressed: software.amazon.awssdk.core.exception.SdkClientException: Request attempt 1 failure: Unable to execute HTTP request: The connection was closed during the request. The request will usually succeed on a retry, but if it does not: consider disabling any proxies you have configured, enabling debug logging, or performing a TCP dump to identify the root cause. If this is a streaming operation, validate that data is being read or written in a timely manner. Channel Information: ChannelDiagnostics(channel=[id: 0x801baead, L:0.0.0.0/0.0.0.0:59534 ! R:bucket.vpce-0117f6033eaf7aee5-2hxd9fg4.s3.us-west-2.vpce.amazonaws.com/10.80.134.179:443], channelAge=PT0.676S, requestCount=1, responseCount=0, lastIdleDuration=PT0.006284125S)\r\nCaused by: java.lang.IllegalStateException: executor not accepting a task\r\n{code}", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions\nDescription: This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4's {{org.junit.Assume}} API to JUnit5's {{org.junit.jupiter.api.Assumptions}} API in order to unify the testing framework and improve maintainability.\r\nh4. Scope of changes:\r\n * Replace usages of {{{}Assume.assumeTrue(...){}}}, {{{}Assume.assumeFalse(...){}}}, etc., with the corresponding methods from JUnit5, such as {{{}Assumptions.assumeTrue(...){}}};\r\n\r\n * Ensure the assertion logic remains consistent with the original behavior;\r\n\r\n * Update any outdated import statements referencing JUnit4's {{{}Assume{}}};\r\n\r\n * Verify that all affected unit tests pass correctly under JUnit5.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Externally Reported Enhancement:\r\n\r\n*Current Limitation*\r\nThe current WorkloadIdentityTokenProvider implementation works well for file-based token scenarios, but it's tightly coupled to file system operations and cannot be easily extended for alternative token sources\r\n\r\n{*}Use Case{*}: *Kubernetes TokenRequest API* \r\nIn modern Kubernetes environments, the recommended approach is to use the TokenRequest API to generate short-lived, on-demand service account tokens rather than relying on projected volume mounts.\r\n\r\n*Proposed Enhancement* \r\nI propose modifying WorkloadIdentityTokenProvider to accept a Supplier for token retrieval instead of being hardcoded to file operations:", "output": "ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce Catalyst server-side geospatial execution classes\nDescription: \nQ: Issue resolved by pull request 52737\n[https://github.com/apache/spark/pull/52737]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add Caching Mechanism to HostResolver to Avoid Redundant Hostname Resolutions\nDescription: *Background:*\r\n\r\n \r\n\r\nCurrently, the two implementations of org.apache.hadoop.security.SecurityUtil.HostResolver, *StandardHostResolver and QualifiedHostResolver* in Hadoop performs hostname resolution each time it is called. *Each heartbeat between the AM and RM causes the RM to invoke the* HostResolver#getByName {*}method once{*}. In large-scale clusters running numerous applications, this results in *a high frequency of redundant hostname resolutions.*\r\n\r\n \r\n\r\n*Proposal:*\r\n\r\n \r\n\r\nIntroduce a caching mechanism in HostResolver to store resolved hostnames for a configurable duration. This would:\r\n\r\n•Reduce redundant DNS queries.\r\n\r\n•Improve performance for frequently used hostnames.\r\n\r\n•Allow configuration options for cache size and TTL (Time-to-Live).\r\n\r\n \r\n\r\n*Suggested Implementation:*\r\n\r\n1.{*}Leverage Existing CachedResolver{*}:\r\n\r\nThe NodesListManager.CachedResolver class in Hadoop already implements a caching mechanism for hostname resolution. Instead of introducing an entirely ne\nQ: HostnameCache depends on the Clock interface, which is currently defined in hadoop-yarn-project. However, this creates an unavoidable circular dependency, as hadoop-common-project cannot depend on hadoop-yarn-project.\r\n\r\nTo resolve this issue, we need to move the Clock interface and its implementation to hadoop-common-project, making it a shared utility. Therefore, a new issue YARN-11765 has been created to refactor and relocate the Clock interface.\r\n\r\nThis refactor ensures that hadoop-common-project remains independent while allowing all projects to use the same Clock implementation without duplication.", "output": "Thanks [~yangjiandan]  for introducing this improvement, it's very useful for large-scale clusters."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\"\nDescription: Experienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests.\r\n\r\nHypothesis: a transient failure meant the server receiving the POST calls to complete the uploads was mistakenly reporting no upload IDs.\r\n\r\nOutcome: all active write operations failed, without any retry attempts. This can lose data and fail jobs, even though the store may recover.\r\n\r\nProposed. The multipart uploads, especially block output stream, retry on this error; treat it as a connectivity issue.\nQ: {code}\r\n[ERROR]   ITestS3AHugeMagicCommits.test_030_postCreationAssertions:192 » AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: JAEYPCZ4P3JYGMTD, Extended Request ID: O/135mw9Xd2aEuFUh0ICWYc8DLXSpBUWaVGkEgEFGf0xO8o+XlZXY0hI+mvennOGt+C/UI7mNrQ=) (SDK Attempt Count: 1)                                                                                                                                                                                   \r\n[ERROR]   ITestS3AHugeMagicCommits>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin in s3a://stevel-london/job-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit                                                                                                                                                      \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 » AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1)                                                                                                                     \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 » FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                                                                                                  \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src  \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src      \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src            \r\n[ERROR]   ITestS3AHugeFilesArrayBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src          \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 » AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/bytebuffer/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: K0K75V8AH7SVBHS3, Extended Request ID: kDosbp+Z2PLZn9tVtRF9QfOqh1MgLbIKYaYFn2JeIptXlBV4v1a/wFukoXnaF7fCp6zx3vR8feE0fScUJEw+WhNW9lzu9dBxssOA62UA2kg=) (SDK Attempt Count: 1)                                                                                                           \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 » FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                   \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                             \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                 \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                       \r\n[ERROR]   ITestS3AHugeFilesByteBufferBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/bytebuffer/src                                                                                                                                                                                     \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 » AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 73T4YAYRWE63WAW5, Extended Request ID: 6ucEY2heh2NsxE8dBrlZp9AE4Tb+hbvnyxea1/yp5H85BEvkQdYsfNlRH5XZM1g4hHPDSoGMVtM=) (SDK Attempt Count: 1)                                                                                                                                                                                       \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 » FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src                                                                                                                     \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src     \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src         \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src               \r\n[ERROR]   ITestS3AHugeFilesDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src             \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 » AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/disk/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: ZSY181YB49GQFR83, Extended Request ID: FrPEfsXO3Gbhxi3m4ZmyYSiyfscQ1QSm/1lKjRPLHEbLWH5vtGked+fHvZl281Dm6u013/5VP6pj42h4XISftk7p9uEIDGw31E7Ymcoviq4=) (SDK Attempt Count: 1)                                                                                                                   \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 » FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src                                                                                                                 \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_040_PositionedReadHugeFile:478->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src     \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_050_readHugeFile:624->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src           \r\n[ERROR]   ITestS3AHugeFilesSSECDiskBlocks>AbstractSTestS3AHugeFiles.test_100_renameHugeFile:679->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/disk/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/disk/src         \r\n[ERROR]   ITestS3AHugeFilesStorageClass.test_010_CreateHugeFile:74->AbstractSTestS3AHugeFiles.test_010_CreateHugeFile:276 » AWSBadRequest Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: APYCQNP1GY02DGDE, Extended Request ID: lE0hQJ67sSwCYSMmO7tDEAvEIOCcpwIbLdfqqrNTpWT0bHIaacaIEzZusajj79rnFQlWudxsMHBIUXdS9ELiKR0T923lcULZy4Essx1LoTs=) (SDK Attempt Count: 1)                                                                                        \r\n[ERROR]   ITestS3AHugeFilesStorageClass.test_030_postCreationAssertions:81->AbstractSTestS3AHugeFiles.test_030_postCreationAssertions:433 » FileNotFound Huge file: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                                                             \r\n[ERROR]   ITestS3AHugeFilesStorageClass>AbstractSTestS3AHugeFiles.test_045_vectoredIOHugeFile:538->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src     \r\n[ERROR]   ITestS3AHugeFilesStorageClass.test_100_renameHugeFile:108->AbstractSTestS3AHugeFiles.assumeHugeFileExists:404->AbstractSTestS3AHugeFiles.assumeFileExists:414 » FileNotFound huge file not created: not found s3a://stevel-london/job-00/test/tests3ascale/array/src/hugefile in s3a://stevel-london/job-00/test/tests3ascale/array/src                                   \r\n[INFO] \r\n[ERROR] Tests run: 124, Failures: 1, Errors: 30, Skipped: 13\r\n[INFO] \r\n{code}\r\n\r\nThis has to be some transient issue with my s3 london bucket, as if in progress upload parts were not being retained. Never seen this before; the expiry time is set to 24h\r\n\r\nWhen these uploads fail we do leave incomplete uploads in progress:\r\n{code}\r\nListing uploads under path \"\"\r\njob-00-fork-0005/test/testCommitOperations 141OKG11JHhWF1GOnunHUd9ZzBJ8cUG9z0LsW_4wUGgCXCvDMQM3kRi5IOCUV8FdCHtg_w8SlipfubRtzCQoT5yEpOLv.cWOiOwjEaBzUjnuJORppfXuKy1piHpLnu98\r\njob-00-fork-0005/test/testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst yBJpm3zh4DjNQIDtyWgEmWVCk5sehVz5Vzn3QGr_tQT2iOonRp5ErXsQy24yIvnzRxBCZqVapy5VepLeu2udZBT5EXLnKRA3bchvzjtKDlipywSzYlL2N_xLUDCT359I\r\njob-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4\r\njob-00-fork-0005/test/testMagicWriteRecovery/file.txt KpvoTuVh85Wzm9XuU1EuxbATjb6D.Zv8vEj3z2S6AvJBHCBssy4iphxNhTkLDs7ceEwak4IPtdXED1vRf3geXT7MRMJn8d6feafvHVEgzbD31odpzTLmOaPrU_mFQXGV\r\njob-00-fork-0005/test/testMagicWriteRecovery/file.txt CnrbWU3pzgEGvjRuDuaP43Xcv1eBF5aLknqYaZA1vwO3b1QUIu9QJSiZjuLMYKT9GKw1QXwqoKo4iuxTY1a18bARx4XMEiL98kZBv0TPMaAfXE.70Olh8Q2kTyDlUCSh\r\njob-00-fork-0005/test/testMagicWriteRecovery/file.txt dEVGPBRsuOAzL5pGA02ve9qJhAlNK8lb8khF6laKjo9U0j_aG1xLkHEfPLrmcrcsLxC3R755Yv_uKbzY_Vnoc.nXCprvutM1TZmLLN_7LHrQ0tY0IjYSS6hVzDVlHbvC\r\njob-00-fork-0006/test/restricted/testCommitEmptyFile/empty-commit.txt NOCjVJqycZhkalrvU26F5oIaJP51q055et2N6b74.2JVjiKL8KwrhOhdrtumOrZ2tZWNqaK4iKZ_iosqgehJOiPbWJwxvrfvA5V.dAUTLNqjtEf5tfWh0UXu.vahDy_S5SSgNLFXK.VB82i5MZtOcw--\r\njob-00/test/tests3ascale/ITestS3AHugeMagicCommits/commit/commit.bin lsYNpdn_oiWLwEVvvM621hCvIwDVaL4y_bbwVpQouW1OBThA.P9cR8fZtxvBjGdMY41UH0dTjxGHtF3BXEY8WXqmcnO9QHs_Jy.os781pE3MGzqgzFyxmd0yN6LFcTbq\r\ntest/restricted/testCommitEmptyFile/empty-commit.txt T3W9V56Bv_FMhKpgcBgJ1H2wOBkPKk23T0JomesBzZyqiIAu3NiROibAgoZUhWSdoTKSJoOgcn3UWYGOvGBbsHteS_N_c1QoTEp0GE7PNlzDfs1GheJ5SOpUgaEY6MaYdNe0mn0gY48FDXpVB2nqiA--\r\ntest/restricted/testCommitEmptyFile/empty-commit.txt .cr4b3xkfze4N24Bj3PAm_ACIyIVuTU4DueDktU1abNu2LJWXH2HKnUu1oOjfnnQwnUXp4VmXBVbZ5aq8E8gVCxN.Oyb7hmGVtESmRjpqIXSW80JrB_0_dqXe.uAT.JH7kEWywAlb4NIqJ5Xz99tvA--\r\nTotal 10 uploads found.\r\n{code}\r\n\r\nMost interesting here is `testIfNoneMatchTwoConcurrentMultipartUploads`, because this initiates then completes an MPU, so as to create a zero byte file. It doesn't upload any parts. \r\n\r\nThe attempt to complete failed.\r\n{code}\r\n[ERROR] org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads -- Time elapsed: 2.783 s <<< ERROR!\r\norg.apache.hadoop.fs.s3a.AWSBadRequestException: Completing multipart upload on job-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1)\r\n        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:265)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)\r\n        at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)\r\n        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\r\n        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.finalizeMultipartUpload(WriteOperationHelper.java:318)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.completeMPUwithRetries(WriteOperationHelper.java:370)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.lambda$complete$3(S3ABlockOutputStream.java:1227)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:493)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:1225)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$1500(S3ABlockOutputStream.java:876)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:545)\r\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\r\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\r\n        at org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.createFileWithFlags(ITestS3APutIfMatchAndIfNoneMatch.java:190)\r\n        at org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads(ITestS3APutIfMatchAndIfNoneMatch.java:380)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at java.util.ArrayList.forEach(ArrayList.java:1259)\r\n        at java.util.ArrayList.forEach(ArrayList.java:1259)\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 9JCJ6M5QRDGJNYYS, Extended Request ID: Z7Q7+LA0o/5B4xoIGhgo+tVppawZ0UBj7X4RNb+0m9RbOAOwD/Apv1o+KmnW0aypjwmfFlarxjo=) (SDK Attempt Count: 1)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:113)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:61)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.retryPolicyDisallowedRetryException(RetryableStageHelper.java:168)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:73)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:53)\r\n        at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:35)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:82)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:43)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\n        at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)\r\n        at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\n        at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)\r\n        at software.amazon.awssdk.services.s3.DefaultS3Client.completeMultipartUpload(DefaultS3Client.java:801)\r\n        at software.amazon.awssdk.services.s3.DelegatingS3Client.lambda$completeMultipartUpload$1(DelegatingS3Client.java:611)\r\n        at software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67)\r\n        at software.amazon.awssdk.services.s3.DelegatingS3Client.completeMultipartUpload(DelegatingS3Client.java:611)\r\n        at org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.completeMultipartUpload(S3AStoreImpl.java:906)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelperCallbacksImpl.completeMultipartUpload(S3AFileSystem.java:1953)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$finalizeMultipartUpload$1(WriteOperationHelper.java:324)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)\r\n        ... 18 more\r\n\r\n{code}\r\n\r\nYet the uploads list afterwards finds it\r\n{code}\r\njob-00-fork-0005/test/testIfNoneMatchTwoConcurrentMultipartUploads AnspJPHUoPJqg61t28OvLfAogi6G9ocyx1Dm6XY2C.a_H_onklM0Nr0LIXaPiYlQjZIiH0fTsQ1e2KhEjS9pGxvSKOXq_4YibiGZmFC6rBolmfACMqIRpoeaqYDgzYW4\r\n{code}", "output": "And stack on a write failure. \r\n{code}\r\n[ERROR] org.apache.hadoop.fs.s3a.scale.ITestS3AHugeFilesArrayBlocks.test_010_CreateHugeFile -- Time elapsed: 2.870 s <<< ERROR!\r\norg.apache.hadoop.fs.s3a.AWSBadRequestException: Completing multipart upload on job-00/test/tests3ascale/array/src/hugefile: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1):InvalidPart: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1)\r\n        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:265)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)\r\n        at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:376)\r\n        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\r\n        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.finalizeMultipartUpload(WriteOperationHelper.java:318)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.completeMPUwithRetries(WriteOperationHelper.java:370)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.lambda$complete$3(S3ABlockOutputStream.java:1227)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.measureDurationOfInvocation(IOStatisticsBinding.java:493)\r\n        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:1225)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$1500(S3ABlockOutputStream.java:876)\r\n        at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:545)\r\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\r\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\r\n        at org.apache.hadoop.fs.s3a.scale.AbstractSTestS3AHugeFiles.test_010_CreateHugeFile(AbstractSTestS3AHugeFiles.java:276)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at java.util.ArrayList.forEach(ArrayList.java:1259)\r\n        at java.util.ArrayList.forEach(ArrayList.java:1259)\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag. (Service: S3, Status Code: 400, Request ID: 1NNBCSX4NCDN7G9X, Extended Request ID: 8vMmeyt1GfjGrf3UL9AN8vlwWSn9860f1gdeIBC3drmcjeQwC6wOPinMD8MSO6ggGw9ywwdcXroGTdVSFLYq0S0VdM/5bYfanDXJ43Eb4QU=) (SDK Attempt Count: 1)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:113)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:61)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.utils.RetryableStageHelper.retryPolicyDisallowedRetryException(RetryableStageHelper.java:168)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:73)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:53)\r\n        at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:35)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:82)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:62)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:43)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\n        at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\n        at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\r\n        at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)\r\n        at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\n        at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)\r\n        at software.amazon.awssdk.services.s3.DefaultS3Client.completeMultipartUpload(DefaultS3Client.java:801)\r\n        at software.amazon.awssdk.services.s3.DelegatingS3Client.lambda$completeMultipartUpload$1(DelegatingS3Client.java:611)\r\n        at software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67)\r\n        at software.amazon.awssdk.services.s3.DelegatingS3Client.completeMultipartUpload(DelegatingS3Client.java:611)\r\n        at org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.completeMultipartUpload(S3AStoreImpl.java:906)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelperCallbacksImpl.completeMultipartUpload(S3AFileSystem.java:1953)\r\n        at org.apache.hadoop.fs.s3a.WriteOperationHelper.lambda$finalizeMultipartUpload$1(WriteOperationHelper.java:324)\r\n        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)\r\n        ... 17 more\r\n\r\n{code}\r\n\r\nwe'd have to map 400 + the error text to a \"MultipartUploadCompleteFailed\" exception and add a policy for it, leaving other 400s as unrecoverable."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Missing null check before using shared resource\nDescription: The shared resource can be assigned to null, while another member method later dereferences it without a null check. This can cause a runtime failure when fencing operations are triggered.\nQ: have you got a stack to share", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support getSchemas for SparkConnectDatabaseMetaData\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Document controller.quorum.auto.join.enable config in upgrade.html\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Wildfly 2.1.4\r\n* doesn't work with openssl 3 (that symbol change...why did they do that?)\r\n\r\n\r\nwe need a version with \r\nhttps://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d\r\n\r\n2.2.5.Final does the openssl 3 support.", "output": "Upgrade to wildfly version with support for openssl 3"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [JDK25] Bump ByteBuddy 1.17.6 and ASM 9.8 to support Java 25 bytecode\nDescription: \nQ: slfan1989 commented on code in PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#discussion_r2281285636\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -146,7 +146,7 @@\n     4.1.118.Final\n     1.1.10.4\n     1.7.1\n-    1.15.11\n+    1.17.6\n\nReview Comment:\n   Should the LICENSE-binary be updated?", "output": "pan3793 commented on code in PR #7879:\nURL: https://github.com/apache/hadoop/pull/7879#discussion_r2281290385\n\n\n##########\nhadoop-project/pom.xml:\n##########\n@@ -146,7 +146,7 @@\n     4.1.118.Final\n     1.1.10.4\n     1.7.1\n-    1.15.11\n+    1.17.6\n\nReview Comment:\n   they are test-only deps, not present in LICENSE/NOTICE files"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header.\r\n\r\nFollowing are types of read we want to identify:\r\n # Direct Read: Read from a given position in remote file. This will be synchronous read\r\n # Normal Read: Read from current seeked position where read ahead was bypassed. This will be synchronous read.\r\n # Prefetch Read: Read triggered from background threads filling up in memory cache. This will be asynchronous read.\r\n # Missed Cache Read: Read triggered after nothing was received from read ahead. This will be synchronous read.\r\n # Footer Read: Read triggered as part of footer read optimization. This will be synchronous.\r\n # Small File Read: Read triggered as a part of small file read. This will be synchronous read.\r\n\r\nWe will add another field in the Tracing Header (Client Request Id) for each request. We can call this field \"Operation Specific Header\" very similar to how we have \"Retry Header\" today. As part of this we will only use it for read operations keeping it empty for other operations. Moving ahead f we need to publish any operation specific info, same header can be used.", "output": "ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done."}
{"instruction": "Answer the question based on the bug.", "input": "Title: [SDP] Sinks\nDescription: \nQ: Issue resolved by pull request 52563\n[https://github.com/apache/spark/pull/52563]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade to com.google.guava 32.1.3-jre to fix CVE-2023-2976 and CVE-2020-8908\nDescription: Upgrade com.google.guava library to 32.1.3-jre to fix:\r\n * [CVE-2023-2976|https://github.com/advisories/GHSA-7g45-4rm6-3mm3]\r\n * [CVE-2020-8908|https://github.com/advisories/GHSA-5mg8-w23w-74h3]\r\n\r\nLink to PR:\r\n[https://github.com/apache/hadoop/pull/7473]\nQ: bartoszkosiorek opened a new pull request, #7473:\nURL: https://github.com/apache/hadoop/pull/7473\n\n   \r\n   \r\n   ### Description of PR\r\n   Upgrade  com.google.guava library to  32.1.3-jre to fix:\r\n   * CVE-2023-2976 \r\n   * CVE-2020-8908\r\n   \r\n   Ticket number:\r\n   https://issues.apache.org/jira/browse/HADOOP-19481\r\n   \r\n   ### How was this patch tested?\r\n   By running  `start-build-env.sh`\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "bartoszkosiorek closed pull request #7458: HADOOP-19481 Upgrade to com.google.guava 32.1.3-jre to fix security issues\nURL: https://github.com/apache/hadoop/pull/7458"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Adjust quorum-related config lower bounds\nDescription: Some config settings are related to each other, or certain configuration constraints haven’t been discussed.\r\n\r\n\r\nSee:\r\n[https://github.com/apache/kafka/pull/18998#discussion_r1988001109]\r\nhttps://github.com/apache/kafka/pull/20318/files#r2465660429", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "[KIP-1217|https://cwiki.apache.org/confluence/x/6QnxFg] deprecates the ClientTelemetryReceiver and ClientTelemetry interfaces. They should be removed in Kafka 5.0.0.", "output": "Follow up for KIP-1217"}
{"instruction": "Answer the question based on the bug.", "input": "Title: SecretManager configuration at runtime\nDescription: In case of TEZ *DAGAppMaster* the Hadoop *SecretManager* code can not read yarn config xml file, therefore the SELECTED_ALGORITHM and SELECTED_LENGTH variables in SecretManager can not be set at runtime.\r\nThis can results with the following exception in FIPS environment:\r\n\r\n{code:java}\r\njava.security.InvalidParameterException: Key size for HMAC must be at least 112 bits in approved mode: SHA-1/HMAC\r\n\tat com.safelogic.cryptocomply.fips.core/com.safelogic.cryptocomply.jcajce.provider.BaseKeyGenerator.engineInit(Unknown Source)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:540)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:517)\r\n\tat org.apache.hadoop.security.token.SecretManager.(SecretManager.java:157)\r\n\tat org.apache.hadoop.yarn.security.client.BaseClientToAMTokenSecretManager.(BaseClientToAMTokenSecretManager.java:38)\r\n\tat org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager.(ClientToAMTokenSecretManager.java:46)\r\n\tat org.apache.tez.co\nQ: K0K0V0K opened a new pull request, #7827:\nURL: https://github.com/apache/hadoop/pull/7827\n\n   ### Description of PR\r\n   \r\n   - static configuration of SecretManager is required because it has some static method what use the selected algorithm\r\n   - in case if class path not contains the config values (for example TEZ DAGAppMaster run) the default values will be loaded at runtime\r\n   - the default values can cause failers in modern environments (they are not FIPS compliant)\r\n   - new SecretManagerConfig created to be able to modify the SecretManager config without core-site.xml present on class path\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   - Unit tests were run\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "let me add the motivation from Tez side as a context, so as [~bkosztolnik] mentioned we faced a problem after YARN-11738,\r\nbecause Tez is supposed to work by runtime payloads (containing all the hadoop config), but in the hadoop layer, the options are initialized in the static initializer, so tez cannot help with that exception, because new Configuration() depends on the core-site.xml config which is *maybe* on the classpath, otherwise, everything turns to default\r\nfirst I tried to make the TezClientToAMTokenSecretManager extend a custom Tez implementation of SecretManager, but it led to compilation issues, as Hadoop layers expect a Hadoop SecretManager (see RPC), so eventually we cannot pass a Tez SecretManager that doesn’t inherit Hadoop’s SecretManager, but as long as we extend the Hadoop one, the static initializer and then the field initializer of keyGen keyGen.init(SELECTED_LENGTH) will kick in immediately\r\n\r\nso this cannot be fixed from Tez, we have 2 options:\r\n1. rework YARN-11738 (as this problem was also implied in this comment upstream)\r\n2. make the core-site.xml localized to tez containers to have it picked up <- this is against tez design, so I would prefer 1)\r\n\r\noptimal way would be completely eliminate static fields from SecretManager, but I'm afraid they are there for a reason, so basically, anything could work for us which makes Tez able to intercept, and configure the SecretManager from a Configuration object, which is different than the default one (which is instantiated by new Configuration())\r\n\r\nso this cannot be fixed from Tez, we have 2 options:\r\n1. rework YARN-11738 (as this problem was also implied in this comment upstream)\r\n2. make the core-site.xml localized to tez containers to have it picked up\r\n\r\nI’m a bit against 2), because it’s also a design decision to make file config resources available to tez containers instead of payload, so I would definitely be in favor of 1), here is where I need the opinion of Hadoop folks"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update ShareGroupCommand to use share partition lag information\nDescription: ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension\nDescription: This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc\r\ninstruction sets, with full functional verification and performance testing completed.\r\n\r\nThe implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction.\r\n\r\nKey Features: \r\n1. Runtime Hardware Detection\r\nThe PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime.\r\n\r\n2. Performance Improvement\r\nHardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Guava has been already updated to 32.0.1 in hadoop-thirdparty.\r\nHowever, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners.\r\n\r\nSync the non-thirdparty Guava version to the thirdparty one.", "output": "Update non-thirdparty Guava version to 32.0.1"}
{"instruction": "Answer the question based on the bug.", "input": "Title: The Content-Security-Policy header must not be overridden\nDescription: [https://github.com/apache/kafka-site/blob/905299a0b7e2e3892d9493c7dcaaf78dce035c00/.htaccess#L13]\r\n\r\nThe Content-Security-Policy header must not be overridden.\r\n\r\nThere is now a standard way to add local exceptions to the CSP:\r\n\r\n[https://infra.apache.org/tools/csp.html]\r\n\r\nPlease update the .htaccess file accordingly.\r\n\r\nPlease note that the policy says that any exceptions must have been approved, and a reference to the approval must be commented in the .htaccess file\nQ: brandboat opened a new pull request, #733:\nURL: https://github.com/apache/kafka-site/pull/733\n\n   The Content-Security-Policy header must not be overridden.\r\n   There is now a standard way to add local exceptions to the CSP:\r\n   https://infra.apache.org/tools/csp.html", "output": "brandboat commented on PR #733:\nURL: https://github.com/apache/kafka-site/pull/733#issuecomment-3447983723\n\n   quickstart local dev env screenshot:"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "[S3 Acceleration|https://aws.amazon.com/s3/transfer-acceleration/] can be used to speed up transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects.\r\n\r\nFor more details on using S3 Acceleration, please refer to [Configuring fast, secure file transfers using Amazon S3 Transfer Acceleration|https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html].", "output": "S3A : Add option fs.s3a.s3-acceleration.enabled to enable S3 Transfer Acceleration"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This goes to add DSv2 support to the optimization rule PushVariantIntoScan. The PushVariantIntoScan rule only supports DSv1 Parquet (ParquetFileFormat) source. It limits the effectiveness of variant type usage on DSv2.", "output": "Fix DSv2 in PushVariantIntoScan by adding SupportsPushDownVariants"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In KAFKA-18225, we discovered that {{ClientQuotaCallback#updateClusterMetadata}} is not supported in KRaft mode, unlike in Zookeeper mode. Kafka 4.0 addressed this gap by implementing the method, but limitations remain:\r\n * A new {{Cluster}} object (immutable and heavyweight) is passed on every metadata update, which may cause memory pressure in large clusters (see KAFKA-18239).\r\n\r\n * Some {{Cluster}} fields are confusing or irrelevant in KRaft, such as {{controller()}} returning a random node for compatibility. Also, listener parsing differs between modes, potentially causing inconsistent partition info (see KAFKA-19122).\r\n\r\nTo resolve these issues, *[KIP-1162|https://cwiki.apache.org/confluence/x/zInoF]* proposes a redesign of the method. However, given that this method remained unimplemented for years without user complaints, we believe it's not worth the complexity to fix it. Instead, we propose deprecating {{updateClusterMetadata}} now and removing it in Kafka 5.0.", "output": "Deprecate ClientQuotaCallback#updateClusterMetadata"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Race condition between log segment flush and file deletion causing log dir to go offline\nDescription: h1. Context\r\n\r\nWe are using Kafka v3.7.1 with Zookeeper, our brokers are configured with multiple disks in a JBOD setup, routine intra-broker data rebalancing is performed using Cruise Control to manage disk utilization. During these rebalance operations, a race condition between a log segment flush operation and the file deletion that is part of the replica's directory move. This race leads to a `NoSuchFileException` when the flush operation targets a file path that has just been deleted by the rebalance process. This exception incorrectly forces the broker to take the entire log directory offline.\r\nh1. Logs / Stack trace\r\n{code:java}\r\n2025-07-23 19:03:30,114 WARN Failed to flush file /var/lib/kafka-08/topic_01-12/00000000024420850595.snapshot (org.apache.kafka.\r\ncommon.utils.Utils)\r\njava.nio.file.NoSuchFileException: /var/lib/kafka-08/topic_01-12/00000000024420850595.snapshot\r\n        at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\r\n        at java\nQ: (https://issues.apache.org/jira/browse/KAFKA-15391) is similar but different, it swallows `NoSuchFileException` for race condition on log directory move/delete, but not on the segment file level.", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: acknowledge Guava license on LimitInputStream\nDescription: When ASF projects copy 3rd party code into their code bases, they are meant to:\r\n* check the orginal license is Category A - https://www.apache.org/legal/resolved.html\r\n* keep the original source code headers\r\n* add something to their LICENSE that mentions the source file and what license is on it\r\n* if the 3rd party project is Apache licensed but has a NOTICE file, we need to copy its contents to our NOTICE\r\n* these requirements are only negated if the original code is submitted to the ASF project by the code's copyright holder (the individual or company that wrote the original code).\r\n\r\n* Hadoop copy https://github.com/apache/hadoop/blob/c357e435fd691b4c184b82325bef6d7c65e5f32b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LimitInputStream.java#L39\r\n* Original code has a Guava copyright that we probably need to keep https://github.com/google/guava/blob/master/guava/src/com/google/common/io/ByteStreams.java\nQ: steveloughran merged PR #7821:\nURL: https://github.com/apache/hadoop/pull/7821", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The currently used libopenssl-3.1.4-1 is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n\r\nThe Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows.\r\n\r\n{code}\r\n00:25:33 SUCCESS: Specified value was saved.\r\n00:25:46 Removing intermediate container 5ce7355571a1\r\n00:25:46 ---> a13a4bc69545\r\n00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n00:25:46 ---> Running in d2dafad446f9\r\n00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n00:25:54 \u001b[0m\u001b[91mAt line:1 char:1\r\n00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ...\r\n00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:Htt\r\n{code}\r\n\r\nThus, we need to upgrade to the latest version to address this.", "output": "Upgrade libopenssl to 3.5.2-1 needed for rsync"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "As part of recent change, loop over listing was added when checking if a directory is empty or not.\r\nContinuation token was not getting updated in subsequent listblob calls leading to infinite loop of list calls.\r\n\r\nCaused by: https://issues.apache.org/jira/browse/HADOOP-19572", "output": "ABFS: [FnsOverBlob][BugFix] IsNonEmptyDirectory Check should loop on listing using updated continuation token"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Upgrade `ZSTD-JNI` to 1.5.7-5\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Skip tests that depend on SecurityManager if the JVM does not support it\nDescription: Due to JEP411, depending on the Java version, SecurityManager either has to be explicitly enabled, or is completely disabled.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: SS | `Add TaskContext information to KafkaDataConsumer release() logs for better debugging`\nDescription: Currently, log messages in the {{release()}} method of {{KafkaDataConsumer}} do not include Spark task context information (e.g., TaskID).\r\n\r\nThis change adds task context details to the {{release()}} logs to improve traceability and debugging of Kafka consumer lifecycle events.\r\n\r\nWe previously added similar context in {{{}KafkaBatchReaderFactory.createReader{}}}, which has proven useful for debugging. Including this information in {{release()}} helps correlate consumer release metrics with the specific Spark tasks that borrowed the consumer.\nQ: Issue resolved by pull request 52745\n[https://github.com/apache/spark/pull/52745]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade hadoop3 docker scripts to 3.4.2\nDescription: The Hadoop 3.4.2 version has been released, and we need to update the Hadoop Docker image to version 3.4.2.\nQ: slfan1989 commented on PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#issuecomment-3355515590\n\n   @jojochuang @adoroszlai @ayushtkn Hadoop 3.4.2 has been released, and we are preparing a corresponding Docker image for Hadoop 3.4.2. I have created this PR to complete the Docker image release. Could you please review this PR? Thank you very much!", "output": "adoroszlai commented on code in PR #8005:\nURL: https://github.com/apache/hadoop/pull/8005#discussion_r2394132623\n\n\n##########\nDockerfile:\n##########\n@@ -14,7 +14,7 @@\n # limitations under the License.\n \n FROM apache/hadoop-runner\n-ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\n+ARG HADOOP_URL=https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar\n\nReview Comment:\n   BTW do you know why the tarball is published without `.gz`?  It still seems to be gzipped:\r\n   \r\n   ```\r\n   $ file hadoop-3.4.2.tar\r\n   hadoop-3.4.2.tar: gzip compressed data, ...\r\n   ```"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Pin robotframework version\nDescription: {{hadoop-runner}} installs {{robotframework}} without version definition.  Re-building the image for unrelated changes may unexpectedly upgrade {{robotframework}}, too.\nQ: adoroszlai opened a new pull request, #8025:\nURL: https://github.com/apache/hadoop/pull/8025\n\n   ## What changes were proposed in this pull request?\r\n   \r\n   `hadoop-runner` installs `robotframework` without version definition.  Re-building the image for unrelated changes may unexpectedly upgrade `robotframework`, too.\r\n   \r\n   This PR proposes to pin `robotframework` to version 6.1.1.\r\n   \r\n   https://issues.apache.org/jira/browse/HADOOP-19722\r\n   \r\n   ## How was this patch tested?\r\n   \r\n   ```\r\n   $ docker build -t hadoop-runner:dev .\r\n   ...\r\n   \r\n   $ docker run -it --rm hadoop-runner:dev robot --version\r\n   Robot Framework 6.1.1 (Python 3.10.12 on linux)\r\n   ```", "output": "smengcl merged PR #8025:\nURL: https://github.com/apache/hadoop/pull/8025"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Refactor RelationResolution to enable code reuse", "output": "Refactor RelationResolution to enable code reuse"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce client-side Geography and Geometry classes\nDescription: \nQ: Issue resolved by pull request 52804\n[https://github.com/apache/spark/pull/52804]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Memory leak using range scans\nDescription: We introduced a regression bug in AK 4.1.0 release, when range scans are use. We are leaking some metrics/sensors.\r\n\r\nThanks to [~eduwerc] and [~hermankj] for reporting this issue.\r\n\r\nAnybody who is using iterator, ie, any type of all/rang/prefix scan etc is affected. Note that certain DSL operators like session-windows, sliding-windows, stream-stream joins, and FK-join all use range scans internally, so the number of affected users is expected to be quite high.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Latest {{hadoop-runner}} images are based on Ubuntu 22.04.  Upgrade to 24.04.", "output": "Upgrade hadoop-runner to Ubuntu 24.04"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ShareConsumer.close() does not remove all sensors.\nDescription: This is a follow up from KAFKA-19542(https://issues.apache.org/jira/browse/KAFKA-19542) where some sensors in the share-consumer are not closed when the consumer closes.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade libxxhash to 0.8.3 in Windows 10\nDescription: The current version of libxxhash - 0.8.1 isn't available on the msys repo. This is causing the Hadoop Jenkins CI for Windows to fail -\r\n\r\n{code}\r\n00:34:06  Step 25/75 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libxxhash-0.8.1-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libxxhash-0.8.1-1-x86_64.pkg.tar.zst\r\n00:34:06   ---> Running in e9a8dd91a514\r\n00:34:15  \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n{code}\r\n\r\nThus, we need to upgrade to 0.8.3.\nQ: Merged PR to trunk - https://github.com/apache/hadoop/pull/7689.", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "not sure why https://github.com/google/gson/pull/2588 didn't attract a CVE\r\n\r\nlinked to https://issues.apache.org/jira/browse/HADOOP-19632 - nimbus-jose-jwt uses gson", "output": "upgrade gson due to security fixes"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support `AUTO` Netty IO Mode\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Exclude `KerberosConfDriverFeatureStep` during benchmarking\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Handle JDK-8225499 IpAddr.toString() format changes in tests\nDescription: The IP address string format has changed in JDK14\r\n[https://bugs.openjdk.org/browse/JDK-8225499|https://bugs.openjdk.org/browse/JDK-8232369]\r\n\r\n\r\n\r\nHDFS-15685 adjust some tests for it, but not all.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Disable topic autocreation for streams consumers.\nDescription: Currently we disable it only for [the main consumer|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1793], but not for the [restore|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1832] or [global|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1865] consumers.", "output": "Patch Available"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: hadoop-client-api exclude webapps/static front-end resources\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-extras.\nDescription: \nQ: slfan1989 opened a new pull request, #7586:\nURL: https://github.com/apache/hadoop/pull/7586\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19433. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-extras.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7586:\nURL: https://github.com/apache/hadoop/pull/7586#issuecomment-2788190158\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 24s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   1m  8s | [/patch-unit-hadoop-tools_hadoop-extras.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/1/artifact/out/patch-unit-hadoop-tools_hadoop-extras.txt) |  hadoop-extras in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 114m 32s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapred.tools.TestGetGroups |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7586 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2e0cb1878cdb 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a75c15e201c7465fc65c345c85c13d80ff6f545e |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/1/testReport/ |\r\n   | Max. process+thread count | 823 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-extras U: hadoop-tools/hadoop-extras |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce Geography and Geometry data types to PySpark API\nDescription: \nQ: Issue resolved by pull request 52627\n[https://github.com/apache/spark/pull/52627]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: processValues() must be declared as value-changing operation\nDescription: When adding `KStreams#processValues()` we missed to declare the operation as \"value changing\". This can lead to an \"incorrectly\" built topology.\r\n\r\nThe main problem is, that `processValues()` is the replacement of `transformValues()` which we removed with AK 4.0.0 release. Thus, if users rewrite existing programs from `transformValues()` to the new `processValues()` (what will be required when upgrading to 4.x release), they might observe this change as a regression.\r\n\r\nThe impact of the changed topology is, that local state is effectively lost, and must be restored from the changelog topic, resulting in downtime after an upgrade.\r\n\r\nNote: the bug does only surface, if topology optimization is used, in particular the \"merge repartition topics\" rewrite.\nQ: mjsax opened a new pull request, #721:\nURL: https://github.com/apache/kafka-site/pull/721\n\n   (no comment)", "output": "mjsax commented on code in PR #721:\nURL: https://github.com/apache/kafka-site/pull/721#discussion_r2331305729\n\n\n##########\n33/streams/developer-guide/dsl-api.html:\n##########\n@@ -3496,6 +3496,9 @@ KTable-KTable Foreign-Key\n                     \n                     \n                 \n+\n+                CAUTION: If you are using \"merge repartition topics\" optimization, it is not recommended to use KStream#processValues to avoid compatibility issues for future upgrades to newer versions of Kafka Streams.\n+                    For more details, see the migration guide in the Kafka Streams 4.0 docs.\n\nReview Comment:\n   This update (same for other version up to, including, `3.8`) are only done here on `kafka-site.git`, but not in `kafka.git`. But should be ok, as we won't do any more 3.x bug-fix releases with high probability.\r\n   \r\n   I did put this line into `3.9` though, as it's the last release in `3.9` series and might get more bug-fix releases, if people are hesitant to migrate to `4.0`."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update `dev/requirements.txt` to install `torch/torchvision` in Python 3.14\nDescription: \nQ: Issue resolved by pull request 52750\n[https://github.com/apache/spark/pull/52750]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title:  StreamsOnTasksAssignedCallbackNeededEvent could not be completed \nDescription: In several tests, for example PlaintextAdminIntegrationTest, we see this exception silently triggered when the consumer is closed.\r\n\r\n \r\n{code:java}\r\norg.apache.kafka.common.errors.TimeoutException: StreamsOnTasksAssignedCallbackNeededEvent could not be completed before the consumer closed[2025-09-05 14:17:40,972] ERROR [Consumer clientId=consumer-stream_group_id_1-1, groupId=stream_group_id_1] Reconciliation failed: callback invocation failed for tasks LocalAssignment{localEpoch=0, activeTasks={subtopology-0=[0, 1, 2]}, standbyTasks={}, warmupTasks={}} (org.apache.kafka.clients.consumer.internals.StreamsMembershipManager:1077)java.util.concurrent.CompletionException: org.apache.kafka.common.errors.TimeoutException: StreamsOnTasksAssignedCallbackNeededEvent could not be completed before the consumer closed\tat java.base/java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:368)\tat java.base/java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:377)\tat java.base/java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:1097)\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\tat java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2162)\tat org.apache.kafka.clients.consumer.internals.events.CompletableEventReaper.completeEventsExceptionallyOnClose(CompletableEventReaper.java:202)\tat org.apache.kafka.clients.consumer.internals.events.Com", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-extras.\nDescription: \nQ: hadoop-yetus commented on PR #7586:\nURL: https://github.com/apache/hadoop/pull/7586#issuecomment-2788190158\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 24s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   1m  8s | [/patch-unit-hadoop-tools_hadoop-extras.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/1/artifact/out/patch-unit-hadoop-tools_hadoop-extras.txt) |  hadoop-extras in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 114m 32s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapred.tools.TestGetGroups |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7586 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2e0cb1878cdb 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a75c15e201c7465fc65c345c85c13d80ff6f545e |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/1/testReport/ |\r\n   | Max. process+thread count | 823 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-extras U: hadoop-tools/hadoop-extras |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7586:\nURL: https://github.com/apache/hadoop/pull/7586#issuecomment-2831792362\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 37s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 56s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  hadoop-tools/hadoop-extras: The patch generated 0 new + 9 unchanged - 1 fixed = 9 total (was 10)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m  3s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   1m  7s | [/patch-unit-hadoop-tools_hadoop-extras.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/2/artifact/out/patch-unit-hadoop-tools_hadoop-extras.txt) |  hadoop-extras in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 129m 22s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.mapred.tools.TestGetGroups |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7586 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c70e89f7739a 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 637d84449b22af336a5e9052648a835f0dd902be |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/2/testReport/ |\r\n   | Max. process+thread count | 818 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-extras U: hadoop-tools/hadoop-extras |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7586/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Reduce default page size by LONG_ARRAY_OFFSET if ZGC or ShenandoahGC and ON_HEAP are used\nDescription: \nQ: Issue resolved by pull request 52754\n[https://github.com/apache/spark/pull/52754]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Clover breaks on double semicolon\nDescription: Building with {{-Pclover}} fails with\r\n{code}\r\n[INFO] Instrumentation error\r\ncom.atlassian.clover.api.CloverException: /home/jenkins/hadoop/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ;\r\n...\r\nFailed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory\r\n{code}\r\n\r\nIt doesn't seem to like a double semicolon in ITestS3APutIfMatchAndIfNoneMatch.java that was added in HADOOP-19256.\nQ: MikaelSmith opened a new pull request, #7956:\nURL: https://github.com/apache/hadoop/pull/7956\n\n   ### Description of PR\r\n   \r\n   Removes the extra semicolon after an import that causes `-Pclover` to fail with\r\n   \r\n       com.atlassian.clover.api.CloverException: hadoop/hadoop-tools/\r\n       hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/\r\n       ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ;`\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   ```\r\n   mvn -e -Pclover install -DskipTests -DskipShade --projects 'hadoop-tools/hadoop-aws'\r\n   ```\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7956:\nURL: https://github.com/apache/hadoop/pull/7956#issuecomment-3287077606\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  15m 13s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 21s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 140m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7956 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d1403c764b02 5.15.0-152-generic #162-Ubuntu SMP Wed Jul 23 09:48:42 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9fc528eeb6112dd9b3209b648fb33da720214dff |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/testReport/ |\r\n   | Max. process+thread count | 709 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7956/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix Maven download link\nDescription: The Jenkins CI for Windows Nightly build is failing since it's unable to download Apache Maven -\r\n\r\n{code}\r\n00:32:18  Step 14/78 : RUN powershell Invoke-WebRequest -URI https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.zip -OutFile $Env:TEMP\\apache-maven-3.8.8-bin.zip\r\n00:32:18   ---> Running in a47cc5638ee5\r\n00:32:22  \u001b[91mInvoke-WebRequest : \r\n00:32:22  \u001b[0m\u001b[91m\r\n00:32:22  \u001b[0m\u001b[91m404 Not Found\r\n00:32:22  \u001b[0m\u001b[91m\r\n00:32:22  \u001b[0m\u001b[91mNot Found\r\n00:32:22  \u001b[0m\u001b[91mThe requested URL was not found on this server.\r\n00:32:22  \u001b[0m\u001b[91m\r\n00:32:22  \u001b[0m\u001b[91mAt line:1 char:1\r\n00:32:22  + Invoke-WebRequest -URI https://downloads.apache.org/maven/maven-3/3.8 ...\r\n00:32:22  \u001b[0m\u001b[91m+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n00:32:22  \u001b[0m\u001b[91m    + CategoryInfo          : InvalidOperation: (System.Net.HttpWebRequest:Htt \r\n00:32:22  \u001b[0m\u001b[91m   pWebRequest) [Invoke-WebRequest], WebException\r\n00:32:22  \u001b[0m\u001b[91m    + FullyQualifiedE\nQ: Merged PR to trunk - https://github.com/apache/hadoop/pull/7768.", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "* In this issue I propose to refactor ParameterizedQuery arguments validation to ParameterizedQueryArgumentsValidator so it can be reused between single-pass and fixed-point analyzer implementations. We also remove one redundant case from the ParameterizedQueryArgumentsValidator.isNotAllowed (Alias shouldn't call isNotAllowed again as it introduces unnecessary overhead to the method) to improve performance.", "output": "Refactor ParameterizedQuery arguments validation"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Latest version of AAL is now available, changes since last time:\r\n * Moves all logging to debug\r\n * Adds in timeout and retries to requests to mitigate (very rare) hanging seen when reading from AsyncClient, related github issue: https://github.com/aws/aws-sdk-java-v2/issues/5755", "output": "S3A Analytics-Accelerator: Upgrade AAL to 1.0.0 "}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support `spark.kubernetes.allocation.maximum`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Metrics from FetchMetricsManager containing a topic tag are duplicated\nDescription: Hello,\r\n\r\nSince Kafka 4.1.0, we're experiencing an issue with Kafka Streams metrics for topics containing dots in their names. (I think the problem is also for simple usage of consumers not only kstream)\r\n\r\nIn FetchMetricsManager, methods like {{recordRecordsFetched()}} now create duplicate sensors for the same topic if they contain dot in their name: \r\n{code:java}\r\nvoid recordRecordsFetched(String topic, int records) { \r\n\r\n\r\n String name = topicRecordsFetchedMetricName(topic); \r\n\r\n\r\n maybeRecordDeprecatedRecordsFetched(name, topic, records);  Map.of(\"topic\", topic)) \r\n\r\n\r\n .withAvg(metricsRegistry.topicRecordsPerRequestAvg) \r\n\r\n\r\n .withMeter(metricsRegistry.topicRecordsConsumedRate, metricsRegistry.topicRecordsConsumedTotal) \r\n\r\n\r\n .build(); \r\n\r\n\r\n recordsFetched.record(records); \r\n\r\n\r\n }  {code}\r\nIt currently record two sensors, one with my original topic name, one time with a topic name with dots replaced by underscore. \r\n\r\n-While we can work around this by reversing the transformation in our case (replacing underscores back to dots in Micrometer filters) or by removing this specific list of metrics, this does not feel like a long-term solution for us.-\r\n\r\nCurrently using spring-boot and micrometer, it's really hard to remove one of those duplicated metrics. \r\n\r\nCould a configuration option be added to disable one of the two sensor to avoid having duplicated metrics?\r\n\r\nThanks in advance", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h2. Steps to reproduce\r\n * program uses a single broker (we see the issue with an in-JVM embedded kafka server)\r\n * create a new topic with 1 partition\r\n * produce a record with {{acks=all}}\r\n * await acknowledgement from the broker\r\n * start a consumer (configured to read from beginning of topic)\r\n * spuriously, _the consumer never sees the record_\r\n\r\nThe problem seems to occur more on a very busy server (e.g. a build server running many tests in parallel). We have not seen it on a modern laptop.\r\n\r\nA delay of 10ms after receiving the acknowledgement, and before starting the consumer, makes the record always visible.\r\n\r\nWe observe this problem in ~1 in 6 runs of [zio-kafka|https://github.com/zio/zio-kafka]'s test suite, when run from a GitHub action. Since we run the test-suite for 3 different JVM versions, more than half of the zio-kafka builds fails due to this issue.\r\nh2. Expected behavior\r\n\r\nA record, produced with {{{}acks=all{}}}, of which producing was acknowledged, should be immediately visible for all consumers.", "output": "Acked record on new topic not immediately visible to consumer"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Simplify recursion detection of `cloudpickle`\nDescription: ", "output": "Closed"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Extract common methods to KafkaOffsetReaderBase\nDescription: When reviewing https://github.com/apache/spark/pull/52729, I found that KafkaOffsetReaderConsumer and KafkaOffsetReaderAdmin have the exactly same getOffsetRangesFromResolvedOffsets method. The method is actually long so seems good to extract them to common one.\nQ: Issue resolved by pull request 52788\n[https://github.com/apache/spark/pull/52788]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Upgrade commons-beanutils to 1.11.0 due to CVE-2025-48734.", "output": "Upgrade commons-beanutils to 1.11.0 due to CVE-2025-48734."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Limitations of KRAFT Dual Write Mode for Production Support\nDescription: We are currently running Kafka version 3.9.0 and are in the process of migrating to KRaft. As part of the migration, we intend to operate in dual-write mode in production for an initial period to help identify and address any issues.\r\n\r\nAre there any known limitations or risks associated with running in dual-write mode? Would you recommend maintaining this mode for production stability, and are there best practices we should follow?\nQ: Hello, could someone please check this?", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add RegexpSingleline module to checkstyle.xml\nDescription: Add RegexpSingleline module to checkstyle.xml\nQ: hfutatzhanghb opened a new pull request, #7505:\nURL: https://github.com/apache/hadoop/pull/7505\n\n   ### Description of PR\r\n   Add RegexpSingleline module to checkstyle.xml.\r\n   In my local,  checkstyle tools use checkstyle.xml can not detect blanks.", "output": "hadoop-yetus commented on PR #7505:\nURL: https://github.com/apache/hadoop/pull/7505#issuecomment-2724481281\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 31s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  71m 24s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 14s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 26s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 19s |  |  hadoop-build-tools in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 109m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7505/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7505 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint |\r\n   | uname | Linux 243e431c62db 5.15.0-134-generic #145-Ubuntu SMP Wed Feb 12 20:08:39 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 91ab935d0061373208673c6054ec0265310b0c2b |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7505/1/testReport/ |\r\n   | Max. process+thread count | 623 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-build-tools U: hadoop-build-tools |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7505/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We recently upgraded vcpkg to install Boost 1.86. We need to update the documentation as well.", "output": "Update build instructions for Windows"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Make open telemetry object instantiation configurable\nDescription: After the 3.7 release of Kafka, the java client will emit many warning messages and more objects deployed due to an open telemetry object instantiation (possibly due to KIP-714). It appears the open telemetry feature was enabled by default and there's no way for a user to disable it. \r\n\r\nUsers receive the INFO message defined [here |https://github.com/apache/kafka/blob/3.7.2/clients/src/main/java/org/apache/kafka/common/telemetry/internals/ClientTelemetryReporter.java#L479] after activating INFO log level for \"org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter\" class. This log line appears lots of times.\r\n\r\nIt appears that the occurrence of this info messages shows ClientTelemetry objects are being created if the feature is not explicitly disabled. Users would like this feature to be opt-in and not enabled by default as it is.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Wrong generic type for UnregisterBrokerOptions\nDescription: This UnregisterBrokerOptions extends from the UpdateFeaturesOptions, which is obviously wrong.\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/UnregisterBrokerOptions.java#L23]\r\n\r\n \r\n\r\nAnd that's why in KafkaAdminClientTest.java, we set timeout via `options.timeoutMs = 10;`. We should set it via timeoutMs(int) method gracefully like other tests.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Release Hadoop 3.4.2\nDescription: Release a minor update to hadoop", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update Hadoop docker image tag 3 to 3.4\nDescription: apache/hadoop:3 is built using Hadoop 3.3.6 binary. https://hub.docker.com/layers/apache/hadoop/3/images/sha256-af361b20bec0dfb13f03279328572ba764926e918c4fe716e197b8be2b08e37f\r\n\r\nNow that Hadoop 3.4 line is getting adopted, it's time to update the tag.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Trigger snapshot generation for next batch when lag is detected\nDescription: We have done work to detect snapshot lag. Now we need to trigger snapshot generation for the next microbatch, when snapshot lag is detected.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "h2. Fencing offset commits\r\n\r\nIn the Kafka protocol, when a consumer commits offsets or a producer tries to add offsets to a transaction, it includes its epoch/generation of the consumer group. The point of this is for the group coordinator to fence against zombie commit requests, that is, commit requests that include an offset for a partition that was since reassigned to a different member. If such a guard was not in place, a zombie offset commit may overwrite offsets of the new owner, or its offsets may be committed to the consumer offset topic but not be included in the result of the new owners offset fetch request.\r\n\r\nIn KIP-848, when receiving an offset commit request that includes the client-side member epoch and a member ID, the group coordinator performs the check\r\n\r\n{{Client-Side Member Epoch == Broker-Side Member Epoch}}\r\n\r\nand, if the check fails, returns a {{STALE_MEMBER_EPOCH}} error for regular offset commits and a {{ILLEGAL_GENERATION}} for transactional offset commits. If the member epoch sent in the request is the current broker-side member epoch, KIP-848 guarantees that the partition cannot also be owned by a different member at the same or a larger epoch. Therefore, this is sufficient for fencing zombie commits. Note that we assume zombie commits will always contain offsets for partitions that were owned by the member at the member epoch sent in the request. Commit requests that commit offsets for partitions that are _not_ owned by the member in that epoch, are not possible in a correct client-side implementation of the protocol.\r\n\r\nNote that the broker-side member epoch is not the group epoch or the target assignment epoch. For details, see KIP-848. Note also that commits can also be fenced because a member falls out of the group (e.g. because it does not revoke partitions within the rebalance timeout). At this point, its commits will be fenced solely based on the member ID (which is not part of the group anymore). We therefore ignore this case i", "output": "Relax offset commit validation to allow member epochs since assignment"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix compilation error of native libraries on newer GCC\nDescription: Building with {{-Pnative}} by using GCC 14 on Fedora 40 failed due to incompatible changes of GCC.\nQ: {noformat}\r\n[WARNING] /hadoop/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/uri.h:60:3: error: ‘uint16_t’ does not name a type\r\n[WARNING]    60 |   uint16_t get_port() const;\r\n[WARNING]       |   ^~~~~~~~\r\n[WARNING] /hadoop/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/uri.h:25:1: note: ‘uint16_t’ is defined in header ‘’; this is probably fixable by adding ‘#include ’\r\n[WARNING]    24 | #include \r\n[WARNING]   +++ |+#include \r\n[WARNING]    25 | #include \r\n...\r\n{noformat}", "output": "iwasakims opened a new pull request, #7644:\nURL: https://github.com/apache/hadoop/pull/7644\n\n   https://issues.apache.org/jira/browse/HADOOP-19551\r\n   \r\n   Building with `-Pnative` by using GCC 14 on Fedora 40 failed due to imcompatible changes of GCC.\r\n   \r\n   `-Wno-error=implicit-function-declaration` should be added to CFLAGS in order to retain old behavior on GCC >= 14.\r\n   \r\n   ```\r\n   [WARNING] make[2]: *** [CMakeFiles/hadoop_static.dir/build.make:286: CMakeFiles/hadoop_static.dir/main/native/src/org/apache/hadoop/crypto/random/OpensslSecureRandom.c.o] Error 1\r\n   [WARNING] make[2]: *** Waiting for unfinished jobs....\r\n   [WARNING] /hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/random/OpensslSecureRandom.c: In function ‘locks_setup’:\r\n   [WARNING] /hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/random/OpensslSecureRandom.c:250:33: error: implicit declaration of function ‘dlsym_CRYPTO_num_locks’; did you mean ‘dlsym_CRYPTO_malloc’? [-Wimplicit-function-declaration]\r\n   [WARNING]   250 |   lock_cs = dlsym_CRYPTO_malloc(dlsym_CRYPTO_num_locks() *  \\\r\n   [WARNING]       |                                 ^~~~~~~~~~~~~~~~~~~~~~\r\n   [WARNING]       |                                 dlsym_CRYPTO_malloc\r\n   [WARNING] /hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/random/OpensslSecureRandom.c:257:3: error: implicit declaration of function ‘dlsym_CRYPTO_set_id_callback’; did you mean ‘CRYPTO_set_id_callback’? [-Wimplicit-function-declaration]\r\n   [WARNING]   257 |   dlsym_CRYPTO_set_id_callback((unsigned long (*)())pthreads_thread_id);\r\n   [WARNING]       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n   [WARNING]       |   CRYPTO_set_id_callback\r\n   [WARNING] /hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/random/OpensslSecureRandom.c:258:3: error: implicit declaration of function ‘dlsym_CRYPTO_set_locking_callback’; did you mean ‘CRYPTO_set_locking_callback’? [-Wimplicit-function-declaration]\r\n   [WARNING]   258 |   dlsym_CRYPTO_set_locking_callback((void (*)())pthreads_locking_callback);\r\n   [WARNING]       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n   [WARNING]       |   CRYPTO_set_locking_callback\r\n   ```\r\n   \r\n   and [cstdint must be explicitly included](https://gcc.gnu.org/gcc-13/porting_to.html#header-dep-changes) on GCC >= 13.\r\n   \r\n   ```\r\n   [WARNING] /hadoop/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/uri.h:60:3: error: ‘uint16_t’ does not name a type\r\n   [WARNING]    60 |   uint16_t get_port() const;\r\n   [WARNING]       |   ^~~~~~~~\r\n   [WARNING] /hadoop/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/uri.h:25:1: note: ‘uint16_t’ is defined in header ‘’; this is probably fixable by adding ‘#include ’\r\n   [WARNING]    24 | #include \r\n   [WARNING]   +++ |+#include \r\n   [WARNING]    25 | #include \r\n   ...\r\n   ```"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "'Long(long)' is deprecated since version 9 and marked for removal.", "output": "[JDK17] Do not use Long(long) and similar constructors"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support AQE in stateless streaming workloads\nDescription: We have been disabling AQE for streaming workloads since stateful operators do not work with AQE. But applying AQE in stateless workloads is still beneficial and we have been blocking the case too aggressively.\r\n\r\nWe have been observed streaming workloads which can enjoy the benefits of AQE e.g. stream-static join with large static table, MERGE INTO in ForeachBatch sink, etc. To accelerate the workloads, we would want to support AQE in stateless streaming workloads.", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: improve the documentation of `RecordsToDelete`\nDescription: document the behavior of  \"-1\" (HIGH_WATERMARK)", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip tests that require JavaScript engine when it's not available\nDescription: Some features require a JavaScript engine, which has been removed from JDK.\r\n\r\nSkip those tests if JavaScript is not available.\nQ: stoty opened a new pull request, #7503:\nURL: https://github.com/apache/hadoop/pull/7503\n\n   ### Description of PR\r\n   \r\n   Skip tests that require JavaScript engine if it is not available in the JVM\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Ran the affected tests with JDK 17, confirmed that they are skipped instead of failing or erroring out\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7503:\nURL: https://github.com/apache/hadoop/pull/7503#issuecomment-2722816991\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   7m 38s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  7s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  21m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   9m 25s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 57s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 14s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m  7s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 55s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 21s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m  2s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   8m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  8s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 10s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   2m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  | 108m  0s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  11m 57s |  |  hadoop-sls in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 255m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7503/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7503 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 47479813e725 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 726153b49ba3288e782af90a5c6bc51ebaea6126 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7503/1/testReport/ |\r\n   | Max. process+thread count | 963 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-tools/hadoop-sls U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7503/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Unbound Error Thrown if some variables are not set for SASL/SSL configuration\nDescription: I was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file — it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set -u}}{*}.\r\n\r\nThis issue is {*}just an enhancement to properly log the error details{*}.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [Gradle 8 --> 9 upgrade] GitHub Actions build issues: JUnit tests parsing fails (for a new/flaky steps)\nDescription: (!) {*}Note{*}: this blocks Gradle upgrade from 8 to 9 (KAFKA-19174) - _Gradle upgrade related stuff are done, but can't be merged due to CI build issues._\r\n\r\nSee here for more details: [https://github.com/apache/kafka/pull/19513#issuecomment-3236158518]\r\n\r\nInterestingly enough: only parsing fails (not tests) and on top of that it happens for flaky/new matrix {*}_only_{*}.\r\n_______________________________________________________________________________________________________________________________\r\n\r\n -{*}(i) Update{*}: after so many testing on Gradle upgrade PR ([https://github.com/apache/kafka/pull/19513]) I can confirm that something strange is going on between GitHub Actions and Develocity instances.-\r\n\r\n-Let me describe just a tip of the issues here; will use 'com.gradle.develocity' plugin version as an example (and please keep in mind that no classes/test are added or changed):-\r\n * Gradle 9/PR branch (with plugin version 3.19, i.e. trunk version): *parsing step fails for JUnit reports* (for flaky/new jobs, as noted above); \r\n * -Gradle 8/trunk branch  (with plugin version upgrade *only* - from 3.19 to 3.19.1): CI build ends up with this error: *upload build to Develocity fails*-\r\n\r\n-(on) From where I see this solution for this issue should be drafted along these lines:-\r\n -  -'com.gradle.develocity' plugin version and configuration-\r\n - -`com.gradle.common-custom-user-data-gradle-plugin` version-\r\n - -github action `setup-gradle` version and configuration-\r\n\r\n[~mumra", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When user tries to use TWS with HDFS as state store, they should get an error explicitly requiring to use RocksDB state provider.", "output": "Throw better error to indicate that TWS is only supported with RocksDB state store provider"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Prefer to detect Java Home from env JAVA_HOME on finding jmap\nDescription: \nQ: Issue resolved by pull request 52665\n[https://github.com/apache/spark/pull/52665]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Skip testRenameFileBeingAppended on Windows\nDescription: The AbstractContractAppendTest#testRenameFileBeingAppended() tries to rename the file while the handle is open. This isn't allowed on Windows and thus the call to rename fails.\r\n\r\nThus, we need to skip this test on Windows.\nQ: GauthamBanasandra opened a new pull request, #7666:\nURL: https://github.com/apache/hadoop/pull/7666\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   * The `AbstractContractAppendTest#testRenameFileBeingAppended()`\r\n      tries to rename the file while the handle\r\n      to the file is open.\r\n   * This isn't allowed on Windows, and thus the\r\n      call to rename fails.\r\n   * Thus, we need to close the file handle before\r\n      attempting to rename it.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   * Verified by running the failing unit test locally\r\n      and ensured that it passed.\r\n   * Jenkins CI validation for Windows.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "GauthamBanasandra commented on PR #7666:\nURL: https://github.com/apache/hadoop/pull/7666#issuecomment-2845595219\n\n   Started the Jenkins CI run for Windows - https://ci-hadoop.apache.org/view/Hadoop/job/hadoop-qbt-trunk-java8-win10-x86_64/910/."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Increase `s.k.o.reconciler.foregroundRequestTimeoutSeconds` to `60s`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Reorder Gradle tasks (in order to bump Shadow plugin version)\nDescription: *Prologue:*\r\n * JIRA ticket: KAFKA-19174\r\n * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027]\r\n\r\n*Scenario:*\r\n * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+\r\n * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x)\r\n\r\n*Action points (what needs to be done):*\r\n * use `com.gradleup.shadow` recent version (9+)\r\n * reorder Gradle tasks so that Gradle command mentioned above can work\r\n\r\n*Definition of done (at the minimum):*\r\n * Gradle command mentioned above works as expected\r\n * also: *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc): [https://kafka.apache.org/quickstart]", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions\nDescription: This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4's {{org.junit.Assume}} API to JUnit5's {{org.junit.jupiter.api.Assumptions}} API in order to unify the testing framework and improve maintainability.\r\nh4. Scope of changes:\r\n * Replace usages of {{{}Assume.assumeTrue(...){}}}, {{{}Assume.assumeFalse(...){}}}, etc., with the corresponding methods from JUnit5, such as {{{}Assumptions.assumeTrue(...){}}};\r\n\r\n * Ensure the assertion logic remains consistent with the original behavior;\r\n\r\n * Update any outdated import statements referencing JUnit4's {{{}Assume{}}};\r\n\r\n * Verify that all affected unit tests pass correctly under JUnit5.\nQ: slfan1989 opened a new pull request, #7858:\nURL: https://github.com/apache/hadoop/pull/7858\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   JIRA: HADOOP-19646. [JDK17] Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7858:\nURL: https://github.com/apache/hadoop/pull/7858#issuecomment-3157797389\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 39s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 23s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7858 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5abd35351f22 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6ca49d5da145efa997723827e84e0d9b047683a3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7858/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Close stale PRs updated over 100 days ago.\nDescription: Close stale PRs on GitHub which updated over 100 days ago, base on the voting thread: https://lists.apache.org/thread/7x6c029fp9k62bxt2995dz6q6j6sg0bh\nQ: Hexiaoqiao opened a new pull request, #7930:\nURL: https://github.com/apache/hadoop/pull/7930\n\n   \r\n   \r\n   ### Description of PR\r\n   This PR adds a GitHub workflow to automatically close stale PRs which have no activity over 100 days.\r\n   \r\n   ### How was this patch tested?\r\n   No.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [Y] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [N] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [N] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [Y] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7930:\nURL: https://github.com/apache/hadoop/pull/7930#issuecomment-3252923998\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  yamllint  |   0m  0s |  |  yamllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  44m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/artifact/out/blanks-eol.txt) |  The patch has 3 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  88m 49s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7930 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets yamllint |\r\n   | uname | Linux 8be7cf7a0c05 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5e03454c04c43efa24691ef3d9245d9333a46e70 |\r\n   | Max. process+thread count | 536 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7930/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Fix WASB ABFS compatibility issues\nDescription: Fix WASB ABFS compatibility issues. Fix issues such as:-\r\n # BlockId computation to be consistent across clients for PutBlock and PutBlockList\r\n # Restrict url encoding of certain json metadata during setXAttr calls.\r\n # Maintain the md5 hash of whole block to validate data integrity during flush.\nQ: anmolanmol1234 opened a new pull request, #7777:\nURL: https://github.com/apache/hadoop/pull/7777\n\n   Jira :- https://issues.apache.org/jira/browse/HADOOP-19604\r\n   \r\n   1. BlockId computation to be consistent across clients for PutBlock and PutBlockList so made use of blockCount instead of offset.\r\n   Block IDs were previously derived from the data offset, which could lead to inconsistency across different clients. The change now uses blockCount (i.e., the index of the block) to compute the Block ID, ensuring deterministic and consistent ID generation for both PutBlock and PutBlockList operations across clients.\r\n   \r\n   2. Restrict URL encoding of certain JSON metadata during setXAttr calls.\r\n   When setting extended attributes (xAttrs), the JSON metadata (hdi_permission) was previously URL-encoded, which could cause unnecessary escaping or compatibility issues. This change ensures that only required metadata are encoded.\r\n   \r\n   3. Maintain the MD5 hash of the whole block to validate data integrity during flush.\r\n   During flush operations, the MD5 hash of the entire block's data is computed and stored. This hash is later used to validate that the block correctly persisted, ensuring data integrity and helping detect corruption or transmission errors.", "output": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3031894608\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 12 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 47s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 16 unchanged - 0 fixed = 20 total (was 16)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 31s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 4 new + 10 unchanged - 0 fixed = 14 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 4 new + 10 unchanged - 0 fixed = 14 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 46s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 34s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2e8591fd6bca 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6b138c098a45024cbab113550c7130911a21df41 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/testReport/ |\r\n   | Max. process+thread count | 528 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Add a section at the bottom of the PR template to ask which AI tooling was used, if any, \r\n\r\n--- \r\n## AI\r\n\r\nIf an AI tool was used: \r\n[ ] The PR includes the phrase \"Generated by  where tool is  the AI tool used\r\n[ ] My use of AI contributions follows the ASF legal policy\r\nhttps://www.apache.org/legal/generative-tooling.html", "output": "Update PR template to ask about AI contribution; other AI hardening"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "[Amazon S3 Multi-Region Access Points|https://aws.amazon.com/s3/features/multi-region-access-points/] provide a global endpoint for applications that need to access data stored in S3 buckets located in multiple AWS regions. This feature simplifies how applications access data distributed across different geographic locations, providing a single endpoint that automatically routes requests to the bucket with the lowest latency.\r\n\r\nThe current implementation in S3A only allows single region access point and the goal is to expand the scope/implementation to include multi-region-access points as well.", "output": "S3A : Add Support For Multi-Region S3 Access Points"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Spark aggregation is incorrect (floating point error)\nDescription: {code:java}\r\nList data = Arrays.asList(\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95),\r\nRowFactory.create(\"2021-01-01T00:00:00.000+0000\", \"pod123\", 99.95)\r\n);\r\n \r\nStructType schema = DataTypes.createStructType(new StructField[] {\r\nDataTypes.createStructField(\"timestamp\", DataTypes.StringType, false),\r\nDataTypes.createStructField(\"id\", DataTypes.StringType, false),\r\nDataTypes.createStructField(\"value\", DataTypes.DoubleType, false)\r\n});\r\n \r\nDataset df = spark.createDataFrame(data, schema);\r\n \r\n// Show the input data\r\nSystem.out.println(\"Input data:\");\r\ndf.show\nQ: This does work with DecimalType - but I am not sure why it should not work with DoubleType given the level of precision of 2 digits.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Implement new behaviour in ProcessorStateManager\nDescription: Update {{ProcessorStateManager}} to start passing offsets to {{StateStore#commit}} and loading offsets using {{{}StateStore#getCommittedOffsets{}}}.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Complex sql with expand operator and code gen enabled, very slow\nDescription: Complex sql with expand operator and code gen enabled, very slow\r\n\r\nsql format like select keya,keyb,count(distinct case when),...,count(distinct case when),sum(a),sum(b) from x group by keya,keyb\r\n\r\nwhen disable whole stage code gen, run will speed up 20x times\r\n\r\nwhen add executor jvm parameter -XX:-TieredCompilation, run will speed up 20x times\r\n\r\nreduce select column count, such as 28 -> 27, can speed up 10x times", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob][Optimizations] Reduce Network Calls In Create and Mkdir Flow\nDescription: Implementing Create and Mkdir file system APIs for FNS(HNS Disabled) accounts on Blob Endpoint involves a lot of checks and marker file creations to handle implicit explicit cases of paths involved in these APIs.\r\n\r\nThis Jira proposes a few optimizations to reduce the network calls wherever possible and in case where create/mkdir is bound to fail, it should fail faster before doing any post checks,", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: ITests to run under JUnit5\nDescription: hadoop-aws tests which need to be parameterized on a class level\r\nare configured to do so through the @ParameterizedClass tag.\r\nFilesystem contract test suites in hadoop-common have\r\nalso been parameterized as appropriate.\r\n\r\nThere are custom JUnit tags declared in org.apache.hadoop.test.tags,\r\nwhich add tag strings to test suites/cases declaring them.\r\nThey can be used on the command line and in IDEs to control\r\nwhich tests are/are not executed.\r\n\r\n@FlakyTest \"flaky\"\r\n@LoadTest \"load\"\r\n@RootFilesystemTest \"rootfilesystem\"\r\n@ScaleTest \"scale\"\r\n\r\nFor anyone migrating tests to JUnit 5\r\n* Methods which subclass an existing test case MUST declare the @Test\r\n  tag again -it is no longer inherited.\r\n* All overridden setup/teardown methods MUST be located and\r\n  @BeforeEach/@AfterEach attribute added respectively\r\n* Subclasses of a parameterized test suite MUST redeclare themselves\r\n  as a @ParameterizedClass, and the binding mechanism again.\r\n* Parameterized test suites SHOULD declare a patt\nQ: Looks mostly test setup; some parameterization.\r\n\r\nPlan:\r\n* move to ParameterizedClass\r\n* fix tests which fail\r\n\r\n{code}\r\n[ERROR] Failures: \r\n[ERROR]   ITestS3AContractBulkDelete>AbstractContractBulkDeleteTest.testBulkDeleteParentDirectoryWithDirectories:241 [Parent non empty directory should not be deleted] \r\nExpected size: but was: in:\r\n\r\n[ERROR]   ITestS3AContractBulkDelete.testBulkDeleteZeroPageSizePrecondition:134 Expected a java.lang.IllegalArgumentException to be thrown, but got the result: : org.apache.hadoop.fs.s3a.impl.BulkDeleteOperation@6d3a56ea\r\n[ERROR]   ITestS3ACopyFromLocalFile.testOptionPropagation:88 [path capability of fs.s3a.optimized.copy.from.local.enabled] \r\nExpecting:\r\n \r\nto be equal to:\r\n \r\nbut was not.\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testDeleteNonExistentFile:755 Doesn't exist ==> expected:  but was: \r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGetFileStatusThrowsExceptionForNonExistentFile:267 Should throw FileNotFoundException\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGlobStatusFilterWithEmptyPathResults:506 expected:  but was: \r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGlobStatusFilterWithSomePathMatchesAndTrivialFilter:528 expected:  but was: \r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGlobStatusWithMultipleWildCardMatches:457 expected:  but was: \r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGlobStatusWithNoMatchesInPath:412 expected:  but was: \r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testListStatus:318 expected:  but was: \r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testListStatusFilterWithNoMatches:353 expected:  but was: \r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testListStatusThrowsExceptionForNonExistentFile:278 Should throw FileNotFoundException\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testMkdirs:210 expected:  but was: \r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testMkdirsFailsForSubdirectoryOfExistingFile:236 expected:  but was: \r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testWriteInNonExistentDirectory:744 Parent doesn't exist ==> expected:  but was: \r\n[ERROR]   ITestS3AMiscOperationCost.testGetContentMissingPath:167->AbstractS3ACostTest.verifyMetricsIntercepting:299->Assertions.assertEquals:664 operation returning java.io.FileNotFoundException: No such file or directory: s3a://stevel-london/job-00-fork-0001/test/testGetContentMissingPath: audit_span_creation ==> expected:  but was: \r\n[ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryDir:145->AbstractS3ACostTest.verifyMetrics:276->Assertions.assertEquals:664 operation returning         none             inf            none             inf            2            1                  0 : audit_span_creation ==> expected:  but was: \r\n[ERROR]   ITestS3AMiscOperationCost.testMkdirOverDir:112->AbstractS3ACostTest.verifyMetrics:276->Assertions.assertEquals:664 operation returning true: audit_span_creation ==> expected:  but was: \r\n[ERROR] Errors: \r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 » IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testAllRangesMergedIntoOne:271->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testAllRangesMergedIntoOne:271->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testConsecutiveRanges:399->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testConsecutiveRanges:399->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testDisjointRanges:251->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testDisjointRanges:251->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testEOFRanges:435->AbstractFSContractTestBase.isSupported:144 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testEOFRanges:435->AbstractFSContractTestBase.isSupported:144 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testEOFRanges416Handling:118->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testEOFRanges416Handling:118->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testEmptyRanges:411->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testEmptyRanges:411->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMinSeekAndMaxSizeConfigsPropagation:160 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMinSeekAndMaxSizeConfigsPropagation:160 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMinSeekAndMaxSizeDefaultValues:186 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMinSeekAndMaxSizeDefaultValues:186 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMultiVectoredReadStatsCollection:371->getTestFileSystemWithReadAheadDisabled:439 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMultiVectoredReadStatsCollection:371->getTestFileSystemWithReadAheadDisabled:439 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testMultipleVectoredReads:541->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testMultipleVectoredReads:541->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNegativeLengthRange:478->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNegativeLengthRange:478->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNegativeOffsetRange:485->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNegativeOffsetRange:485->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNormalReadAfterVectoredRead:504->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNormalReadAfterVectoredRead:504->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testNormalReadVsVectoredReadStatsCollection:250->getTestFileSystemWithReadAheadDisabled:439 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testNormalReadVsVectoredReadStatsCollection:250->getTestFileSystemWithReadAheadDisabled:439 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullRange:358->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullRange:358->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullRangeList:369->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullRangeList:369->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullReleaseOperation:493->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullReleaseOperation:493->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testOverlappingRanges:313->AbstractFSContractTestBase.isSupported:144 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testOverlappingRanges:313->AbstractFSContractTestBase.isSupported:144 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSameRanges:334->AbstractFSContractTestBase.isSupported:144 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSameRanges:334->AbstractFSContractTestBase.isSupported:144 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSomeRandomNonOverlappingRanges:383->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSomeRandomNonOverlappingRanges:383->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSomeRangesMergedSomeUnmerged:293->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSomeRangesMergedSomeUnmerged:293->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testStopVectoredIoOperationsCloseStream:207->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testStopVectoredIoOperationsCloseStream:207->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testStopVectoredIoOperationsUnbuffer:230->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testStopVectoredIoOperationsUnbuffer:230->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredIOEndToEnd:570->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredIOEndToEnd:570->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAfterNormalRead:522->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAfterNormalRead:522->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:210->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:210->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:189->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:189->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadWholeFile:229->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadWholeFile:229->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 » NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadWholeFilePlusOne:451->AbstractFSContractTestBase.isSupported:144 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadWholeFilePlusOne:451->AbstractFSContractTestBase.isSupported:144 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testDeleteEmptyDirectory:793 » PathIsNotEmptyDirectory `s3a://stevel-london/job-00-fork-0005/test/test/hadoop': Directory is not empty\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testDeleteRecursively:765->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGetWrappedInputStream:1120->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testInputStreamClosedTwice:1098->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameDirectoryAsNonEmptyDirectory:1048->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/dir/file1 already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameDirectoryAsNonExistentDirectory:990->FSMainOperationsBaseTest.doTestRenameDirectoryAsNonExistentDirectory:1007->FSMainOperationsBaseTest.rename:1159 » FileAlreadyExists rename destination s3a://stevel-london/job-00-fork-0005/test/test/new/newdir already exists.\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameFileAsExistingDirectory:920->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameFileAsExistingFile:899->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameFileToDestinationWithParentFile:846->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameFileToExistingParent:868->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameFileToItself:878->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testWriteReadAndDeleteEmptyFile:660->FSMainOperationsBaseTest.writeReadAndDelete:691 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testWriteReadAndDeleteHalfABlock:665->FSMainOperationsBaseTest.writeReadAndDelete:691 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testWriteReadAndDeleteOneBlock:670->FSMainOperationsBaseTest.writeReadAndDelete:691 » FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3APrefetchingLruEviction>AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:89 » NumberFormat Cannot parse null string\r\n[ERROR]   ITestS3APrefetchingLruEviction>AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:89 » NumberFormat Cannot parse null string\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 » IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3ACommitterMRJob.test_200_execute:282 » NullPointer Cannot read the array length because \"blkLocations\" is null\r\n[ERROR]   ITestS3ACommitterMRJob.test_200_execute:282 » NullPointer Cannot read the array length because \"blkLocations\" is null\r\n[ERROR]   ITestS3ACommitterMRJob.test_200_execute:282 » NullPointer Cannot read the array length because \"blkLocations\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testAMWorkflow:1475->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testAbortJobNoWorkDone:1188->AbstractITCommitProtocol.executeWork:608->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testAbortJobNotTask:1289->AbstractITCommitProtocol.executeWork:608->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testAbortTaskNoWorkDone:1181->AbstractITCommitProtocol.executeWork:608->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testAbortTaskThenJob:1213->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitJobButNotTask:1195->AbstractITCommitProtocol.executeWork:608->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitLifecycle:820->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitWithStorageClassConfig:864->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol.testCommitterCleanup:224->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitterWithDuplicatedCommit:898->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitterWithFailure:1003->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitterWithNoOutputs:1074->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testConcurrentCommitTaskWithSubDir:1318->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testFailAbort:1256->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testMapFileOutputCommitter:1095->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testOutputFormatIntegration:1409->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testParallelJobsToAdjacentPaths:1504->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testParallelJobsToSameDestination:1586->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testRecoveryAndCleanup:640->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testRequirePropagatedUUID:1784->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testS3ACommitterFactoryBinding:1829->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testSelfGeneratedUUID:1730->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testTwoTaskAttemptsCommit:936->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestS3AConditionalCreateBehavior.testConditionalWrite » ParameterResolution No ParameterResolver registered for parameter [boolean arg0] in constructor [public org.apache.hadoop.fs.s3a.impl.ITestS3AConditionalCreateBehavior(boolean)].\r\n[ERROR]   ITestS3AConditionalCreateBehavior.testWriteWithEtag » ParameterResolution No ParameterResolver registered for parameter [boolean arg0] in constructor [public org.apache.hadoop.fs.s3a.impl.ITestS3AConditionalCreateBehavior(boolean)].\r\n[ERROR]   ITestS3AConditionalCreateBehavior.testWriteWithPerformanceFlagAndOverwriteFalse » ParameterResolution No ParameterResolver registered for parameter [boolean arg0] in constructor [public org.apache.hadoop.fs.s3a.impl.ITestS3AConditionalCreateBehavior(boolean)].\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testConditionalCreateWhenPerformanceFlagEnabledAndOverwriteDisabled:637->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteDeletedFileWithEtag:484->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteFileWithEmptyEtag:508->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteWithCorrectEtag:432->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteWithOutdatedEtag:460->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst:522->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchConflictOnMultipartUpload:312->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchConflictOnOverwrite:288->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchMultipartUploadWithRaceCondition:333->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchOverwriteEmptyFileWithFile:397->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchOverwriteEmptyWithEmptyFile:414->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchOverwriteWithEmptyFile:379->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads:356->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 » NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n{code}", "output": "hadoop-yetus commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3089822111\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 16s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  44m  7s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  20m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 33s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 26s |  |  branch/hadoop-client-modules/hadoop-client-integration-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 30s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 11s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 46s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 55s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   3m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 42s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 35s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  hadoop-client-modules/hadoop-client-integration-tests has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 20s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 18s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  18m 32s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 28s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 26s |  |  hadoop-client-integration-tests in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 211m 27s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7814 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 63d1f6ad482c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f6e890ab053f9169cc683ac5ac5046f4bc4be111 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/testReport/ |\r\n   | Max. process+thread count | 1509 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-client-modules/hadoop-client-integration-tests U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Drop temporary functions in regular UDF tests\nDescription: \nQ: Issue resolved by pull request 52674\n[https://github.com/apache/spark/pull/52674]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Revisit `JUnit` assert usage in test cases\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Bump Avro 1.12.1\nDescription: \nQ: This is resolved via [https://github.com/apache/spark/pull/52664]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Auto migrate .position offsets\nDescription: When opening a {{{}RocksDbStateStore{}}}, {{.position}} offsets should be migrated into the offsets column family.", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Java 22 breaks Subject propagation for new Threads (when SecurityManager is not enabled).\r\n\r\nPreviously, the Subject set by Subject.doAs() / Subject.callAs() automatically propagated to any new Threads created (via new Thread(), not Executors).\r\n\r\nWith JDK22, this is no longer the case, new Threads do NOT inherit the Subject.\r\n\r\nAs Hadoop heavily relies on the original behavior, we somehow need to solve this problem.", "output": "Restore Subject propagation semantics for Java 22+"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: S3A Analytics-Accelerator: Upgrade AAL to 1.0.0 \nDescription: Latest version of AAL is now available, changes since last time:\r\n * Moves all logging to debug\r\n * Adds in timeout and retries to requests to mitigate (very rare) hanging seen when reading from AsyncClient, related github issue: https://github.com/aws/aws-sdk-java-v2/issues/5755", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-datajoin.\nDescription: \nQ: slfan1989 opened a new pull request, #7618:\nURL: https://github.com/apache/hadoop/pull/7618\n\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   HADOOP-19430. [JDK17] Upgrade JUnit from 4 to 5 in hadoop-datajoin.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7618:\nURL: https://github.com/apache/hadoop/pull/7618#issuecomment-2808864846\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 13s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 12s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 14s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 13s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 27s |  |  hadoop-datajoin in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  68m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7618/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7618 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 29ec6f5791bb 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / dd8d3f30c5c1e0beeaf093acdf28bc1ec7426c16 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7618/1/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-datajoin U: hadoop-tools/hadoop-datajoin |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7618/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "NPE in test200 of ITestS3ACommitterMRJob.\r\n\r\nCause is\r\n* test dir setup and propagation calls Path.getRoot().toUri() which returns the /home dir\r\n* somehow the locatedFileStatus of that path being 1 so a codepath in FileInputFormat NPEs.\r\n\r\nFix is in test code, though FileInputFormat has its NPE messages improved to help understand what is going wrong.", "output": "S3A: ITestS3ACommitterMRJob failing on Junit5"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Native-image (dockerimage) does not work with compression.type=zstd\nDescription: When sending records to a topic created like this:\r\n{quote}            admin.createTopics(List.of(new NewTopic(\"foo\", 24, (short) 1)\r\n                    .configs(Map.of(TopicConfig.COMPRESSION_TYPE_CONFIG,\"zstd\"))));\r\n{quote}\r\nWith a producer configured with:\r\n{quote}producerProps.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"zstd\");\r\n{quote}\r\nThe server fails with:\r\n{quote}[TIMESTAMP] ERROR [ReplicaManager broker=1] Error processing append operation on partition Z6OCnSwIS9KVFCN2gLaxnQ:foo-1 (kafka.server.ReplicaManager)\r\njava.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?]\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:423) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:405) ~[?:?]\r\n...\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?]\r\nSuppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.J", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: State Store Row Checksum implementation\nDescription: Introduce row checksum creation and verification for state store, both {*}HDFS and RocksDB state store{*}. This will help detect corruption at the row level and help prevent corruption.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Shading of Guava in `spark-network-common` has changed from 3.x to 4.x such that conflicting classes prevent downstream code from using Guava. \r\n\r\nIn spark jars prior to 4.x, `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class` was not a class included in the jar. Post 4.x, Guava is still shaded to `sparkproject` but `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class`, used in Guava's `InternetDomainName` and elsewhere, is now present in the jar. \r\n{code:java}\r\n$ jar tf spark-network-common_2.13-4.0.0.jar | rg PublicSuffixPatterns \r\ncom/google/thirdparty/publicsuffix/PublicSuffixPatterns.class {code}\r\nIf a library depends on spark but also depends on Guava, calls to `InternetDomainName` that call `PublicSuffixPatterns` will fail with `Exception in thread \"main\" java.lang.NoSuchFieldError: EXACT`. \r\n\r\nInspecting the code locations in such a library via `classOf[InternetDomainName].getProtectionDomain.getCodeSource.getLocation` and `classOf[PublicSuffixPatterns].getProtectionDomain.getCodeSource.getLocation` reveals that `InternetDomainName` is sourced from Guava, `target/bg-jobs/sbt_2062a9c3/target/5fcb43b5/1685140132000/guava-32.0.0-jre.jar`, while `PublicSuffixPatterns` is sourced instead from spark jar, `target/bg-jobs/sbt_2062a9c3/target/db746978/1747651686000/spark-network-common_2.13-4.0.0.jar`.", "output": "spark-network-common no longer shades all of Guava"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The Spark optimizer's {{InferFiltersFromConstraints}} rule is currently unable to infer filter conditions when join constraints involve complex expressions. While it works for simple attribute equalities ({{{}a = b{}}}), it can't infer constraint from anything more complex than that.\r\n\r\n*Example (works as expected):*\r\n{code:sql}\r\nSELECT *\r\nFROM t1\r\nJOIN t2 ON t1.a = t2.b\r\nWHERE t2.b = 1\r\n{code}\r\nIn this case, the optimizer correctly infers the additional constraint {{{}t1.a = 1{}}}.\r\n\r\n*Example (does not work):*\r\n{code:sql}\r\nSELECT *\r\nFROM t1\r\nJOIN right ON t1.a = t2.b + 2\r\nWHERE t2.b = 1\r\n{code}\r\nIn this case, it is clear that {{t1.a = 3}} (since {{b = 1}} and {{{}a = b + 2{}}}), but the optimizer does not infer this constraint.\r\n\r\n*How to Reproduce:*\r\n{code:scala}\r\nspark.sql(\"CREATE TABLE t1(a INT)\")\r\nspark.sql(\"CREATE TABLE t2(b INT)\")\r\n\r\nspark.sql(\"\"\"\r\nSELECT * \r\nFROM t1 \r\nINNER JOIN t2 ON t2.b = t1.a + 2 \r\nWHERE t1.a = 1\r\n\"\"\").explain\r\n{code}\r\n{code:java}\r\n== Physical Plan ==\r\nAdaptiveSparkPlan\r\n+- BroadcastHashJoin [(a#2 + 2)], [b#3], Inner, BuildRight, false\r\n   :- Filter (isnotnull(a#2) AND (a#2 = 1))\r\n   :  +- FileScan spark_catalog.default.t1[a#2]\r\n      +- Filter isnotnull(b#3)\r\n         +- FileScan spark_catalog.default.t2[b#3]\r\n{code}\r\n*Expected Behavior:*\r\nThe optimizer should be able to statically evaluate and infer that {{t2.b = 3}} given the join condition and the filter on {{{}t1.a{}}}.\r\n\r\n*Impact:*\r\nThis limits the optimizer's ability to push down filters and optimize query execution plans for queries with complex join conditions.", "output": "InferFiltersFromConstraints rule does not infer filter conditions for complex join expressions"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support logging in UDTFs\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update 3.4.2 docs landing page to highlight changes shipped in the release\nDescription: The landing page for the 3.4.2 docs mostly lists changes that already shipped in 3.4.0. Update this to highlight 3.4.2 changes.\nQ: ahmarsuhail commented on PR #7887:\nURL: https://github.com/apache/hadoop/pull/7887#issuecomment-3200748949\n\n   @anujmodi2021 could you please review the ABFS changes, and let me know if there's anything else you want to highlight. \r\n   \r\n    @steveloughran anything else in S3A we want to highlight?\r\n   \r\n   I'll merge this in tomorrow and kick off the new build.", "output": "hadoop-yetus commented on PR #7887:\nURL: https://github.com/apache/hadoop/pull/7887#issuecomment-3201009350\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   7m  4s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ branch-3.4.2 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 22s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  branch-3.4.2 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  18m 37s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  71m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7887/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7887 |\r\n   | Optional Tests | dupname asflicense mvnsite codespell detsecrets |\r\n   | uname | Linux 85cc1ac66933 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4.2 / 62e2fc17c26865e8b191c7b34617af0ea7a5fa95 |\r\n   | Max. process+thread count | 717 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project U: hadoop-project |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7887/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Document newly added `core` module configurations\nDescription: \nQ: Issue resolved by pull request 52626\n[https://github.com/apache/spark/pull/52626]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "There is a potential risk of Long.MAX_VALUE overflow in the sessionExpirationTimeNanos calculation within the SaslServerAuthenticator class.\r\nLocation:\r\n !image-2025-08-01-10-12-04-784.png! \r\nThe calculation sessionExpirationTimeNanos = authenticationEndNanos + 1000 * 1000 * retvalSessionLifetimeMs can potentially overflow when:\r\nretvalSessionLifetimeMs is very large \r\nauthenticationEndNanos is already a large value\r\nThe multiplication 1000 * 1000 * retvalSessionLifetimeMs exceeds Long.MAX_VALUE - authenticationEndNanos", "output": "Potential Long.MAX_VALUE overflow in sessionExpirationTimeNanos calculation in SaslServerAuthenticator "}
{"instruction": "Answer the question based on the bug.", "input": "Title: Clean up TaskManagerTest\nDescription: See https://github.com/apache/kafka/pull/20392#issuecomment-3241457533\nQ: Hi [~lucasbru], since the tests in the cleanup of this file are many, I would like to propose to make incremental changes to this cleanup.\r\n\r\n- Removal of dead tests and address these 3 comments - [#1|https://github.com/apache/kafka/pull/19275#discussion_r2107811068], [#2|https://github.com/apache/kafka/pull/19275#discussion_r2107814832] and [#3|https://github.com/apache/kafka/pull/19275#discussion_r2107828813] made in the previous stale PR ([#19275|https://github.com/apache/kafka/pull/19275]).\r\n- Identify and replace tryToCompleteRestoration() with checkStateUpdater() in all tests that require no additional mocking\r\n- Modify tests that may require to be rewritten (I still am doing my analysis of this and may need some help)\r\n\r\nSteps 1 and 2 seem to be straightforward and also I think splitting would make it easier for you to review. Step 3 is where I may need some guidance and help. \r\n\r\nDo you think this is a good approach?", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade JUnit from 4 to 5 in hadoop-rumen.\nDescription: \nQ: hadoop-yetus commented on PR #7552:\nURL: https://github.com/apache/hadoop/pull/7552#issuecomment-2763304720\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/1/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix >. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   0m 16s | [/results-checkstyle-hadoop-tools_hadoop-rumen.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-rumen.txt) |  hadoop-tools/hadoop-rumen: The patch generated 4 new + 5 unchanged - 0 fixed = 9 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 54s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 32s |  |  hadoop-rumen in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 132m  3s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7552 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7083e5353efc 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f09b946cd84d837717ad417c71edfe32bd9fedce |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/1/testReport/ |\r\n   | Max. process+thread count | 524 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-rumen U: hadoop-tools/hadoop-rumen |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7552:\nURL: https://github.com/apache/hadoop/pull/7552#issuecomment-2768523113\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 49s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 49s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 32s |  |  hadoop-rumen in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 129m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7552 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 76049961deae 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2379524344c7e47489c292fa839fe28fb63c47e1 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/2/testReport/ |\r\n   | Max. process+thread count | 578 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-rumen U: hadoop-tools/hadoop-rumen |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7552/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix S3A failing to initialize S3 buckets having namespace with dot followed by number\nDescription: S3A fails to initialize when S3 bucket namespace is having dot followed by a number. \r\n\r\n{*}Specific Problem{*}: URI parsing fails when S3 bucket names contain a dot followed by a number (like {{{}bucket-v1.1-us-east-1{}}}). Java's\r\nURI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null.\r\n\r\n \r\n\r\n{{}}\r\n{code:java}\r\nhadoop dfs -ls s3a://bucket-v1.1-us-east-1/\r\n\r\nWARNING: Use of this script to execute dfs is deprecated.\r\nWARNING: Attempting to execute replacement \"hdfs dfs\" instead.\r\n\r\n2025-09-08 06:13:06,670 WARN fs.FileSystem: Failed to initialize filesystem s3://bucket-v1.1-us-east-1/: java.lang.IllegalArgumentException: bucket is null/empty\r\n-ls: bucket is null/empty{code}\r\n \r\n\r\n{*}Please Note{*}: Although there has been discussion on not allowing S3 buckets with such a namespace ([https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/]) , Amazon S3 still allows you to create a bucket with \nQ: shameersss1 commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3267057573\n\n   Test `ITestBucketTool,ITestS3ACommitterMRJob` are failing even without the change.", "output": "shameersss1 commented on PR #7942:\nURL: https://github.com/apache/hadoop/pull/7942#issuecomment-3267074576\n\n   @steveloughran  : Could you please review the changes."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Install `pyarrow/torch/torchvision` packages to `Python 3.14` Dockefile\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK17] Add ubuntu:noble as a build platform with JDK-17 as default\nDescription: Add a new Dockerfile to compile Hadoop on latest ubuntu:noble (24.04) with JDK17 as the default compiler.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Ingress Related operations on blob Endpoint (Create + Append + Flush filesystem calls)\r\n\r\nThis Jira tracks implementing these tests.", "output": "ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Ingress Operations"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, {{log.dir}} and {{log.dirs}} exhibit similar behavior. With the changes introduced in KIP-1161, we now have an opportunity to simplify and clean up their usage for better consistency.\r\n\r\nsee the discussion: https://github.com/apache/kafka/pull/20334#discussion_r2291866372", "output": "Deprecated the config log.dir"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The {{com.huaweicloud:esdk-obs-java}} dependency , used exclusively by the {{hadoop-huaweicloud}} uses {{com.squareup.okio:okio:1.17.2}} which has [CVE-2023-3635|https://nvd.nist.gov/vuln/detail/cve-2023-3635].\r\nUpgrading it will use a newer fixed version of {{okio}}, which will mitigate the vulnerability.", "output": "Upgrade com.huaweicloud:esdk-obs-java for CVE-2023-3635"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Introduce the framework for adding ST expressions in Catalyst\nDescription: \nQ: Issue resolved by pull request 52784\n[https://github.com/apache/spark/pull/52784]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Publish Hadoop Docker Image to DockerHub for amd64 and arm64 \nDescription: Update GitHub Actions workflow to publish the apache/hadoop Docker image to DockerHub for multiple arch's: amd64 and arm64.\nQ: smengcl commented on PR #7795:\nURL: https://github.com/apache/hadoop/pull/7795#issuecomment-3090703143\n\n   > tag step is untested\r\n   \r\n   Is it possible to test it on your own fork and Docker Hub (registry), just to verify if it works?", "output": "Copilot commented on code in PR #7795:\nURL: https://github.com/apache/hadoop/pull/7795#discussion_r2217013279\n\n\n##########\n.github/workflows/build-and-tag-hadoop-image.yaml:\n##########\n@@ -0,0 +1,138 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+name: build-and-tag-hadoop-image\n+\n+# This workflow builds the Hadoop docker image.\n+# For non-PR runs, it also pushes the image to the registry, tagging it based on the branch name.\n+\n+on:\n+  pull_request:\n+    types: [opened, synchronize]\n+    branches:\n+      - 'docker-hadoop-**'\n+      - '!docker-hadoop-runner-**'\n+  push:\n+    branches:\n+      - 'docker-hadoop-**'\n+      - '!docker-hadoop-runner-**'\n+\n+permissions:\n+  contents: read\n+  packages: write\n+\n+jobs:\n+  build:\n+    runs-on: ubuntu-latest\n+    steps:\n+      - name: Generate image ID\n+        id: meta\n+        uses: docker/metadata-action@8e5442c4ef9f78752691e2d8f8d19755c6f78e81\n+        with:\n+          images: |\n+            ghcr.io/${{ github.repository_owner }}/hadoop\n+          tags: |\n+            type=match,pattern=docker-hadoop-(.*),value={{branch}},group=1\n+          flavor: |\n+            latest=false\n+\n+      - name: Check if image exists\n+        id: pull\n+        run: |\n+          success=false\n+          if docker pull \"$DOCKER_METADATA_OUTPUT_TAGS\"; then\n+            success=true\n+          fi\n+\n+          echo \"success=$success\" >> $GITHUB_OUTPUT\n+\n+      - name: Set up QEMU\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/setup-qemu-action@49b3bc8e6bdd4a60e6116a5414239cba5943d3cf\n+\n+      - name: Set up Docker Buildx\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/setup-buildx-action@c47758b77c9736f4b2ef4073d4d51994fabfe349\n+\n+      - name: Login to GitHub Container Registry\n+        id: login\n+        if: ${{ github.event_name != 'pull_request' && steps.pull.outputs.success == 'false' }}\n+        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567\n+        with:\n+          registry: ghcr.io\n+          username: ${{ github.repository_owner }}\n+          password: ${{ secrets.GITHUB_TOKEN }}\n+\n+      - name: Build and push image\n+        id: build\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/build-push-action@4f58ea79222b3b9dc2c8bbdd6debcef730109a75\n+        with:\n+          platforms: linux/amd64,linux/arm64\n+          push: ${{ github.event_name != 'pull_request' }}\n+          tags: ${{ steps.meta.outputs.tags }}\n+          labels: ${{ steps.meta.outputs.labels }}\n+\n+  tag:\n+    needs: build\n+    if: ${{ github.ref_type == 'tag' }}\n+    runs-on: ubuntu-latest\n+    env:\n+      DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}\n+      IMAGE_ID: ${{ needs.build.outputs.image-id }}\n+      REGISTRIES: ghcr.io # docker.io is appended dynamically\n+    steps:\n+      - name: Generate tags\n+        uses: docker/metadata-action@8e5442c4ef9f78752691e2d8f8d19755c6f78e81\n+        with:\n+          images: |\n+            ${{ github.repository_owner }}/hadoop\n\nReview Comment:\n   This image reference is missing the registry prefix. For DockerHub, it should be `docker.io/${{ github.repository_owner }}/hadoop` or just `apache/hadoop` if targeting the official apache organization.\n   ```suggestion\n               docker.io/${{ github.repository_owner }}/hadoop\n   ```\n\n\n\n##########\n.github/workflows/build-and-tag-hadoop-image.yaml:\n##########\n@@ -0,0 +1,138 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+name: build-and-tag-hadoop-image\n+\n+# This workflow builds the Hadoop docker image.\n+# For non-PR runs, it also pushes the image to the registry, tagging it based on the branch name.\n+\n+on:\n+  pull_request:\n+    types: [opened, synchronize]\n+    branches:\n+      - 'docker-hadoop-**'\n+      - '!docker-hadoop-runner-**'\n+  push:\n+    branches:\n+      - 'docker-hadoop-**'\n+      - '!docker-hadoop-runner-**'\n+\n+permissions:\n+  contents: read\n+  packages: write\n+\n+jobs:\n+  build:\n+    runs-on: ubuntu-latest\n+    steps:\n+      - name: Generate image ID\n+        id: meta\n+        uses: docker/metadata-action@8e5442c4ef9f78752691e2d8f8d19755c6f78e81\n+        with:\n+          images: |\n+            ghcr.io/${{ github.repository_owner }}/hadoop\n+          tags: |\n+            type=match,pattern=docker-hadoop-(.*),value={{branch}},group=1\n+          flavor: |\n+            latest=false\n+\n+      - name: Check if image exists\n+        id: pull\n+        run: |\n+          success=false\n+          if docker pull \"$DOCKER_METADATA_OUTPUT_TAGS\"; then\n+            success=true\n+          fi\n+\n+          echo \"success=$success\" >> $GITHUB_OUTPUT\n+\n+      - name: Set up QEMU\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/setup-qemu-action@49b3bc8e6bdd4a60e6116a5414239cba5943d3cf\n+\n+      - name: Set up Docker Buildx\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/setup-buildx-action@c47758b77c9736f4b2ef4073d4d51994fabfe349\n+\n+      - name: Login to GitHub Container Registry\n+        id: login\n+        if: ${{ github.event_name != 'pull_request' && steps.pull.outputs.success == 'false' }}\n+        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567\n+        with:\n+          registry: ghcr.io\n+          username: ${{ github.repository_owner }}\n+          password: ${{ secrets.GITHUB_TOKEN }}\n+\n+      - name: Build and push image\n+        id: build\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/build-push-action@4f58ea79222b3b9dc2c8bbdd6debcef730109a75\n+        with:\n+          platforms: linux/amd64,linux/arm64\n+          push: ${{ github.event_name != 'pull_request' }}\n+          tags: ${{ steps.meta.outputs.tags }}\n+          labels: ${{ steps.meta.outputs.labels }}\n+\n+  tag:\n+    needs: build\n+    if: ${{ github.ref_type == 'tag' }}\n+    runs-on: ubuntu-latest\n+    env:\n+      DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}\n+      IMAGE_ID: ${{ needs.build.outputs.image-id }}\n\nReview Comment:\n   The build job does not define any outputs, but the tag job is trying to reference `needs.build.outputs.image-id`. This will result in an empty IMAGE_ID variable and cause the tag job to fail.\n\n\n\n##########\n.github/workflows/build-and-tag-hadoop-image.yaml:\n##########\n@@ -0,0 +1,138 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+name: build-and-tag-hadoop-image\n+\n+# This workflow builds the Hadoop docker image.\n+# For non-PR runs, it also pushes the image to the registry, tagging it based on the branch name.\n+\n+on:\n+  pull_request:\n+    types: [opened, synchronize]\n+    branches:\n+      - 'docker-hadoop-**'\n+      - '!docker-hadoop-runner-**'\n+  push:\n+    branches:\n+      - 'docker-hadoop-**'\n+      - '!docker-hadoop-runner-**'\n+\n+permissions:\n+  contents: read\n+  packages: write\n+\n+jobs:\n+  build:\n+    runs-on: ubuntu-latest\n+    steps:\n+      - name: Generate image ID\n+        id: meta\n+        uses: docker/metadata-action@8e5442c4ef9f78752691e2d8f8d19755c6f78e81\n+        with:\n+          images: |\n+            ghcr.io/${{ github.repository_owner }}/hadoop\n+          tags: |\n+            type=match,pattern=docker-hadoop-(.*),value={{branch}},group=1\n+          flavor: |\n+            latest=false\n+\n+      - name: Check if image exists\n+        id: pull\n+        run: |\n+          success=false\n+          if docker pull \"$DOCKER_METADATA_OUTPUT_TAGS\"; then\n+            success=true\n+          fi\n+\n+          echo \"success=$success\" >> $GITHUB_OUTPUT\n+\n+      - name: Set up QEMU\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/setup-qemu-action@49b3bc8e6bdd4a60e6116a5414239cba5943d3cf\n+\n+      - name: Set up Docker Buildx\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/setup-buildx-action@c47758b77c9736f4b2ef4073d4d51994fabfe349\n+\n+      - name: Login to GitHub Container Registry\n+        id: login\n+        if: ${{ github.event_name != 'pull_request' && steps.pull.outputs.success == 'false' }}\n+        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567\n+        with:\n+          registry: ghcr.io\n+          username: ${{ github.repository_owner }}\n+          password: ${{ secrets.GITHUB_TOKEN }}\n+\n+      - name: Build and push image\n+        id: build\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/build-push-action@4f58ea79222b3b9dc2c8bbdd6debcef730109a75\n+        with:\n+          platforms: linux/amd64,linux/arm64\n+          push: ${{ github.event_name != 'pull_request' }}\n+          tags: ${{ steps.meta.outputs.tags }}\n+          labels: ${{ steps.meta.outputs.labels }}\n+\n+  tag:\n+    needs: build\n+    if: ${{ github.ref_type == 'tag' }}\n+    runs-on: ubuntu-latest\n+    env:\n+      DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}\n+      IMAGE_ID: ${{ needs.build.outputs.image-id }}\n+      REGISTRIES: ghcr.io # docker.io is appended dynamically\n+    steps:\n+      - name: Generate tags\n+        uses: docker/metadata-action@8e5442c4ef9f78752691e2d8f8d19755c6f78e81\n+        with:\n+          images: |\n+            ${{ github.repository_owner }}/hadoop\n+          tags: |\n+            type=match,pattern=docker-image-(.*),value={{ ref_name }},group=1\n+          flavor: |\n+            latest=false\n+\n+      - name: Add Docker Hub to targets\n+        if: ${{ env.DOCKERHUB_USER }}\n+        run: |\n+          echo \"REGISTRIES=${{ env.REGISTRIES }} docker.io\" >> $GITHUB_ENV\n+\n+      - name: Pull image\n+        run: |\n+          docker pull \"$IMAGE_ID\"\n+\n+      - name: Login to GitHub Container Registry\n+        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567\n+        with:\n+          registry: ghcr.io\n+          username: ${{ github.repository_owner }}\n+          password: ${{ secrets.GITHUB_TOKEN }}\n+\n+      - name: Login to Docker Hub\n+        if: ${{ env.DOCKERHUB_USER }}\n+        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567\n+        with:\n+          username: ${{ env.DOCKERHUB_USER }}\n+          password: ${{ secrets.DOCKERHUB_TOKEN }}\n+\n+      - name: Apply tags to existing image\n+        run: |\n+          set -x\n+          for registry in $REGISTRIES; do\n+            opts=\"$(echo \"$DOCKER_METADATA_OUTPUT_TAGS\" | sed \"s@^@--tag $registry/@g\" | xargs echo)\"\n\nReview Comment:\n   The sed command will incorrectly prepend the registry to already-qualified tag names, potentially creating malformed tags like `docker.io/apache/hadoop:3.4.1` becoming `--tag docker.io/docker.io/apache/hadoop:3.4.1`.\n   ```suggestion\n               opts=\"$(echo \"$DOCKER_METADATA_OUTPUT_TAGS\" | sed -E \"s@^[^/]+:[^:]+$@--tag $registry/&@g\" | xargs echo)\"\n   ```\n\n\n\n##########\n.github/workflows/build-and-tag-hadoop-image.yaml:\n##########\n@@ -0,0 +1,138 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+name: build-and-tag-hadoop-image\n+\n+# This workflow builds the Hadoop docker image.\n+# For non-PR runs, it also pushes the image to the registry, tagging it based on the branch name.\n+\n+on:\n+  pull_request:\n+    types: [opened, synchronize]\n+    branches:\n+      - 'docker-hadoop-**'\n+      - '!docker-hadoop-runner-**'\n+  push:\n+    branches:\n+      - 'docker-hadoop-**'\n+      - '!docker-hadoop-runner-**'\n+\n+permissions:\n+  contents: read\n+  packages: write\n+\n+jobs:\n+  build:\n+    runs-on: ubuntu-latest\n+    steps:\n+      - name: Generate image ID\n+        id: meta\n+        uses: docker/metadata-action@8e5442c4ef9f78752691e2d8f8d19755c6f78e81\n+        with:\n+          images: |\n+            ghcr.io/${{ github.repository_owner }}/hadoop\n+          tags: |\n+            type=match,pattern=docker-hadoop-(.*),value={{branch}},group=1\n+          flavor: |\n+            latest=false\n+\n+      - name: Check if image exists\n+        id: pull\n+        run: |\n+          success=false\n+          if docker pull \"$DOCKER_METADATA_OUTPUT_TAGS\"; then\n+            success=true\n+          fi\n+\n+          echo \"success=$success\" >> $GITHUB_OUTPUT\n+\n+      - name: Set up QEMU\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/setup-qemu-action@49b3bc8e6bdd4a60e6116a5414239cba5943d3cf\n+\n+      - name: Set up Docker Buildx\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/setup-buildx-action@c47758b77c9736f4b2ef4073d4d51994fabfe349\n+\n+      - name: Login to GitHub Container Registry\n+        id: login\n+        if: ${{ github.event_name != 'pull_request' && steps.pull.outputs.success == 'false' }}\n+        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567\n+        with:\n+          registry: ghcr.io\n+          username: ${{ github.repository_owner }}\n+          password: ${{ secrets.GITHUB_TOKEN }}\n+\n+      - name: Build and push image\n+        id: build\n+        if: ${{ steps.pull.outputs.success == 'false' }}\n+        uses: docker/build-push-action@4f58ea79222b3b9dc2c8bbdd6debcef730109a75\n+        with:\n+          platforms: linux/amd64,linux/arm64\n+          push: ${{ github.event_name != 'pull_request' }}\n+          tags: ${{ steps.meta.outputs.tags }}\n+          labels: ${{ steps.meta.outputs.labels }}\n+\n+  tag:\n+    needs: build\n+    if: ${{ github.ref_type == 'tag' }}\n+    runs-on: ubuntu-latest\n+    env:\n+      DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}\n+      IMAGE_ID: ${{ needs.build.outputs.image-id }}\n+      REGISTRIES: ghcr.io # docker.io is appended dynamically\n+    steps:\n+      - name: Generate tags\n+        uses: docker/metadata-action@8e5442c4ef9f78752691e2d8f8d19755c6f78e81\n+        with:\n+          images: |\n+            ${{ github.repository_owner }}/hadoop\n+          tags: |\n+            type=match,pattern=docker-image-(.*),value={{ ref_name }},group=1\n\nReview Comment:\n   The syntax `{{ ref_name }}` is incorrect for GitHub Actions. It should be `${{ github.ref_name }}` to properly reference the tag name.\n   ```suggestion\n               type=match,pattern=docker-image-(.*),value=${{ github.ref_name }},group=1\n   ```"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add timezone to make timestamp absolute time.\nDescription: ", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "github action builds of PRs for hadoopHthirdparty fail because of throttling NVE throttling of requests; needs an update to a later version with either retries or use of a github source cve list.\r\n\r\ndependency checker 11+ \r\n\r\n{code}\r\nMandatory Upgrade Notice\r\nUpgrading to 10.0.2 or later is mandatory\r\n\r\nOlder versions of dependency-check are causing numerous, duplicative requests that end in processing failures are causing unnecassary load on the NVD API. Dependency-check 10.0.2 uses an updated User-Agent header that will allow the NVD to block calls from the older client.\r\n{code}\r\n\r\n----\r\n\r\nThe upgraded dependency checker now *requires* java11+, and *prefers* the provision of an API key for the national vulnerabilities database. It also skips the sonatype check as that no longer supports anonymous checks at all", "output": "hadoop-thirdparty build to update maven plugin dependencies"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Building with {{-Pnative}} by using GCC 14 on Fedora 40 failed due to incompatible changes of GCC.", "output": "Fix compilation error of native libraries on newer GCC"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, log messages in the {{release()}} method of {{KafkaDataConsumer}} do not include Spark task context information (e.g., TaskID).\r\n\r\nThis change adds task context details to the {{release()}} logs to improve traceability and debugging of Kafka consumer lifecycle events.\r\n\r\nWe previously added similar context in {{{}KafkaBatchReaderFactory.createReader{}}}, which has proven useful for debugging. Including this information in {{release()}} helps correlate consumer release metrics with the specific Spark tasks that borrowed the consumer.", "output": "SS | `Add TaskContext information to KafkaDataConsumer release() logs for better debugging`"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Failures in the StateUpdater thread may lead to inability to shut down a stream thread\nDescription: If during rebalance a failure occurs in the StateUpdater thread, and this failure leads to the thread shutdown, the Stream thread may get into an infinite wait state during it's shutdown.\r\n\r\nSee the attached test that reproduces the issue.\nQ: Hi [~goyarpit] \r\nI am already working on it.\r\nI will let you know if I need help. thanks!", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove `Java 17` requirement from `deploy.gradle`\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: [FnsOverBlob] Support Fixed SAS token at container level\nDescription: The ABFS driver currently lacks support for multiple SAS tokens for the same storage account across different containers.\r\n\r\nWe are now introducing this support.\r\n\r\nTo use fixed SAS token at container level the configuration to be used is:\r\n{quote}fs.azure.sas.fixed.token..\r\n{quote}", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Document controller.quorum.auto.join.enable config in upgrade.html\nDescription: \nQ: [~chia7712] Someone else can take it over and I can give an initial review. It would be good to make others aware of this new functionality for KRaft at a high level.", "output": "[~isding_l] Please provide the best documentation you can :)"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The default value of io.compression.codec.lzo.buffersize should not be in the core-site-default.xml file.\r\n\r\nThe io.compression.codec.lzo.buffersize configuration file is the core configuration file of LZO. The default value is 245 MB. The default value should be controlled by ZLO and should not be contained in the core-site file.\r\n\r\n \r\n\r\n!image-2025-05-12-16-21-43-114.png!", "output": "The default value of io.compression.codec.lzo.buffersize should not be in the core-site-default.xml file."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly\nDescription: \nQ: Issue resolved by pull request 52738\n[https://github.com/apache/spark/pull/52738]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: testVectoredReadAfterNormalRead() failing with 412 response from S3\nDescription: This is surfacing on a bucket using versionid for change detection: block reads are failing in the test {{ITestS3AContractAnalyticsStreamVectoredRead.testVectoredReadAfterNormalRead()}}\r\n\r\n{code}\r\nCaused by: software.amazon.awssdk.services.s3.model.S3Exception: At least one of the pre-conditions you specified did not hold (Service: S3, Status Code: 412, Request ID: 0AN2EB8QXC75HH0T, Extended Request ID: U5l/UnIF4n3NO1mrZVzS2vv72F3LgUoVJxR4XodUSaTWCerfjmmpH45CbFGKkTkfgfnykwzseGo=)\r\n        at software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:104)\r\n\r\n{code}\r\n\r\n* this is the normal readFully() call, before the vectored one\r\n* it worked last week\r\n* also found on branch-3.4 before the SDK update, so not an issue caused by the SDK unless my maven repo is badly contaminated\r\n* seems unrelated to versioning -still there when disabled.\r\n* applies on unversioned s3 express store too.\r\n\r\nAbout the main way I could see this surface is if the test file is less\nQ: Ok able to reproduce, it happens when you do \r\n\r\n \r\n\r\nfs.s3a.encryption.algorithm\r\nSSE-KMS\r\n\r\n\r\n\r\nfs.s3a.bucket.bucket.encryption.key\r\nKEY\r\n\r\n \r\nand run the whole AbstractContractVectoredReadTest. It'll pass if you run just testVectoredReadAfterNormalRead(). \r\n \r\nSo guessing a previous test is caching this data in AAL, looking some more. Do you think this is still a blocker?", "output": "ok took me a few hours but I get it now. \r\n\r\n \r\n\r\nThis happens when encryption is enabled (eg: SSE-KMS), because in that case the etag is not the md5 of the object content. When making a GET request, AAL does \r\n \r\nGetRequest.builder()\r\n.s3Uri(this.objectKey.getS3URI())\r\n.range(this.range)\r\n.etag(this.objectKey.getEtag())\r\n \r\nthe etag comes from cached HeadObjectResponse. When you run the whole test suite, head object gets cached from a previous HEAD. Then in this test, the cached value gets used, but since with SSE-KMS the value will change every time the vectored object vectored_file.txt gets created, this will fail. \r\n \r\nWhen not using SSE-KMS, the etag is always the same, as it is just the md5 of the object content which never changes."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Deprecate --property and replace with --reader-property in console producer\nDescription: The --property argument in kafka-console-producer.sh should be deprecated and replaced with --reader-property.", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When `TO_TIMESTAMP` is called with an invalid datetime pattern (e.g., `'yyyyMMddHHMIss'` with invalid letter `'I'`), the `IllegalArgumentException` from Java's `DateTimeFormatterBuilder` escapes during constant folding optimization, resulting in a raw exception without proper error codes or `SQLSTATE`.", "output": "Wrap IllegalArgumentException with proper error code for invalid datetime patterns"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "(!) {*}Note{*}: this blocks Gradle upgrade from 8 to 9 (KAFKA-19174) - _Gradle upgrade related stuff are done, but can't be merged due to CI build issues._\r\n\r\nSee here for more details: [https://github.com/apache/kafka/pull/19513#issuecomment-3236158518]\r\n\r\nInterestingly enough: only parsing fails (not tests) and on top of that it happens for flaky/new matrix {*}_only_{*}.\r\n_______________________________________________________________________________________________________________________________\r\n\r\n -{*}(i) Update{*}: after so many testing on Gradle upgrade PR ([https://github.com/apache/kafka/pull/19513]) I can confirm that something strange is going on between GitHub Actions and Develocity instances.-\r\n\r\n-Let me describe just a tip of the issues here; will use 'com.gradle.develocity' plugin version as an example (and please keep in mind that no classes/test are added or changed):-\r\n * Gradle 9/PR branch (with plugin version 3.19, i.e. trunk version): *parsing step fails for JUnit reports* (for flaky/new jobs, as noted above); \r\n * -Gradle 8/trunk branch  (with plugin version upgrade *only* - from 3.19 to 3.19.1): CI build ends up with this error: *upload build to Develocity fails*-\r\n\r\n-(on) From where I see this solution for this issue should be drafted along these lines:-\r\n -  -'com.gradle.develocity' plugin version and configuration-\r\n - -`com.gradle.common-custom-user-data-gradle-plugin` version-\r\n - -github action `setup-gradle` version and configuration-\r\n\r\n[~mumrah]  [~chia7712] I will create a separate PR and tag you there (it will be a trunk-based PR, I do believe it will be easier to test/solve these issues on a trunk level (and only then to apply Gradle upgrade).", "output": "[Gradle 8 --> 9 upgrade] GitHub Actions build issues: JUnit tests parsing fails (for a new/flaky steps)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consistency of command-line arguments for verifiable producer/consumer\nDescription: This implements KIP-1147 for kafka-verifiable-producer.sh and kafka-verifiable-consumer.sh.\nQ: [~schofielaj] I've assigned this issue to myself in advance. If someone else has already taken it, please feel free to assign it to another.", "output": "Go for it."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We could have a wrapper class to work around the problem in https://issues.apache.org/jira/browse/SPARK-52022", "output": "Add SparkThrowable wrapper to workaround Py4J limitation"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "New test added in [https://github.com/apache/hadoop/pull/7367,]\r\n\r\nexpects an IndexOutOfBoundsException to be thrown, when offset is negative.\r\n\r\n \r\n\r\nAAL throws IllegalArgumentException. For len, the PR says it's ok to throw either IllegalArgumentException or IndexOutOfBoundsException, so should the same thing happen of offset?", "output": "S3A Analaytics-Accelerator: ITestS3AContractOpen.testInputStreamReadNegativePosition() failing"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see https://github.com/apache/kafka/pull/20421#issuecomment-3236341849", "output": "The normal topic auto-creation paths should use num.partitions and default.replication.factor from the controller properties"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`\nDescription: HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found.\r\n\r\nI think the cause is:\r\n * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration`\r\n * However, code paths to load AWS credential provider call `S3AUtils.getInstanceFromReflection`, which uses the classloader that loaded the S3AUtils class. That's likely to be the built-in application classloader, which won't be able to load classes in a Spark job JAR.\r\n\r\nAnd the fix seems small:\r\n * Change `S3AUtils.getInstanceFromReflection` to load classes using the `Configuration`'s classloader. Luckily we already have the Configuration in this method.\nQ: brandonvin opened a new pull request, #8048:\nURL: https://github.com/apache/hadoop/pull/8048\n\n   …lassloader\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   Follow-up to [HADOOP-18993](https://issues.apache.org/jira/browse/HADOOP-18993) and [HADOOP-19733](https://issues.apache.org/jira/browse/HADOOP-19733) before it.\r\n   \r\n   With `fs.s3a.classloader.isolation` set to `false` in a Spark application, it was still impossible to load a credentials provider class from the Spark application jar.\r\n   \r\n   `fs.s3a.classloader.isolation` works by saving a reference to the intended classloader in the `Configuration`.\r\n   \r\n   However, loading credentials providers goes through\r\n   `S3AUtils#getInstanceFromReflection`, which always used the classloader that loaded `S3AUtils`.\r\n   \r\n   With this patch, credentials providers will be loaded using the `Configuration`'s classloader.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Unit tests in `org.apache.hadoop.fs.s3a.ITestS3AFileSystemIsolatedClassloader`.\r\n   \r\n   Manual testing in a Spark application.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "brandonvin commented on code in PR #8048:\nURL: https://github.com/apache/hadoop/pull/8048#discussion_r2453905622\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemIsolatedClassloader.java:\n##########\n@@ -77,19 +109,9 @@ private void assertInNewFilesystem(Map confToSet, Consumer mapOf() {\n-    return new HashMap<>();\n-  }\n-\n-  private Map mapOf(String key, String value) {\n-    HashMap m = new HashMap<>();\n-    m.put(key, value);\n-    return m;\n-  }\n\nReview Comment:\n   Since I added test cases that set 2 key-value pairs, I switched to `Map.of` instead of extending these. Not sure if there was a reason to avoid `Map.of` here."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: [JDK17] Set Up CI Support JDK17 & JDK21\nDescription: Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to upgrade the project’s Java compatibility. This pipeline will serve as the foundation for testing and validating future JDK 17 support and ensuring smooth integration with existing CI/CD processes.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix `gradlew` content: add missing parts (old Groovy template version is being referenced)\nDescription: {panel:bgColor=#ffffce}\r\n*_Prologue: Kafka developers can't commit jar's into Gir repo (that includes Gradle wrapper jar_*\r\nRelated links:\r\n * https://issues.apache.org/jira/browse/KAFKA-2098?focusedCommentId=14481979&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-14481979\r\n * https://issues.apache.org/jira/browse/KAFKA-1490\r\n * [https://stackoverflow.com/questions/39856714/gradle-wrapper-without-the-jar]\r\n * [https://discuss.gradle.org/t/how-hand-compile-gradle-wrapper-jar-when-gradle-cannot-be-installed-and-cannot-check-jar-files-into-git/4813]{panel}\r\n \r\n\r\n*(i) Intro:* Groovy file _*unixStartScript.txt*_ (that servers as a template for a {_}*gradlew*{_}) path/module was changed in Gradle version 8.8.0\r\n\r\n _*(x) Problem description:*_\r\n # at the moment Kafka trunk branch is using Gradle version [8.14.1|https://github.com/apache/kafka/blob/8deb6c6911616f887ebb2678f3f12ee1da09a618/gradle/wrapper/gradle-wrapper.properties] but thing is that _*gradlew*_ i\nQ: -Canceling patch (but code will be available for a review elsewhere).-\r\n\r\nIn more details: this ticket is a prerequisite for KAFKA-19174 (and hence same GitHub PR will be used for both tickets).", "output": "New patch (with code for KAFKA-19174) is submitted here:https://github.com/apache/kafka/pull/19513 \r\n\r\n-Progress is kind of blocked by these issues: KAFKA-19636 -\r\n\r\nSee this comment for a latest update:\r\nhttps://github.com/apache/kafka/pull/19513#issuecomment-3213821218"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Today, the engine currently treats both the changelog and snapshot files with the same importance, so when a state store needs to be loaded it reads the latest snapshot and applies the subsequent changes on it. If the snapshot is bad or corrupted, then the query will fail and be completely down and blocked, needing manual intervention. This leads to user clearing their query checkpoint and having to do full recomputation.\r\n\r\nThis shouldn’t be the case. The changelog should be treated as the “source of truth” and the snapshot is just a disposable materialization of the log.\r\n\r\nIntroducing Automatic snapshot repair, which will automatically repair the checkpoint by skipping bad snapshots and rebuilding the current state from the last good snapshot (works even if there’s none) and applying the changelogs on it. This eliminates the need for manual intervention and unblocks the pipeline to keep it running.\r\n\r\nAlso emit metrics about number of state stores that were auto repaired in a given batch, so that you can build alert and dashboard for it.", "output": "Automatic Snapshot Repair for State store"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "fixing a doc of from_protobuf function pyspark.", "output": "fixing a doc of from_protobuf function pyspark."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Kafka Connect connectors sent out zombie records during rebalance\nDescription: Hi, we run Debezium connectors on Kafka Connect. We identified several \"zombie\" records that are delivered by the connectors during or after the rebalance. Since the downstream consumers require ordering, this issue breaks several things where previous primitives were build upon.\r\n\r\nHere are an overview of the setup:\r\n * Connector type: Debezium Mongo Connector\r\n * Kafka Connect version: 3.2\r\n * Number of workers: 3-4\r\n * Kafka producer configs: at-least once settings, ack=all, max inflight requests=1\r\n\r\nThe following conclusion are based on our investigation:\r\n{quote}When a Kafka Connect worker (part of a connector cluster) is overloaded or degraded, the connector on it may become temporarily unhealthy. The Kafka Connect cluster will rebalance the connector by \"moving\" it to another worker. When the connector is started on the new worker, the events will resume normally without any data loss and depending on the previously committed offsets, there might be a small amount of duplicate events due to replay but eventually the total ordering is still guaranteed. \r\n\r\nHowever, the producer of the old worker may not have been gracefully shut down. When the old worker recovered, some old events that were already placed in the producer's internal queue got sent out to Kafka before the producer was forcefully closed. This caused the \"out-of-band\" duplicate events, which we referred to as \"ghost duplicates\" or \"zombie records.\r\n{quote}\r\nCan you verify our conclusion and do you have any", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: region resolution within AWS infra always goes to us-east-2\nDescription: I think this is new to the V2 SDK, or at least our region logic there.\r\nWhen you try to connect to a bucket without specifying the region, then even if you're running in the same region of the store a HEAD request is still made to US Central. This adds latency, makes us-central a SPoF and if the VM/container has network rules which blocks such access, we actually timeout and eventually fail.\r\n\r\nWhile Machine configurations should ideally have the fs.s3a.endpoint.region setting configured, that information is actually provided as IAM metadata. Therefore it would be possible \r\n\r\nThis is actually included in the default region chain according to the SDK docs \"If running in EC2, check the EC2 metadata service for the region\", so maybe this isn't being picked up because\r\n\r\n# cross region access is being checked for first.\r\n# the region chain we are setting off doesn't check the EC2 metadata service.\r\n\r\nThe SDK region chain does do the right thing within AWS infra. How do we restore that whi\nQ: in the meantime, I propose removing the warning \"you are using the default chain\".\r\n\r\nThe default chain is exactly the one to use within EC2", "output": "Steve, this is from our region logic: [https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/DefaultS3ClientFactory.java#L355]\r\n\r\nif no region is configured, and none could be determined from the endpoint, US_EAST_2 is used as default.  We actually don't end up using the EC2 metadata service, unless the region is configured as an empty string \"\" I believe. If region is never configured, we will never use the metadata service.\r\n\r\nAgree on using the default chain with EC2, but will need to update that region logic again..."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move Gauge#value to MetricValueProvider\nDescription: from: https://github.com/apache/kafka/pull/3705#discussion_r140830112", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove `blacklist` alternative config names\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Unexpected Fatal error InvalidTxnStateException on Kafka Streams (KIP-890)\nDescription: We are seeing cases where a Kafka Streams (KS) thread stalls for ~20 seconds, resulting in a visible gap in the logs. During this stall, the broker correctly aborts the open transaction (triggered by the 10-second transaction timeout). So far, this is expected behavior. However, when the KS thread resumes, instead of receiving the expected InvalidProducerEpochException (which we already handle gracefully as part of transaction abort), the client is instead hit with an InvalidTxnStateException. KS currently treats this as a fatal error, causing the application to fail.\nQ: 4.1: https://github.com/apache/kafka/commit/a10c1f3ea1454dbd644ea65a885c965529e1d37d", "output": "4.0: https://github.com/apache/kafka/commit/4b0ba424837a76c94b2432f6e2ac4237f92e1ff7"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: kafka-broker-api tool should support to get controller api version\nDescription: like tool kafka-broker-api-versions.sh, we can get all RPC version from broker.\r\n\r\nIt should also support for controller.\r\n\r\nKIP: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1220%3A+kafka-broker-api-versions+tool+support+controllers]", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: `ci-complete` needs to work with active branches after the JDK is updated\nDescription: The JDK version used by `ci-complete` is inconsistent with other active branches, which is causing the build report task to fail", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `Spotless` to 8.0.0\nDescription: \nQ: Issue resolved by pull request 399\n[https://github.com/apache/spark-kubernetes-operator/pull/399]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Read Buffer Manager V2 should not be allowed untill implemented\nDescription: Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used.\r\nThis is to stop any user explicitly enabling the config to enable RBMV2.\nQ: anujmodi2021 opened a new pull request, #8002:\nURL: https://github.com/apache/hadoop/pull/8002\n\n   Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used.\r\n   This is to stop any user explicitly enabling the config to enable RBMV2.\r\n   \r\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19710", "output": "hadoop-yetus commented on PR #8002:\nURL: https://github.com/apache/hadoop/pull/8002#issuecomment-3346210665\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   9m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  30m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 20s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  90m  2s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/8002 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7ed8c85f9284 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / bc356262478fb82c786dc3bee7c312b2b1a29634 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/testReport/ |\r\n   | Max. process+thread count | 566 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-8002/1/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "This fix addresses an issue where no warning log was displayed when the `plugin.path` configuration contained empty string elements.", "output": "Fix PluginUtils#pluginLocations warning log not being printed"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support Spatial Reference System mapping in PySpark\nDescription: ", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add basic logging support\nDescription: \nQ: Issue resolved by pull request 52689\n[https://github.com/apache/spark/pull/52689]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade libopenssl to 3.1.2 on Windows\nDescription: The current libopenssl 3.1.1 isn't available for download from the repo.msys2.org website. Hence, we're upgrading to 3.1.2.\nQ: hadoop-yetus commented on PR #7680:\nURL: https://github.com/apache/hadoop/pull/7680#issuecomment-2869042256\n\n   (!) A patch to the testing environment has been detected. \r\n   Re-executing against the patched versions to perform further tests. \r\n   The console is at https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7680/1/console in case of problems.", "output": "hadoop-yetus commented on PR #7680:\nURL: https://github.com/apache/hadoop/pull/7680#issuecomment-2869074147\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 59s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 23s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  19m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  48m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7680/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7680 |\r\n   | Optional Tests | dupname asflicense |\r\n   | uname | Linux 4ea4ad644652 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / cc216c5708c169223285cc22697f594ed1fb0d71 |\r\n   | Max. process+thread count | 557 (vs. ulimit of 5500) |\r\n   | modules | C:  U:  |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7680/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Parallel remote reads causes memory leak in broker\nDescription: This issue is caused with changes from https://issues.apache.org/jira/browse/KAFKA-14915\r\n\r\nBroker heap memory gets filled up and throws OOM error when remote reads are triggered for multiple partitions within a FETCH request. \r\n\r\nSteps to reproduce: \r\n\r\n1. Start a one node broker and configure LocalTieredStorage as remote storage. \r\n2. Create a topic with 5 partitions. \r\n3. Produce message and ensure that few segments are uploaded to remote.\r\n4. Start a consumer to read from those 5 partitions. Seek the offset to beginning for 4 partitions and to end for 1 partition. This is to simulate that the FETCH request read from both remote-log and local-log.\r\n5. The broker crashes with the OOM error.\r\n6. The DelayedRemoteFetch / RemoteLogReadResult references are being held by the purgatory, so the broker crashes.\r\n\r\ncc [~showuon] [~satish.duggana]\nQ: [~ckamal] , I tired to reproduce it using your steps, but the heap only increase ~ 100 MB, not a big issue IMO. Have you figured it out where do we leak the memory?", "output": "[~showuon]\r\nAble to reproduce the issue consistently. Uploaded the RemoteReadMemoryLeakReproducer. The leak was due to that the DelayedRemoteFetchPurgatory holding the references of previously completed DelayedRemoteFetch objects. DelayedRemoteFetch contains the RemoteReadResult internally.\r\n\r\n> Have you figured it out where do we leak the memory? \r\nIn a given FETCH request, if 1 out of 5 partition, read the data from local log, then the watcherKey for that partition holds the reference of the DelayedRemoteFetch in the purgatory; if there are no other remote-read happens for that partition, then it won't get removed until the reaper thread cleans it up after the purgeInterval (entries) of 1000. \r\n{code:java}\r\n% sh kafka-topics.sh --create --topic apple --partitions 5 --replication-factor 1 --bootstrap-server localhost:9092 --config remote.storage.enable=true --config local.retention.ms=60000 --config retention.ms=7200000 --config segment.bytes=104857600 --config file.delete.delay.ms=1000\r\n\r\n% for i in `seq 1 100`; do echo $i; sleep 1; sh kafka-producer-perf-test.sh --topic apple --num-records 1200000000 --record-size 1024 --throughput 1000 --producer-props bootstrap.servers=localhost:9092; done {code}"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Implementation for https://cwiki.apache.org/confluence/display/KAFKA/KIP-1170%3A+Unify+cluster+metadata+bootstrapping", "output": "KIP-1170: Unify cluster metadata bootstrapping"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Several arguments were replaced in [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1147%3A+Improve+consistency+of+command-line+arguments], initially deprecating in 4.2. The deprecated arguments will be removed in 5.0.", "output": "Remove command-line arguments deprecated to implement KIP-1147"}
{"instruction": "Answer the question based on the bug.", "input": "Title: document the encoding of nullable struct\nDescription: In [https://kafka.apache.org/protocol#protocol_types,] we didn't specify the encoding of a struct. In particular, how a nullable struct is represented. We should document that if a struct is nullable, the first byte indicates whether is null and the rest of the bytes are the serialization of each field.\nQ: Hi [~junrao], if you are not working on this, may i take it over, thanks.", "output": "[~isding_l] : Thanks for your interest. Feel free to take it."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "the dependency checker of hadoop-thirdparty PRs fails with an NPE in the plugin.\r\n{code}\r\nError:  Failed to execute goal org.owasp:dependency-check-maven:6.1.5:aggregate (default-cli) on project hadoop-thirdparty: Fatal exception(s) analyzing Apache Hadoop Third-party Libs: One or more exceptions occurred during analysis:\r\nError:  \tUpdateException: java.util.concurrent.ExecutionException: java.lang.NullPointerException\r\nError:  \t\tcaused by ExecutionException: java.lang.NullPointerException\r\nError:  \t\tcaused by NullPointerException: null\r\n{code}\r\n\r\n\r\nThere is a much newer version of the library, but it is a java11 release. we will need to find the most recent release without this issue that is java8 only.\r\n\r\nAgain, this highlights why trying to continue build on java8 is futile", "output": "NPE in dependency-check of hadoop-thirdparty"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Read Policy set in openFileOptions should be considered for enabling various optimizations\nDescription: AbfsInputStream should take in account the Read Policy set by user with Open File Options. Based on the read policy set, appropriate optimizations should be enabled and kicked in.\nQ: key ones are random, whole file and sequential, with recognising avro, parquet, a bonus\r\n\r\n* parquet does now open files with \"parquet\" as first entry\r\n* distcp always uses whole-file where a few large 64MB+  blocks deliver great performance", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: google gs connector registration failing\nDescription: Surfaced during HADOOP-19696 and work with all the cloud connectors on the classpath.\r\n\r\nThere's a missing dependency causing the gcs connector to fail to register *when the first filesystem is instantiated*, because the service registration process loads the gcs connector class, instantiates one and asks for its schema.\r\n\r\nAs well as a sign of a problem, it's better to just add an entry in core-default.xml as this saves all classloading overhead. It's what the other asf bundled ones do.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add Support for Propagating Access Token via RPC Header in HDFS\nDescription: *Description:*\r\nTo support modern authentication models (e.g., bearer tokens, OAuth2), we propose adding support in HDFS to propagate an access token via the RPC request header. This enables downstream services (e.g., NameNode, Router) to validate access tokens in a secure and standardized way.\r\n\r\nThe token will be passed in a dedicated field in the {{{}RpcRequestHeaderProto{}}}, mimicking the behavior of an HTTP {{Authorization: Bearer }} header. The caller context or UGI may extract this token and use it for authorization decisions or auditing.\r\n\r\n*Benefits:*\r\n * Enables secure, token-based authentication in multi-tenant environments\r\n\r\n * Lays the foundation for fine-grained, per-request authorization\r\n\r\n*Scope:*\r\n * Add optional {{authorization_token}} field to RPC header\r\n\r\n * Ensure token is thread-local or caller-context scoped\r\n\r\n * Wire it through relevant client and server code paths\r\n\r\n * Provide configuration to enable/disable this feature\r\n\r\n*Notes:*\r\nThis feature is inten\nQ: ctrezzo merged PR #7803:\nURL: https://github.com/apache/hadoop/pull/7803", "output": "mccormickt12 opened a new pull request, #7844:\nURL: https://github.com/apache/hadoop/pull/7844\n\n   Add a new auth header to the rpc header proto for access token support. This should support different access tokens within the same connection.\r\n   \r\n   Contributed-by: Tom McCormick \r\n   \r\n   \r\n   \r\n   ### Description of PR\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: InsertHeader drops part of the value when header value is a number\nDescription: When I use the SMT *org.apache.kafka.connect.transforms.InsertHeader* and the header value is a number, part of the value is dropped. Example: \r\n\r\n{code:yaml}\r\n# Insert KafkaHeaders for avro serialization\r\ntransforms: insertSpecHeader\r\n\r\ntransforms.insertSpecHeader.type: org.apache.kafka.connect.transforms.InsertHeader\r\ntransforms.insertSpecHeader.header: \"specversion\"\r\ntransforms.insertSpecHeader.value.literal: \"2.0\"\r\n{code}\r\n\r\nThen, the record is produced with the header *\"specversion\": \"2\"*\r\n\r\nIs KafkaConnect doing a sort of casting and treating the value as float even though I am using literal?\nQ: Hi, I'd like to work on this issue as my first contribution to Kafka. Could you please assign it to me? Thank you!", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support `spark.kubernetes.executor.useDriverPodIP`\nDescription: \nQ: Issue resolved by pull request 52650\n[https://github.com/apache/spark/pull/52650]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Avoid noisy NPE logs when closing consumer after constructor failures\nDescription: If there's a failure in the kafka consumer constructor, we attempt to close it https://github.com/lianetm/kafka/blob/2329def2ff9ca4f7b9426af159b6fa19a839dc4d/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L540\r\nIn that case, it could be the case that some components may have not been created, so we should consider some null checks to avoid noisy logs about NPE. \r\n\r\nThis noisy logs have been reported with the console share consumer in a similar scenario, so this task is to review and do a similar fix for the Async if needed.\nQ: Hi [~lianetm], I'm new to contributing to Kafka and would be happy to take a stab at this. Can I assign this ticket to myself?", "output": "Hi [~francisgodinho]! Thanks for your interest! There is already someone from the team working on this :S \r\nBut stay on the lookout for new issue that we create, and also maybe check what's already out with minor/trivial complexity (ex. [this filter|https://issues.apache.org/jira/browse/KAFKA-15642?jql=project%20%3D%20KAFKA%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened%2C%20%22Patch%20Available%22)%20AND%20priority%20in%20(Minor%2C%20Trivial)%20AND%20component%20%3D%20clients] for the clients space, but also check other components). Welcome to the community!"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move ThrottledChannelExpirationTest to server-common module\nDescription: With *KAFKA-19817 moving all related components to `server-common`, we can rewrite and migrate `ThrottledChannelExpirationTest`. Also, the test that was moved in KAFKA-17372* should be merged into it.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support IO_URING Netty IO Mode\nDescription: ", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: [Connect] Provide a configuration to disable SNI required and SNI host checking for the REST API\nDescription: h3. Summary\r\n\r\nThe upgrade to Jetty 12 in Kafka 4.0 enables a strict SNI host check by default for the Connect REST API. This change breaks HTTPS request forwarding between Connect workers when they connect via IP address, causing requests to fail with a 400: Invalid SNI error.\r\nh3. The Problem\r\n\r\nPrior to Kafka 4.0, the Jetty server used for the Connect REST API did not enforce a strict match between the TLS SNI hostname and the HTTP Host header.\r\n\r\nWith the upgrade to Jetty 12, this check is now enabled by default at the HTTP level. This causes legitimate HTTPS requests to fail in environments where the client connects using an IP address or a hostname that is not listed in the server's TLS certificate.\r\n\r\nThis results in the following error:\r\n{code:java}\r\norg.eclipse.jetty.http.BadMessageException: 400: Invalid SNI\r\n{code}\r\nh3. Impacted Use Case: Inter-Node Request Forwarding\r\n\r\nThis change specifically breaks the request forwarding mechanism between Connect workers in a common depl\nQ: We are routing by IP.", "output": "Yes exactly so if there are multiple host names on the web server , without an sni it is possible you can get an error if the web server does not know what ssl to handshake you with. so there are some draw backs disabling this I think ."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Make \"analytics\" the default input stream in S3A. \r\n\r\nGoals\r\n* Parquet performance through applications running queries over the data (spark etc)\r\n* Performance for other formats good as/better than today. Examples: avro manifests in iceberg, ORC in hive/spark\r\n* Performance for other uses as good as today (whole-file/sequential reads of parquet data in distcp etc)\r\n* better resilience to bad uses (incomplete reads not retaining http streams, buffer allocations on long-retained data)\r\n* efficient on applications like Impala, which caches parquet footers itself, and uses unbuffer() to discard all stream-side resources. Maybe just throw alway all state on unbuffer() and stop trying to be sophisticated, or support some new openFile flag which can be used to disable footer parsing", "output": "S3A: Analytics accelerator for S3 to be enabled by default"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Enable to ducker-ak to \"isolate\" the ducker containers\nDescription: We can't run the e2e tests through ducker-ak on the same machine for now. That is a bit troublesome when I want to run e2e for different PRs. Perhaps we could introduce a \"prefix\" to ducker to isolate the containers.", "output": "In Progress"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Let approx_top_k handle NULLs\nDescription: Spark uses FrequentItemsSketch of Apache DataSketches in the approx_top_k function, which does not consider NULL values by itself ([https://github.com/apache/datasketches-java/blob/main/src/main/java/org/apache/datasketches/frequencies/FrequentItemsSketch.java#L587).] However, NULL value could be meaningful in some use cases and users might want to include NULL in the approx_top_k output. Therefore, this ticket aims to add a nullCounter associated with the FrequentItemsSketch to count for NULL in the approx_top_k aggregation.\nQ: Issue resolved by pull request 52655\n[https://github.com/apache/spark/pull/52655]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "If the share consumer application takes a long time to process records, the share consume request manager can do a better job of time-outs and liveness.", "output": "Improve handling of long processing times"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix ByteBuf leaks in TestShuffleChannelHandler\nDescription: A lot of ByteBuf leaks are reported when running with JDK23.\r\n\r\nI have not idea why those are not detected with JDK8, 11 or 17. Maybe something about the logging setup ?\nQ: hadoop-yetus commented on PR #7500:\nURL: https://github.com/apache/hadoop/pull/7500#issuecomment-2719316865\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  8s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-shuffle.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7500/1/artifact/out/results-checkstyle-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-shuffle.txt) |  hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 59s |  |  hadoop-mapreduce-client-shuffle in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 114m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7500/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7500 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2b91e841e013 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e252de3bcf40124251e60f4dc777752311fab9b0 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7500/1/testReport/ |\r\n   | Max. process+thread count | 544 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7500/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on PR #7500:\nURL: https://github.com/apache/hadoop/pull/7500#issuecomment-2719387042\n\n   @stoty Overall it looks good. Can we fix the checkstyle issue?"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Fix dependency exclusion list of hadoop-client-runtime.\nDescription: \nQ: pan3793 commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3194939043\n\n   cc @slfan1989 @cnauroth, this is caused by Jersey upgrading.", "output": "pan3793 commented on PR #7878:\nURL: https://github.com/apache/hadoop/pull/7878#issuecomment-3194946944\n\n   AFAIK, there is no solid integration test for the Hadoop Shaded client, I will continue to test it with Spark and fix issues I encounter."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Kafka Broker Fails to Start After Upgrade to 4.1.0 Due to Invalid segment.bytes Configuration\nDescription: After upgrading our Kafka brokers to version {*}4.1.0{*}, the broker fails to start due to a misconfigured topic-level {{segment.bytes}} setting. The new version enforces a *minimum value of 1MB (1048576 bytes)* for this configuration, and any value below this threshold causes the broker to terminate during startup.\r\n*Error Details:*\r\n**\r\n \r\n{code:java}\r\n[2025-09-12 14:39:51,285] ERROR Encountered fatal fault: Error starting LogManager (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler) org.apache.kafka.common.config.ConfigException: Invalid value 75000 for configuration segment.bytes: Value must be at least 1048576 at org.apache.kafka.common.config.ConfigDef$Range.ensureValid(ConfigDef.java:989) ~[kafka-clients-4.1.0.jar:?]\r\n \r\n{code}\r\nIn our setup, some topics were previously configured with a lower segment.bytes value (e.g., 75000), which was allowed in earlier Kafka versions but is now invalid.\r\n\r\nAs a result Kafka broker cannot start, leading to downtime and unavailabil\nQ: Hi [~mithaj], the new 1MB segment.bytes constraint was introduced in 4.0, KIP: https://cwiki.apache.org/confluence/x/FAqpEQ.\r\nPerhaps you can try go back to the previous version, and use bin/kafka-configs.sh alter the topic segment.bytes? (Make it >= 1MB)", "output": "Hi [~brandboat] , thank you for the suggestion.\r\n\r\nUnfortunately, reverting to a previous Kafka version to update the {{segment.bytes}} configuration is not a viable option for us. Kafka is embedded in our product, which follows a bi-weekly release cycle. Even if we were to fix the configuration in one release and upgrade Kafka in a subsequent one, there's no guarantee that all customers will upgrade sequentially. As a result, we require a mechanism to bypass this validation during the Kafka upgrade itself, ensuring compatibility regardless of the upgrade path.\r\n\r\nAs an additional note, we had previously tested an upgrade to Kafka 4.0.0 and did not encounter this {{segment.bytes}} issue - brokers started successfully. However, we had to roll back due to KAFKA-19427, which caused out-of-memory errors. This prompted us to move directly to 4.1.0, where the stricter enforcement of the {{segment.bytes}} minimum led to this startup failure.\r\n\r\nWe’re looking for guidance on how to handle this scenario cleanly, ideally without requiring a rollback or risking data loss."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Support `spark.io.mode.default`\nDescription: \nQ: Issue resolved by pull request 52717\n[https://github.com/apache/spark/pull/52717]", "output": "No reply."}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A Analytics-Accelerator: Upgrade AAL to 1.0.0 \nDescription: Latest version of AAL is now available, changes since last time:\r\n * Moves all logging to debug\r\n * Adds in timeout and retries to requests to mitigate (very rare) hanging seen when reading from AsyncClient, related github issue: https://github.com/aws/aws-sdk-java-v2/issues/5755\nQ: ahmarsuhail commented on PR #7469:\nURL: https://github.com/apache/hadoop/pull/7469#issuecomment-2700895757\n\n   @mukund-thakur @steveloughran - PR to use AAL 1.0.0. No major changes since 0.0.4, moves logging to debug and adds timeouts and retries for a hanging issue we're seeing (happens really rarely)", "output": "fuatbasik commented on code in PR #7469:\nURL: https://github.com/apache/hadoop/pull/7469#discussion_r1981377089\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -64,8 +64,6 @@\n public class ITestS3AAnalyticsAcceleratorStreamReading extends AbstractS3ATestBase {\n \n   private static final String PHYSICAL_IO_PREFIX = \"physicalio\";\n\nReview Comment:\n   we can get rid of this   private static final String PHYSICAL_IO_PREFIX = \"physicalio\"; too right?"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update `dev/requirements.txt` to install `torch/torchvision` in Python 3.14\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Unify the command-line parser\nDescription: see https://github.com/apache/kafka/pull/20438#discussion_r2319682177", "output": "In Progress"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Rewrite AbortedTxn by generated protocol\nDescription: as title", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Throw better error to indicate that TWS is only supported with RocksDB state store provider\nDescription: When user tries to use TWS with HDFS as state store, they should get an error explicitly requiring to use RocksDB state provider.\nQ: I am working on this.", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Support partitionSizeLimitPerResponse for topicId–based describeTopics request\nDescription: DescribeTopicsOptions.partitionSizeLimitPerResponse is a prerequisite for supporting pagination, which is useful for requests involving a large number of topics.\r\n\r\nCurrently, when calling Admin#describeTopics(TopicCollection) with topic IDs, KafkaAdminClient uses a MetadataRequest under the hood. However, MetadataRequest does not support the partitionSizeLimitPerResponse option. This means that pagination is effectively unsupported for topic ID–based requests.\r\n\r\nIn contrast, partitionSizeLimitPerResponse is supported by DescribeTopicPartitionsRequest. However, this RPC currently only supports topic names, not topic IDs. To enable full pagination support across both topic names and topic IDs, we would need to extend DescribeTopicPartitionsRequest to accept topic IDs as an additional field.", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Mark DescribeStreamsGroupTest testDescribeStreamsGroupWithShortTimeout() as flaky\nDescription: According to [Develocity|https://develocity.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&tests.container=org.apache.kafka.tools.streams.DescribeStreamsGroupTest&tests.sortField=FLAKY], {{testDescribeStreamsGroupWithShortTimeout}} is very flaky.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: KRaft voter auto join will add a removed voter immediately\nDescription: In v4.2.0, we are able to auto join a controller with the configuration `controller.quorum.auto.join.enable=true` set ([KIP-853|https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes#KIP853:KRaftControllerMembershipChanges-Controllerautojoining](KAFKA-19078)). This is a good improvement for controller addition, but it has a UX issue, which is that when a controller is removed via removeVoterRequest, it will be added immediately due to `controller.quorum.auto.join.enable=true`. In the KIP, we also mention you have to stop the controller before removing the controller:\r\n\r\n \r\n{noformat}\r\ncontroller.quorum.auto.join.enable:\r\n\r\nControls whether a KRaft controller should automatically join the cluster \r\nmetadata partition for its cluster id. If the configuration is set to \r\ntrue the controller must be stopped before removing the controller with kafka-metadata-quorum remove-controller.{noformat}\r\n \r\n\r\nThis is not a user friendly behavior in my opinion\nQ: [~kevinwu2412] [~chia7712] [~jsancio] , any thought on this?", "output": "If we only allow anto-join voter to join the cluster once (it can be retried until success), is it simpler than using removed voter RPC?"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The implementation of permission restriction by https://issues.apache.org/jira/browse/KAFKA-10705 is very restrictive on groups. Existing group write permissions should be kept.\r\n\r\nWe could also make this configurable, which would require a KIP.\r\n\r\nKIP-1230 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1230%3A+Add+config+for+file+system+permissions]", "output": "Relax state directory file system restrictions"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Implement Geography and Geometry accessors across Catalyst\nDescription: \nQ: Issue resolved by pull request 52723\n[https://github.com/apache/spark/pull/52723]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Remove no-op `spark.shuffle.blockTransferService` configuration\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add support for podman when running system tests\nDescription: Add support for using podman within system tests.", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: AbfsConfiguration should store account type information (HNS or FNS)\nDescription: Currently, both {{AbfsClient}} and {{AzureBlobFileSystemStore}} store information about whether the account is FNS or HNS (i.e., whether namespace is enabled). This information should instead be stored at the {{AbfsConfiguration}} level, allowing both the client and the store to retrieve it from there when needed.\nQ: anujmodi2021 commented on code in PR #7765:\nURL: https://github.com/apache/hadoop/pull/7765#discussion_r2174789793\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1525,8 +1529,8 @@ void setMaxBackoffIntervalMilliseconds(int maxBackoffInterval) {\n   }\n \n   @VisibleForTesting\n-  void setIsNamespaceEnabledAccount(String isNamespaceEnabledAccount) {\n-    this.isNamespaceEnabledAccount = isNamespaceEnabledAccount;\n+  public void setIsNamespaceEnabledAccount(Trilean isNamespaceEnabledAccount) {\n\nReview Comment:\n   We should be setting this value on abfsconfiguration object only when we know the exact value. Do we really need this to be Trilean? I think we should pass the definitive argument and do the Boolean to Trilean conversion here just to make sure that no one accidently sets UNKNOWN here.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -443,7 +428,7 @@ private synchronized boolean getNamespaceEnabledInformationFromServer(\n    */\n   @VisibleForTesting\n   boolean isNamespaceEnabled() throws TrileanConversionException {\n-    return this.isNamespaceEnabled.toBoolean();\n+    return abfsConfiguration.getIsNamespaceEnabledAccount().toBoolean();\n\nReview Comment:\n   Better to use getter here and other places for abfsconfiguration. This will make sure same object is being referred everywhere and can easily be mocked if needed in future.", "output": "hadoop-yetus commented on PR #7765:\nURL: https://github.com/apache/hadoop/pull/7765#issuecomment-3019105733\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m  1s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 56s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 45s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 22s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   0m 23s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 22s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 43s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 27s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 156m 10s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7765 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 8bfda2d14615 5.15.0-138-generic #148-Ubuntu SMP Fri Mar 14 19:05:48 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ad210efe4512e9c3054dc8ee508ab047e3459c4b |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/testReport/ |\r\n   | Max. process+thread count | 541 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7765/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Consumer throughput drops by 10 times with Kafka v3.9.0 in ZK mode\nDescription: Kafka consumer throughput in best-case drops by ~10 times after upgrading to kafka v3.9.0 from v3.5.1. Note that this is in ZK mode and KRAFT migration is not done yet.\nQ: [~gargsha]—Can you share the consumer configuration? Thanks!", "output": "Hi,\r\n\r\nUpdating here on behalf of [~gargsha] -\r\n\r\n \r\n\r\n*Topic configs :*\r\n\r\n{{cleanup.policy=compact,delete; retention.ms=1200000; min.insync.replicas=2}}\r\n\r\n*Consumer configs :*\r\n\r\n{{fetch.min.bytes=131072,fetch.max.wait.ms=500}}\r\n\r\n( acks=1 for the Producer )\r\n\r\n \r\n\r\nI have generally observed this case for when the topic has a high throughput producer(s) running on it with upwards of *590 MB/sec* ( *600 K msgs/sec* with *1000 b* message size each ). Can also simulate this case with running multiple producers on the topic to achieve close to 600 MB/sec.\r\n\r\n \r\n\r\nIn the above test scenario, the consumer throughput drops down to {*}< 90MB/sec{*}. This is for a 1 Producer + 1 Consumer test, which should be the ideal test scenario and was getting me upto *250+ MB/sec* consumer throughput in the old {*}v3.5.1 Kafka cluster{*}.\r\n\r\n \r\n\r\nMy best guess is some form of loss in *insync replicas* ( with the high throughput producers as *acks=1* ), as only the leader replica would stay in-sync in case of a high throughput on the topic. And if this can have a performance impact on the consumer in some way ?? \r\n\r\nTo reason on this I had conducted the below test ( *min.insync.replicas=2* vs *min.insync.replicas=1* ) -\r\n\r\n \r\n \r\n|*test*|*Test Name*|*Observed message rate (K msgs/sec*|*Test description*|\r\n|1| | | |\r\n|Consumer|Throughput test run ( Java client ) - Min ISR = 2 - 1 producer & 1 consumer|*{color:#de350b}89.21{color}*|Test Configs = acks:1, partitions:1, rf:3, min-isr:2, producers:1, consumers:1, batch_size:256KB|\r\n|Producer|Throughput test run ( Java client ) - Min ISR = 2 - 1 producer & 1 consumer|617.20|Test Configs = acks:1, partitions:1, rf:3, min-isr:2, producers:1, consumers:1, batch_size:256KB|\r\n|2| | | |\r\n|Consumer|Throughput test run ( Java client ) - Min ISR = 1 - 1 producer & 1 consumer|*{color:#00875a}298.99{color}*|Test Configs = acks:1, partitions:1, rf:3, min-isr:1, producers:1, consumers:1, batch_size:256KB|\r\n|Producer|Throughput test run ( Java client ) - Min ISR = 1 - 1 producer & 1 consumer|597.20|Test Configs = acks:1, partitions:1, rf:3, min-isr:1, producers:1, consumers:1, batch_size:256KB|\r\n\r\n \r\n\r\nJust to note I have also observed this heavy drop in consumer throughput in a *KRaft mode cluster ( v3.9.0 ).*\r\n\r\nLet me know if to share more details regarding the tests I conducted or any other configs, for getting to debug this."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{panel:bgColor=#ffffce}\r\n*_Prologue: Kafka developers can't commit jar's into Gir repo (that includes Gradle wrapper jar_*\r\nRelated links:\r\n * https://issues.apache.org/jira/browse/KAFKA-2098?focusedCommentId=14481979&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-14481979\r\n * https://issues.apache.org/jira/browse/KAFKA-1490\r\n * [https://stackoverflow.com/questions/39856714/gradle-wrapper-without-the-jar]\r\n * [https://discuss.gradle.org/t/how-hand-compile-gradle-wrapper-jar-when-gradle-cannot-be-installed-and-cannot-check-jar-files-into-git/4813]{panel}\r\n \r\n\r\n*(i) Intro:* Groovy file _*unixStartScript.txt*_ (that servers as a template for a {_}*gradlew*{_}) path/module was changed in Gradle version 8.8.0\r\n\r\n _*(x) Problem description:*_\r\n # at the moment Kafka trunk branch is using Gradle version [8.14.1|https://github.com/apache/kafka/blob/8deb6c6911616f887ebb2678f3f12ee1da09a618/gradle/wrapper/gradle-wrapper.properties] but thing is that _*gradlew*_ is referencing Gradle 8.7.0 template file _*unixStartScript.txt*_\r\n # it means that _*gradlew*_ is missing all recent changes for a template file _*unixStartScript.txt*_\r\n\r\n*_Related GitHub Gradle links for unixStartScript.txt:_*\r\n * Gradle = 8.8.0 version:\r\n ** file path: [https://github.com/gradle/gradle/blob/v8.8.0/platforms/jvm/plugins-application/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt]\r\n ** Git history for versions [8.8.0, 8.14.1]: [https://github.com/gradle/gradle/commits/v8.14.1/platforms/jvm/plugins-application/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt]\r\n\r\n!screenshot-1.png!\r\n\r\n*Other related links:*\r\n * [https://github.com/gradle/gradle/compare/v8.7.0...v8.8.0]\r\n * [https://github.com/gradle/gradle/commit/c5ec05d90780454c7c20c66533b804d80d67639f] Moving code out of plugins project to more appropriate sub-projects\r\n * [https://github.com/gradle/gradle/commit/9041afac3b20f4878b5b0d59c1fb6b3b0982b53f] Refactor start scri", "output": "Fix `gradlew` content: add missing parts (old Groovy template version is being referenced)"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Upgrade `JUnit` to 6.0.0\nDescription: \nQ: Issue resolved by pull request 52561\n[https://github.com/apache/spark/pull/52561]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Keep in sync with recently upgraded thirdparty Guava", "output": "Update non-thirdparty Guava version to  33.4.8-jre"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Enable `spark.io.compression.lzf.parallel.enabled` by default\nDescription: \nQ: Issue resolved by pull request 52603\n[https://github.com/apache/spark/pull/52603]", "output": "No reply."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: XML to Variant conversion throws ArithmeticException on decimals with extreme exponents\nDescription: When parsing XML data with `parse_xml` that contains decimal numbers with very large \r\nexponents (e.g., \"1E+2147483647\"), the conversion to Variant type fails with:\r\n\r\n```\r\njava.lang.ArithmeticException: BigInteger would overflow supported range\r\n    at java.base/java.math.BigDecimal.setScale(BigDecimal.java:3000)\r\n    at org.apache.spark.sql.catalyst.xml.StaxXmlParser$.org$apache$spark$sql$catalyst$xml$StaxXmlParser$$appendXMLCharacterToVariant(StaxXmlParser.scala:1285)\r\n```", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "see this comment https://github.com/apache/kafka/pull/19589#discussion_r2274143020", "output": "Fix the busy loop occurring in the broker observer"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Release Hadoop 3.4.2\nDescription: Release a minor update to hadoop\nQ: [~ahmar] Currently, there seem to be some issues with Hadoop 3.4.2, and the full tar package hasn't been uploaded. Could you please help check this? Thank you very much! \r\n\r\nWe can refer to the information described in this PR.\r\n\r\nhttps://github.com/apache/hive/pull/6049\r\n\r\ncc: [~ayushtkn]", "output": "No reply."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "The PR https://github.com/apache/kafka/pull/20294  will add a number of exclusions to the SpotBugs configurations, and it would be good to revisit them later to ensure that we are not overlooking potential bugs", "output": "Revisit gradle/spotbugs-exclude.xml"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "See [https://bugs.openjdk.org/browse/JDK-8307059] , The Generational ZGC separate the Cycles and Pauses as Minor and Major,like this:\r\n * Old ZGC: \"ZGC Cycles\", \"ZGC Pauses\"\r\n * Generational ZGC: \"ZGC Minor Cycles\", \"ZGC Minor Pauses\", \"ZGC Major Cycles\", \"ZGC Major Pauses\"\r\n\r\nlet us separate it same and give 2 new metric about these, may be better...\r\n\r\n-----------------------------------------------------------------------------------------\r\n\r\nImpl to support get minor/major GC time/count from Hadoop Jmx", "output": "JVM GC Metrics supports fine-grained metrics"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "{{hadoop-runner}} installs {{robotframework}} without version definition.  Re-building the image for unrelated changes may unexpectedly upgrade {{robotframework}}, too.", "output": "Pin robotframework version"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update Boost to 1.86.0\nDescription: The currentlly used Boost 1.72.0 does not compile with recent C++ compilers, which prevents compiling on recent systems (at least without replacing the C++ compiler)\r\n\r\nhdfs-native does not compile with 1.87.0, but does with 1.86.0.\nQ: stoty opened a new pull request, #7444:\nURL: https://github.com/apache/hadoop/pull/7444\n\n   ### Description of PR\r\n   \r\n   Update Boost library version to 1.86.0\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Test suite on CI\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "hadoop-yetus commented on PR #7444:\nURL: https://github.com/apache/hadoop/pull/7444#issuecomment-2692416011\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  43m 56s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  shellcheck  |   0m  0s |  |  Shellcheck was not available.  |\r\n   | +0 :ok: |  shelldocs  |   0m  0s |  |  Shelldocs was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m  3s |  |  Maven dependency ordering for branch  |\r\n   | -1 :x: |  mvninstall  |  37m 21s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |  13m  1s | [/branch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/branch-compile-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  mvnsite  |   5m  7s | [/branch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/branch-mvnsite-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  shadedclient  | 103m 21s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |  35m 38s | [/patch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-mvninstall-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  compile  |   2m 48s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  cc  |   2m 48s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | -1 :x: |  javac  |   2m 48s | [/patch-compile-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-compile-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -1 :x: |  mvnsite  |   4m 47s | [/patch-mvnsite-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-mvnsite-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 58s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  | 533m 25s | [/patch-unit-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/patch-unit-root.txt) |  root in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 767m 29s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.tools.TestHadoopArchiveLogs |\r\n   |   | hadoop.util.TestNativeCodeLoader |\r\n   |   | hadoop.crypto.TestCryptoCodec |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7444 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets mvnsite unit shellcheck shelldocs compile cc javac |\r\n   | uname | Linux a978e95f2477 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4efbd1479550371e00cbfef28812b25ba2144a3a |\r\n   | Default Java | Red Hat, Inc.-1.8.0_412-b08 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/testReport/ |\r\n   | Max. process+thread count | 3137 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7444/1/console |\r\n   | versions | git=2.9.5 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "When parsing XML data with `parse_xml` that contains decimal numbers with very large \r\nexponents (e.g., \"1E+2147483647\"), the conversion to Variant type fails with:\r\n\r\n```\r\njava.lang.ArithmeticException: BigInteger would overflow supported range\r\n    at java.base/java.math.BigDecimal.setScale(BigDecimal.java:3000)\r\n    at org.apache.spark.sql.catalyst.xml.StaxXmlParser$.org$apache$spark$sql$catalyst$xml$StaxXmlParser$$appendXMLCharacterToVariant(StaxXmlParser.scala:1285)\r\n```", "output": "XML to Variant conversion throws ArithmeticException on decimals with extreme exponents"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Restore metrics are calculated incorrectly\nDescription: Restore metrics are calculated by measuring the time of the init method: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java#L135]\r\n\r\nBut the restoration process has been moved to the poll loop: [https://github.com/apache/kafka/commit/b78d7ba5d58fa77b831244444eef005a62883f9b]", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: S3A: Upgrade AWS V2 SDK to 2.29.52\nDescription: Upgrade to 2.29.52 -the last version compatible with third party stores until there are fixes in the AWS SDK or workarounds added in the S3A connector\r\n\r\nThis SDK update doesn't need to come with some changes to disable some new features (default integrity protections),\r\nand to apply critical changes related to the SDK\r\n\r\nDefault integrity protection came with 2.30, and is on unless disabled.\r\nhttps://github.com/aws/aws-sdk-java-v2/issues/5801\r\n\r\nAs well as being incompatible with third party stores, it has also affected S3 multiregion Access Points: https://github.com/aws/aws-sdk-java-v2/issues/5878\r\n\r\nThis has broken most interaction with third party stores, hence fixes in Iceberg https://github.com/apache/iceberg/pull/12264 and Trinio https://github.com/trinodb/trino/pull/24954\r\n\r\nThere's also [AWS v2.30 SDK InputStream behavior changes #5859](AWS v2.30 SDK InputStream behavior changes).\r\nIt looks like our code is safer from that, but it did require code review.\r\n\r\nSDK 2.30.19 seems\nQ: steveloughran opened a new pull request, #7479:\nURL: https://github.com/apache/hadoop/pull/7479\n\n   \r\n   Upgrade to 2.30.27\r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   Regression testing in progress\r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "steveloughran commented on PR #7479:\nURL: https://github.com/apache/hadoop/pull/7479#issuecomment-2706147948\n\n   testing in progress; also writing a new, expanded and very strict doc on qualifying a release, based on the experience of recent upgrades."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Wrong generic type for UnregisterBrokerOptions\nDescription: This UnregisterBrokerOptions extends from the UpdateFeaturesOptions, which is obviously wrong.\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/UnregisterBrokerOptions.java#L23]\r\n\r\n \r\n\r\nAnd that's why in KafkaAdminClientTest.java, we set timeout via `options.timeoutMs = 10;`. We should set it via timeoutMs(int) method gracefully like other tests.\nQ: Thank you, [~brandboat] !", "output": "great find"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update the documents of arrow-batching related configures\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Migrate to AssertJ for Assertion Verification\nDescription: Currently, the unit tests in the project do not fully leverage modern assertion capabilities, resulting in lower readability and maintainability of the test code. To improve test clarity and extensibility, we plan to migrate the existing assertion library to {*}AssertJ{*}.\r\n\r\n \r\n\r\n*Objective:*\r\n * Migrate all assertions in existing tests from JUnit or other assertion libraries to AssertJ.\r\n\r\n * Utilize AssertJ’s rich assertion methods (e.g., fluent API, more readable assertions) to enhance the expressiveness of unit tests.\r\n\r\n * Ensure that all existing unit tests continue to run correctly after migration.\r\n\r\n \r\n\r\n*Implementation Steps:*\r\n # Analyze existing unit test code to identify assertions that need to be replaced.\r\n\r\n # Replace existing assertions with AssertJ assertion syntax.\r\n\r\n # Run unit tests to ensure the tests pass and function correctly after migration.\r\n\r\n # Update relevant documentation to ensure the team is aware of how to use AssertJ for assertions.", "output": "In Progress"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Add a config entry to make IPC.Client checkAsyncCall off by default\nDescription: Add a config entry to make IPC.Client checkAsyncCall off by default\nQ: hfutatzhanghb opened a new pull request, #7521:\nURL: https://github.com/apache/hadoop/pull/7521\n\n   ### Description of PR\r\n    Add a config entry to make IPC.Client checkAsyncCall off by default.\r\n   \r\n   ### How was this patch tested?\r\n   Add unit test.", "output": "hfutatzhanghb commented on PR #7521:\nURL: https://github.com/apache/hadoop/pull/7521#issuecomment-2735176237\n\n   Hi, @KeeProMise , Please help review this PR when you have free time, Thanks ahead.  And i will close HDFS-17758 https://github.com/apache/hadoop/pull/7501 ."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Separate the controller config and admin config when add controller\nDescription: Currently, when adding a controller via CLI, we have to run:\r\n{code:java}\r\n$ bin/kafka-metadata-quorum.sh --command-config config/controller.properties --bootstrap-server localhost:9092 add-controller\r\n\r\nor\r\n\r\n$ bin/kafka-metadata-quorum.sh --command-config config/controller.properties --bootstrap-controller localhost:9093 add-controller{code}\r\nThe controller.properties file is expected to be the controller property file that to be added. But if we want to pass configs to the admin client, what can we do?\r\n{code:java}\r\nbin/kafka-metadata-quorum.sh --help\r\nusage: kafka-metadata-quorum [-h] [--command-config COMMAND_CONFIG] (--bootstrap-server BOOTSTRAP_SERVER | --bootstrap-controller BOOTSTRAP_CONTROLLER)\r\n                             {describe,add-controller,remove-controller} ...This tool describes kraft metadata quorum status.\r\n\r\n...\r\n\r\n --command-config COMMAND_CONFIG\r\n   Property file containing configs to be passed to Admin Client.  For add-controller,  the file is used to specify\nQ: Hi [~showuon], if you are not working on this, may i take it. Thanks.", "output": "Go take it [~isding_l] ! It needs a KIP, for your information."}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move `KRaftClusterTest` from core module to server module\nDescription: It should include following tasks\r\n # rewrite by java\r\n # move to server module", "output": "Open"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "We are using TopologyTestDriver \r\n\r\nIt looks like the test driver is syncing the entire state directory to disk for every record processed:\r\n\r\n!image-2025-09-08-14-07-05-768.png!", "output": "TopologyTestDriver spends most of its time forcing changes to disk with persistent state stores"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Add `word-count-preview.yaml` Example\nDescription: ", "output": "Resolved"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "As per guide: 'Service users will always be scheduled into the highest-priority queue and won’t be included in the priority computation of normal user calls.'\r\n\r\nHowever, this functionality was never implemented due to some redundant issue during initial development. So this decay-scheduler.service-users needs to be removed from document.\r\n\r\n \r\n\r\n[https://hadoop.apache.org/docs/r3.4.1/hadoop-project-dist/hadoop-common/FairCallQueue.html]\r\n\r\nRef: https://issues.apache.org/jira/browse/HADOOP-10281", "output": "Remove decay-scheduler.service-users in the fair call guide"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))\nDescription: \nQ: bhattmanish98 opened a new pull request, #7967:\nURL: https://github.com/apache/hadoop/pull/7967\n\n   JIRA – https://issues.apache.org/jira/browse/HADOOP-19672\r\n   \r\n   In case of a network error while using the Apache client, we allow the client to switch over from Apache to JDK. This network fallback occurs in two scenarios:\r\n   \r\n   1. During file system initialization – When warming up the cache, if no connection is created (indicating an issue with the Apache client), the system will fall back to the JDK.\r\n   2. During a network call – If an I/O or Unknown Host exception occurs for three consecutive retries, the system will fall back to the JDK.\r\n   \r\n   This fallback is applied at the JVM level, so all file system calls will use the JDK client once the switch occurs.\r\n   \r\n   There is also a possibility of recovery. During cache warmup, if connections are successfully created using the Apache client, the system will automatically switch back to the Apache client.", "output": "hadoop-yetus commented on PR #7967:\nURL: https://github.com/apache/hadoop/pull/7967#issuecomment-3293191749\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 36s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  57m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 50s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 2 unchanged - 0 fixed = 3 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 59s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 47s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7967 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b7ef8b2a88ce 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1dad672faf42fbfb0ada2a95456b922a50b2becf |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/testReport/ |\r\n   | Max. process+thread count | 693 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7967/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Curator 5.2.0 has a default \"connection timeout\" of 15s:\r\n\r\n[https://github.com/apache/curator/blob/apache-curator-5.2.0/curator-framework/src/main/java/org/apache/curator/framework/CuratorFrameworkFactory.java#L63]\r\n\r\nAnd it will throw a warning if the Zookeeper session timeout is less than the Curator connection timeout:\r\n\r\n[https://github.com/apache/curator/blob/apache-curator-5.2.0/curator-client/src/main/java/org/apache/curator/CuratorZookeeperClient.java#L117-L120]\r\n\r\nThe Hadoop default for ZK timeout is set to 10s:\r\n\r\n[https://github.com/apache/hadoop/blob/0dd9bf8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeys.java#L414-L416]\r\n\r\nWhich means without setting the \"connection timeout\" a default installation will keep warning about the connection timeout for Curator being lower than the requested session timeout. This sets both timeout values to override the Curator default.\r\n\r\nAnother option is to change the default Hadoop ZK timeout from 10s to 15s.", "output": "Set Curator Connection Timeout"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Currently, the check for max inflight records happen prior fetching data for some share partition but the fetched data can be acquired and have records which are past max inflight records. This is evident when some records are released from the inflight records and the next fetch results in data past max inflight records.", "output": "Restrict records acquisition post max inflight records"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Client#call decrease asyncCallCounter incorrectlly when exceptions occur\nDescription: Client#call decrease asyncCallCounter incorrectlly when exceptions occur\nQ: hadoop-yetus commented on PR #7384:\nURL: https://github.com/apache/hadoop/pull/7384#issuecomment-2656410395\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  23m 49s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/1/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | +1 :green_heart: |  compile  |   9m 50s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 54s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m 47s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 50s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   8m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 51s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 38s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  12m 12s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 42s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 128m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7384 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 524428b2d7da 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 220e74e3e6fd5d1083bbce660942e8de08ad361f |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/1/testReport/ |\r\n   | Max. process+thread count | 3149 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "hadoop-yetus commented on PR #7384:\nURL: https://github.com/apache/hadoop/pull/7384#issuecomment-2656424263\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | -1 :x: |  test4tests  |   0m  0s |  |  The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  10m 15s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   8m 54s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  0s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m 43s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 57s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   8m 57s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 38s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  0s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  12m 12s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 129m 22s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7384 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 0cc6665dc13f 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / cd3893c41b392c401396acf27488fcc7f5f64f26 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/2/testReport/ |\r\n   | Max. process+thread count | 1261 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7384/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "In the release process, you release manager has to manually trigger the Docker Build Test Workflow for both the jvm and native images ([https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=34840886#ReleaseProcess-CreateJVMApacheKafkaDockerArtifacts(Forversions>=3.7.0)|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=34840886#ReleaseProcess-CreateJVMApacheKafkaDockerArtifacts(Forversions>=3.7.0)]).\r\n\r\nThe release script could trigger these jobs automatically using the GitHub REST API.", "output": "Trigger Docker image builds in release script"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Move AutoTopicCreationManager to server module\nDescription: Move AutoTopicCreationManager and AutoTopicCreationManagerTest to server module.", "output": "Open"}
{"instruction": "Answer the question based on the bug.", "input": "Title: ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Rename Operation\nDescription: We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Rename File and Rename directory operations on blob Endpoint\r\n\r\nThis Jira tracks implementing these tests.\nQ: bhattmanish98 opened a new pull request, #7386:\nURL: https://github.com/apache/hadoop/pull/7386\n\n   We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint. So, we have added test cases for all those scenarios in this PR.", "output": "hadoop-yetus commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2657734081\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 9 new + 2 unchanged - 0 fixed = 11 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 58s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 134m 13s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7386 |\r\n   | JIRA Issue | HADOOP-19445 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f1c6768f1b54 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0bfe00728dc19c14378d0a2aa58842330a1b5f4a |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/testReport/ |\r\n   | Max. process+thread count | 604 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update build instructions for Windows\nDescription: We recently upgraded vcpkg to install Boost 1.86. We need to update the documentation as well.\nQ: hadoop-yetus commented on PR #7673:\nURL: https://github.com/apache/hadoop/pull/7673#issuecomment-2849038608\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  shadedclient  |  37m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  shadedclient  |  33m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  73m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7673/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7673 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets |\r\n   | uname | Linux 15bb8d6521d4 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 90268e02346b3cab22d79071e7ba7b763d046214 |\r\n   | Max. process+thread count | 671 (vs. ulimit of 5500) |\r\n   | modules | C: . U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7673/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.", "output": "slfan1989 commented on PR #7673:\nURL: https://github.com/apache/hadoop/pull/7673#issuecomment-2849092061\n\n   @GauthamBanasandra LGTM."}
{"instruction": "Answer the question based on the bug.", "input": "Title: Percentile estimation functions\nDescription: Similar to https://issues.apache.org/jira/browse/SPARK-53885, it would be useful to have the following percentile estimation functions based on data sketch. These functions provide a lightweight, mergeable representation of percentile distributions, enabling efficient approximate percentile computation over large or streaming datasets without maintaining full data samples.\r\n * *APPROX_PERCENTILE_ACCUMULATE(expr[, maxItemsTracked]):* Creates an intermediate state object that incrementally accumulates values for percentile estimation using a sketch-based algorithm.\r\n\r\n * *APPROX_PERCENTILE_ESTIMATE(state [, k] ])* : Returns the approximate percentile value(s) from a previously accumulated sketch state.\r\n\r\n * *APPROX_PERCENTILE_COMBINE(expr[, maxItemsTracked])* : Merges two intermediate sketch states to enable distributed or parallel percentile estimation.\nQ: cc [~cboumalh] [~yhuang95] — would you be interested in taking on these new functions?", "output": "[~Gengliang.Wang] I can take these on."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Java sync client is giving the best performance for our use case, especially for readVectored() where a large number of requests can be made concurrently.", "output": "S3A Analytics-Accelerator: Move AAL to use Java sync client"}
{"instruction": "Summarize this bug in 1 sentence.", "input": "Fix WASB ABFS compatibility issues. Fix issues such as:-\r\n # BlockId computation to be consistent across clients for PutBlock and PutBlockList\r\n # Restrict url encoding of certain json metadata during setXAttr calls.\r\n # Maintain the md5 hash of whole block to validate data integrity during flush.", "output": "ABFS: Fix WASB ABFS compatibility issues"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Update streams documentation with KIP-1147 changes\nDescription: docs/streams/quickstart.html and docsk/streams/developer-guide/datatypes.html both need to be updated to change the flags for kafka-console-consumer.sh to rename --property to --formatter-property.\nQ: Hi [~schofielaj] , should I update the docs from `--property` to `--reader-property` for consoleProducer as mentioned in https://github.com/apache/kafka/pull/20554#issuecomment-3308459767 , or would you like to do it in another ticket?", "output": "You can do the docs change in this ticket, but I'll get the code change for replacing --property with --reader-property in another ticket."}
{"instruction": "Summarize this bug in 1 sentence.", "input": "RouterAdminProtocolTranslatorPB should also use ShadedProtobufHelper.", "output": "[pb-upgrade] RouterAdminProtocolTranslatorPB use ShadedProtobufHelper"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Improve heartbeat request manager initial HB interval \nDescription: With KIP-848, consumer HB interval config moved to the broker, so currently, the consumer HB request manager starts with a 0ms interval (mainly to send a first HB right away after the consumer subscribe + poll). Once a response is received, the consumer takes the interval from the response and starts using it. \r\n\r\nThat 0ms initial interval makes that the HB mgr poll continuously executes logic on a tight loop that may not really be needed. It mostly has to wait for a response (or a failure).\r\n\r\nProbably worse than this, is the impact on the app thread, given that pollTimeout takes into account the maxTimeToWait from the network thread, that is directly impacted by the timeToNextHeartbeat\r\n * [https://github.com/apache/kafka/blob/388739f5d847d7a16e389d9891f806547f023476/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1764-L1766]\r\n * [https://github.com/apache/kafka/blob/781bc7a54b8c4f7c86f0d6bb9ef8399d86d0735e/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L255]\r\n\r\nWe should review and consider setting a non-zero initial interval (while we wait for the actual interval from the broker). One option to consider would be using the request timeout maybe (just a first thought)\r\n\r\nHigh level goals here would be to:\r\n * maintain the behaviour of sending a first HB without delay \r\n * ensure no unneeded activity on the HB mgr poll in the background, in tight loop, while we're just waiting ", "output": "Open"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: Update `dev/requirements.txt` to skip `torch` and `torchvision` in Python 3.14\nDescription: ", "output": "Resolved"}
{"instruction": "Predict final status (Open/In Progress/Resolved/Closed).", "input": "Title: cos use token credential will lost token field\nDescription: Hi,\r\n\r\nI've discovered a bug when accessing COSN using temporary credentials (access key, secret key, and session token).\r\n\r\nIn the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n\r\nFurthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n!image-2025-08-11-10-37-12-451.png|width=1048,height=540! \r\n!image-2025-08-11-10-42-36-375.png!", "output": "Resolved"}
{"instruction": "Answer the question based on the bug.", "input": "Title: Remove broken Centos 7 C++ precommit checks from CI\nDescription: The Netty update in HADOOP-19335 required a newer grpc compiler which requires newer libraries than even the add-on C++ environment has in Centos 7.\r\n\r\nCentos 7 is EOL, and fixing Centos 7 would require rebuilding and replacing the C++ library at which point why even bother ?\nQ: stoty opened a new pull request, #7493:\nURL: https://github.com/apache/hadoop/pull/7493\n\n   ### Description of PR\r\n   \r\n   Remove the Centos 7 C++ precommit check from CI.\r\n   \r\n   ### How was this patch tested?\r\n   \r\n   It wasn't, I am not able test Jenkins changes locally.\r\n   \r\n   ### For code changes:\r\n   \r\n   - [X ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?", "output": "pan3793 commented on PR #7493:\nURL: https://github.com/apache/hadoop/pull/7493#issuecomment-2710815286\n\n   This sounds like a silent breaking change. I know CentOS 7 has been sunset, but ... we have dozens of thousands of Hadoop nodes running on CentOS 7 as of today ...\r\n   \r\n   Please at least discuss the drop support of CentOS 7 in the mailing list ..."}
