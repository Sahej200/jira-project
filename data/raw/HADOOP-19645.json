{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13624892", "self": "https://issues.apache.org/jira/rest/api/2/issue/13624892", "key": "HADOOP-19645", "fields": {"summary": "ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done.", "description": "There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header.\r\n\r\nFollowing are types of read we want to identify:\r\n # Direct Read: Read from a given position in remote file. This will be synchronous read\r\n # Normal Read: Read from current seeked position where read ahead was bypassed. This will be synchronous read.\r\n # Prefetch Read: Read triggered from background threads filling up in memory cache. This will be asynchronous read.\r\n # Missed Cache Read: Read triggered after nothing was received from read ahead. This will be synchronous read.\r\n # Footer Read: Read triggered as part of footer read optimization. This will be synchronous.\r\n # Small File Read: Read triggered as a part of small file read. This will be synchronous read.\r\n\r\nWe will add another field in the Tracing Header (Client Request Id) for each request. We can call this field \"Operation Specific Header\" very similar to how we have \"Retry Header\" today. As part of this we will only use it for read operations keeping it empty for other operations. Moving ahead f we need to publish any operation specific info, same header can be used.", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anujmodi", "name": "anujmodi", "key": "JIRAUSER307456", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Anuj Modi", "active": true, "timeZone": "Asia/Kolkata"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18010847", "id": "18010847", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135193019\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 55s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 14 new + 1 unchanged - 0 fixed = 15 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m 59s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 51s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c64bcd4ab2e9 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ea1572a40346a9ddfa4e80f4f1a4925308205175 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-30T07:39:37.419+0000", "updated": "2025-07-30T07:39:37.419+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18010848", "id": "18010848", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135296997\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 47s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 17 new + 1 unchanged - 0 fixed = 18 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 28s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 28s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   2m 59s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 30s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.azurebfs.services.TestApacheHttpClientFallback |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7be945cb4d05 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 42ecdd05eb6954bdfd26d5022bf4c8a517c607ba |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/testReport/ |\r\n   | Max. process+thread count | 540 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-30T08:16:17.523+0000", "updated": "2025-07-30T08:16:17.523+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18010878", "id": "18010878", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135629353\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  2s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  2s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 58s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 18 new + 1 unchanged - 0 fixed = 19 total (was 1)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  46m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 42s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 148m 56s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5f43a0e97825 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 224f712fee069bd839f8c27a979367e61cec8c17 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/testReport/ |\r\n   | Max. process+thread count | 596 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-30T09:59:11.370+0000", "updated": "2025-07-30T09:59:11.370+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18010886", "id": "18010886", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3135851673\n\n   -----------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 34\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 172\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 34\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 395\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 35\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 234\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 58\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 285\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 29\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 400\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 35\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 301\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 29\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 346\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 53\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 356\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 34\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 858, Failures: 0, Errors: 0, Skipped: 397\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 35\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-30T11:15:47.177+0000", "updated": "2025-07-30T11:15:47.177+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011079", "id": "18011079", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "Copilot commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2244303281\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +289,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n+    String retryHeader = String.format(\"%d\", retryCount);\n+    if (previousFailure == null) {\n+      return retryHeader;\n+    }\n+    if (CONNECTION_TIMEOUT_ABBREVIATION.equals(previousFailure) && retryPolicyAbbreviation != null) {\n+      return String.format(\"%s_%s_%s\", retryHeader, previousFailure, retryPolicyAbbreviation);\n+    }\n+    return String.format(\"%s_%s\", retryHeader, previousFailure);\n+  }\n+\n+  private String getOperationSpecificHeader(FSOperationType opType) {\n+    // Similar header can be added for other operations in the future.\n+    switch (opType) {\n+      case READ:\n+        return readSpecificHeader();\n+      default:\n+        return EMPTY_STRING; // no operation specific header\n+    }\n+  }\n+\n+  private String readSpecificHeader() {\n+    // More information on read can be added to this header in the future.\n+    // As underscore separated values.\n+    String readHeader = String.format(\"%s\", readType.toString());\n\nReview Comment:\n   The String.format with \"%s\" is unnecessary here. Use readType.toString() directly for better readability and performance.\n   ```suggestion\n       String readHeader = readType.toString();\n   ```\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,31 +213,35 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n+    case ALL_ID_FORMAT:\n       header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n+          clientCorrelationID + \":\" +\n+          clientRequestId + \":\" +\n+          fileSystemID + \":\" +\n+          getPrimaryRequestIdForHeader(retryCount > 0) + \":\" +\n+          streamID + \":\" +\n+          opType + \":\" +\n+          getRetryHeader(previousFailure, retryPolicyAbbreviation) + \":\" +\n+          ingressHandler + \":\" +\n+          position + \":\" +\n+          operatedBlobCount + \":\" +\n+          httpOperation.getTracingContextSuffix() + \":\" +\n+          getOperationSpecificHeader(opType);\n+\n       metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n       break;\n     case TWO_ID_FORMAT:\n-      header = clientCorrelationID + \":\" + clientRequestId;\n+      header =\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n+          clientCorrelationID + \":\" + clientRequestId;\n       metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n       break;\n     default:\n       //case SINGLE_ID_FORMAT\n-      header = clientRequestId;\n+      header =\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n\nReview Comment:\n   The hardcoded V1 version is used in multiple places. Consider using TracingHeaderVersion.getCurrentVersion() consistently to centralize version management.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,31 +213,35 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n+    case ALL_ID_FORMAT:\n       header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n\nReview Comment:\n   The hardcoded V1 version is used in multiple places. Consider using TracingHeaderVersion.getCurrentVersion() consistently to centralize version management.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,31 +213,35 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n+    case ALL_ID_FORMAT:\n       header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n+          clientCorrelationID + \":\" +\n+          clientRequestId + \":\" +\n+          fileSystemID + \":\" +\n+          getPrimaryRequestIdForHeader(retryCount > 0) + \":\" +\n+          streamID + \":\" +\n+          opType + \":\" +\n+          getRetryHeader(previousFailure, retryPolicyAbbreviation) + \":\" +\n+          ingressHandler + \":\" +\n+          position + \":\" +\n+          operatedBlobCount + \":\" +\n+          httpOperation.getTracingContextSuffix() + \":\" +\n+          getOperationSpecificHeader(opType);\n+\n       metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n       break;\n     case TWO_ID_FORMAT:\n-      header = clientCorrelationID + \":\" + clientRequestId;\n+      header =\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n+          clientCorrelationID + \":\" + clientRequestId;\n       metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n       break;\n     default:\n       //case SINGLE_ID_FORMAT\n-      header = clientRequestId;\n+      header =\n+          AbfsHttpConstants.TracingHeaderVersion.V1 + \":\" +\n\nReview Comment:\n   The hardcoded V1 version is used in multiple places. Consider using TracingHeaderVersion.getCurrentVersion() consistently to centralize version management.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderValidator.java:\n##########\n@@ -81,82 +85,93 @@ public TracingHeaderValidator(String clientCorrelationId, String fileSystemId,\n   }\n \n   private void validateTracingHeader(String tracingContextHeader) {\n-    String[] idList = tracingContextHeader.split(\":\");\n+    String[] idList = tracingContextHeader.split(\":\", -1);\n\nReview Comment:\n   [nitpick] Consider defining the split limit (-1) as a named constant to improve code readability and maintainability.\n   ```suggestion\n       String[] idList = tracingContextHeader.split(\":\", SPLIT_NO_LIMIT);\n   ```\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-31T04:10:40.693+0000", "updated": "2025-07-31T04:10:40.693+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011105", "id": "18011105", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3138813312\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  9s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 6 new + 5 unchanged - 0 fixed = 11 total (was 5)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 23s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 27s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  79m 44s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ba7a35768638 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0d926b14ba007e88c0099c5880f78176988d2442 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-31T07:13:19.664+0000", "updated": "2025-07-31T07:13:19.664+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011190", "id": "18011190", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2245426192\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +289,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n+    String retryHeader = String.format(\"%d\", retryCount);\n+    if (previousFailure == null) {\n+      return retryHeader;\n+    }\n+    if (CONNECTION_TIMEOUT_ABBREVIATION.equals(previousFailure) && retryPolicyAbbreviation != null) {\n+      return String.format(\"%s_%s_%s\", retryHeader, previousFailure, retryPolicyAbbreviation);\n+    }\n+    return String.format(\"%s_%s\", retryHeader, previousFailure);\n+  }\n+\n+  private String getOperationSpecificHeader(FSOperationType opType) {\n+    // Similar header can be added for other operations in the future.\n+    switch (opType) {\n+      case READ:\n+        return readSpecificHeader();\n+      default:\n+        return EMPTY_STRING; // no operation specific header\n+    }\n+  }\n+\n+  private String readSpecificHeader() {\n+    // More information on read can be added to this header in the future.\n+    // As underscore separated values.\n+    String readHeader = String.format(\"%s\", readType.toString());\n\nReview Comment:\n   Rataining it as in future we might add more info to the same field\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-31T13:36:31.313+0000", "updated": "2025-07-31T13:36:31.313+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011196", "id": "18011196", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2245456486\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java:\n##########\n@@ -128,6 +128,7 @@ public final class AbfsHttpConstants {\n   public static final String STAR = \"*\";\n   public static final String COMMA = \",\";\n   public static final String COLON = \":\";\n+  public static final String HYPHEN = \"-\";\n\nReview Comment:\n   We already have CHAR_HYPHEN defined for this.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n+  }\n+\n+  public int getFieldCount() {\n+    return V1.fieldCount;\n+  }\n+\n+  public String getVersion() {\n+    return V1.version;\n\nReview Comment:\n   Same as above, it should be `return this.version`?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +286,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n\nReview Comment:\n   Please add javadoc to all newly added methods\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n+  }\n+\n+  public int getFieldCount() {\n+    return V1.fieldCount;\n\nReview Comment:\n   Shouldn't it be just `return this.fieldCount`?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n\nReview Comment:\n   Java Doc missing\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-31T14:00:13.187+0000", "updated": "2025-07-31T14:00:13.187+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011212", "id": "18011212", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3140309047\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 46s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 44s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  78m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b44a21d0bd2d 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9bb6cdbeda33155f0957108cfdf87b63dcefe53a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/5/testReport/ |\r\n   | Max. process+thread count | 676 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-31T14:58:54.479+0000", "updated": "2025-07-31T14:58:54.479+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011345", "id": "18011345", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2246885435\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -544,7 +555,9 @@ private int readInternal(final long position, final byte[] b, final int offset,\n       }\n \n       // got nothing from read-ahead, do our own read now\n-      receivedBytes = readRemote(position, b, offset, length, new TracingContext(tracingContext));\n+      TracingContext tc = new TracingContext(tracingContext);\n+      tc.setReadType(ReadType.MISSEDCACHE_READ);\n+      receivedBytes = readRemote(position, b, offset, length, tc);\n       return receivedBytes;\n     } else {\n       LOG.debug(\"read ahead disabled, reading remote\");\n\nReview Comment:\n   Should we add readtype as normal read for this TC as well?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T04:46:21.813+0000", "updated": "2025-08-01T04:46:21.813+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011349", "id": "18011349", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2246929766\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -442,6 +451,7 @@ private int optimisedRead(final byte[] b, final int off, final int len,\n     //  bCursor that means the user requested data has not been read.\n     if (fCursor < contentLength && bCursor > limit) {\n       restorePointerState();\n+      tracingContext.setReadType(ReadType.NORMAL_READ);\n\nReview Comment:\n   Before readOneBlock we're setting TC as normal read both here and line 439. In readOneBlock method- we're setting TC again to normal read- do we need it twice?\r\n   We can keep it once in the method only otherwise\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T05:18:14.198+0000", "updated": "2025-08-01T05:18:14.198+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011351", "id": "18011351", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2246993787\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n+          + position + COLON\n+          + operatedBlobCount + COLON\n+          + httpOperation.getTracingContextSuffix() + COLON\n+          + getOperationSpecificHeader(opType);\n\nReview Comment:\n   should we keep the op specific header before adding the HTTP client? It would get all req related info together and then network client. \r\n   Eg- .....:RE:1_EGR:NR:JDK\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T05:52:06.955+0000", "updated": "2025-08-01T05:52:06.955+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011354", "id": "18011354", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247014906\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +286,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n\nReview Comment:\n   we can remove the addFailureReasons method- it has no usage now\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T06:01:46.957+0000", "updated": "2025-08-01T06:01:46.957+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011362", "id": "18011362", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247066421\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n\nReview Comment:\n   Since the next versions would be V1.1/V1.2- so should we consider starting with V1.0/V1.1?\r\n   And with the version updates- would we update the version field in V1 only or new V1.1 enum?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T06:24:44.962+0000", "updated": "2025-08-01T06:24:44.962+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011381", "id": "18011381", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247336266\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Footer Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(false).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(true).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Small File Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(true).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(false).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls);\n+  }\n+\n+  private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs, int fileSize, ReadType readType, int numOfReadCalls) throws Exception {\n+    Path testPath = new Path(\"testFile\");\n+    byte[] fileContent = getRandomBytesArray(fileSize);\n+    try (FSDataOutputStream oStream = fs.create(testPath)) {\n+      oStream.write(fileContent);\n+      oStream.flush();\n+    }\n+    try (FSDataInputStream iStream = fs.open(testPath)) {\n+      int bytesRead = iStream.read(new byte[fileSize], 0,\n+          fileSize);\n+      Assertions.assertThat(fileSize)\n+          .describedAs(\"Read size should match file size\")\n+          .isEqualTo(bytesRead);\n+    }\n+\n+    ArgumentCaptor<String> captor1 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> captor2 = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<byte[]> captor3 = ArgumentCaptor.forClass(byte[].class);\n+    ArgumentCaptor<Integer> captor4 = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Integer> captor5 = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<String> captor6 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<String> captor7 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class);\n+    ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class);\n+\n+    verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read(\n+        captor1.capture(), captor2.capture(), captor3.capture(),\n+        captor4.capture(), captor5.capture(), captor6.capture(),\n+        captor7.capture(), captor8.capture(), captor9.capture());\n+    TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1);\n+    verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType);\n+  }\n+\n+  private void verifyHeaderForReadTypeInTracingContextHeader(TracingContext tracingContext, ReadType readType) {\n+    AbfsHttpOperation mockOp = Mockito.mock(AbfsHttpOperation.class);\n+    doReturn(EMPTY_STRING).when(mockOp).getTracingContextSuffix();\n+    tracingContext.constructHeader(mockOp, null, null);\n+    String[] idList = tracingContext.getHeader().split(COLON, SPLIT_NO_LIMIT);\n+    Assertions.assertThat(idList).describedAs(\"Client Request Id should have all fields\").hasSize(\n+        TracingHeaderVersion.getCurrentVersion().getFieldCount());\n+    Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Operation Type Should Be Read\")\n+        .contains(FSOperationType.READ.toString());\n+    Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Read type in tracing context header should match\")\n+        .contains(readType.toString());\n+  }\n+\n+//  private testReadTypeInTracingContextHeaderInternal(ReadType readType) throws Exception {\n\nReview Comment:\n   Nit- we can remove this\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T08:39:08.330+0000", "updated": "2025-08-01T08:39:08.330+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011388", "id": "18011388", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247415571\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -77,8 +81,7 @@ public class TracingContext {\n    * this field shall not be set.\n    */\n   private String primaryRequestIdForRetry;\n-\n-  private Integer operatedBlobCount = null;\n+  private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set.\n\nReview Comment:\n   why is it changed from null to 1 ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:15:09.074+0000", "updated": "2025-08-01T09:15:09.074+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011389", "id": "18011389", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247428588\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n\nReview Comment:\n   So every time we add a new header, we need to add a new version ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:21:15.085+0000", "updated": "2025-08-01T09:21:15.085+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011390", "id": "18011390", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247438249\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n+  }\n+\n+  public int getFieldCount() {\n+    return V1.fieldCount;\n\nReview Comment:\n   +1\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:25:37.946+0000", "updated": "2025-08-01T09:25:37.946+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011391", "id": "18011391", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247441472\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n\nReview Comment:\n   will this need to be updated everytime a new version is introduced ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:27:14.393+0000", "updated": "2025-08-01T09:27:14.393+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011392", "id": "18011392", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247441472\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n\nReview Comment:\n   this needs to be updated everytime a new version is introduced, can it be dynamically fetched ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:27:40.350+0000", "updated": "2025-08-01T09:27:40.350+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011393", "id": "18011393", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247447664\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n\nReview Comment:\n   should we use getCurrentVersion here ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:29:30.416+0000", "updated": "2025-08-01T09:29:30.416+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011394", "id": "18011394", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247451122\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n\nReview Comment:\n   these empty string checks are needed \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:30:49.644+0000", "updated": "2025-08-01T09:30:49.644+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011395", "id": "18011395", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247460310\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n+          + position + COLON\n+          + operatedBlobCount + COLON\n+          + httpOperation.getTracingContextSuffix() + COLON\n+          + getOperationSpecificHeader(opType);\n+\n+      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : EMPTY_STRING;\n       break;\n     case TWO_ID_FORMAT:\n-      header = clientCorrelationID + \":\" + clientRequestId;\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n\nReview Comment:\n   same as above getCurrentVersion ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:35:15.314+0000", "updated": "2025-08-01T09:35:15.314+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011396", "id": "18011396", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247472876\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestTracingContext.java:\n##########\n@@ -326,8 +329,8 @@ fileSystemId, FSOperationType.CREATE_FILESYSTEM, tracingHeaderFormat, new Tracin\n   }\n \n   private void checkHeaderForRetryPolicyAbbreviation(String header, String expectedFailureReason, String expectedRetryPolicyAbbreviation) {\n-    String[] headerContents = header.split(\":\");\n-    String previousReqContext = headerContents[6];\n+    String[] headerContents = header.split(\":\", SPLIT_NO_LIMIT);\n\nReview Comment:\n   colon constant here as well since we are changing at other places\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:41:25.750+0000", "updated": "2025-08-01T09:41:25.750+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011398", "id": "18011398", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247491342\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n\nReview Comment:\n   should we also verify that it is normal_read for all the three calls made ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:47:30.975+0000", "updated": "2025-08-01T09:47:30.975+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011399", "id": "18011399", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247493079\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n\nReview Comment:\n   same here verify that 2 calls have prefetch_read\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:47:55.997+0000", "updated": "2025-08-01T09:47:55.997+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011401", "id": "18011401", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247491342\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n\nReview Comment:\n   should we also verify that it is normal_read for all the three calls made, currently it verifies for contains \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:49:26.061+0000", "updated": "2025-08-01T09:49:26.061+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011402", "id": "18011402", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2247501549\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Footer Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(false).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(true).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Small File Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(true).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(false).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls);\n+  }\n\nReview Comment:\n   One test for direct read as well ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T09:50:01.077+0000", "updated": "2025-08-01T09:50:01.077+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011559", "id": "18011559", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "violetnspct commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2249151187\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -578,6 +591,7 @@ int readRemote(long position, byte[] b, int offset, int length, TracingContext t\n         streamStatistics.remoteReadOperation();\n       }\n       LOG.trace(\"Trigger client.read for path={} position={} offset={} length={}\", path, position, offset, length);\n+      tracingContext.setPosition(String.valueOf(position));\n\nReview Comment:\n   Is there a test to verify position is correctly added to tracing context? Position is a key identifier for read operations.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-02T07:07:30.970+0000", "updated": "2025-08-02T07:07:30.970+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011697", "id": "18011697", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250024646\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -544,7 +555,9 @@ private int readInternal(final long position, final byte[] b, final int offset,\n       }\n \n       // got nothing from read-ahead, do our own read now\n-      receivedBytes = readRemote(position, b, offset, length, new TracingContext(tracingContext));\n+      TracingContext tc = new TracingContext(tracingContext);\n+      tc.setReadType(ReadType.MISSEDCACHE_READ);\n+      receivedBytes = readRemote(position, b, offset, length, tc);\n       return receivedBytes;\n     } else {\n       LOG.debug(\"read ahead disabled, reading remote\");\n\nReview Comment:\n   This is coming directly from readOneBlock() so will always be normal\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-03T15:41:45.557+0000", "updated": "2025-08-03T15:41:45.557+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011710", "id": "18011710", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3148617353\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 56s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 47s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  77m 23s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5948820d65fb 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 132893fcbf3d7eb4b31ca01dbaef26c186560dd3 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/6/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-03T18:26:50.065+0000", "updated": "2025-08-03T18:26:50.065+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011758", "id": "18011758", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250346945\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java:\n##########\n@@ -128,6 +128,7 @@ public final class AbfsHttpConstants {\n   public static final String STAR = \"*\";\n   public static final String COMMA = \",\";\n   public static final String COLON = \":\";\n+  public static final String HYPHEN = \"-\";\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +286,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:02:31.698+0000", "updated": "2025-08-04T04:02:31.698+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011759", "id": "18011759", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250347466\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n+  }\n+\n+  public int getFieldCount() {\n+    return V1.fieldCount;\n\nReview Comment:\n   Fixed, Thanks for pointing out\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n+  }\n+\n+  public int getFieldCount() {\n+    return V1.fieldCount;\n+  }\n+\n+  public String getVersion() {\n+    return V1.version;\n\nReview Comment:\n   Fixed\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n\nReview Comment:\n   Added\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:02:56.709+0000", "updated": "2025-08-04T04:02:56.709+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011760", "id": "18011760", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250348114\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -442,6 +451,7 @@ private int optimisedRead(final byte[] b, final int off, final int len,\n     //  bCursor that means the user requested data has not been read.\n     if (fCursor < contentLength && bCursor > limit) {\n       restorePointerState();\n+      tracingContext.setReadType(ReadType.NORMAL_READ);\n\nReview Comment:\n   Nice Catch, that seemed redundant, hence removed\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:03:40.747+0000", "updated": "2025-08-04T04:03:40.747+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011761", "id": "18011761", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250348323\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n+          + position + COLON\n+          + operatedBlobCount + COLON\n+          + httpOperation.getTracingContextSuffix() + COLON\n+          + getOperationSpecificHeader(opType);\n\nReview Comment:\n   Sounds Better, Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -265,6 +286,34 @@ private String addFailureReasons(final String header,\n     return String.format(\"%s_%s\", header, previousFailure);\n   }\n \n+  private String getRetryHeader(final String previousFailure, String retryPolicyAbbreviation) {\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:03:55.075+0000", "updated": "2025-08-04T04:03:55.075+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011762", "id": "18011762", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250349750\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n\nReview Comment:\n   We will have simple version strings like v0, v1, v2 and so on. This will help reduce char count in clientReqId.\r\n   \r\n   With any new changes in the schema of Tracing Header (add/delete/rearrange) we need to bump up version and update the schema and getCurrentVersion method to return the latest version.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Footer Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(false).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(true).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Small File Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(true).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(false).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls);\n+  }\n+\n+  private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs, int fileSize, ReadType readType, int numOfReadCalls) throws Exception {\n+    Path testPath = new Path(\"testFile\");\n+    byte[] fileContent = getRandomBytesArray(fileSize);\n+    try (FSDataOutputStream oStream = fs.create(testPath)) {\n+      oStream.write(fileContent);\n+      oStream.flush();\n+    }\n+    try (FSDataInputStream iStream = fs.open(testPath)) {\n+      int bytesRead = iStream.read(new byte[fileSize], 0,\n+          fileSize);\n+      Assertions.assertThat(fileSize)\n+          .describedAs(\"Read size should match file size\")\n+          .isEqualTo(bytesRead);\n+    }\n+\n+    ArgumentCaptor<String> captor1 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<Long> captor2 = ArgumentCaptor.forClass(Long.class);\n+    ArgumentCaptor<byte[]> captor3 = ArgumentCaptor.forClass(byte[].class);\n+    ArgumentCaptor<Integer> captor4 = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<Integer> captor5 = ArgumentCaptor.forClass(Integer.class);\n+    ArgumentCaptor<String> captor6 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<String> captor7 = ArgumentCaptor.forClass(String.class);\n+    ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class);\n+    ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class);\n+\n+    verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read(\n+        captor1.capture(), captor2.capture(), captor3.capture(),\n+        captor4.capture(), captor5.capture(), captor6.capture(),\n+        captor7.capture(), captor8.capture(), captor9.capture());\n+    TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1);\n+    verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType);\n+  }\n+\n+  private void verifyHeaderForReadTypeInTracingContextHeader(TracingContext tracingContext, ReadType readType) {\n+    AbfsHttpOperation mockOp = Mockito.mock(AbfsHttpOperation.class);\n+    doReturn(EMPTY_STRING).when(mockOp).getTracingContextSuffix();\n+    tracingContext.constructHeader(mockOp, null, null);\n+    String[] idList = tracingContext.getHeader().split(COLON, SPLIT_NO_LIMIT);\n+    Assertions.assertThat(idList).describedAs(\"Client Request Id should have all fields\").hasSize(\n+        TracingHeaderVersion.getCurrentVersion().getFieldCount());\n+    Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Operation Type Should Be Read\")\n+        .contains(FSOperationType.READ.toString());\n+    Assertions.assertThat(tracingContext.getHeader()).describedAs(\"Read type in tracing context header should match\")\n+        .contains(readType.toString());\n+  }\n+\n+//  private testReadTypeInTracingContextHeaderInternal(ReadType readType) throws Exception {\n\nReview Comment:\n   Removed\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:05:56.730+0000", "updated": "2025-08-04T04:05:56.730+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011763", "id": "18011763", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250351378\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -77,8 +81,7 @@ public class TracingContext {\n    * this field shall not be set.\n    */\n   private String primaryRequestIdForRetry;\n-\n-  private Integer operatedBlobCount = null;\n+  private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set.\n\nReview Comment:\n   Because it was coming out as null in ClientReqId. Having a null value does not looks good and can be prone to NPE if someone used this value anywhere.\r\n   Since this is set only in rename/delete other ops are prone to NPE.\r\n   \r\n   As to why set to 1, I thought for every operation this has to be 1. I am open to suggestions for a better default value but strongly feel null should be avoided.\r\n   \r\n   Thoughts?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:08:11.838+0000", "updated": "2025-08-04T04:08:11.838+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011764", "id": "18011764", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250351858\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderVersion.java:\n##########\n@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderVersion {\n+\n+  V0(\"\", 8),\n+  V1(\"v1\", 13);\n+\n+  private final String version;\n+  private final int fieldCount;\n+\n+  TracingHeaderVersion(String version, int fieldCount) {\n+    this.version = version;\n+    this.fieldCount = fieldCount;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return version;\n+  }\n+\n+  public static TracingHeaderVersion getCurrentVersion() {\n+    return V1;\n\nReview Comment:\n   We need to update it to the latest version every time we do a version upgrade.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n\nReview Comment:\n   Fixed\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:08:51.850+0000", "updated": "2025-08-04T04:08:51.850+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011765", "id": "18011765", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353075\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n\nReview Comment:\n   With empty checks we cannot have a fixed schema. We need the proper defined schema where each position after split is fixed for all the headers and analysis can be done easily without worrying about the position of info we need to analyse.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -193,32 +213,33 @@ public void setListener(Listener listener) {\n   public void constructHeader(AbfsHttpOperation httpOperation, String previousFailure, String retryPolicyAbbreviation) {\n     clientRequestId = UUID.randomUUID().toString();\n     switch (format) {\n-    case ALL_ID_FORMAT: // Optional IDs (e.g. streamId) may be empty\n-      header =\n-          clientCorrelationID + \":\" + clientRequestId + \":\" + fileSystemID + \":\"\n-              + getPrimaryRequestIdForHeader(retryCount > 0) + \":\" + streamID\n-              + \":\" + opType + \":\" + retryCount;\n-      header = addFailureReasons(header, previousFailure, retryPolicyAbbreviation);\n-      if (!(ingressHandler.equals(EMPTY_STRING))) {\n-        header += \":\" + ingressHandler;\n-      }\n-      if (!(position.equals(EMPTY_STRING))) {\n-        header += \":\" + position;\n-      }\n-      if (operatedBlobCount != null) {\n-        header += (\":\" + operatedBlobCount);\n-      }\n-      header += (\":\" + httpOperation.getTracingContextSuffix());\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+    case ALL_ID_FORMAT:\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n+          + clientCorrelationID + COLON\n+          + clientRequestId + COLON\n+          + fileSystemID + COLON\n+          + getPrimaryRequestIdForHeader(retryCount > 0) + COLON\n+          + streamID + COLON\n+          + opType + COLON\n+          + getRetryHeader(previousFailure, retryPolicyAbbreviation) + COLON\n+          + ingressHandler + COLON\n+          + position + COLON\n+          + operatedBlobCount + COLON\n+          + httpOperation.getTracingContextSuffix() + COLON\n+          + getOperationSpecificHeader(opType);\n+\n+      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : EMPTY_STRING;\n       break;\n     case TWO_ID_FORMAT:\n-      header = clientCorrelationID + \":\" + clientRequestId;\n-      metricHeader += !(metricResults.trim().isEmpty()) ? metricResults  : \"\";\n+      header = TracingHeaderVersion.V1.getVersion() + COLON\n\nReview Comment:\n   Fixed\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:10:26.878+0000", "updated": "2025-08-04T04:10:26.878+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011766", "id": "18011766", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353287\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestTracingContext.java:\n##########\n@@ -326,8 +329,8 @@ fileSystemId, FSOperationType.CREATE_FILESYSTEM, tracingHeaderFormat, new Tracin\n   }\n \n   private void checkHeaderForRetryPolicyAbbreviation(String header, String expectedFailureReason, String expectedRetryPolicyAbbreviation) {\n-    String[] headerContents = header.split(\":\");\n-    String previousReqContext = headerContents[6];\n+    String[] headerContents = header.split(\":\", SPLIT_NO_LIMIT);\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:10:41.885+0000", "updated": "2025-08-04T04:10:41.885+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011767", "id": "18011767", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353532\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -781,6 +794,132 @@ public void testDefaultReadaheadQueueDepth() throws Exception {\n     in.close();\n   }\n \n+  @Test\n+  public void testReadTypeInTracingContextHeader() throws Exception {\n+    AzureBlobFileSystem spiedFs = Mockito.spy(getFileSystem());\n+    AzureBlobFileSystemStore spiedStore = Mockito.spy(spiedFs.getAbfsStore());\n+    AbfsConfiguration spiedConfig = Mockito.spy(spiedStore.getAbfsConfiguration());\n+    AbfsClient spiedClient = Mockito.spy(spiedStore.getClient());\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadBufferSize();\n+    Mockito.doReturn(ONE_MB).when(spiedConfig).getReadAheadBlockSize();\n+    Mockito.doReturn(spiedClient).when(spiedStore).getClient();\n+    Mockito.doReturn(spiedStore).when(spiedFs).getAbfsStore();\n+    Mockito.doReturn(spiedConfig).when(spiedStore).getAbfsConfiguration();\n+    int numOfReadCalls = 0;\n+    int fileSize = 0;\n+\n+    /*\n+     * Test to verify Normal Read Type.\n+     * Disabling read ahead ensures that read type is normal read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3; // 3 blocks of 1MB each.\n+    doReturn(false).when(spiedConfig).isReadAheadV2Enabled();\n+    doReturn(false).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, NORMAL_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Missed Cache Read Type.\n+     * Setting read ahead depth to 0 ensure that nothing can be got from prefetch.\n+     * In such a case Input Stream will do a sequential read with missed cache read type.\n+     */\n+    fileSize = ONE_MB; // To make sure only one block is read.\n+    numOfReadCalls += 1; // 1 block of 1MB.\n+    Mockito.doReturn(0).when(spiedConfig).getReadAheadQueueDepth();\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, MISSEDCACHE_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Prefetch Read Type.\n+     * Setting read ahead depth to 2 with prefetch enabled ensures that prefetch is done.\n+     * First read here might be Normal or Missed Cache but the rest 2 should be Prefetched Read.\n+     */\n+    fileSize = 3 * ONE_MB; // To make sure multiple blocks are read.\n+    numOfReadCalls += 3;\n+    doReturn(true).when(spiedConfig).isReadAheadEnabled();\n+    Mockito.doReturn(3).when(spiedConfig).getReadAheadQueueDepth();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, PREFETCH_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Footer Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(false).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(true).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, FOOTER_READ, numOfReadCalls);\n+\n+    /*\n+     * Test to verify Small File Read Type.\n+     * Having file size less than footer read size and disabling small file opt\n+     */\n+    fileSize = 8 * ONE_KB;\n+    numOfReadCalls += 1; // Full file will be read along with footer.\n+    doReturn(true).when(spiedConfig).readSmallFilesCompletely();\n+    doReturn(false).when(spiedConfig).optimizeFooterRead();\n+    testReadTypeInTracingContextHeaderInternal(spiedFs, fileSize, SMALLFILE_READ, numOfReadCalls);\n+  }\n\nReview Comment:\n   Added\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:10:56.890+0000", "updated": "2025-08-04T04:10:56.890+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011768", "id": "18011768", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250353998\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -578,6 +591,7 @@ int readRemote(long position, byte[] b, int offset, int length, TracingContext t\n         streamStatistics.remoteReadOperation();\n       }\n       LOG.trace(\"Trigger client.read for path={} position={} offset={} length={}\", path, position, offset, length);\n+      tracingContext.setPosition(String.valueOf(position));\n\nReview Comment:\n   Thanks for the suggestion. I updated the current test to assert on position as well.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T04:11:26.906+0000", "updated": "2025-08-04T04:11:26.906+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011777", "id": "18011777", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3149195033\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 52s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  79m  1s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5a4dce188fdc 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6345ec99eeb23cd09b5d7197e777b1dec23a35e0 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/7/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T05:40:57.724+0000", "updated": "2025-08-04T05:40:57.724+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011811", "id": "18011811", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250858558\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -77,8 +81,7 @@ public class TracingContext {\n    * this field shall not be set.\n    */\n   private String primaryRequestIdForRetry;\n-\n-  private Integer operatedBlobCount = null;\n+  private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set.\n\nReview Comment:\n   But there was a null check before it was added to the header which would avoid the NPE\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T08:59:29.953+0000", "updated": "2025-08-04T08:59:29.953+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011813", "id": "18011813", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250885278\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -886,40 +928,54 @@ private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs,\n     ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class);\n     ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class);\n \n-    verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read(\n+    verify(fs.getAbfsStore().getClient(), times(totalReadCalls)).read(\n         captor1.capture(), captor2.capture(), captor3.capture(),\n         captor4.capture(), captor5.capture(), captor6.capture(),\n         captor7.capture(), captor8.capture(), captor9.capture());\n-    TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1);\n-    verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType);\n+    List<TracingContext> tracingContextList = captor9.getAllValues();\n+    if (readType == PREFETCH_READ) {\n+      /*\n+       * For Prefetch Enabled, first read can be Normal or Missed Cache Read.\n+       * Sow e will assert only for last 2 calls which should be Prefetched Read.\n\nReview Comment:\n   nit typo: so\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T09:10:44.693+0000", "updated": "2025-08-04T09:10:44.693+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011828", "id": "18011828", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2250972747\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderValidator.java:\n##########\n@@ -206,7 +207,7 @@ public void updateReadType(ReadType readType) {\n   }\n \n   /**\n-   * Sets the value of the number of blobs operated on\n+   * Sets the value of the number of blobs operated on976345\n\nReview Comment:\n   typo issue\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -886,40 +928,54 @@ private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs,\n     ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class);\n     ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class);\n \n-    verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read(\n+    verify(fs.getAbfsStore().getClient(), times(totalReadCalls)).read(\n         captor1.capture(), captor2.capture(), captor3.capture(),\n         captor4.capture(), captor5.capture(), captor6.capture(),\n         captor7.capture(), captor8.capture(), captor9.capture());\n-    TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1);\n-    verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType);\n+    List<TracingContext> tracingContextList = captor9.getAllValues();\n+    if (readType == PREFETCH_READ) {\n+      /*\n+       * For Prefetch Enabled, first read can be Normal or Missed Cache Read.\n+       * Sow e will assert only for last 2 calls which should be Prefetched Read.\n+       * Since calls are asynchronous, we can not guarantee the order of calls.\n+       * Therefore, we cannot assert on exact position here.\n+       */\n+      for (int i = tracingContextList.size() - (numOfReadCalls - 1); i < tracingContextList.size(); i++) {\n+        verifyHeaderForReadTypeInTracingContextHeader(tracingContextList.get(i), readType, -1);\n+      }\n+    } else if (readType == DIRECT_READ) {\n+      int expectedReadPos = ONE_MB/3;\n\nReview Comment:\n   comment for why are we starting with this position will help in clarity\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestApacheHttpClientFallback.java:\n##########\n@@ -61,15 +61,15 @@ private TracingContext getSampleTracingContext(int[] jdkCallsRegister,\n           answer.callRealMethod();\n           AbfsHttpOperation op = answer.getArgument(0);\n           if (op instanceof AbfsAHCHttpOperation) {\n-            Assertions.assertThat(tc.getHeader()).contains(APACHE_IMPL);\n+            Assertions.assertThat(tc.getHeader()).endsWith(APACHE_IMPL);\n             apacheCallsRegister[0]++;\n           }\n           if (op instanceof AbfsJdkHttpOperation) {\n             jdkCallsRegister[0]++;\n             if (AbfsApacheHttpClient.usable()) {\n-              Assertions.assertThat(tc.getHeader()).contains(JDK_IMPL);\n+              Assertions.assertThat(tc.getHeader()).endsWith(JDK_IMPL);\n\nReview Comment:\n   this might fail if we add new header where the network library is not maintained as the last header, so contains looks better to me\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T09:52:41.909+0000", "updated": "2025-08-04T09:52:41.909+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011844", "id": "18011844", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2251145110\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -64,9 +67,10 @@ public class TracingContext {\n   //final concatenated ID list set into x-ms-client-request-id header\n   private String header = EMPTY_STRING;\n   private String ingressHandler = EMPTY_STRING;\n-  private String position = EMPTY_STRING;\n+  private String position = String.valueOf(0); // position of read/write in remote file\n\nReview Comment:\n   Any reason we are changing this default value?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T11:07:17.366+0000", "updated": "2025-08-04T11:07:17.366+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011874", "id": "18011874", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2251498573\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -77,8 +81,7 @@ public class TracingContext {\n    * this field shall not be set.\n    */\n   private String primaryRequestIdForRetry;\n-\n-  private Integer operatedBlobCount = null;\n+  private Integer operatedBlobCount = 1; // Only relevant for rename-delete over blob endpoint where it will be explicitly set.\n\nReview Comment:\n   Yes but we decided to keep the header schema fix and publishing this value as null does not look good in Client Request Id as it can be exposed to user.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T13:28:58.399+0000", "updated": "2025-08-04T13:28:58.399+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011875", "id": "18011875", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2251499536\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java:\n##########\n@@ -64,9 +67,10 @@ public class TracingContext {\n   //final concatenated ID list set into x-ms-client-request-id header\n   private String header = EMPTY_STRING;\n   private String ingressHandler = EMPTY_STRING;\n-  private String position = EMPTY_STRING;\n+  private String position = String.valueOf(0); // position of read/write in remote file\n\nReview Comment:\n   No reason, will revert.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T13:29:18.399+0000", "updated": "2025-08-04T13:29:18.399+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011877", "id": "18011877", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#discussion_r2251505718\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -886,40 +928,54 @@ private void testReadTypeInTracingContextHeaderInternal(AzureBlobFileSystem fs,\n     ArgumentCaptor<ContextEncryptionAdapter> captor8 = ArgumentCaptor.forClass(ContextEncryptionAdapter.class);\n     ArgumentCaptor<TracingContext> captor9 = ArgumentCaptor.forClass(TracingContext.class);\n \n-    verify(fs.getAbfsStore().getClient(), times(numOfReadCalls)).read(\n+    verify(fs.getAbfsStore().getClient(), times(totalReadCalls)).read(\n         captor1.capture(), captor2.capture(), captor3.capture(),\n         captor4.capture(), captor5.capture(), captor6.capture(),\n         captor7.capture(), captor8.capture(), captor9.capture());\n-    TracingContext tracingContext = captor9.getAllValues().get(numOfReadCalls - 1);\n-    verifyHeaderForReadTypeInTracingContextHeader(tracingContext, readType);\n+    List<TracingContext> tracingContextList = captor9.getAllValues();\n+    if (readType == PREFETCH_READ) {\n+      /*\n+       * For Prefetch Enabled, first read can be Normal or Missed Cache Read.\n+       * Sow e will assert only for last 2 calls which should be Prefetched Read.\n+       * Since calls are asynchronous, we can not guarantee the order of calls.\n+       * Therefore, we cannot assert on exact position here.\n+       */\n+      for (int i = tracingContextList.size() - (numOfReadCalls - 1); i < tracingContextList.size(); i++) {\n+        verifyHeaderForReadTypeInTracingContextHeader(tracingContextList.get(i), readType, -1);\n+      }\n+    } else if (readType == DIRECT_READ) {\n+      int expectedReadPos = ONE_MB/3;\n\nReview Comment:\n   Already added in comment above.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T13:30:48.542+0000", "updated": "2025-08-04T13:30:48.542+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011903", "id": "18011903", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3151118542\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 36s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  79m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2f9b7814e558 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9059784765d6d3519f9649968b07fd94ef9798e8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/8/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T14:53:23.296+0000", "updated": "2025-08-04T14:53:23.296+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18011950", "id": "18011950", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837#issuecomment-3152193026\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 51s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 56s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 142m  5s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7837 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3280cde505cb 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c787d3b39af6bacd3d9a003fe3b6f81ccc10e194 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/9/testReport/ |\r\n   | Max. process+thread count | 536 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7837/9/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T20:09:47.921+0000", "updated": "2025-08-04T20:09:47.921+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18012062", "id": "18012062", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7837:\nURL: https://github.com/apache/hadoop/pull/7837\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-05T07:54:08.465+0000", "updated": "2025-08-05T07:54:08.465+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18014684", "id": "18014684", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 opened a new pull request, #7881:\nURL: https://github.com/apache/hadoop/pull/7881\n\n   ### Description of PR\r\n   Backport for 3.4\r\n   Jira: https://issues.apache.org/jira/browse/HADOOP-19645\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-18T17:44:36.905+0000", "updated": "2025-08-18T17:44:36.905+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18014685", "id": "18014685", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on PR #7881:\nURL: https://github.com/apache/hadoop/pull/7881#issuecomment-3197856944\n\n   ------------------------------\r\n   :::: AGGREGATED TEST RESULT ::::\r\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 32\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 172\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 32\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 395\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 33\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 234\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 56\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 285\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 27\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 400\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 33\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 301\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 27\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 346\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 51\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 876, Failures: 0, Errors: 0, Skipped: 356\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 32\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 860, Failures: 0, Errors: 0, Skipped: 397\r\n   [WARNING] Tests run: 182, Failures: 0, Errors: 0, Skipped: 33\r\n   [WARNING] Tests run: 274, Failures: 0, Errors: 0, Skipped: 24\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-18T17:45:06.942+0000", "updated": "2025-08-18T17:45:06.942+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18014720", "id": "18014720", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7881:\nURL: https://github.com/apache/hadoop/pull/7881#issuecomment-3198211215\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  12m 11s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 7 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 33s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  branch-3.4 passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  branch-3.4 passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  32m 30s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  32m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 27s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 127m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7881/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7881 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux da6887b03eda 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / dc1821731086534899f15dc2abcc2d9bf2c61ae4 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7881/1/testReport/ |\r\n   | Max. process+thread count | 728 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7881/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-18T19:53:28.610+0000", "updated": "2025-08-18T19:53:28.610+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13624892/comment/18014790", "id": "18014790", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7881:\nURL: https://github.com/apache/hadoop/pull/7881\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-19T04:58:38.454+0000", "updated": "2025-08-19T04:58:38.454+0000"}], "maxResults": 57, "total": 57, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/5", "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png", "name": "Resolved", "id": "5", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3", "id": 3, "key": "done", "colorName": "green", "name": "Done"}}}}