{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13631304", "self": "https://issues.apache.org/jira/rest/api/2/issue/13631304", "key": "SPARK-53878", "fields": {"summary": "Fix race condition issue related to ObservedMetrics", "description": "In Spark Connect environment, QueryExecution#observedMetrics can be called by two threads concurrently.\r\n\r\n\u00a0\r\n\r\nThread1(ObservationManager)\r\n{code:java}\r\nprivate def tryComplete(qe: QueryExecution): Unit = {\r\n  val allMetrics = qe.observedMetrics\r\n  qe.logical.foreach {\r\n    case c: CollectMetrics =>\r\n      allMetrics.get(c.name).foreach { metrics =>\r\n        val observation = observations.remove((c.name, c.dataframeId))\r\n        if (observation != null) {\r\n          observation.setMetricsAndNotify(metrics)\r\n        }\r\n      }\r\n    case _ =>\r\n  }\r\n}\r\n{code}\r\nThread2(SparkConnectPlanExecution)\r\n{code:java}\r\nprivate def createObservedMetricsResponse(\r\n    sessionId: String,\r\n    observationAndPlanIds: Map[String, Long],\r\n    dataframe: DataFrame): Option[ExecutePlanResponse] = {\r\n  val observedMetrics = dataframe.queryExecution.observedMetrics.collect {\r\n    case (name, row) if !executeHolder.observations.contains(name) =>\r\n      val values = SparkConnectPlanExecution.toObservedMetricsValues(row)\r\n      name -> values\r\n  } {code}\r\n\r\nThis can cause race condition issues. We can see CI failure caused by this issue.\r\nhttps://github.com/apache/spark/actions/runs/18422173471/job/52497913985\r\n\r\n{code}\r\n======================================================================\r\nERROR [0.181s]: test_observe_with_map_type (pyspark.sql.tests.connect.test_parity_observation.DataFrameObservationParityTests.test_observe_with_map_type)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/testing/utils.py\", line 228, in wrapper\r\n    lastValue = condition(*args, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_observation.py\", line 226, in test_observe_with_map_type\r\n    assertDataFrameEqual(df, [Row(id=id) for id in range(10)])\r\n  File \"/__w/spark/spark/python/pyspark/testing/utils.py\", line 1098, in assertDataFrameEqual\r\n    actual_list = actual.collect()\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/dataframe.py\", line 1817, in collect\r\n    table, schema = self._to_table()\r\n                    ^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/dataframe.py\", line 1830, in _to_table\r\n    table, schema, self._execution_info = self._session.client.to_table(\r\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 946, in to_table\r\n    table, schema, metrics, observed_metrics, _ = self._execute_and_fetch(req, observations)\r\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1642, in _execute_and_fetch\r\n    for response in self._execute_and_fetch_as_iterator(\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1619, in _execute_and_fetch_as_iterator\r\n    self._handle_error(error)\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1893, in _handle_error\r\n    self._handle_rpc_error(error)\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 1966, in _handle_rpc_error\r\n    raise convert_exception(\r\npyspark.errors.exceptions.connect.IllegalArgumentException: requirement failed\r\n\r\nJVM stacktrace:\r\njava.lang.IllegalArgumentException\r\n\tat scala.Predef$.require(Predef.scala:324)\r\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapData.<init>(ArrayBasedMapData.scala:31)\r\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.build(ArrayBasedMapBuilder.scala:130)\r\n\tat org.apache.spark.sql.catalyst.expressions.CreateMap.eval(complexTypeCreator.scala:260)\r\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:162)\r\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedUnsafeProjection.apply(InterpretedUnsafeProjection.scala:84)\r\n\tat org.apache.spark.sql.execution.AggregatingAccumulator.$anonfun$value$2(AggregatingAccumulator.scala:199)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:162)\r\n\tat org.apache.spark.sql.execution.AggregatingAccumulator.withSQLConf(AggregatingAccumulator.scala:106)\r\n\tat org.apache.spark.sql.execution.AggregatingAccumulator.value(AggregatingAccumulator.scala:188)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec.collectedMetrics(CollectMetricsExec.scala:59)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$$anonfun$1.applyOrElse(CollectMetricsExec.scala:111)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$$anonfun$1.applyOrElse(CollectMetricsExec.scala:109)\r\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:338)\r\n\tat scala.PartialFunction$Lifted.apply(PartialFunction.scala:334)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.$anonfun$collect$1(AdaptiveSparkPlanHelper.scala:86)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.$anonfun$collect$1$adapted(AdaptiveSparkPlanHelper.scala:86)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.foreach(AdaptiveSparkPlanHelper.scala:45)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.foreach$(AdaptiveSparkPlanHelper.scala:44)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.foreach(CollectMetricsExec.scala:101)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collect(AdaptiveSparkPlanHelper.scala:86)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collect$(AdaptiveSparkPlanHelper.scala:83)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.collect(CollectMetricsExec.scala:101)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.$anonfun$collectWithSubqueries$1(AdaptiveSparkPlanHelper.scala:113)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:294)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:79)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collectWithSubqueries(AdaptiveSparkPlanHelper.scala:113)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper.collectWithSubqueries$(AdaptiveSparkPlanHelper.scala:112)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.collectWithSubqueries(CollectMetricsExec.scala:101)\r\n\tat org.apache.spark.sql.execution.CollectMetricsExec$.collect(CollectMetricsExec.scala:109)\r\n\tat org.apache.spark.sql.execution.QueryExecution.observedMetrics(QueryExecution.scala:276)\r\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.createObservedMetricsResponse(SparkConnectPlanExecution.scala:322)\r\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:82)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\r\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:394)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:394)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:113)\r\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:184)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:103)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:393)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\r\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:333)\r\n{code}", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=sarutak", "name": "sarutak", "key": "sarutak", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=sarutak&avatarId=20842", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sarutak&avatarId=20842", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sarutak&avatarId=20842", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sarutak&avatarId=20842"}, "displayName": "Kousuke Saruta", "active": true, "timeZone": "Asia/Tokyo"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631304/comment/18029606", "id": "18029606", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "body": "Issue resolved by pull request 52575\n[https://github.com/apache/spark/pull/52575]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=dongjoon", "name": "dongjoon", "key": "dongjoon", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=dongjoon&avatarId=42503", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dongjoon&avatarId=42503", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dongjoon&avatarId=42503", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dongjoon&avatarId=42503"}, "displayName": "Dongjoon Hyun", "active": true, "timeZone": "America/Los_Angeles"}, "created": "2025-10-13T20:24:53.973+0000", "updated": "2025-10-13T20:24:53.973+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/5", "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png", "name": "Resolved", "id": "5", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3", "id": 3, "key": "done", "colorName": "green", "name": "Done"}}}}