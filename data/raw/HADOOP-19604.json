{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13622233", "self": "https://issues.apache.org/jira/rest/api/2/issue/13622233", "key": "HADOOP-19604", "fields": {"summary": "ABFS: Fix WASB ABFS compatibility issues", "description": "Fix WASB ABFS compatibility issues. Fix issues such as:-\r\n # BlockId computation to be consistent across clients for PutBlock and PutBlockList\r\n # Restrict url encoding of certain json metadata during setXAttr calls.\r\n # Maintain the md5 hash of whole block to validate data integrity during flush.", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=asrani_anmol", "name": "asrani_anmol", "key": "JIRAUSER281089", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34046", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"}, "displayName": "Anmol Asrani", "active": true, "timeZone": "Etc/UTC"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/17992438", "id": "17992438", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 opened a new pull request, #7777:\nURL: https://github.com/apache/hadoop/pull/7777\n\n   Jira :- https://issues.apache.org/jira/browse/HADOOP-19604\r\n   \r\n   1. BlockId computation to be consistent across clients for PutBlock and PutBlockList so made use of blockCount instead of offset.\r\n   Block IDs were previously derived from the data offset, which could lead to inconsistency across different clients. The change now uses blockCount (i.e., the index of the block) to compute the Block ID, ensuring deterministic and consistent ID generation for both PutBlock and PutBlockList operations across clients.\r\n   \r\n   2. Restrict URL encoding of certain JSON metadata during setXAttr calls.\r\n   When setting extended attributes (xAttrs), the JSON metadata (hdi_permission) was previously URL-encoded, which could cause unnecessary escaping or compatibility issues. This change ensures that only required metadata are encoded.\r\n   \r\n   3. Maintain the MD5 hash of the whole block to validate data integrity during flush.\r\n   During flush operations, the MD5 hash of the entire block's data is computed and stored. This hash is later used to validate that the block correctly persisted, ensuring data integrity and helping detect corruption or transmission errors.\r\n   \r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T08:37:21.386+0000", "updated": "2025-07-03T08:37:21.386+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/17993047", "id": "17993047", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3031894608\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 12 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 47s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 16 unchanged - 0 fixed = 20 total (was 16)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 31s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 4 new + 10 unchanged - 0 fixed = 14 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 4 new + 10 unchanged - 0 fixed = 14 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 46s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 34s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2e8591fd6bca 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6b138c098a45024cbab113550c7130911a21df41 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/testReport/ |\r\n   | Max. process+thread count | 528 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T11:18:06.910+0000", "updated": "2025-07-03T11:18:06.910+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/17993058", "id": "17993058", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3031953572\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  25m 18s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  1s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 23s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 15 unchanged - 0 fixed = 19 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 4 new + 10 unchanged - 0 fixed = 14 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 26s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 4 new + 10 unchanged - 0 fixed = 14 total (was 10)  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 163m 59s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 15ce74eb5c22 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8292192be43adff87a99987e83f64455e9d42207 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T11:41:17.867+0000", "updated": "2025-07-03T11:41:17.867+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18003810", "id": "18003810", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2192701461\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1914,7 +1924,9 @@ private List<AbfsHttpHeader> getMetadataHeadersList(final Hashtable<String, Stri\n       // AzureBlobFileSystem supports only ASCII Characters in property values.\n       if (isPureASCII(value)) {\n         try {\n-          value = encodeMetadataAttribute(value);\n+          if (!XML_TAG_HDI_PERMISSION.equalsIgnoreCase(entry.getKey())) {\n\nReview Comment:\n   Seems like we want to encode all other headers except XML_TAG_HDI_PERMISSION.\r\n   Can we add a comment around this explaining the reason?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1097,8 +1105,10 @@ public AbfsRestOperation flush(byte[] buffer,\n         AbfsRestOperation op1 = getPathStatus(path, true, tracingContext,\n             contextEncryptionAdapter);\n         String metadataMd5 = op1.getResult().getResponseHeader(CONTENT_MD5);\n-        if (!md5Hash.equals(metadataMd5)) {\n-          throw ex;\n+        if (blobMd5 != null) {\n\nReview Comment:\n   Nit: combine if statements using &&\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n\nReview Comment:\n   Nit: outputstream is passed as a parameter to this constructor. I think its better to use that parameter directly. Calling a getter for this is giving an impression as if some other input stream is called upon.\r\n   \r\n   Beside that getter is not used any where else.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n\nReview Comment:\n   Nit: streamId and streamIdGuid are local variables and used only for generating block Ids. They should be moved inside that method only as it was previously.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n+    this.blockId = generateBlockId(streamIdGuid, blockIdLength);\n   }\n \n   /**\n-   * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n-   * @return String representing the block ID generated.\n+   * Generates a Base64-encoded block ID string based on the given position, stream ID, and desired raw length.\n\nReview Comment:\n   How did we arrive at this logic?\r\n   Is there some server side recommendation to follow this?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -982,6 +985,11 @@ public AbfsRestOperation appendBlock(final String path,\n     if (requestParameters.getLeaseId() != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, requestParameters.getLeaseId()));\n     }\n+    if (isChecksumValidationEnabled()) {\n+      if (requestParameters.getMd5() != null) {\n\nReview Comment:\n   This check on reqParams and adding of headers is repeating. May be we can retain the original method and update its implementation as needed now.\r\n   That method anyway wasn't used anywhere else as of now.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n+    this.blockId = generateBlockId(streamIdGuid, blockIdLength);\n   }\n \n   /**\n-   * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n-   * @return String representing the block ID generated.\n+   * Generates a Base64-encoded block ID string based on the given position, stream ID, and desired raw length.\n+   * The block ID is composed using the stream UUID and the block index, which is derived from\n+   * the given position divided by the output stream's buffer size. The resulting string is\n+   * optionally adjusted to match the specified raw length, padded or trimmed as needed, and\n+   * then Base64-encoded.\n+   *\n+   * @param streamId   The UUID representing the stream, used as a prefix in the block ID.\n+   * @param rawLength  The desired length of the raw block ID string before Base64 encoding.\n+   *                   If 0, no length adjustment is made.\n+   * @return A Base64-encoded block ID string suitable for use in block-based storage APIs.\n    */\n-  private String generateBlockId(long position) {\n-    String streamId = getOutputStream().getStreamID();\n-    String streamIdHash = Integer.toString(streamId.hashCode());\n-    String blockId = String.format(\"%d_%s\", position, streamIdHash);\n-    byte[] blockIdByteArray = new byte[BLOCK_ID_LENGTH];\n-    System.arraycopy(blockId.getBytes(StandardCharsets.UTF_8), 0, blockIdByteArray, 0, Math.min(BLOCK_ID_LENGTH, blockId.length()));\n-    return new String(Base64.encodeBase64(blockIdByteArray), StandardCharsets.UTF_8);\n+  private String generateBlockId(UUID streamId, int rawLength) {\n+    String rawBlockId = String.format(\"%s-%06d\", streamId, blockIndex);\n+\n+    if (rawLength != 0) {\n+      // Adjust to match expected decoded length\n+      if (rawBlockId.length() < rawLength) {\n+        rawBlockId = String.format(\"%-\" + rawLength + \"s\", rawBlockId)\n\nReview Comment:\n   Nit: use constants everywhere there is a hardcoded string.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-08T14:42:05.762+0000", "updated": "2025-07-08T14:42:05.762+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18003963", "id": "18003963", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2193890805\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n+    this.blockId = generateBlockId(streamIdGuid, blockIdLength);\n   }\n \n   /**\n-   * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n-   * @return String representing the block ID generated.\n+   * Generates a Base64-encoded block ID string based on the given position, stream ID, and desired raw length.\n+   * The block ID is composed using the stream UUID and the block index, which is derived from\n+   * the given position divided by the output stream's buffer size. The resulting string is\n+   * optionally adjusted to match the specified raw length, padded or trimmed as needed, and\n+   * then Base64-encoded.\n+   *\n+   * @param streamId   The UUID representing the stream, used as a prefix in the block ID.\n+   * @param rawLength  The desired length of the raw block ID string before Base64 encoding.\n+   *                   If 0, no length adjustment is made.\n+   * @return A Base64-encoded block ID string suitable for use in block-based storage APIs.\n    */\n-  private String generateBlockId(long position) {\n-    String streamId = getOutputStream().getStreamID();\n-    String streamIdHash = Integer.toString(streamId.hashCode());\n-    String blockId = String.format(\"%d_%s\", position, streamIdHash);\n-    byte[] blockIdByteArray = new byte[BLOCK_ID_LENGTH];\n-    System.arraycopy(blockId.getBytes(StandardCharsets.UTF_8), 0, blockIdByteArray, 0, Math.min(BLOCK_ID_LENGTH, blockId.length()));\n-    return new String(Base64.encodeBase64(blockIdByteArray), StandardCharsets.UTF_8);\n+  private String generateBlockId(UUID streamId, int rawLength) {\n+    String rawBlockId = String.format(\"%s-%06d\", streamId, blockIndex);\n+\n+    if (rawLength != 0) {\n+      // Adjust to match expected decoded length\n+      if (rawBlockId.length() < rawLength) {\n+        rawBlockId = String.format(\"%-\" + rawLength + \"s\", rawBlockId)\n+            .replace(' ', '_');\n+      } else if (rawBlockId.length() > rawLength) {\n+        rawBlockId = rawBlockId.substring(0, rawLength);\n\nReview Comment:\n   should we use ternary logic here?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T02:43:31.046+0000", "updated": "2025-07-09T02:43:31.046+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18003999", "id": "18003999", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194294234\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureDFSIngressHandler.java:\n##########\n@@ -178,13 +181,35 @@ protected synchronized AbfsRestOperation remoteFlush(final long offset,\n       tracingContextFlush.setIngressHandler(DFS_FLUSH);\n       tracingContextFlush.setPosition(String.valueOf(offset));\n     }\n+    byte[] digest = null;\n+    String fullBlobMd5 = null;\n+    try {\n+      // Clone the MessageDigest to avoid resetting the original state\n+      MessageDigest clonedMd5 = (MessageDigest) getAbfsOutputStream().getFullBlobContentMd5().clone();\n+      digest = clonedMd5.digest();\n+    } catch (CloneNotSupportedException e) {\n+      LOG.warn(\"Failed to clone MessageDigest instance\", e);\n+    }\n+    if (digest != null && digest.length != 0) {\n+      fullBlobMd5 = Base64.getEncoder().encodeToString(digest);\n+    }\n\nReview Comment:\n   Since this code is common in both DFS and Blob ingress handler class, maybe we can add it as a protected helper method in the abstract class AzureIngressHandler?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T07:37:17.080+0000", "updated": "2025-07-09T07:37:17.080+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004072", "id": "18004072", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194576349\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureBlockManager.java:\n##########\n@@ -134,7 +134,7 @@ protected long getBlockCount() {\n    *\n    * @param blockCount the count of blocks to set\n    */\n-  public void setBlockCount(final long blockCount) {\n+  protected void setBlockCount(final long blockCount) {\n\nReview Comment:\n   why this change?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T09:49:34.085+0000", "updated": "2025-07-09T09:49:34.085+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004087", "id": "18004087", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194669740\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n\nReview Comment:\n   Since this is common across all the blocks, we don't want the computation to be done repeatedly hence have put it in the constructor\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T10:32:29.131+0000", "updated": "2025-07-09T10:32:29.131+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004088", "id": "18004088", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194680208\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n+    this.blockId = generateBlockId(streamIdGuid, blockIdLength);\n   }\n \n   /**\n-   * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n-   * @return String representing the block ID generated.\n+   * Generates a Base64-encoded block ID string based on the given position, stream ID, and desired raw length.\n+   * The block ID is composed using the stream UUID and the block index, which is derived from\n+   * the given position divided by the output stream's buffer size. The resulting string is\n+   * optionally adjusted to match the specified raw length, padded or trimmed as needed, and\n+   * then Base64-encoded.\n+   *\n+   * @param streamId   The UUID representing the stream, used as a prefix in the block ID.\n+   * @param rawLength  The desired length of the raw block ID string before Base64 encoding.\n+   *                   If 0, no length adjustment is made.\n+   * @return A Base64-encoded block ID string suitable for use in block-based storage APIs.\n    */\n-  private String generateBlockId(long position) {\n-    String streamId = getOutputStream().getStreamID();\n-    String streamIdHash = Integer.toString(streamId.hashCode());\n-    String blockId = String.format(\"%d_%s\", position, streamIdHash);\n-    byte[] blockIdByteArray = new byte[BLOCK_ID_LENGTH];\n-    System.arraycopy(blockId.getBytes(StandardCharsets.UTF_8), 0, blockIdByteArray, 0, Math.min(BLOCK_ID_LENGTH, blockId.length()));\n-    return new String(Base64.encodeBase64(blockIdByteArray), StandardCharsets.UTF_8);\n+  private String generateBlockId(UUID streamId, int rawLength) {\n+    String rawBlockId = String.format(\"%s-%06d\", streamId, blockIndex);\n+\n+    if (rawLength != 0) {\n+      // Adjust to match expected decoded length\n+      if (rawBlockId.length() < rawLength) {\n+        rawBlockId = String.format(\"%-\" + rawLength + \"s\", rawBlockId)\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T10:38:48.807+0000", "updated": "2025-07-09T10:38:48.807+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004089", "id": "18004089", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194680253\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAppend.java:\n##########\n@@ -1132,7 +1130,7 @@ public void testFlushSuccessWithConnectionResetOnResponseValidMd5() throws Excep\n           Mockito.nullable(String.class),\n           Mockito.anyString(),\n           Mockito.nullable(ContextEncryptionAdapter.class),\n-          Mockito.any(TracingContext.class)\n+          Mockito.any(TracingContext.class), Mockito.anyString()\n\nReview Comment:\n   should this be Mockito.nullable(String.class) as the md5 here can be null?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T10:38:49.679+0000", "updated": "2025-07-09T10:38:49.679+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004090", "id": "18004090", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194681213\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n+    this.blockId = generateBlockId(streamIdGuid, blockIdLength);\n   }\n \n   /**\n-   * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n-   * @return String representing the block ID generated.\n+   * Generates a Base64-encoded block ID string based on the given position, stream ID, and desired raw length.\n\nReview Comment:\n   This logic is used across clients. They follow the pattern of UUID followed by index\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T10:39:24.745+0000", "updated": "2025-07-09T10:39:24.745+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004091", "id": "18004091", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194691026\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemChecksum.java:\n##########\n@@ -61,9 +64,37 @@ public class ITestAzureBlobFileSystemChecksum extends AbstractAbfsIntegrationTes\n   private static final int MB_15 = 15 * ONE_MB;\n   private static final int MB_16 = 16 * ONE_MB;\n   private static final String INVALID_MD5_TEXT = \"Text for Invalid MD5 Computation\";\n+  private MessageDigest md = null;\n \n   public ITestAzureBlobFileSystemChecksum() throws Exception {\n     super();\n+    try {\n+      md = MessageDigest.getInstance(MD5);\n+    } catch (NoSuchAlgorithmException e) {\n+      // MD5 algorithm not available; md will remain null\n+      // Log this in production code if needed\n\nReview Comment:\n   we can remove this line\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T10:44:45.307+0000", "updated": "2025-07-09T10:44:45.307+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004092", "id": "18004092", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194694687\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1914,7 +1924,9 @@ private List<AbfsHttpHeader> getMetadataHeadersList(final Hashtable<String, Stri\n       // AzureBlobFileSystem supports only ASCII Characters in property values.\n       if (isPureASCII(value)) {\n         try {\n-          value = encodeMetadataAttribute(value);\n+          if (!XML_TAG_HDI_PERMISSION.equalsIgnoreCase(entry.getKey())) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T10:46:33.647+0000", "updated": "2025-07-09T10:46:33.647+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004093", "id": "18004093", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194697331\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemChecksum.java:\n##########\n@@ -84,10 +115,10 @@ public void testAppendWithChecksumAtDifferentOffsets() throws Exception {\n     byte[] data = generateRandomBytes(MB_4);\n     int pos = 0;\n \n-    pos += appendWithOffsetHelper(os, client, path, data, fs, pos, 0);\n-    pos += appendWithOffsetHelper(os, client, path, data, fs, pos, ONE_MB);\n-    pos += appendWithOffsetHelper(os, client, path, data, fs, pos, MB_2);\n-    appendWithOffsetHelper(os, client, path, data, fs, pos, MB_4 - 1);\n+    pos += appendWithOffsetHelper(os, client, path, data, fs, pos, 0,  getMd5(data, 0, data.length));\n\nReview Comment:\n   nit: double spaces\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T10:47:50.539+0000", "updated": "2025-07-09T10:47:50.539+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004094", "id": "18004094", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194699350\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1097,8 +1105,10 @@ public AbfsRestOperation flush(byte[] buffer,\n         AbfsRestOperation op1 = getPathStatus(path, true, tracingContext,\n             contextEncryptionAdapter);\n         String metadataMd5 = op1.getResult().getResponseHeader(CONTENT_MD5);\n-        if (!md5Hash.equals(metadataMd5)) {\n-          throw ex;\n+        if (blobMd5 != null) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T10:48:50.579+0000", "updated": "2025-07-09T10:48:50.579+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004098", "id": "18004098", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194735850\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -982,6 +985,11 @@ public AbfsRestOperation appendBlock(final String path,\n     if (requestParameters.getLeaseId() != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, requestParameters.getLeaseId()));\n     }\n+    if (isChecksumValidationEnabled()) {\n+      if (requestParameters.getMd5() != null) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T11:08:26.956+0000", "updated": "2025-07-09T11:08:26.956+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004100", "id": "18004100", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194738434\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n+    this.blockId = generateBlockId(streamIdGuid, blockIdLength);\n   }\n \n   /**\n-   * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n-   * @return String representing the block ID generated.\n+   * Generates a Base64-encoded block ID string based on the given position, stream ID, and desired raw length.\n+   * The block ID is composed using the stream UUID and the block index, which is derived from\n+   * the given position divided by the output stream's buffer size. The resulting string is\n+   * optionally adjusted to match the specified raw length, padded or trimmed as needed, and\n+   * then Base64-encoded.\n+   *\n+   * @param streamId   The UUID representing the stream, used as a prefix in the block ID.\n+   * @param rawLength  The desired length of the raw block ID string before Base64 encoding.\n+   *                   If 0, no length adjustment is made.\n+   * @return A Base64-encoded block ID string suitable for use in block-based storage APIs.\n    */\n-  private String generateBlockId(long position) {\n-    String streamId = getOutputStream().getStreamID();\n-    String streamIdHash = Integer.toString(streamId.hashCode());\n-    String blockId = String.format(\"%d_%s\", position, streamIdHash);\n-    byte[] blockIdByteArray = new byte[BLOCK_ID_LENGTH];\n-    System.arraycopy(blockId.getBytes(StandardCharsets.UTF_8), 0, blockIdByteArray, 0, Math.min(BLOCK_ID_LENGTH, blockId.length()));\n-    return new String(Base64.encodeBase64(blockIdByteArray), StandardCharsets.UTF_8);\n+  private String generateBlockId(UUID streamId, int rawLength) {\n+    String rawBlockId = String.format(\"%s-%06d\", streamId, blockIndex);\n+\n+    if (rawLength != 0) {\n+      // Adjust to match expected decoded length\n+      if (rawBlockId.length() < rawLength) {\n+        rawBlockId = String.format(\"%-\" + rawLength + \"s\", rawBlockId)\n+            .replace(' ', '_');\n+      } else if (rawBlockId.length() > rawLength) {\n+        rawBlockId = rawBlockId.substring(0, rawLength);\n\nReview Comment:\n   that will make the readability a bit difficult\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T11:09:56.864+0000", "updated": "2025-07-09T11:09:56.864+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004101", "id": "18004101", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194739674\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureBlockManager.java:\n##########\n@@ -134,7 +134,7 @@ protected long getBlockCount() {\n    *\n    * @param blockCount the count of blocks to set\n    */\n-  public void setBlockCount(final long blockCount) {\n+  protected void setBlockCount(final long blockCount) {\n\nReview Comment:\n   The modifier level was incorrect earlier, corrected it\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T11:10:42.155+0000", "updated": "2025-07-09T11:10:42.155+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004102", "id": "18004102", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194743196\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAppend.java:\n##########\n@@ -1132,7 +1130,7 @@ public void testFlushSuccessWithConnectionResetOnResponseValidMd5() throws Excep\n           Mockito.nullable(String.class),\n           Mockito.anyString(),\n           Mockito.nullable(ContextEncryptionAdapter.class),\n-          Mockito.any(TracingContext.class)\n+          Mockito.any(TracingContext.class), Mockito.anyString()\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T11:12:47.303+0000", "updated": "2025-07-09T11:12:47.303+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004106", "id": "18004106", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194758435\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemChecksum.java:\n##########\n@@ -61,9 +64,37 @@ public class ITestAzureBlobFileSystemChecksum extends AbstractAbfsIntegrationTes\n   private static final int MB_15 = 15 * ONE_MB;\n   private static final int MB_16 = 16 * ONE_MB;\n   private static final String INVALID_MD5_TEXT = \"Text for Invalid MD5 Computation\";\n+  private MessageDigest md = null;\n \n   public ITestAzureBlobFileSystemChecksum() throws Exception {\n     super();\n+    try {\n+      md = MessageDigest.getInstance(MD5);\n+    } catch (NoSuchAlgorithmException e) {\n+      // MD5 algorithm not available; md will remain null\n+      // Log this in production code if needed\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T11:20:02.856+0000", "updated": "2025-07-09T11:20:02.856+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004107", "id": "18004107", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194769475\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureDFSIngressHandler.java:\n##########\n@@ -178,13 +181,35 @@ protected synchronized AbfsRestOperation remoteFlush(final long offset,\n       tracingContextFlush.setIngressHandler(DFS_FLUSH);\n       tracingContextFlush.setPosition(String.valueOf(offset));\n     }\n+    byte[] digest = null;\n+    String fullBlobMd5 = null;\n+    try {\n+      // Clone the MessageDigest to avoid resetting the original state\n+      MessageDigest clonedMd5 = (MessageDigest) getAbfsOutputStream().getFullBlobContentMd5().clone();\n+      digest = clonedMd5.digest();\n+    } catch (CloneNotSupportedException e) {\n+      LOG.warn(\"Failed to clone MessageDigest instance\", e);\n+    }\n+    if (digest != null && digest.length != 0) {\n+      fullBlobMd5 = Base64.getEncoder().encodeToString(digest);\n+    }\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T11:25:53.183+0000", "updated": "2025-07-09T11:25:53.183+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004112", "id": "18004112", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194775260\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemChecksum.java:\n##########\n@@ -84,10 +115,10 @@ public void testAppendWithChecksumAtDifferentOffsets() throws Exception {\n     byte[] data = generateRandomBytes(MB_4);\n     int pos = 0;\n \n-    pos += appendWithOffsetHelper(os, client, path, data, fs, pos, 0);\n-    pos += appendWithOffsetHelper(os, client, path, data, fs, pos, ONE_MB);\n-    pos += appendWithOffsetHelper(os, client, path, data, fs, pos, MB_2);\n-    appendWithOffsetHelper(os, client, path, data, fs, pos, MB_4 - 1);\n+    pos += appendWithOffsetHelper(os, client, path, data, fs, pos, 0,  getMd5(data, 0, data.length));\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T11:28:18.433+0000", "updated": "2025-07-09T11:28:18.433+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004164", "id": "18004164", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3052745637\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 32s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 29s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  35m 51s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 14 unchanged - 1 fixed = 18 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  1s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 49s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 123m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux a8e224ca75bd 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5e6298a6dc7328fb1de1aa8e6a6202102d4d64a0 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/8/testReport/ |\r\n   | Max. process+thread count | 558 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T13:47:54.322+0000", "updated": "2025-07-09T13:47:54.322+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004173", "id": "18004173", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3052822743\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 45s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m  8s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/9/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 5 new + 14 unchanged - 1 fixed = 19 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 59s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 139m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux 741e89db1722 5.15.0-138-generic #148-Ubuntu SMP Fri Mar 14 19:05:48 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d951cc081529a99a69e4fcfb9c9523bbd700c21d |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/9/testReport/ |\r\n   | Max. process+thread count | 568 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/9/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T14:08:58.903+0000", "updated": "2025-07-09T14:08:58.903+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004175", "id": "18004175", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3052857189\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m  2s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 56s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 18s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/6/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 5 new + 14 unchanged - 1 fixed = 19 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 160m  4s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux 9823447b7bc1 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d951cc081529a99a69e4fcfb9c9523bbd700c21d |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/6/testReport/ |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T14:19:44.009+0000", "updated": "2025-07-09T14:19:44.009+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004177", "id": "18004177", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3052859007\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 53s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 10s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 5 new + 14 unchanged - 1 fixed = 19 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 44s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 158m 55s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux a92bcecdf77a 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d951cc081529a99a69e4fcfb9c9523bbd700c21d |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/7/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T14:20:15.188+0000", "updated": "2025-07-09T14:20:15.188+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004214", "id": "18004214", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2195482892\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +48,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = outputStream.getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n\nReview Comment:\n   Can streamId be null? streamId.getBytes can raise null pointer exception. Better to handle it,\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -982,6 +983,9 @@ public AbfsRestOperation appendBlock(final String path,\n     if (requestParameters.getLeaseId() != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, requestParameters.getLeaseId()));\n     }\n+    if (isChecksumValidationEnabled()) {\n+    addCheckSumHeaderForWrite(requestHeaders, requestParameters);\n\nReview Comment:\n   NIT: formatting required - there must be one tab in the start\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1032,7 +1036,7 @@ public AbfsRestOperation flush(final String path,\n       final String cachedSasToken,\n       final String leaseId,\n       final ContextEncryptionAdapter contextEncryptionAdapter,\n-      final TracingContext tracingContext) throws AzureBlobFileSystemException {\n+      final TracingContext tracingContext, String blobMd5) throws AzureBlobFileSystemException {\n\nReview Comment:\n   Add new argument in the comments @param. Please make this change wherever required.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -878,27 +878,29 @@ public boolean appendSuccessCheckOp(AbfsRestOperation op, final String path,\n    * @param leaseId if there is an active lease on the path.\n    * @param contextEncryptionAdapter to provide encryption context.\n    * @param tracingContext for tracing the server calls.\n+   * @param blobMd5  The Base64-encoded MD5 hash of the blob for data integrity validation.\n    * @return executed rest operation containing response from server.\n    * @throws AzureBlobFileSystemException if rest operation fails.\n    */\n   public abstract AbfsRestOperation flush(String path, long position,\n       boolean retainUncommittedData, boolean isClose,\n       String cachedSasToken, String leaseId,\n-      ContextEncryptionAdapter contextEncryptionAdapter, TracingContext tracingContext)\n+      ContextEncryptionAdapter contextEncryptionAdapter, TracingContext tracingContext, String blobMd5)\n       throws AzureBlobFileSystemException;\n \n   /**\n-   * Flush previously uploaded data to a file.\n-   * @param buffer containing blockIds to be flushed.\n-   * @param path on which data has to be flushed.\n-   * @param isClose specify if this is the last flush to the file.\n-   * @param cachedSasToken to be used for the authenticating operation.\n-   * @param leaseId if there is an active lease on the path.\n-   * @param eTag to specify conditional headers.\n-   * @param contextEncryptionAdapter to provide encryption context.\n-   * @param tracingContext for tracing the server calls.\n-   * @return executed rest operation containing response from server.\n-   * @throws AzureBlobFileSystemException if rest operation fails.\n+   * Flushes previously uploaded data to the specified path.\n\nReview Comment:\n   NIT - Format can be consistent across places.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AppendRequestParameters.java:\n##########\n@@ -77,6 +81,7 @@ public AppendRequestParameters(final long position,\n    * @param leaseId leaseId of the blob to be appended\n    * @param isExpectHeaderEnabled true if the expect header is enabled\n    * @param blobParams parameters specific to append operation on Blob Endpoint.\n+   *  @param md5  The Base64-encoded MD5 hash of the block for data integrity validation.\n\nReview Comment:\n   NIT: Format can be connected - extra space before @param and after md5\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n+    this.blockId = generateBlockId(streamIdGuid, blockIdLength);\n   }\n \n   /**\n-   * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n-   * @return String representing the block ID generated.\n+   * Generates a Base64-encoded block ID string based on the given position, stream ID, and desired raw length.\n\nReview Comment:\n   In the comment, you have mentioned that block id is generated based on position, stream Id and raw length. Where exactly are we using position in this method? Am I missing something here?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1060,7 +1064,7 @@ public AbfsRestOperation flush(byte[] buffer,\n       final String leaseId,\n       final String eTag,\n       ContextEncryptionAdapter contextEncryptionAdapter,\n-      final TracingContext tracingContext) throws AzureBlobFileSystemException {\n+      final TracingContext tracingContext, String blobMd5) throws AzureBlobFileSystemException {\n\nReview Comment:\n   Same as above.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +48,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n\nReview Comment:\n   Add newly added parameter in method comment.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T16:57:24.019+0000", "updated": "2025-07-09T16:57:24.019+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004215", "id": "18004215", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2195479605\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AppendRequestParameters.java:\n##########\n@@ -77,6 +81,7 @@ public AppendRequestParameters(final long position,\n    * @param leaseId leaseId of the blob to be appended\n    * @param isExpectHeaderEnabled true if the expect header is enabled\n    * @param blobParams parameters specific to append operation on Blob Endpoint.\n+   *  @param md5  The Base64-encoded MD5 hash of the block for data integrity validation.\n\nReview Comment:\n   NIT: Format can be corrected - extra space before @param and after md5\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T16:57:48.623+0000", "updated": "2025-07-09T16:57:48.623+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004218", "id": "18004218", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2195535019\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +48,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T17:12:40.489+0000", "updated": "2025-07-09T17:12:40.489+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004219", "id": "18004219", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2195536278\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/AppendRequestParameters.java:\n##########\n@@ -77,6 +81,7 @@ public AppendRequestParameters(final long position,\n    * @param leaseId leaseId of the blob to be appended\n    * @param isExpectHeaderEnabled true if the expect header is enabled\n    * @param blobParams parameters specific to append operation on Blob Endpoint.\n+   *  @param md5  The Base64-encoded MD5 hash of the block for data integrity validation.\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T17:13:28.926+0000", "updated": "2025-07-09T17:13:28.926+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004220", "id": "18004220", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2195538540\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +48,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = outputStream.getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n\nReview Comment:\n   StreamId can never be null as this is set in constructor of AbfsOutputStream itself, this.outputStreamId = createOutputStreamId();\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T17:15:00.623+0000", "updated": "2025-07-09T17:15:00.623+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004221", "id": "18004221", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2195546022\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -982,6 +983,9 @@ public AbfsRestOperation appendBlock(final String path,\n     if (requestParameters.getLeaseId() != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, requestParameters.getLeaseId()));\n     }\n+    if (isChecksumValidationEnabled()) {\n+    addCheckSumHeaderForWrite(requestHeaders, requestParameters);\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T17:17:08.561+0000", "updated": "2025-07-09T17:17:08.561+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004222", "id": "18004222", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2195566845\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1032,7 +1036,7 @@ public AbfsRestOperation flush(final String path,\n       final String cachedSasToken,\n       final String leaseId,\n       final ContextEncryptionAdapter contextEncryptionAdapter,\n-      final TracingContext tracingContext) throws AzureBlobFileSystemException {\n+      final TracingContext tracingContext, String blobMd5) throws AzureBlobFileSystemException {\n\nReview Comment:\n   taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1060,7 +1064,7 @@ public AbfsRestOperation flush(byte[] buffer,\n       final String leaseId,\n       final String eTag,\n       ContextEncryptionAdapter contextEncryptionAdapter,\n-      final TracingContext tracingContext) throws AzureBlobFileSystemException {\n+      final TracingContext tracingContext, String blobMd5) throws AzureBlobFileSystemException {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T17:21:25.908+0000", "updated": "2025-07-09T17:21:25.908+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004223", "id": "18004223", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2195571739\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -878,27 +878,29 @@ public boolean appendSuccessCheckOp(AbfsRestOperation op, final String path,\n    * @param leaseId if there is an active lease on the path.\n    * @param contextEncryptionAdapter to provide encryption context.\n    * @param tracingContext for tracing the server calls.\n+   * @param blobMd5  The Base64-encoded MD5 hash of the blob for data integrity validation.\n    * @return executed rest operation containing response from server.\n    * @throws AzureBlobFileSystemException if rest operation fails.\n    */\n   public abstract AbfsRestOperation flush(String path, long position,\n       boolean retainUncommittedData, boolean isClose,\n       String cachedSasToken, String leaseId,\n-      ContextEncryptionAdapter contextEncryptionAdapter, TracingContext tracingContext)\n+      ContextEncryptionAdapter contextEncryptionAdapter, TracingContext tracingContext, String blobMd5)\n       throws AzureBlobFileSystemException;\n \n   /**\n-   * Flush previously uploaded data to a file.\n-   * @param buffer containing blockIds to be flushed.\n-   * @param path on which data has to be flushed.\n-   * @param isClose specify if this is the last flush to the file.\n-   * @param cachedSasToken to be used for the authenticating operation.\n-   * @param leaseId if there is an active lease on the path.\n-   * @param eTag to specify conditional headers.\n-   * @param contextEncryptionAdapter to provide encryption context.\n-   * @param tracingContext for tracing the server calls.\n-   * @return executed rest operation containing response from server.\n-   * @throws AzureBlobFileSystemException if rest operation fails.\n+   * Flushes previously uploaded data to the specified path.\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T17:22:18.755+0000", "updated": "2025-07-09T17:22:18.755+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004227", "id": "18004227", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2195612399\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n+    this.blockId = generateBlockId(streamIdGuid, blockIdLength);\n   }\n \n   /**\n-   * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n-   * @return String representing the block ID generated.\n+   * Generates a Base64-encoded block ID string based on the given position, stream ID, and desired raw length.\n\nReview Comment:\n   corrected it\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T17:36:21.578+0000", "updated": "2025-07-09T17:36:21.578+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004228", "id": "18004228", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2194738434\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobBlock.java:\n##########\n@@ -42,23 +42,40 @@ public class AbfsBlobBlock extends AbfsBlock {\n    * @param offset       Used to generate blockId based on offset.\n    * @throws IOException exception is thrown.\n    */\n-  AbfsBlobBlock(AbfsOutputStream outputStream, long offset) throws IOException {\n+  AbfsBlobBlock(AbfsOutputStream outputStream, long offset, int blockIdLength, long blockIndex) throws IOException {\n     super(outputStream, offset);\n-    this.blockId = generateBlockId(offset);\n+    this.blockIndex = blockIndex;\n+    String streamId = getOutputStream().getStreamID();\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n+    this.blockId = generateBlockId(streamIdGuid, blockIdLength);\n   }\n \n   /**\n-   * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n-   * @return String representing the block ID generated.\n+   * Generates a Base64-encoded block ID string based on the given position, stream ID, and desired raw length.\n+   * The block ID is composed using the stream UUID and the block index, which is derived from\n+   * the given position divided by the output stream's buffer size. The resulting string is\n+   * optionally adjusted to match the specified raw length, padded or trimmed as needed, and\n+   * then Base64-encoded.\n+   *\n+   * @param streamId   The UUID representing the stream, used as a prefix in the block ID.\n+   * @param rawLength  The desired length of the raw block ID string before Base64 encoding.\n+   *                   If 0, no length adjustment is made.\n+   * @return A Base64-encoded block ID string suitable for use in block-based storage APIs.\n    */\n-  private String generateBlockId(long position) {\n-    String streamId = getOutputStream().getStreamID();\n-    String streamIdHash = Integer.toString(streamId.hashCode());\n-    String blockId = String.format(\"%d_%s\", position, streamIdHash);\n-    byte[] blockIdByteArray = new byte[BLOCK_ID_LENGTH];\n-    System.arraycopy(blockId.getBytes(StandardCharsets.UTF_8), 0, blockIdByteArray, 0, Math.min(BLOCK_ID_LENGTH, blockId.length()));\n-    return new String(Base64.encodeBase64(blockIdByteArray), StandardCharsets.UTF_8);\n+  private String generateBlockId(UUID streamId, int rawLength) {\n+    String rawBlockId = String.format(\"%s-%06d\", streamId, blockIndex);\n+\n+    if (rawLength != 0) {\n+      // Adjust to match expected decoded length\n+      if (rawBlockId.length() < rawLength) {\n+        rawBlockId = String.format(\"%-\" + rawLength + \"s\", rawBlockId)\n+            .replace(' ', '_');\n+      } else if (rawBlockId.length() > rawLength) {\n+        rawBlockId = rawBlockId.substring(0, rawLength);\n\nReview Comment:\n   that will make readability a bit difficult\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T17:36:41.593+0000", "updated": "2025-07-09T17:36:41.593+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004252", "id": "18004252", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3053817235\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  9s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 59s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/10/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 14 unchanged - 1 fixed = 15 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 145m 19s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux c2390b1a8380 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9ae0198739af5ba345b4fd85ecf8b43f1efb9222 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/10/testReport/ |\r\n   | Max. process+thread count | 527 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/10/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T19:52:27.915+0000", "updated": "2025-07-09T19:52:27.915+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18004370", "id": "18004370", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3056136093\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 32s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  hadoop-tools/hadoop-azure: The patch generated 0 new + 14 unchanged - 1 fixed = 14 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 15s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 44s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 140m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/11/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux ffa5ea9ef982 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5f48139ca271a716f66d8b358a8eb07de79ff0c0 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/11/testReport/ |\r\n   | Max. process+thread count | 528 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/11/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-10T07:50:13.759+0000", "updated": "2025-07-10T07:50:13.759+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18005095", "id": "18005095", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2204530092\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java:\n##########\n@@ -117,19 +118,20 @@ public void verifyShortWriteRequest() throws Exception {\n     AbfsConfiguration abfsConf;\n     final Configuration conf = new Configuration();\n     conf.set(accountKey1, accountValue1);\n-    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    abfsConf = Mockito.spy(new AbfsConfiguration(conf, accountName1));\n     AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n     when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.getAbfsConfiguration()).thenReturn(abfsConf);\n     when(client.append(anyString(), any(byte[].class),\n         any(AppendRequestParameters.class), any(),\n         any(), any(TracingContext.class)))\n         .thenReturn(op);\n     when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean(), any(),\n-        isNull(), any(), any(TracingContext.class))).thenReturn(op);\n+        isNull(), any(), any(TracingContext.class), anyString())).thenReturn(op);\n     when(clientHandler.getClient(any())).thenReturn(client);\n     when(clientHandler.getDfsClient()).thenReturn(client);\n-\n-    AbfsOutputStream out = new AbfsOutputStream(\n+    AbfsOutputStream out;\n+    out = Mockito.spy(new AbfsOutputStream(\n\nReview Comment:\n   can we have this in the same line?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-14T10:40:41.457+0000", "updated": "2025-07-14T10:40:41.457+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18005096", "id": "18005096", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2204535597\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -42,10 +51,28 @@\n  * Test compatibility between ABFS client and WASB client.\n  */\n public class ITestWasbAbfsCompatibility extends AbstractAbfsIntegrationTest {\n+\n\nReview Comment:\n   Nit: we can remove these spaces here\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-14T10:42:21.488+0000", "updated": "2025-07-14T10:42:21.488+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18005098", "id": "18005098", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2204544838\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAppend.java:\n##########\n@@ -1041,16 +1041,14 @@ public void testWriteAsyncOpFailedAfterCloseCalled() throws Exception {\n \n   /**\n    * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n    * @return String representing the block ID generated.\n    */\n-  private String generateBlockId(AbfsOutputStream os, long position) {\n+  private String generateBlockId(AbfsOutputStream os) {\n     String streamId = os.getStreamID();\n-    String streamIdHash = Integer.toString(streamId.hashCode());\n-    String blockId = String.format(\"%d_%s\", position, streamIdHash);\n-    byte[] blockIdByteArray = new byte[BLOCK_ID_LENGTH];\n-    System.arraycopy(blockId.getBytes(), 0, blockIdByteArray, 0, Math.min(BLOCK_ID_LENGTH, blockId.length()));\n-    return new String(Base64.encodeBase64(blockIdByteArray), StandardCharsets.UTF_8);\n+    UUID streamIdGuid = UUID.nameUUIDFromBytes(streamId.getBytes(StandardCharsets.UTF_8));\n+    long blockIndex = os.getBlockManager().getBlockCount();\n+    String rawBlockId = String.format(\"%s-%06d\", streamIdGuid, blockIndex);\n\nReview Comment:\n   we can use the constant BLOCK_ID_FORMAT\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-14T10:46:26.684+0000", "updated": "2025-07-14T10:46:26.684+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18005100", "id": "18005100", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2204613657\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAppend.java:\n##########\n@@ -1041,16 +1041,14 @@ public void testWriteAsyncOpFailedAfterCloseCalled() throws Exception {\n \n   /**\n    * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n    * @return String representing the block ID generated.\n    */\n-  private String generateBlockId(AbfsOutputStream os, long position) {\n+  private String generateBlockId(AbfsOutputStream os) {\n\nReview Comment:\n   Better to use production code in tests as well.\r\n   Any issue in code better to catch in production and fix there itself.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemChecksum.java:\n##########\n@@ -61,9 +64,36 @@ public class ITestAzureBlobFileSystemChecksum extends AbstractAbfsIntegrationTes\n   private static final int MB_15 = 15 * ONE_MB;\n   private static final int MB_16 = 16 * ONE_MB;\n   private static final String INVALID_MD5_TEXT = \"Text for Invalid MD5 Computation\";\n+  private MessageDigest md = null;\n \n   public ITestAzureBlobFileSystemChecksum() throws Exception {\n     super();\n+    try {\n+      md = MessageDigest.getInstance(MD5);\n+    } catch (NoSuchAlgorithmException e) {\n+      // MD5 algorithm not available; md will remain null\n+    }\n+  }\n+\n+  /**\n+   * Computes the MD5 checksum of a specified portion of the input byte array.\n+   *\n+   * @param data The byte array containing the data to compute the MD5 checksum for.\n+   * @param off The starting offset in the byte array.\n+   * @param length The number of bytes to include in the checksum computation.\n+   * @return The Base64-encoded MD5 checksum of the specified data, or null if the digest is empty.\n+   * @throws IllegalArgumentException If the offset or length is invalid for the given byte array.\n+   */\n+  public String getMd5(byte[] data, int off, int length) {\n\nReview Comment:\n   A similar method is present in production code.\r\n   Can we use that itself in tests so that production method can also be covered in test flow?\r\n   \r\n   If some issuei is found later then someone might just fix here and production code will still remain buggy.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureIngressHandler.java:\n##########\n@@ -206,4 +208,32 @@ protected InvalidIngressServiceException getIngressHandlerSwitchException(\n    * @return the block manager\n    */\n   public abstract AbfsClient getClient();\n+\n+  /**\n+   * Computes the Base64-encoded MD5 hash of the full blob content.\n+   *\n+   * <p>This method clones the current state of the {@link MessageDigest} instance\n+   * associated with the blob content to avoid resetting its original state. It then\n+   * calculates the MD5 digest and encodes it into a Base64 string.</p>\n+   *\n+   * @return A Base64-encoded string representing the MD5 hash of the full blob content,\n+   *         or {@code null} if the digest could not be computed.\n+   */\n+  protected String computeFullBlobMd5() {\n\nReview Comment:\n   Is there a unit test adde around this?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n\nReview Comment:\n   Can we convert all these comments to javadocs for test methods also inlcuding the expected outcome of eac scenario\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n+  @Test\n+  public void testScenario1() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 2: - Create and write via WASB, read via ABFS and then write the same file via ABFS\n+  @Test\n+  public void testScenario2() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Write\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT1.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 3: - Create and write via ABFS and the read via WASB\n+  @Test\n+  public void testScenario3() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 4:- Create via WASB, write via ABFS and then write via WASB\n+  @Test\n+  public void testScenario4() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 5:- Create via ABFS, write via WASB, read via ABFS (Checksum validation disabled)\n+  @Test\n+  public void testScenario5() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 6: - Create via ABFS, write via WASB, read via ABFS (Checksum validation enabled)\n+  @Test\n+  public void testScenario6() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    assumeBlobServiceType();\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 7 :- Create via WASB and then create overwrite true using ABFS\n+  @Test\n+  public void testScenario7() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 8 :- Create via WASB and then create overwrite false using ABFS\n+  @Test\n+  public void testScenario8() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      abfs.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\"AlreadyExists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 9 :- Create via ABFS and then create overwrite true using WASB\n+  @Test\n+  public void testScenario9() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 10 :- Create via ABFS and then create overwrite false using WASB\n+  @Test\n+  public void testScenario10() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      wasb.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().toLowerCase().contains(\"exists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 11 :- Create via ABFS and then write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario11() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 12 :- Create and write via ABFS and delete via WASB\n+  @Test\n+  public void testScenario12() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 13:- Create via ABFS, write via WASB, and read via wasb\n+  @Test\n+  public void testScenario13() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 14:- Create via ABFS, write via WASB, and delete via wasb\n+  @Test\n+  public void testScenario14() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 15 :- Create and write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario15() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 16: Create via WASB, write via ABFS, and delete via WASB\n+  @Test\n+  public void testScenario16() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = abfs.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 17: Create, setXAttr and getXAttr via ABFS\n+  @Test\n+  public void testScenario17() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // -", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-14T11:31:03.938+0000", "updated": "2025-07-14T11:31:03.938+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18005124", "id": "18005124", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2204855738\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n+  @Test\n+  public void testScenario1() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 2: - Create and write via WASB, read via ABFS and then write the same file via ABFS\n+  @Test\n+  public void testScenario2() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Write\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT1.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 3: - Create and write via ABFS and the read via WASB\n+  @Test\n+  public void testScenario3() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 4:- Create via WASB, write via ABFS and then write via WASB\n+  @Test\n+  public void testScenario4() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 5:- Create via ABFS, write via WASB, read via ABFS (Checksum validation disabled)\n+  @Test\n+  public void testScenario5() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 6: - Create via ABFS, write via WASB, read via ABFS (Checksum validation enabled)\n+  @Test\n+  public void testScenario6() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    assumeBlobServiceType();\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 7 :- Create via WASB and then create overwrite true using ABFS\n+  @Test\n+  public void testScenario7() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 8 :- Create via WASB and then create overwrite false using ABFS\n+  @Test\n+  public void testScenario8() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      abfs.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\"AlreadyExists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 9 :- Create via ABFS and then create overwrite true using WASB\n+  @Test\n+  public void testScenario9() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 10 :- Create via ABFS and then create overwrite false using WASB\n+  @Test\n+  public void testScenario10() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      wasb.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().toLowerCase().contains(\"exists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 11 :- Create via ABFS and then write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario11() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 12 :- Create and write via ABFS and delete via WASB\n+  @Test\n+  public void testScenario12() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 13:- Create via ABFS, write via WASB, and read via wasb\n+  @Test\n+  public void testScenario13() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 14:- Create via ABFS, write via WASB, and delete via wasb\n+  @Test\n+  public void testScenario14() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 15 :- Create and write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario15() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 16: Create via WASB, write via ABFS, and delete via WASB\n+  @Test\n+  public void testScenario16() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = abfs.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 17: Create, setXAttr and getXAttr via ABFS\n+  @Test\n+  public void testScenario17() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // -", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-14T12:48:18.208+0000", "updated": "2025-07-14T12:48:18.208+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18005297", "id": "18005297", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2206458334\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java:\n##########\n@@ -117,19 +118,20 @@ public void verifyShortWriteRequest() throws Exception {\n     AbfsConfiguration abfsConf;\n     final Configuration conf = new Configuration();\n     conf.set(accountKey1, accountValue1);\n-    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    abfsConf = Mockito.spy(new AbfsConfiguration(conf, accountName1));\n     AbfsPerfTracker tracker = new AbfsPerfTracker(\"test\", accountName1, abfsConf);\n     when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+    when(client.getAbfsConfiguration()).thenReturn(abfsConf);\n     when(client.append(anyString(), any(byte[].class),\n         any(AppendRequestParameters.class), any(),\n         any(), any(TracingContext.class)))\n         .thenReturn(op);\n     when(client.flush(anyString(), anyLong(), anyBoolean(), anyBoolean(), any(),\n-        isNull(), any(), any(TracingContext.class))).thenReturn(op);\n+        isNull(), any(), any(TracingContext.class), anyString())).thenReturn(op);\n     when(clientHandler.getClient(any())).thenReturn(client);\n     when(clientHandler.getDfsClient()).thenReturn(client);\n-\n-    AbfsOutputStream out = new AbfsOutputStream(\n+    AbfsOutputStream out;\n+    out = Mockito.spy(new AbfsOutputStream(\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-15T05:52:07.277+0000", "updated": "2025-07-15T05:52:07.277+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007350", "id": "18007350", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3076025629\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m  4s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 11 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  73m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 20s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 42s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  hadoop-tools/hadoop-azure: The patch generated 0 new + 14 unchanged - 1 fixed = 14 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 12s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  4s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 191m 21s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/12/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux 96b4f6382445 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ef1db63a55c53d21364fab43488ba4ea130beb3a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/12/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/12/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-15T23:15:21.363+0000", "updated": "2025-07-15T23:15:21.363+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007740", "id": "18007740", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2212583440\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAppend.java:\n##########\n@@ -1041,16 +1041,14 @@ public void testWriteAsyncOpFailedAfterCloseCalled() throws Exception {\n \n   /**\n    * Helper method that generates blockId.\n-   * @param position The offset needed to generate blockId.\n    * @return String representing the block ID generated.\n    */\n-  private String generateBlockId(AbfsOutputStream os, long position) {\n+  private String generateBlockId(AbfsOutputStream os) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T07:52:34.909+0000", "updated": "2025-07-17T07:52:34.909+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007742", "id": "18007742", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2212611621\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemChecksum.java:\n##########\n@@ -61,9 +64,36 @@ public class ITestAzureBlobFileSystemChecksum extends AbstractAbfsIntegrationTes\n   private static final int MB_15 = 15 * ONE_MB;\n   private static final int MB_16 = 16 * ONE_MB;\n   private static final String INVALID_MD5_TEXT = \"Text for Invalid MD5 Computation\";\n+  private MessageDigest md = null;\n \n   public ITestAzureBlobFileSystemChecksum() throws Exception {\n     super();\n+    try {\n+      md = MessageDigest.getInstance(MD5);\n+    } catch (NoSuchAlgorithmException e) {\n+      // MD5 algorithm not available; md will remain null\n+    }\n+  }\n+\n+  /**\n+   * Computes the MD5 checksum of a specified portion of the input byte array.\n+   *\n+   * @param data The byte array containing the data to compute the MD5 checksum for.\n+   * @param off The starting offset in the byte array.\n+   * @param length The number of bytes to include in the checksum computation.\n+   * @return The Base64-encoded MD5 checksum of the specified data, or null if the digest is empty.\n+   * @throws IllegalArgumentException If the offset or length is invalid for the given byte array.\n+   */\n+  public String getMd5(byte[] data, int off, int length) {\n\nReview Comment:\n   makes sense, taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -42,10 +51,28 @@\n  * Test compatibility between ABFS client and WASB client.\n  */\n public class ITestWasbAbfsCompatibility extends AbstractAbfsIntegrationTest {\n+\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T08:03:50.869+0000", "updated": "2025-07-17T08:03:50.869+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007743", "id": "18007743", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2212635589\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n+  @Test\n+  public void testScenario1() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 2: - Create and write via WASB, read via ABFS and then write the same file via ABFS\n+  @Test\n+  public void testScenario2() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Write\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT1.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 3: - Create and write via ABFS and the read via WASB\n+  @Test\n+  public void testScenario3() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 4:- Create via WASB, write via ABFS and then write via WASB\n+  @Test\n+  public void testScenario4() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 5:- Create via ABFS, write via WASB, read via ABFS (Checksum validation disabled)\n+  @Test\n+  public void testScenario5() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 6: - Create via ABFS, write via WASB, read via ABFS (Checksum validation enabled)\n+  @Test\n+  public void testScenario6() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    assumeBlobServiceType();\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 7 :- Create via WASB and then create overwrite true using ABFS\n+  @Test\n+  public void testScenario7() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 8 :- Create via WASB and then create overwrite false using ABFS\n+  @Test\n+  public void testScenario8() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      abfs.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\"AlreadyExists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 9 :- Create via ABFS and then create overwrite true using WASB\n+  @Test\n+  public void testScenario9() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 10 :- Create via ABFS and then create overwrite false using WASB\n+  @Test\n+  public void testScenario10() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      wasb.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().toLowerCase().contains(\"exists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 11 :- Create via ABFS and then write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario11() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 12 :- Create and write via ABFS and delete via WASB\n+  @Test\n+  public void testScenario12() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 13:- Create via ABFS, write via WASB, and read via wasb\n+  @Test\n+  public void testScenario13() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 14:- Create via ABFS, write via WASB, and delete via wasb\n+  @Test\n+  public void testScenario14() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 15 :- Create and write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario15() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 16: Create via WASB, write via ABFS, and delete via WASB\n+  @Test\n+  public void testScenario16() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = abfs.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 17: Create, setXAttr and getXAttr via ABFS\n+  @Test\n+  public void testScenario17() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // -", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T08:12:16.647+0000", "updated": "2025-07-17T08:12:16.647+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007746", "id": "18007746", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2212651469\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureIngressHandler.java:\n##########\n@@ -206,4 +208,32 @@ protected InvalidIngressServiceException getIngressHandlerSwitchException(\n    * @return the block manager\n    */\n   public abstract AbfsClient getClient();\n+\n+  /**\n+   * Computes the Base64-encoded MD5 hash of the full blob content.\n+   *\n+   * <p>This method clones the current state of the {@link MessageDigest} instance\n+   * associated with the blob content to avoid resetting its original state. It then\n+   * calculates the MD5 digest and encodes it into a Base64 string.</p>\n+   *\n+   * @return A Base64-encoded string representing the MD5 hash of the full blob content,\n+   *         or {@code null} if the digest could not be computed.\n+   */\n+  protected String computeFullBlobMd5() {\n\nReview Comment:\n   yes testFlushSuccessWithConnectionResetOnResponseValidMd5 and testFlushSuccessWithConnectionResetOnResponseInvalidMd5 test this new code\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T08:19:42.124+0000", "updated": "2025-07-17T08:19:42.124+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007780", "id": "18007780", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2213043323\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureDFSIngressHandler.java:\n##########\n@@ -178,13 +179,24 @@ protected synchronized AbfsRestOperation remoteFlush(final long offset,\n       tracingContextFlush.setIngressHandler(DFS_FLUSH);\n       tracingContextFlush.setPosition(String.valueOf(offset));\n     }\n+    String fullBlobMd5 = computeFullBlobMd5();\n     LOG.trace(\"Flushing data at offset {} and path {}\", offset, getAbfsOutputStream().getPath());\n-    return getClient()\n-        .flush(getAbfsOutputStream().getPath(), offset, retainUncommitedData,\n-            isClose,\n-            getAbfsOutputStream().getCachedSasTokenString(), leaseId,\n-            getAbfsOutputStream().getContextEncryptionAdapter(),\n-            tracingContextFlush);\n+    AbfsRestOperation op;\n+    try {\n+      op = getClient()\n+          .flush(getAbfsOutputStream().getPath(), offset, retainUncommitedData,\n+              isClose,\n+              getAbfsOutputStream().getCachedSasTokenString(), leaseId,\n+              getAbfsOutputStream().getContextEncryptionAdapter(),\n+              tracingContextFlush, fullBlobMd5);\n+    } catch (AbfsRestOperationException ex) {\n+      LOG.error(\"Error in remote flush for path {} and offset {}\",\n+          getAbfsOutputStream().getPath(), offset, ex);\n+      throw ex;\n+    } finally {\n+      getAbfsOutputStream().getFullBlobContentMd5().reset();\n\nReview Comment:\n   done\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T11:06:38.530+0000", "updated": "2025-07-17T11:06:38.530+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007781", "id": "18007781", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2213048850\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1914,7 +1920,11 @@ private List<AbfsHttpHeader> getMetadataHeadersList(final Hashtable<String, Stri\n       // AzureBlobFileSystem supports only ASCII Characters in property values.\n       if (isPureASCII(value)) {\n         try {\n-          value = encodeMetadataAttribute(value);\n+          // URL encoding this JSON metadata, set by the WASB Client during file creation, causes compatibility issues.\n\nReview Comment:\n   Create via WASB, setXAttr via ABFS, create overwrite via WASB -> there is a test added for this scenario where this flow is getting tested, do you suggest adding a mocked test as well ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T11:09:28.682+0000", "updated": "2025-07-17T11:09:28.682+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007784", "id": "18007784", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2213054261\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java:\n##########\n@@ -117,19 +118,20 @@ public void verifyShortWriteRequest() throws Exception {\n     AbfsConfiguration abfsConf;\n     final Configuration conf = new Configuration();\n     conf.set(accountKey1, accountValue1);\n-    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    abfsConf = Mockito.spy(new AbfsConfiguration(conf, accountName1));\n\nReview Comment:\n   corrected\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsOutputStream.java:\n##########\n@@ -182,19 +186,21 @@ public void verifyWriteRequest() throws Exception {\n     AbfsConfiguration abfsConf;\n     final Configuration conf = new Configuration();\n     conf.set(accountKey1, accountValue1);\n-    abfsConf = new AbfsConfiguration(conf, accountName1);\n+    abfsConf = Mockito.spy(new AbfsConfiguration(conf, accountName1));\n\nReview Comment:\n   corrected\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T11:12:13.832+0000", "updated": "2025-07-17T11:12:13.832+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007794", "id": "18007794", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2213149217\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n+  @Test\n+  public void testScenario1() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 2: - Create and write via WASB, read via ABFS and then write the same file via ABFS\n+  @Test\n+  public void testScenario2() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Write\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT1.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 3: - Create and write via ABFS and the read via WASB\n+  @Test\n+  public void testScenario3() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 4:- Create via WASB, write via ABFS and then write via WASB\n+  @Test\n+  public void testScenario4() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 5:- Create via ABFS, write via WASB, read via ABFS (Checksum validation disabled)\n+  @Test\n+  public void testScenario5() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 6: - Create via ABFS, write via WASB, read via ABFS (Checksum validation enabled)\n+  @Test\n+  public void testScenario6() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    assumeBlobServiceType();\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 7 :- Create via WASB and then create overwrite true using ABFS\n+  @Test\n+  public void testScenario7() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 8 :- Create via WASB and then create overwrite false using ABFS\n+  @Test\n+  public void testScenario8() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      abfs.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\"AlreadyExists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 9 :- Create via ABFS and then create overwrite true using WASB\n+  @Test\n+  public void testScenario9() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 10 :- Create via ABFS and then create overwrite false using WASB\n+  @Test\n+  public void testScenario10() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      wasb.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().toLowerCase().contains(\"exists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 11 :- Create via ABFS and then write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario11() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 12 :- Create and write via ABFS and delete via WASB\n+  @Test\n+  public void testScenario12() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 13:- Create via ABFS, write via WASB, and read via wasb\n+  @Test\n+  public void testScenario13() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 14:- Create via ABFS, write via WASB, and delete via wasb\n+  @Test\n+  public void testScenario14() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 15 :- Create and write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario15() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 16: Create via WASB, write via ABFS, and delete via WASB\n+  @Test\n+  public void testScenario16() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = abfs.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 17: Create, setXAttr and getXAttr via ABFS\n+  @Test\n+  public void testScenario17() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // -", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T12:00:43.612+0000", "updated": "2025-07-17T12:00:43.612+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007796", "id": "18007796", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2213154927\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n+  @Test\n+  public void testScenario1() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 2: - Create and write via WASB, read via ABFS and then write the same file via ABFS\n+  @Test\n+  public void testScenario2() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Write\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT1.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 3: - Create and write via ABFS and the read via WASB\n+  @Test\n+  public void testScenario3() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 4:- Create via WASB, write via ABFS and then write via WASB\n+  @Test\n+  public void testScenario4() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 5:- Create via ABFS, write via WASB, read via ABFS (Checksum validation disabled)\n+  @Test\n+  public void testScenario5() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 6: - Create via ABFS, write via WASB, read via ABFS (Checksum validation enabled)\n+  @Test\n+  public void testScenario6() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    assumeBlobServiceType();\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 7 :- Create via WASB and then create overwrite true using ABFS\n+  @Test\n+  public void testScenario7() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 8 :- Create via WASB and then create overwrite false using ABFS\n+  @Test\n+  public void testScenario8() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      abfs.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\"AlreadyExists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 9 :- Create via ABFS and then create overwrite true using WASB\n+  @Test\n+  public void testScenario9() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 10 :- Create via ABFS and then create overwrite false using WASB\n+  @Test\n+  public void testScenario10() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      wasb.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().toLowerCase().contains(\"exists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 11 :- Create via ABFS and then write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario11() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 12 :- Create and write via ABFS and delete via WASB\n+  @Test\n+  public void testScenario12() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 13:- Create via ABFS, write via WASB, and read via wasb\n+  @Test\n+  public void testScenario13() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 14:- Create via ABFS, write via WASB, and delete via wasb\n+  @Test\n+  public void testScenario14() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 15 :- Create and write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario15() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 16: Create via WASB, write via ABFS, and delete via WASB\n+  @Test\n+  public void testScenario16() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = abfs.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 17: Create, setXAttr and getXAttr via ABFS\n+  @Test\n+  public void testScenario17() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // -", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T12:03:52.021+0000", "updated": "2025-07-17T12:03:52.021+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007799", "id": "18007799", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2213166847\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T12:10:05.026+0000", "updated": "2025-07-17T12:10:05.026+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007803", "id": "18007803", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2213186309\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n+  @Test\n+  public void testScenario1() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 2: - Create and write via WASB, read via ABFS and then write the same file via ABFS\n+  @Test\n+  public void testScenario2() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Write\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT1.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 3: - Create and write via ABFS and the read via WASB\n+  @Test\n+  public void testScenario3() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 4:- Create via WASB, write via ABFS and then write via WASB\n+  @Test\n+  public void testScenario4() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 5:- Create via ABFS, write via WASB, read via ABFS (Checksum validation disabled)\n+  @Test\n+  public void testScenario5() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 6: - Create via ABFS, write via WASB, read via ABFS (Checksum validation enabled)\n+  @Test\n+  public void testScenario6() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    assumeBlobServiceType();\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 7 :- Create via WASB and then create overwrite true using ABFS\n+  @Test\n+  public void testScenario7() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 8 :- Create via WASB and then create overwrite false using ABFS\n+  @Test\n+  public void testScenario8() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      abfs.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\"AlreadyExists\"));\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T12:20:01.457+0000", "updated": "2025-07-17T12:20:01.457+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007805", "id": "18007805", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2213194988\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n+  @Test\n+  public void testScenario1() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 2: - Create and write via WASB, read via ABFS and then write the same file via ABFS\n+  @Test\n+  public void testScenario2() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Write\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT1.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 3: - Create and write via ABFS and the read via WASB\n+  @Test\n+  public void testScenario3() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 4:- Create via WASB, write via ABFS and then write via WASB\n+  @Test\n+  public void testScenario4() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 5:- Create via ABFS, write via WASB, read via ABFS (Checksum validation disabled)\n+  @Test\n+  public void testScenario5() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 6: - Create via ABFS, write via WASB, read via ABFS (Checksum validation enabled)\n+  @Test\n+  public void testScenario6() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    assumeBlobServiceType();\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 7 :- Create via WASB and then create overwrite true using ABFS\n+  @Test\n+  public void testScenario7() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 8 :- Create via WASB and then create overwrite false using ABFS\n+  @Test\n+  public void testScenario8() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      abfs.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\"AlreadyExists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 9 :- Create via ABFS and then create overwrite true using WASB\n+  @Test\n+  public void testScenario9() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.create(path, true);\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T12:24:02.846+0000", "updated": "2025-07-17T12:24:02.846+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007806", "id": "18007806", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2213200151\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n+  @Test\n+  public void testScenario1() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 2: - Create and write via WASB, read via ABFS and then write the same file via ABFS\n+  @Test\n+  public void testScenario2() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Write\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT1.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 3: - Create and write via ABFS and the read via WASB\n+  @Test\n+  public void testScenario3() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 4:- Create via WASB, write via ABFS and then write via WASB\n+  @Test\n+  public void testScenario4() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 5:- Create via ABFS, write via WASB, read via ABFS (Checksum validation disabled)\n+  @Test\n+  public void testScenario5() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 6: - Create via ABFS, write via WASB, read via ABFS (Checksum validation enabled)\n+  @Test\n+  public void testScenario6() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    assumeBlobServiceType();\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 7 :- Create via WASB and then create overwrite true using ABFS\n+  @Test\n+  public void testScenario7() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 8 :- Create via WASB and then create overwrite false using ABFS\n+  @Test\n+  public void testScenario8() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      abfs.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\"AlreadyExists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 9 :- Create via ABFS and then create overwrite true using WASB\n+  @Test\n+  public void testScenario9() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 10 :- Create via ABFS and then create overwrite false using WASB\n+  @Test\n+  public void testScenario10() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      wasb.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().toLowerCase().contains(\"exists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 11 :- Create via ABFS and then write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario11() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 12 :- Create and write via ABFS and delete via WASB\n+  @Test\n+  public void testScenario12() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n\nReview Comment:\n   The outputstream name is confusing corrected it\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T12:26:28.002+0000", "updated": "2025-07-17T12:26:28.002+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18007812", "id": "18007812", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2213257097\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n+  @Test\n+  public void testScenario1() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 2: - Create and write via WASB, read via ABFS and then write the same file via ABFS\n+  @Test\n+  public void testScenario2() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Write\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT1.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 3: - Create and write via ABFS and the read via WASB\n+  @Test\n+  public void testScenario3() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 4:- Create via WASB, write via ABFS and then write via WASB\n+  @Test\n+  public void testScenario4() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 5:- Create via ABFS, write via WASB, read via ABFS (Checksum validation disabled)\n+  @Test\n+  public void testScenario5() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 6: - Create via ABFS, write via WASB, read via ABFS (Checksum validation enabled)\n+  @Test\n+  public void testScenario6() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    assumeBlobServiceType();\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 7 :- Create via WASB and then create overwrite true using ABFS\n+  @Test\n+  public void testScenario7() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 8 :- Create via WASB and then create overwrite false using ABFS\n+  @Test\n+  public void testScenario8() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      abfs.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\"AlreadyExists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 9 :- Create via ABFS and then create overwrite true using WASB\n+  @Test\n+  public void testScenario9() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 10 :- Create via ABFS and then create overwrite false using WASB\n+  @Test\n+  public void testScenario10() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      wasb.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().toLowerCase().contains(\"exists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 11 :- Create via ABFS and then write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario11() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 12 :- Create and write via ABFS and delete via WASB\n+  @Test\n+  public void testScenario12() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 13:- Create via ABFS, write via WASB, and read via wasb\n+  @Test\n+  public void testScenario13() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 14:- Create via ABFS, write via WASB, and delete via wasb\n+  @Test\n+  public void testScenario14() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.delete(path, true);\n+  }\n+\n+  // Scenario 15 :- Create and write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario15() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 16: Create via WASB, write via ABFS, and delete via WASB\n+  @Test\n+  public void testScenario16() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = abfs.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n\nReview Comment:\n   corrected\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T12:49:28.316+0000", "updated": "2025-07-17T12:49:28.316+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18008020", "id": "18008020", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3088951155\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 12 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  78m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 31s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/13/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 14 unchanged - 1 fixed = 18 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 46s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 175m 18s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/13/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux cdaf6b71f4ba 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4b4b7a4db2deaa11362e04e490dc02961bc0cfeb |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/13/testReport/ |\r\n   | Max. process+thread count | 528 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/13/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-18T10:23:23.066+0000", "updated": "2025-07-18T10:23:23.066+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18008078", "id": "18008078", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3089547616\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 12 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  74m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 37s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  hadoop-tools/hadoop-azure: The patch generated 0 new + 14 unchanged - 1 fixed = 14 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 59s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 48s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 173m 17s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/14/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux faeccd26a3c7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 513511a9a9812854846e0331e700618eac81de57 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/14/testReport/ |\r\n   | Max. process+thread count | 533 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/14/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-18T13:50:19.414+0000", "updated": "2025-07-18T13:50:19.414+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18008591", "id": "18008591", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2218241928\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1914,7 +1920,11 @@ private List<AbfsHttpHeader> getMetadataHeadersList(final Hashtable<String, Stri\n       // AzureBlobFileSystem supports only ASCII Characters in property values.\n       if (isPureASCII(value)) {\n         try {\n-          value = encodeMetadataAttribute(value);\n+          // URL encoding this JSON metadata, set by the WASB Client during file creation, causes compatibility issues.\n\nReview Comment:\n   That should be sufficient\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T05:42:18.505+0000", "updated": "2025-07-21T05:42:18.505+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18008592", "id": "18008592", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#discussion_r2218244690\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -201,4 +325,1650 @@ public void testSetWorkingDirectory() throws Exception {\n     assertEquals(path3, wasb.getWorkingDirectory());\n     assertEquals(path3, abfs.getWorkingDirectory());\n   }\n+\n+  // Scenario wise testing\n+\n+  //Scenario 1: - Create and write via WASB, read via ABFS\n+  @Test\n+  public void testScenario1() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 2: - Create and write via WASB, read via ABFS and then write the same file via ABFS\n+  @Test\n+  public void testScenario2() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+    // Check file status\n+    ContractTestUtils.assertIsFile(wasb, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Write\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT1.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 3: - Create and write via ABFS and the read via WASB\n+  @Test\n+  public void testScenario3() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(wasb.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + wasb,\n+          TEST_CONTEXT, line);\n+    }\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 4:- Create via WASB, write via ABFS and then write via WASB\n+  @Test\n+  public void testScenario4() throws Exception {\n+    AzureBlobFileSystem abfs = getFileSystem();\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    wasb.create(path, true);\n+    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+      abfsOutputStream.flush();\n+      abfsOutputStream.hsync();\n+    }\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 5:- Create via ABFS, write via WASB, read via ABFS (Checksum validation disabled)\n+  @Test\n+  public void testScenario5() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  //Scenario 6: - Create via ABFS, write via WASB, read via ABFS (Checksum validation enabled)\n+  @Test\n+  public void testScenario6() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    assumeBlobServiceType();\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 7 :- Create via WASB and then create overwrite true using ABFS\n+  @Test\n+  public void testScenario7() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 8 :- Create via WASB and then create overwrite false using ABFS\n+  @Test\n+  public void testScenario8() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      abfs.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\"AlreadyExists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 9 :- Create via ABFS and then create overwrite true using WASB\n+  @Test\n+  public void testScenario9() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    wasb.create(path, true);\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 10 :- Create via ABFS and then create overwrite false using WASB\n+  @Test\n+  public void testScenario10() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    try {\n+      wasb.create(path, false);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().toLowerCase().contains(\"exists\"));\n+    }\n+\n+    // Remove file\n+    assertDeleted(abfs, path, true);\n+  }\n+\n+  // Scenario 11 :- Create via ABFS and then write via WASB and delete via ABFS\n+  @Test\n+  public void testScenario11() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    abfs.create(path, true);\n+    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n+      nativeFsStream.flush();\n+      nativeFsStream.hsync();\n+    }\n+\n+    // Check file status\n+    ContractTestUtils.assertIsFile(abfs, path);\n+\n+    try (BufferedReader br = new BufferedReader(\n+        new InputStreamReader(abfs.open(path)))) {\n+      String line = br.readLine();\n+      assertEquals(\"Wrong text from \" + abfs,\n+          TEST_CONTEXT, line);\n+    }\n+    abfs.delete(path, true);\n+  }\n+\n+  // Scenario 12 :- Create and write via ABFS and delete via WASB\n+  @Test\n+  public void testScenario12() throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+        getIsNamespaceEnabled(abfs));\n+    NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+    Path testFile = path(\"/testReadFile\");\n+    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n+\n+    // Write\n+    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n+      nativeFsStream.write(TEST_CONTEXT.getBytes());\n\nReview Comment:\n   Can we do this correction for all the tests. I still see some places where output strem is created via abfs bu still named native.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T05:44:28.624+0000", "updated": "2025-07-21T05:44:28.624+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18008669", "id": "18008669", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3095928491\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 12 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 36s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  hadoop-tools/hadoop-azure: The patch generated 0 new + 14 unchanged - 1 fixed = 14 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 165m 34s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/15/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7777 |\r\n   | Optional Tests | dupname asflicense codespell detsecrets xmllint compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle |\r\n   | uname | Linux dda14e00c156 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / dbb743f569c6528fca6376a2582f78184c4099f1 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/15/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7777/15/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T09:37:53.218+0000", "updated": "2025-07-21T09:37:53.218+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18009140", "id": "18009140", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777#issuecomment-3105885054\n\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 859, Failures: 0, Errors: 0, Skipped: 208\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 862, Failures: 0, Errors: 0, Skipped: 160\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 10\r\n    \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 701, Failures: 0, Errors: 0, Skipped: 238\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n    \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 859, Failures: 0, Errors: 0, Skipped: 219\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 706, Failures: 0, Errors: 0, Skipped: 133\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 698, Failures: 0, Errors: 0, Skipped: 240\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 703, Failures: 0, Errors: 0, Skipped: 146\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 700, Failures: 0, Errors: 0, Skipped: 188\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 733, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 698, Failures: 0, Errors: 0, Skipped: 237\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T06:08:15.132+0000", "updated": "2025-07-23T06:08:15.132+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18009141", "id": "18009141", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7777:\nURL: https://github.com/apache/hadoop/pull/7777\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T06:09:11.683+0000", "updated": "2025-07-23T06:09:11.683+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18009169", "id": "18009169", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 opened a new pull request, #7819:\nURL: https://github.com/apache/hadoop/pull/7819\n\n   Jira :- https://issues.apache.org/jira/browse/HADOOP-19604\r\n   \r\n   BlockId computation to be consistent across clients for PutBlock and PutBlockList so made use of blockCount instead of offset.\r\n   Block IDs were previously derived from the data offset, which could lead to inconsistency across different clients. The change now uses blockCount (i.e., the index of the block) to compute the Block ID, ensuring deterministic and consistent ID generation for both PutBlock and PutBlockList operations across clients.\r\n   \r\n   Restrict URL encoding of certain JSON metadata during setXAttr calls.\r\n   When setting extended attributes (xAttrs), the JSON metadata (hdi_permission) was previously URL-encoded, which could cause unnecessary escaping or compatibility issues. This change ensures that only required metadata are encoded.\r\n   \r\n   Maintain the MD5 hash of the whole block to validate data integrity during flush.\r\n   During flush operations, the MD5 hash of the entire block's data is computed and stored. This hash is later used to validate that the block correctly persisted, ensuring data integrity and helping detect corruption or transmission errors.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T07:50:57.866+0000", "updated": "2025-07-23T07:50:57.866+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18011833", "id": "18011833", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 opened a new pull request, #7853:\nURL: https://github.com/apache/hadoop/pull/7853\n\n   This PR adds a configuration flag to control the use of full blob MD5 validation during the PutBlockList (flush) operation. The functionality to validate the MD5 hash of the entire blob already existed, but it could not be toggled. With this change, the feature is now configurable and is disabled by default. When the config is set to false, the system uses the default block ID hash for integrity checks. When set to true, it performs full blob MD5 validation. This config has been introduced because full blob MD5 computation can lead to increased latency and higher CPU usage, especially for large blobs.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T10:11:10.439+0000", "updated": "2025-08-04T10:11:10.439+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18011865", "id": "18011865", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3150604036\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  21m 42s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 55s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 3 unchanged - 0 fixed = 5 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 43s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 166m 33s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7853 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 49770805f4f5 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3c249d92770fde82e4e1d0036dd1780740223cbe |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/1/testReport/ |\r\n   | Max. process+thread count | 538 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T12:59:06.723+0000", "updated": "2025-08-04T12:59:06.723+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012048", "id": "18012048", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3153826606\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  2s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  2s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 24s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  compile  |   3m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 57s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |   2m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |   2m 37s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 41s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 25s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 25s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 26s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 26s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 45s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 25s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   5m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 24s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 57s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  21m 36s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7853 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 741feb1e3351 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4c24330ce3c53ad29b5aa575b079cea7bf892aaa |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/testReport/ |\r\n   | Max. process+thread count | 78 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-05T07:25:52.377+0000", "updated": "2025-08-05T07:25:52.377+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012081", "id": "18012081", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3154389439\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  8s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 24s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/3/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/3/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  compile  |   5m 14s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 54s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  48m 29s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  48m 51s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 14s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/3/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 107m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7853 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a64467a686cd 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1200cb26cfb06ba56ba2f9211f57abd93997db27 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/3/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-05T09:45:48.380+0000", "updated": "2025-08-05T09:45:48.380+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012100", "id": "18012100", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3154831690\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 22s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 43s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m 30s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7853 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 4f92536b957f 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 493484e8eb8bfb62877642610699c147a75591cd |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/4/testReport/ |\r\n   | Max. process+thread count | 531 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-05T11:32:25.086+0000", "updated": "2025-08-05T11:32:25.086+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012587", "id": "18012587", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "Copilot commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2260273456\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1103,8 +1107,21 @@ public AbfsRestOperation flush(byte[] buffer,\n         AbfsRestOperation op1 = getPathStatus(path, true, tracingContext,\n             contextEncryptionAdapter);\n         String metadataMd5 = op1.getResult().getResponseHeader(CONTENT_MD5);\n-        if (blobMd5 != null && !blobMd5.equals(metadataMd5)) {\n-          throw ex;\n+        /*\n+         * Validate the response by comparing the server's MD5 metadata against either:\n+         * 1. The full blob content MD5 (if full blob checksum validation is enabled), or\n+         * 2. The full block ID buffer MD5 (fallback if blob checksum validation is disabled)\n+         */\n+        if (getAbfsConfiguration().isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n\nReview Comment:\n   The configuration check `getAbfsConfiguration().isFullBlobChecksumValidationEnabled()` is performed inside the response validation logic, but this same check was already done earlier in the method. Consider storing the result in a variable to avoid redundant method calls.\n   ```suggestion\n           if (fullBlobChecksumValidationEnabled && blobMd5 != null) {\n   ```\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1073,11 +1073,15 @@ public AbfsRestOperation flush(byte[] buffer,\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_LENGTH, String.valueOf(buffer.length)));\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_TYPE, APPLICATION_XML));\n     requestHeaders.add(new AbfsHttpHeader(IF_MATCH, eTag));\n+    String md5Hash = null;\n     if (leaseId != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\n     }\n-    if (blobMd5 != null) {\n+    if (isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, blobMd5));\n+    } else {\n+      md5Hash = computeMD5Hash(buffer, 0, buffer.length);\n+      requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, md5Hash));\n\nReview Comment:\n   The variable `md5Hash` is declared but only used in the else branch. Consider declaring it within the else block to improve code clarity and reduce the scope of the variable.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java:\n##########\n@@ -481,6 +481,8 @@ public void testResetCalledOnExceptionInRemoteFlush() throws Exception {\n       //expected exception\n     }\n     // Verify that reset was called on the message digest\n-    Mockito.verify(mockMessageDigest, Mockito.times(1)).reset();\n+    if (spiedClient.isChecksumValidationEnabled()) {\n\nReview Comment:\n   The test is checking `isChecksumValidationEnabled()` but the code change suggests this should be checking `isFullBlobChecksumValidationEnabled()` since the MD5 reset behavior is now conditional on full blob checksum validation, not general checksum validation.\n   ```suggestion\n       if (spiedClient.isFullBlobChecksumValidationEnabled()) {\n   ```\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1705,6 +1709,10 @@ public void setIsChecksumValidationEnabled(boolean isChecksumValidationEnabled)\n     this.isChecksumValidationEnabled = isChecksumValidationEnabled;\n   }\n \n+  public boolean isFullBlobChecksumValidationEnabled() {\n\nReview Comment:\n   The getter method `isFullBlobChecksumValidationEnabled()` lacks a corresponding setter method, which breaks the pattern established by other configuration properties like `isChecksumValidationEnabled`. This could cause issues for programmatic configuration changes.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-07T13:07:21.385+0000", "updated": "2025-08-07T13:07:21.385+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012602", "id": "18012602", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2260558410\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java:\n##########\n@@ -481,6 +481,8 @@ public void testResetCalledOnExceptionInRemoteFlush() throws Exception {\n       //expected exception\n     }\n     // Verify that reset was called on the message digest\n\nReview Comment:\n   Can we add a similar test where with config disabled we assert that methods to compute md5 hash were not called at all?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -438,6 +438,10 @@ public class AbfsConfiguration{\n       FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION)\n   private boolean isChecksumValidationEnabled;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_ABFS_ENABLE_FULL_BLOB_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_FULL_BLOB_ABFS_CHECKSUM_VALIDATION)\n\nReview Comment:\n   Do we need \"ABFS\" in Configuration Key variable name? We always have AZURE alone.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientTestUtil.java:\n##########\n@@ -166,7 +166,12 @@ public static void setMockAbfsRestOperationForFlushOperation(\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_LENGTH, String.valueOf(buffer.length)));\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_TYPE, APPLICATION_XML));\n     requestHeaders.add(new AbfsHttpHeader(IF_MATCH, eTag));\n-    requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, blobMd5));\n+    if (spiedClient.isFullBlobChecksumValidationEnabled()) {\n\nReview Comment:\n   Can be simplified by having a single statement for adding blobMd5 to request header.\r\n   blobMd5 value can be computed using a trilean operator. \r\n   \r\n   Similar can be used in production code to make sure one of the way is used to compute Md5 and variable is never null.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1103,8 +1107,21 @@ public AbfsRestOperation flush(byte[] buffer,\n         AbfsRestOperation op1 = getPathStatus(path, true, tracingContext,\n             contextEncryptionAdapter);\n         String metadataMd5 = op1.getResult().getResponseHeader(CONTENT_MD5);\n-        if (blobMd5 != null && !blobMd5.equals(metadataMd5)) {\n-          throw ex;\n+        /*\n+         * Validate the response by comparing the server's MD5 metadata against either:\n+         * 1. The full blob content MD5 (if full blob checksum validation is enabled), or\n+         * 2. The full block ID buffer MD5 (fallback if blob checksum validation is disabled)\n+         */\n+        if (getAbfsConfiguration().isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n\nReview Comment:\n   We can simply call the new method introduced in client here as well `isFullBlobChecksumValidationEnabled`\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java:\n##########\n@@ -481,6 +481,8 @@ public void testResetCalledOnExceptionInRemoteFlush() throws Exception {\n       //expected exception\n     }\n     // Verify that reset was called on the message digest\n-    Mockito.verify(mockMessageDigest, Mockito.times(1)).reset();\n+    if (spiedClient.isChecksumValidationEnabled()) {\n+      Mockito.verify(mockMessageDigest, Mockito.times(1)).reset();\n\nReview Comment:\n   Nit: assert message.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1073,11 +1073,15 @@ public AbfsRestOperation flush(byte[] buffer,\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_LENGTH, String.valueOf(buffer.length)));\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_TYPE, APPLICATION_XML));\n     requestHeaders.add(new AbfsHttpHeader(IF_MATCH, eTag));\n+    String md5Hash = null;\n     if (leaseId != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\n     }\n-    if (blobMd5 != null) {\n+    if (isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, blobMd5));\n+    } else {\n\nReview Comment:\n   Is there a config even to avoid this checksum computation?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -113,7 +114,10 @@ public void testReadFile() throws Exception {\n     boolean[] createFileWithAbfs = new boolean[]{false, true, false, true};\n     boolean[] readFileWithAbfs = new boolean[]{false, true, true, false};\n \n-    AzureBlobFileSystem abfs = getFileSystem();\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_FULL_BLOB_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n\nReview Comment:\n   When creating new instance, we need to close it. Can you check for all the tests in this class and make sure any new instance sis getting closed within test itself?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java:\n##########\n@@ -544,7 +547,12 @@ private void uploadBlockAsync(AbfsBlock blockToUpload,\n     outputStreamStatistics.bytesToUpload(bytesLength);\n     outputStreamStatistics.writeCurrentBuffer();\n     DataBlocks.BlockUploadData blockUploadData = blockToUpload.startUpload();\n-    String md5Hash = getMd5();\n+    String md5Hash;\n+    if (getClient().getAbfsConfiguration().getIsChecksumValidationEnabled()) {\n\nReview Comment:\n   `isChecksumValidationEnabled()` can be used here?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1103,8 +1107,21 @@ public AbfsRestOperation flush(byte[] buffer,\n         AbfsRestOperation op1 = getPathStatus(path, true, tracingContext,\n             contextEncryptionAdapter);\n         String metadataMd5 = op1.getResult().getResponseHeader(CONTENT_MD5);\n-        if (blobMd5 != null && !blobMd5.equals(metadataMd5)) {\n-          throw ex;\n+        /*\n+         * Validate the response by comparing the server's MD5 metadata against either:\n+         * 1. The full blob content MD5 (if full blob checksum validation is enabled), or\n+         * 2. The full block ID buffer MD5 (fallback if blob checksum validation is disabled)\n+         */\n+        if (getAbfsConfiguration().isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n+          // Full blob content MD5 mismatch \u2014 integrity check failed\n+          if (!blobMd5.equals(metadataMd5)) {\n\nReview Comment:\n   `blobMd5` and `md5Hash` are the 2 variables used here for holding the md5 hash of data. For a single flush case there will either be a valid blobMd5 or a valid md5Hash. Can we not merge them into a single variable and use that everywher? Can there be a case where both are null??\r\n   \r\n   If not we can simply use same variable for them and avoid null checks and any chances of mixmathching variables as they 2 sounds very similar.\r\n   \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-07T14:47:09.145+0000", "updated": "2025-08-07T14:47:09.145+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012746", "id": "18012746", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262065498\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -1423,6 +1423,17 @@ protected boolean isChecksumValidationEnabled() {\n     return getAbfsConfiguration().getIsChecksumValidationEnabled();\n   }\n \n+  /**\n+   * Conditions check for allowing checksum support for write operation.\n+   * Server will support this if client sends the MD5 Hash as a request header.\n+   * For azure stoage service documentation and more details refer to\n\nReview Comment:\n   Nit: storage spelling\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:38:30.985+0000", "updated": "2025-08-08T06:38:30.985+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012747", "id": "18012747", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262066009\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1073,11 +1073,15 @@ public AbfsRestOperation flush(byte[] buffer,\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_LENGTH, String.valueOf(buffer.length)));\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_TYPE, APPLICATION_XML));\n     requestHeaders.add(new AbfsHttpHeader(IF_MATCH, eTag));\n+    String md5Hash = null;\n     if (leaseId != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\n     }\n-    if (blobMd5 != null) {\n+    if (isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, blobMd5));\n+    } else {\n+      md5Hash = computeMD5Hash(buffer, 0, buffer.length);\n+      requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, md5Hash));\n\nReview Comment:\n   used later in catch block hence declared outside\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:38:43.157+0000", "updated": "2025-08-08T06:38:43.157+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012748", "id": "18012748", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262071050\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -438,6 +438,10 @@ public class AbfsConfiguration{\n       FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION)\n   private boolean isChecksumValidationEnabled;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_ABFS_ENABLE_FULL_BLOB_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_FULL_BLOB_ABFS_CHECKSUM_VALIDATION)\n\nReview Comment:\n   Took reference from the existing config for checksum validation which had abfs as well. Will remove abfs\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:40:31.121+0000", "updated": "2025-08-08T06:40:31.121+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012750", "id": "18012750", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262080793\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java:\n##########\n@@ -544,7 +547,12 @@ private void uploadBlockAsync(AbfsBlock blockToUpload,\n     outputStreamStatistics.bytesToUpload(bytesLength);\n     outputStreamStatistics.writeCurrentBuffer();\n     DataBlocks.BlockUploadData blockUploadData = blockToUpload.startUpload();\n-    String md5Hash = getMd5();\n+    String md5Hash;\n+    if (getClient().getAbfsConfiguration().getIsChecksumValidationEnabled()) {\n+      md5Hash = getMd5();\n\nReview Comment:\n   We can change it to: String md5Hash = tisChecksumValidationEnabled() ? getMd5() : null;\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:45:46.473+0000", "updated": "2025-08-08T06:45:46.473+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012752", "id": "18012752", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262083673\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1103,8 +1107,21 @@ public AbfsRestOperation flush(byte[] buffer,\n         AbfsRestOperation op1 = getPathStatus(path, true, tracingContext,\n             contextEncryptionAdapter);\n         String metadataMd5 = op1.getResult().getResponseHeader(CONTENT_MD5);\n-        if (blobMd5 != null && !blobMd5.equals(metadataMd5)) {\n-          throw ex;\n+        /*\n+         * Validate the response by comparing the server's MD5 metadata against either:\n+         * 1. The full blob content MD5 (if full blob checksum validation is enabled), or\n+         * 2. The full block ID buffer MD5 (fallback if blob checksum validation is disabled)\n+         */\n+        if (getAbfsConfiguration().isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:47:25.219+0000", "updated": "2025-08-08T06:47:25.219+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012771", "id": "18012771", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262091650\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1073,11 +1073,15 @@ public AbfsRestOperation flush(byte[] buffer,\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_LENGTH, String.valueOf(buffer.length)));\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_TYPE, APPLICATION_XML));\n     requestHeaders.add(new AbfsHttpHeader(IF_MATCH, eTag));\n+    String md5Hash = null;\n     if (leaseId != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\n     }\n-    if (blobMd5 != null) {\n+    if (isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, blobMd5));\n+    } else {\n\nReview Comment:\n   In flush for idempotency we need to compute md5, so we have conditional checks in append but in flush it's not needed\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:52:01.429+0000", "updated": "2025-08-08T06:52:01.429+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012772", "id": "18012772", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262094329\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1103,8 +1107,21 @@ public AbfsRestOperation flush(byte[] buffer,\n         AbfsRestOperation op1 = getPathStatus(path, true, tracingContext,\n             contextEncryptionAdapter);\n         String metadataMd5 = op1.getResult().getResponseHeader(CONTENT_MD5);\n-        if (blobMd5 != null && !blobMd5.equals(metadataMd5)) {\n-          throw ex;\n+        /*\n+         * Validate the response by comparing the server's MD5 metadata against either:\n+         * 1. The full blob content MD5 (if full blob checksum validation is enabled), or\n+         * 2. The full block ID buffer MD5 (fallback if blob checksum validation is disabled)\n+         */\n+        if (getAbfsConfiguration().isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n+          // Full blob content MD5 mismatch \u2014 integrity check failed\n+          if (!blobMd5.equals(metadataMd5)) {\n\nReview Comment:\n   The way in which the 2 md5 are computed are different, blobMd5 computes the entire blob data's md5 while md5Hash is used only for the md5 computation of the list of blockId's and hence we are maintaining 2 separate variables. \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:53:37.188+0000", "updated": "2025-08-08T06:53:37.188+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012773", "id": "18012773", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262095889\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java:\n##########\n@@ -544,7 +547,12 @@ private void uploadBlockAsync(AbfsBlock blockToUpload,\n     outputStreamStatistics.bytesToUpload(bytesLength);\n     outputStreamStatistics.writeCurrentBuffer();\n     DataBlocks.BlockUploadData blockUploadData = blockToUpload.startUpload();\n-    String md5Hash = getMd5();\n+    String md5Hash;\n+    if (getClient().getAbfsConfiguration().getIsChecksumValidationEnabled()) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:54:32.287+0000", "updated": "2025-08-08T06:54:32.287+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012774", "id": "18012774", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262098033\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -1423,6 +1423,17 @@ protected boolean isChecksumValidationEnabled() {\n     return getAbfsConfiguration().getIsChecksumValidationEnabled();\n   }\n \n+  /**\n+   * Conditions check for allowing checksum support for write operation.\n+   * Server will support this if client sends the MD5 Hash as a request header.\n+   * For azure stoage service documentation and more details refer to\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:55:37.640+0000", "updated": "2025-08-08T06:55:37.640+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012775", "id": "18012775", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262099031\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java:\n##########\n@@ -544,7 +547,12 @@ private void uploadBlockAsync(AbfsBlock blockToUpload,\n     outputStreamStatistics.bytesToUpload(bytesLength);\n     outputStreamStatistics.writeCurrentBuffer();\n     DataBlocks.BlockUploadData blockUploadData = blockToUpload.startUpload();\n-    String md5Hash = getMd5();\n+    String md5Hash;\n+    if (getClient().getAbfsConfiguration().getIsChecksumValidationEnabled()) {\n+      md5Hash = getMd5();\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:56:12.463+0000", "updated": "2025-08-08T06:56:12.463+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012776", "id": "18012776", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262099471\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureDFSIngressHandler.java:\n##########\n@@ -179,7 +179,10 @@ protected synchronized AbfsRestOperation remoteFlush(final long offset,\n       tracingContextFlush.setIngressHandler(DFS_FLUSH);\n       tracingContextFlush.setPosition(String.valueOf(offset));\n     }\n-    String fullBlobMd5 = computeFullBlobMd5();\n+    String fullBlobMd5 = null;\n+    if (getClient().isFullBlobChecksumValidationEnabled()) {\n\nReview Comment:\n   Since the check getClient().isFullBlobChecksumValidationEnabled() is common in both DFS, Blob ingress handlers- we can add it as a common method in the abstract class itself\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T06:56:27.495+0000", "updated": "2025-08-08T06:56:27.495+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012794", "id": "18012794", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262249546\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java:\n##########\n@@ -481,6 +481,8 @@ public void testResetCalledOnExceptionInRemoteFlush() throws Exception {\n       //expected exception\n     }\n     // Verify that reset was called on the message digest\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T08:12:04.458+0000", "updated": "2025-08-08T08:12:04.458+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012796", "id": "18012796", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262266342\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java:\n##########\n@@ -481,6 +481,8 @@ public void testResetCalledOnExceptionInRemoteFlush() throws Exception {\n       //expected exception\n     }\n     // Verify that reset was called on the message digest\n-    Mockito.verify(mockMessageDigest, Mockito.times(1)).reset();\n+    if (spiedClient.isChecksumValidationEnabled()) {\n+      Mockito.verify(mockMessageDigest, Mockito.times(1)).reset();\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T08:19:02.197+0000", "updated": "2025-08-08T08:19:02.197+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012797", "id": "18012797", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262094329\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1103,8 +1107,21 @@ public AbfsRestOperation flush(byte[] buffer,\n         AbfsRestOperation op1 = getPathStatus(path, true, tracingContext,\n             contextEncryptionAdapter);\n         String metadataMd5 = op1.getResult().getResponseHeader(CONTENT_MD5);\n-        if (blobMd5 != null && !blobMd5.equals(metadataMd5)) {\n-          throw ex;\n+        /*\n+         * Validate the response by comparing the server's MD5 metadata against either:\n+         * 1. The full blob content MD5 (if full blob checksum validation is enabled), or\n+         * 2. The full block ID buffer MD5 (fallback if blob checksum validation is disabled)\n+         */\n+        if (getAbfsConfiguration().isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n+          // Full blob content MD5 mismatch \u2014 integrity check failed\n+          if (!blobMd5.equals(metadataMd5)) {\n\nReview Comment:\n   The way in which the 2 md5 are computed are different, blobMd5 computes the entire blob data's md5 while md5Hash is used only for the md5 computation of the list of blockId's and hence we are maintaining 2 separate variables. \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T08:29:55.927+0000", "updated": "2025-08-08T08:29:55.927+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012798", "id": "18012798", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262302799\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1103,8 +1107,21 @@ public AbfsRestOperation flush(byte[] buffer,\n         AbfsRestOperation op1 = getPathStatus(path, true, tracingContext,\n             contextEncryptionAdapter);\n         String metadataMd5 = op1.getResult().getResponseHeader(CONTENT_MD5);\n-        if (blobMd5 != null && !blobMd5.equals(metadataMd5)) {\n-          throw ex;\n+        /*\n+         * Validate the response by comparing the server's MD5 metadata against either:\n+         * 1. The full blob content MD5 (if full blob checksum validation is enabled), or\n+         * 2. The full block ID buffer MD5 (fallback if blob checksum validation is disabled)\n+         */\n+        if (getAbfsConfiguration().isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n+          // Full blob content MD5 mismatch \u2014 integrity check failed\n+          if (!blobMd5.equals(metadataMd5)) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T08:32:08.934+0000", "updated": "2025-08-08T08:32:08.934+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012799", "id": "18012799", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262303420\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientTestUtil.java:\n##########\n@@ -166,7 +166,12 @@ public static void setMockAbfsRestOperationForFlushOperation(\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_LENGTH, String.valueOf(buffer.length)));\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_TYPE, APPLICATION_XML));\n     requestHeaders.add(new AbfsHttpHeader(IF_MATCH, eTag));\n-    requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, blobMd5));\n+    if (spiedClient.isFullBlobChecksumValidationEnabled()) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T08:32:26.085+0000", "updated": "2025-08-08T08:32:26.085+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012844", "id": "18012844", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262739105\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -113,7 +114,10 @@ public void testReadFile() throws Exception {\n     boolean[] createFileWithAbfs = new boolean[]{false, true, false, true};\n     boolean[] readFileWithAbfs = new boolean[]{false, true, true, false};\n \n-    AzureBlobFileSystem abfs = getFileSystem();\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_FULL_BLOB_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T11:50:36.895+0000", "updated": "2025-08-08T11:50:36.895+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012845", "id": "18012845", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262742932\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AzureDFSIngressHandler.java:\n##########\n@@ -179,7 +179,10 @@ protected synchronized AbfsRestOperation remoteFlush(final long offset,\n       tracingContextFlush.setIngressHandler(DFS_FLUSH);\n       tracingContextFlush.setPosition(String.valueOf(offset));\n     }\n-    String fullBlobMd5 = computeFullBlobMd5();\n+    String fullBlobMd5 = null;\n+    if (getClient().isFullBlobChecksumValidationEnabled()) {\n\nReview Comment:\n   That would not result in any major improvement as it's just called at 2 places\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T11:51:31.951+0000", "updated": "2025-08-08T11:51:31.951+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012848", "id": "18012848", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262766252\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1705,6 +1709,10 @@ public void setIsChecksumValidationEnabled(boolean isChecksumValidationEnabled)\n     this.isChecksumValidationEnabled = isChecksumValidationEnabled;\n   }\n \n+  public boolean isFullBlobChecksumValidationEnabled() {\n\nReview Comment:\n   setter is not used anywhere\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T11:59:37.655+0000", "updated": "2025-08-08T11:59:37.655+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012852", "id": "18012852", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2262817511\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java:\n##########\n@@ -481,6 +481,8 @@ public void testResetCalledOnExceptionInRemoteFlush() throws Exception {\n       //expected exception\n     }\n     // Verify that reset was called on the message digest\n-    Mockito.verify(mockMessageDigest, Mockito.times(1)).reset();\n+    if (spiedClient.isChecksumValidationEnabled()) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T12:16:38.806+0000", "updated": "2025-08-08T12:16:38.806+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012871", "id": "18012871", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3168127827\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 59s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 44s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/5/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 3 new + 3 unchanged - 0 fixed = 6 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 44s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m 56s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7853 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b07bf329deaa 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f8a0a649689783cd2dc5e17b18ec4044dd0166d4 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/5/testReport/ |\r\n   | Max. process+thread count | 531 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T14:22:29.574+0000", "updated": "2025-08-08T14:22:29.574+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18012878", "id": "18012878", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3168189873\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  42m 33s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 42s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m 32s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7853 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a034cec179e6 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 20f358218ab04b08474e0829fe8dc653d813132b |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/6/testReport/ |\r\n   | Max. process+thread count | 563 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T14:45:14.429+0000", "updated": "2025-08-08T14:45:14.429+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18014797", "id": "18014797", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2284098667\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -447,34 +473,37 @@ public void testScenario3() throws Exception {\n    */\n   @Test\n   public void testScenario4() throws Exception {\n-    AzureBlobFileSystem abfs = getFileSystem();\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-    assumeBlobServiceType();\n-    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n-\n-    Path testFile = path(\"/testReadFile\");\n-    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n-\n-    // Write\n-    wasb.create(path, true);\n-    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n-      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n-      abfsOutputStream.flush();\n-      abfsOutputStream.hsync();\n-    }\n-\n-    try (FSDataOutputStream nativeFsStream = wasb.append(path)) {\n-      nativeFsStream.write(TEST_CONTEXT1.getBytes());\n-      nativeFsStream.flush();\n-      nativeFsStream.hsync();\n+    try (AzureBlobFileSystem abfs = getFileSystem()) {\n+      Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n\nReview Comment:\n   Let's move all the assume into a common place and they could be first line in each method.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -299,32 +311,34 @@ public void testUrlConversion() {\n   @Test\n   public void testSetWorkingDirectory() throws Exception {\n     //create folders\n-    AzureBlobFileSystem abfs = getFileSystem();\n-    // test only valid for non-namespace enabled account\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n-\n-    Path d1 = path(\"/d1\");\n-    Path d1d4 = new Path(d1 + \"/d2/d3/d4\");\n-    assertMkdirs(abfs, d1d4);\n-\n-    //set working directory to path1\n-    Path path1 = new Path(d1 + \"/d2\");\n-    wasb.setWorkingDirectory(path1);\n-    abfs.setWorkingDirectory(path1);\n-    assertEquals(path1, wasb.getWorkingDirectory());\n-    assertEquals(path1, abfs.getWorkingDirectory());\n-\n-    //set working directory to path2\n-    Path path2 = new Path(\"d3/d4\");\n-    wasb.setWorkingDirectory(path2);\n-    abfs.setWorkingDirectory(path2);\n-\n-    Path path3 = d1d4;\n-    assertEquals(path3, wasb.getWorkingDirectory());\n-    assertEquals(path3, abfs.getWorkingDirectory());\n+    try (AzureBlobFileSystem abfs = getFileSystem()) {\n+      // test only valid for non-namespace enabled account\n+      Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n\nReview Comment:\n   we can use base class method `assumeHnsDisabled()` for better readability here and other places.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java:\n##########\n@@ -481,6 +482,103 @@ public void testResetCalledOnExceptionInRemoteFlush() throws Exception {\n       //expected exception\n     }\n     // Verify that reset was called on the message digest\n-    Mockito.verify(mockMessageDigest, Mockito.times(1)).reset();\n+    if (spiedClient.isFullBlobChecksumValidationEnabled()) {\n+      Assertions.assertThat(Mockito.mockingDetails(mockMessageDigest).getInvocations()\n+          .stream()\n+          .filter(i -> i.getMethod().getName().equals(\"reset\"))\n+          .count())\n+          .as(\"Expected MessageDigest.reset() to be called exactly once when checksum validation is enabled\")\n+          .isEqualTo(1);\n+    }\n+  }\n+\n+  /**\n+   * Tests that the message digest is reset when an exception occurs during remote flush.\n+   * Simulates a failure in the flush operation and verifies reset is called on MessageDigest.\n+   */\n+  @Test\n+  public void testNoChecksumComputedWhenConfigFalse()  throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem fs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeTrue(!getIsNamespaceEnabled(fs));\n+    AzureBlobFileSystemStore store = Mockito.spy(fs.getAbfsStore());\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+\n+    // Create spies for the client handler and blob client\n+    AbfsClientHandler clientHandler = Mockito.spy(store.getClientHandler());\n+    AbfsBlobClient blobClient = Mockito.spy(clientHandler.getBlobClient());\n+\n+    // Set up the spies to return the mocked objects\n+    Mockito.doReturn(clientHandler).when(store).getClientHandler();\n+    Mockito.doReturn(blobClient).when(clientHandler).getBlobClient();\n+    Mockito.doReturn(blobClient).when(clientHandler).getIngressClient();\n+    AbfsOutputStream abfsOutputStream = Mockito.spy(\n+        (AbfsOutputStream) fs.create(new Path(\"/test/file\")).getWrappedStream());\n+    AzureIngressHandler ingressHandler = Mockito.spy(\n+        abfsOutputStream.getIngressHandler());\n+    Mockito.doReturn(ingressHandler).when(abfsOutputStream).getIngressHandler();\n+    Mockito.doReturn(blobClient).when(ingressHandler).getClient();\n+    FSDataOutputStream os = Mockito.spy(\n+        new FSDataOutputStream(abfsOutputStream, null));\n+    AbfsOutputStream out = (AbfsOutputStream) os.getWrappedStream();\n+    byte[] bytes = new byte[1024 * 1024 * 4];\n+    new Random().nextBytes(bytes);\n+    // Write some bytes and attempt to flush, which should retry\n+    out.write(bytes);\n+    out.hsync();\n+    Assertions.assertThat(Mockito.mockingDetails(blobClient).getInvocations()\n+        .stream()\n+        .filter(i -> i.getMethod().getName().equals(\"addCheckSumHeaderForWrite\"))\n+        .count())\n+        .as(\"Expected addCheckSumHeaderForWrite() to be called exactly 0 times\")\n+        .isZero();\n+  }\n+\n+  /**\n+   * Tests that the message digest is reset when an exception occurs during remote flush.\n+   * Simulates a failure in the flush operation and verifies reset is called on MessageDigest.\n+   */\n+  @Test\n+  public void testChecksumComputedWhenConfigTrue()  throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem fs = (AzureBlobFileSystem) fileSystem;\n\nReview Comment:\n   try() needed here as it is a new Instance\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -113,121 +114,129 @@ public void testReadFile() throws Exception {\n     boolean[] createFileWithAbfs = new boolean[]{false, true, false, true};\n     boolean[] readFileWithAbfs = new boolean[]{false, true, true, false};\n \n-    AzureBlobFileSystem abfs = getFileSystem();\n-    // test only valid for non-namespace enabled account\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n-\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ENABLE_FULL_BLOB_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    try (AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem) {\n+      // test only valid for non-namespace enabled account\n+      Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+          getIsNamespaceEnabled(abfs));\n+      Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+\n+      NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+      Path testFile = path(\"/testReadFile\");\n+      for (int i = 0; i < 4; i++) {\n+        Path path = new Path(testFile + \"/~12/!008/testfile\" + i);\n+        final FileSystem createFs = createFileWithAbfs[i] ? abfs : wasb;\n+        // Read\n+        final FileSystem readFs = readFileWithAbfs[i] ? abfs : wasb;\n+        // Write\n+        try (FSDataOutputStream nativeFsStream = createFs.create(path, true)) {\n+          nativeFsStream.write(TEST_CONTEXT.getBytes());\n+          nativeFsStream.flush();\n+          nativeFsStream.hsync();\n+        }\n+\n+        // Check file status\n+        ContractTestUtils.assertIsFile(createFs, path);\n+\n+        try (BufferedReader br = new BufferedReader(\n+            new InputStreamReader(readFs.open(path)))) {\n+          String line = br.readLine();\n+          assertEquals(\"Wrong text from \" + readFs,\n+              TEST_CONTEXT, line);\n+        }\n+\n+        // Remove file\n+        assertDeleted(readFs, path, true);\n+      }\n+    }\n+  }\n \n-    Path testFile = path(\"/testReadFile\");\n-    for (int i = 0; i < 4; i++) {\n-      Path path = new Path(testFile + \"/~12/!008/testfile\" + i);\n-      final FileSystem createFs = createFileWithAbfs[i] ? abfs : wasb;\n-      // Read\n-      final FileSystem readFs = readFileWithAbfs[i] ? abfs : wasb;\n+  /**\n+   * Flow: Create and write a file using WASB, then read and append to it using ABFS. Finally, delete the file via ABFS after verifying content consistency.\n+   * Expected: WASB successfully creates the file and writes content. ABFS reads, appends, and deletes the file without data loss or errors.\n+   */\n+  @Test\n+  public void testwriteFile() throws Exception {\n+    try (AzureBlobFileSystem abfs = getFileSystem()) {\n\nReview Comment:\n   Nit: Here we are not creating a new instance. We are using the instance created by base class and that will be autoclosed during tear down. try() is redundant here.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java:\n##########\n@@ -481,6 +482,103 @@ public void testResetCalledOnExceptionInRemoteFlush() throws Exception {\n       //expected exception\n     }\n     // Verify that reset was called on the message digest\n-    Mockito.verify(mockMessageDigest, Mockito.times(1)).reset();\n+    if (spiedClient.isFullBlobChecksumValidationEnabled()) {\n+      Assertions.assertThat(Mockito.mockingDetails(mockMessageDigest).getInvocations()\n+          .stream()\n+          .filter(i -> i.getMethod().getName().equals(\"reset\"))\n+          .count())\n+          .as(\"Expected MessageDigest.reset() to be called exactly once when checksum validation is enabled\")\n+          .isEqualTo(1);\n+    }\n+  }\n+\n+  /**\n+   * Tests that the message digest is reset when an exception occurs during remote flush.\n+   * Simulates a failure in the flush operation and verifies reset is called on MessageDigest.\n+   */\n+  @Test\n+  public void testNoChecksumComputedWhenConfigFalse()  throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n\nReview Comment:\n   try() needed here\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -299,32 +311,34 @@ public void testUrlConversion() {\n   @Test\n   public void testSetWorkingDirectory() throws Exception {\n     //create folders\n-    AzureBlobFileSystem abfs = getFileSystem();\n-    // test only valid for non-namespace enabled account\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n-\n-    Path d1 = path(\"/d1\");\n-    Path d1d4 = new Path(d1 + \"/d2/d3/d4\");\n-    assertMkdirs(abfs, d1d4);\n-\n-    //set working directory to path1\n-    Path path1 = new Path(d1 + \"/d2\");\n-    wasb.setWorkingDirectory(path1);\n-    abfs.setWorkingDirectory(path1);\n-    assertEquals(path1, wasb.getWorkingDirectory());\n-    assertEquals(path1, abfs.getWorkingDirectory());\n-\n-    //set working directory to path2\n-    Path path2 = new Path(\"d3/d4\");\n-    wasb.setWorkingDirectory(path2);\n-    abfs.setWorkingDirectory(path2);\n-\n-    Path path3 = d1d4;\n-    assertEquals(path3, wasb.getWorkingDirectory());\n-    assertEquals(path3, abfs.getWorkingDirectory());\n+    try (AzureBlobFileSystem abfs = getFileSystem()) {\n+      // test only valid for non-namespace enabled account\n+      Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n\nReview Comment:\n   Nit: Also I suppose all the tests in this class need FNS account, may be we canmove this assume in constuctor itself to skip whole file instead of checking individual tests\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -113,121 +114,129 @@ public void testReadFile() throws Exception {\n     boolean[] createFileWithAbfs = new boolean[]{false, true, false, true};\n     boolean[] readFileWithAbfs = new boolean[]{false, true, true, false};\n \n-    AzureBlobFileSystem abfs = getFileSystem();\n-    // test only valid for non-namespace enabled account\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n-\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ENABLE_FULL_BLOB_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    try (AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem) {\n+      // test only valid for non-namespace enabled account\n+      Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+          getIsNamespaceEnabled(abfs));\n+      Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+\n+      NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+      Path testFile = path(\"/testReadFile\");\n+      for (int i = 0; i < 4; i++) {\n+        Path path = new Path(testFile + \"/~12/!008/testfile\" + i);\n+        final FileSystem createFs = createFileWithAbfs[i] ? abfs : wasb;\n+        // Read\n+        final FileSystem readFs = readFileWithAbfs[i] ? abfs : wasb;\n+        // Write\n+        try (FSDataOutputStream nativeFsStream = createFs.create(path, true)) {\n+          nativeFsStream.write(TEST_CONTEXT.getBytes());\n+          nativeFsStream.flush();\n+          nativeFsStream.hsync();\n+        }\n+\n+        // Check file status\n+        ContractTestUtils.assertIsFile(createFs, path);\n+\n+        try (BufferedReader br = new BufferedReader(\n+            new InputStreamReader(readFs.open(path)))) {\n+          String line = br.readLine();\n+          assertEquals(\"Wrong text from \" + readFs,\n+              TEST_CONTEXT, line);\n+        }\n+\n+        // Remove file\n+        assertDeleted(readFs, path, true);\n+      }\n+    }\n+  }\n \n-    Path testFile = path(\"/testReadFile\");\n-    for (int i = 0; i < 4; i++) {\n-      Path path = new Path(testFile + \"/~12/!008/testfile\" + i);\n-      final FileSystem createFs = createFileWithAbfs[i] ? abfs : wasb;\n-      // Read\n-      final FileSystem readFs = readFileWithAbfs[i] ? abfs : wasb;\n+  /**\n+   * Flow: Create and write a file using WASB, then read and append to it using ABFS. Finally, delete the file via ABFS after verifying content consistency.\n+   * Expected: WASB successfully creates the file and writes content. ABFS reads, appends, and deletes the file without data loss or errors.\n+   */\n+  @Test\n+  public void testwriteFile() throws Exception {\n+    try (AzureBlobFileSystem abfs = getFileSystem()) {\n+      Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+          getIsNamespaceEnabled(abfs));\n+      assumeBlobServiceType();\n+      Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+      NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+      Path testFile = path(\"/testReadFile\");\n+      Path path = new Path(\n+          testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n       // Write\n-      try (FSDataOutputStream nativeFsStream = createFs.create(path, true)) {\n+      try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n         nativeFsStream.write(TEST_CONTEXT.getBytes());\n         nativeFsStream.flush();\n         nativeFsStream.hsync();\n       }\n \n       // Check file status\n-      ContractTestUtils.assertIsFile(createFs, path);\n+      ContractTestUtils.assertIsFile(wasb, path);\n \n       try (BufferedReader br = new BufferedReader(\n-          new InputStreamReader(readFs.open(path)))) {\n+          new InputStreamReader(abfs.open(path)))) {\n         String line = br.readLine();\n-        assertEquals(\"Wrong text from \" + readFs,\n+        assertEquals(\"Wrong text from \" + abfs,\n             TEST_CONTEXT, line);\n       }\n-\n+      try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+        abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+        abfsOutputStream.flush();\n+        abfsOutputStream.hsync();\n+      }\n       // Remove file\n-      assertDeleted(readFs, path, true);\n+      assertDeleted(abfs, path, true);\n     }\n   }\n \n-  /**\n-   * Flow: Create and write a file using WASB, then read and append to it using ABFS. Finally, delete the file via ABFS after verifying content consistency.\n-   * Expected: WASB successfully creates the file and writes content. ABFS reads, appends, and deletes the file without data loss or errors.\n-   */\n-  @Test\n-  public void testwriteFile() throws Exception {\n-    AzureBlobFileSystem abfs = getFileSystem();\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-    assumeBlobServiceType();\n-    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n-\n-    Path testFile = path(\"/testReadFile\");\n-    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n-    // Write\n-    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n-      nativeFsStream.write(TEST_CONTEXT.getBytes());\n-      nativeFsStream.flush();\n-      nativeFsStream.hsync();\n-    }\n-\n-    // Check file status\n-    ContractTestUtils.assertIsFile(wasb, path);\n-\n-    try (BufferedReader br = new BufferedReader(\n-        new InputStreamReader(abfs.open(path)))) {\n-      String line = br.readLine();\n-      assertEquals(\"Wrong text from \" + abfs,\n-          TEST_CONTEXT, line);\n-    }\n-    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n-      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n-      abfsOutputStream.flush();\n-      abfsOutputStream.hsync();\n-    }\n-    // Remove file\n-    assertDeleted(abfs, path, true);\n-  }\n-\n   /**\n    * Flow: Create and write a file using ABFS, append to the file using WASB, then write again using ABFS.\n    * Expected: File is created and written correctly by ABFS, appended by WASB, and final ABFS write reflects all updates without errors.\n    */\n \n   @Test\n   public void testwriteFile1() throws Exception {\n-    AzureBlobFileSystem abfs = getFileSystem();\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-    assumeBlobServiceType();\n-    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n-\n-    Path testFile = path(\"/testReadFile\");\n-    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n-    // Write\n-    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n-      nativeFsStream.write(TEST_CONTEXT.getBytes());\n-      nativeFsStream.flush();\n-      nativeFsStream.hsync();\n-    }\n+    try (AzureBlobFileSystem abfs = getFileSystem()) {\n\nReview Comment:\n   Same as above and a few places below, no need to close this.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-19T05:32:31.813+0000", "updated": "2025-08-19T05:32:31.813+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18015133", "id": "18015133", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2287859561\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -113,121 +114,129 @@ public void testReadFile() throws Exception {\n     boolean[] createFileWithAbfs = new boolean[]{false, true, false, true};\n     boolean[] readFileWithAbfs = new boolean[]{false, true, true, false};\n \n-    AzureBlobFileSystem abfs = getFileSystem();\n-    // test only valid for non-namespace enabled account\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n-\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ENABLE_FULL_BLOB_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    try (AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem) {\n+      // test only valid for non-namespace enabled account\n+      Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+          getIsNamespaceEnabled(abfs));\n+      Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+\n+      NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+      Path testFile = path(\"/testReadFile\");\n+      for (int i = 0; i < 4; i++) {\n+        Path path = new Path(testFile + \"/~12/!008/testfile\" + i);\n+        final FileSystem createFs = createFileWithAbfs[i] ? abfs : wasb;\n+        // Read\n+        final FileSystem readFs = readFileWithAbfs[i] ? abfs : wasb;\n+        // Write\n+        try (FSDataOutputStream nativeFsStream = createFs.create(path, true)) {\n+          nativeFsStream.write(TEST_CONTEXT.getBytes());\n+          nativeFsStream.flush();\n+          nativeFsStream.hsync();\n+        }\n+\n+        // Check file status\n+        ContractTestUtils.assertIsFile(createFs, path);\n+\n+        try (BufferedReader br = new BufferedReader(\n+            new InputStreamReader(readFs.open(path)))) {\n+          String line = br.readLine();\n+          assertEquals(\"Wrong text from \" + readFs,\n+              TEST_CONTEXT, line);\n+        }\n+\n+        // Remove file\n+        assertDeleted(readFs, path, true);\n+      }\n+    }\n+  }\n \n-    Path testFile = path(\"/testReadFile\");\n-    for (int i = 0; i < 4; i++) {\n-      Path path = new Path(testFile + \"/~12/!008/testfile\" + i);\n-      final FileSystem createFs = createFileWithAbfs[i] ? abfs : wasb;\n-      // Read\n-      final FileSystem readFs = readFileWithAbfs[i] ? abfs : wasb;\n+  /**\n+   * Flow: Create and write a file using WASB, then read and append to it using ABFS. Finally, delete the file via ABFS after verifying content consistency.\n+   * Expected: WASB successfully creates the file and writes content. ABFS reads, appends, and deletes the file without data loss or errors.\n+   */\n+  @Test\n+  public void testwriteFile() throws Exception {\n+    try (AzureBlobFileSystem abfs = getFileSystem()) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-20T11:25:23.032+0000", "updated": "2025-08-20T11:25:23.032+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18015139", "id": "18015139", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2287903415\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -113,121 +114,129 @@ public void testReadFile() throws Exception {\n     boolean[] createFileWithAbfs = new boolean[]{false, true, false, true};\n     boolean[] readFileWithAbfs = new boolean[]{false, true, true, false};\n \n-    AzureBlobFileSystem abfs = getFileSystem();\n-    // test only valid for non-namespace enabled account\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n-\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ENABLE_FULL_BLOB_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    try (AzureBlobFileSystem abfs = (AzureBlobFileSystem) fileSystem) {\n+      // test only valid for non-namespace enabled account\n+      Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+          getIsNamespaceEnabled(abfs));\n+      Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+\n+      NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+      Path testFile = path(\"/testReadFile\");\n+      for (int i = 0; i < 4; i++) {\n+        Path path = new Path(testFile + \"/~12/!008/testfile\" + i);\n+        final FileSystem createFs = createFileWithAbfs[i] ? abfs : wasb;\n+        // Read\n+        final FileSystem readFs = readFileWithAbfs[i] ? abfs : wasb;\n+        // Write\n+        try (FSDataOutputStream nativeFsStream = createFs.create(path, true)) {\n+          nativeFsStream.write(TEST_CONTEXT.getBytes());\n+          nativeFsStream.flush();\n+          nativeFsStream.hsync();\n+        }\n+\n+        // Check file status\n+        ContractTestUtils.assertIsFile(createFs, path);\n+\n+        try (BufferedReader br = new BufferedReader(\n+            new InputStreamReader(readFs.open(path)))) {\n+          String line = br.readLine();\n+          assertEquals(\"Wrong text from \" + readFs,\n+              TEST_CONTEXT, line);\n+        }\n+\n+        // Remove file\n+        assertDeleted(readFs, path, true);\n+      }\n+    }\n+  }\n \n-    Path testFile = path(\"/testReadFile\");\n-    for (int i = 0; i < 4; i++) {\n-      Path path = new Path(testFile + \"/~12/!008/testfile\" + i);\n-      final FileSystem createFs = createFileWithAbfs[i] ? abfs : wasb;\n-      // Read\n-      final FileSystem readFs = readFileWithAbfs[i] ? abfs : wasb;\n+  /**\n+   * Flow: Create and write a file using WASB, then read and append to it using ABFS. Finally, delete the file via ABFS after verifying content consistency.\n+   * Expected: WASB successfully creates the file and writes content. ABFS reads, appends, and deletes the file without data loss or errors.\n+   */\n+  @Test\n+  public void testwriteFile() throws Exception {\n+    try (AzureBlobFileSystem abfs = getFileSystem()) {\n+      Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n+          getIsNamespaceEnabled(abfs));\n+      assumeBlobServiceType();\n+      Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+      NativeAzureFileSystem wasb = getWasbFileSystem();\n+\n+      Path testFile = path(\"/testReadFile\");\n+      Path path = new Path(\n+          testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n       // Write\n-      try (FSDataOutputStream nativeFsStream = createFs.create(path, true)) {\n+      try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n         nativeFsStream.write(TEST_CONTEXT.getBytes());\n         nativeFsStream.flush();\n         nativeFsStream.hsync();\n       }\n \n       // Check file status\n-      ContractTestUtils.assertIsFile(createFs, path);\n+      ContractTestUtils.assertIsFile(wasb, path);\n \n       try (BufferedReader br = new BufferedReader(\n-          new InputStreamReader(readFs.open(path)))) {\n+          new InputStreamReader(abfs.open(path)))) {\n         String line = br.readLine();\n-        assertEquals(\"Wrong text from \" + readFs,\n+        assertEquals(\"Wrong text from \" + abfs,\n             TEST_CONTEXT, line);\n       }\n-\n+      try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n+        abfsOutputStream.write(TEST_CONTEXT.getBytes());\n+        abfsOutputStream.flush();\n+        abfsOutputStream.hsync();\n+      }\n       // Remove file\n-      assertDeleted(readFs, path, true);\n+      assertDeleted(abfs, path, true);\n     }\n   }\n \n-  /**\n-   * Flow: Create and write a file using WASB, then read and append to it using ABFS. Finally, delete the file via ABFS after verifying content consistency.\n-   * Expected: WASB successfully creates the file and writes content. ABFS reads, appends, and deletes the file without data loss or errors.\n-   */\n-  @Test\n-  public void testwriteFile() throws Exception {\n-    AzureBlobFileSystem abfs = getFileSystem();\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-    assumeBlobServiceType();\n-    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n-\n-    Path testFile = path(\"/testReadFile\");\n-    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n-    // Write\n-    try (FSDataOutputStream nativeFsStream = wasb.create(path, true)) {\n-      nativeFsStream.write(TEST_CONTEXT.getBytes());\n-      nativeFsStream.flush();\n-      nativeFsStream.hsync();\n-    }\n-\n-    // Check file status\n-    ContractTestUtils.assertIsFile(wasb, path);\n-\n-    try (BufferedReader br = new BufferedReader(\n-        new InputStreamReader(abfs.open(path)))) {\n-      String line = br.readLine();\n-      assertEquals(\"Wrong text from \" + abfs,\n-          TEST_CONTEXT, line);\n-    }\n-    try (FSDataOutputStream abfsOutputStream = abfs.append(path)) {\n-      abfsOutputStream.write(TEST_CONTEXT.getBytes());\n-      abfsOutputStream.flush();\n-      abfsOutputStream.hsync();\n-    }\n-    // Remove file\n-    assertDeleted(abfs, path, true);\n-  }\n-\n   /**\n    * Flow: Create and write a file using ABFS, append to the file using WASB, then write again using ABFS.\n    * Expected: File is created and written correctly by ABFS, appended by WASB, and final ABFS write reflects all updates without errors.\n    */\n \n   @Test\n   public void testwriteFile1() throws Exception {\n-    AzureBlobFileSystem abfs = getFileSystem();\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-    assumeBlobServiceType();\n-    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n-\n-    Path testFile = path(\"/testReadFile\");\n-    Path path = new Path(testFile + \"/~12/!008/testfile_\" + UUID.randomUUID());\n-    // Write\n-    try (FSDataOutputStream nativeFsStream = abfs.create(path, true)) {\n-      nativeFsStream.write(TEST_CONTEXT.getBytes());\n-      nativeFsStream.flush();\n-      nativeFsStream.hsync();\n-    }\n+    try (AzureBlobFileSystem abfs = getFileSystem()) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-20T11:43:31.132+0000", "updated": "2025-08-20T11:43:31.132+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18015144", "id": "18015144", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2287958397\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -299,32 +311,34 @@ public void testUrlConversion() {\n   @Test\n   public void testSetWorkingDirectory() throws Exception {\n     //create folders\n-    AzureBlobFileSystem abfs = getFileSystem();\n-    // test only valid for non-namespace enabled account\n-    Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n-        getIsNamespaceEnabled(abfs));\n-\n-    NativeAzureFileSystem wasb = getWasbFileSystem();\n-\n-    Path d1 = path(\"/d1\");\n-    Path d1d4 = new Path(d1 + \"/d2/d3/d4\");\n-    assertMkdirs(abfs, d1d4);\n-\n-    //set working directory to path1\n-    Path path1 = new Path(d1 + \"/d2\");\n-    wasb.setWorkingDirectory(path1);\n-    abfs.setWorkingDirectory(path1);\n-    assertEquals(path1, wasb.getWorkingDirectory());\n-    assertEquals(path1, abfs.getWorkingDirectory());\n-\n-    //set working directory to path2\n-    Path path2 = new Path(\"d3/d4\");\n-    wasb.setWorkingDirectory(path2);\n-    abfs.setWorkingDirectory(path2);\n-\n-    Path path3 = d1d4;\n-    assertEquals(path3, wasb.getWorkingDirectory());\n-    assertEquals(path3, abfs.getWorkingDirectory());\n+    try (AzureBlobFileSystem abfs = getFileSystem()) {\n+      // test only valid for non-namespace enabled account\n+      Assume.assumeFalse(\"Namespace enabled account does not support this test\",\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-20T12:06:14.302+0000", "updated": "2025-08-20T12:06:14.302+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18015159", "id": "18015159", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2288074252\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java:\n##########\n@@ -481,6 +482,103 @@ public void testResetCalledOnExceptionInRemoteFlush() throws Exception {\n       //expected exception\n     }\n     // Verify that reset was called on the message digest\n-    Mockito.verify(mockMessageDigest, Mockito.times(1)).reset();\n+    if (spiedClient.isFullBlobChecksumValidationEnabled()) {\n+      Assertions.assertThat(Mockito.mockingDetails(mockMessageDigest).getInvocations()\n+          .stream()\n+          .filter(i -> i.getMethod().getName().equals(\"reset\"))\n+          .count())\n+          .as(\"Expected MessageDigest.reset() to be called exactly once when checksum validation is enabled\")\n+          .isEqualTo(1);\n+    }\n+  }\n+\n+  /**\n+   * Tests that the message digest is reset when an exception occurs during remote flush.\n+   * Simulates a failure in the flush operation and verifies reset is called on MessageDigest.\n+   */\n+  @Test\n+  public void testNoChecksumComputedWhenConfigFalse()  throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem fs = (AzureBlobFileSystem) fileSystem;\n+    Assume.assumeTrue(!getIsNamespaceEnabled(fs));\n+    AzureBlobFileSystemStore store = Mockito.spy(fs.getAbfsStore());\n+    assumeBlobServiceType();\n+    Assume.assumeFalse(\"Not valid for APPEND BLOB\", isAppendBlobEnabled());\n+\n+    // Create spies for the client handler and blob client\n+    AbfsClientHandler clientHandler = Mockito.spy(store.getClientHandler());\n+    AbfsBlobClient blobClient = Mockito.spy(clientHandler.getBlobClient());\n+\n+    // Set up the spies to return the mocked objects\n+    Mockito.doReturn(clientHandler).when(store).getClientHandler();\n+    Mockito.doReturn(blobClient).when(clientHandler).getBlobClient();\n+    Mockito.doReturn(blobClient).when(clientHandler).getIngressClient();\n+    AbfsOutputStream abfsOutputStream = Mockito.spy(\n+        (AbfsOutputStream) fs.create(new Path(\"/test/file\")).getWrappedStream());\n+    AzureIngressHandler ingressHandler = Mockito.spy(\n+        abfsOutputStream.getIngressHandler());\n+    Mockito.doReturn(ingressHandler).when(abfsOutputStream).getIngressHandler();\n+    Mockito.doReturn(blobClient).when(ingressHandler).getClient();\n+    FSDataOutputStream os = Mockito.spy(\n+        new FSDataOutputStream(abfsOutputStream, null));\n+    AbfsOutputStream out = (AbfsOutputStream) os.getWrappedStream();\n+    byte[] bytes = new byte[1024 * 1024 * 4];\n+    new Random().nextBytes(bytes);\n+    // Write some bytes and attempt to flush, which should retry\n+    out.write(bytes);\n+    out.hsync();\n+    Assertions.assertThat(Mockito.mockingDetails(blobClient).getInvocations()\n+        .stream()\n+        .filter(i -> i.getMethod().getName().equals(\"addCheckSumHeaderForWrite\"))\n+        .count())\n+        .as(\"Expected addCheckSumHeaderForWrite() to be called exactly 0 times\")\n+        .isZero();\n+  }\n+\n+  /**\n+   * Tests that the message digest is reset when an exception occurs during remote flush.\n+   * Simulates a failure in the flush operation and verifies reset is called on MessageDigest.\n+   */\n+  @Test\n+  public void testChecksumComputedWhenConfigTrue()  throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, true);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n+    AzureBlobFileSystem fs = (AzureBlobFileSystem) fileSystem;\n\nReview Comment:\n   taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/ITestAbfsOutputStream.java:\n##########\n@@ -481,6 +482,103 @@ public void testResetCalledOnExceptionInRemoteFlush() throws Exception {\n       //expected exception\n     }\n     // Verify that reset was called on the message digest\n-    Mockito.verify(mockMessageDigest, Mockito.times(1)).reset();\n+    if (spiedClient.isFullBlobChecksumValidationEnabled()) {\n+      Assertions.assertThat(Mockito.mockingDetails(mockMessageDigest).getInvocations()\n+          .stream()\n+          .filter(i -> i.getMethod().getName().equals(\"reset\"))\n+          .count())\n+          .as(\"Expected MessageDigest.reset() to be called exactly once when checksum validation is enabled\")\n+          .isEqualTo(1);\n+    }\n+  }\n+\n+  /**\n+   * Tests that the message digest is reset when an exception occurs during remote flush.\n+   * Simulates a failure in the flush operation and verifies reset is called on MessageDigest.\n+   */\n+  @Test\n+  public void testNoChecksumComputedWhenConfigFalse()  throws Exception {\n+    Configuration conf = getRawConfiguration();\n+    conf.setBoolean(FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, false);\n+    FileSystem fileSystem = FileSystem.newInstance(conf);\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-20T12:54:17.803+0000", "updated": "2025-08-20T12:54:17.803+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18015176", "id": "18015176", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2260846732\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1073,11 +1073,15 @@ public AbfsRestOperation flush(byte[] buffer,\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_LENGTH, String.valueOf(buffer.length)));\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_TYPE, APPLICATION_XML));\n     requestHeaders.add(new AbfsHttpHeader(IF_MATCH, eTag));\n+    String md5Hash = null;\n     if (leaseId != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\n     }\n-    if (blobMd5 != null) {\n+    if (isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, blobMd5));\n+    } else {\n+      md5Hash = computeMD5Hash(buffer, 0, buffer.length);\n+      requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, md5Hash));\n\nReview Comment:\n   +1\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-20T13:23:08.299+0000", "updated": "2025-08-20T13:23:08.299+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18015177", "id": "18015177", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2260846732\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1073,11 +1073,15 @@ public AbfsRestOperation flush(byte[] buffer,\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_LENGTH, String.valueOf(buffer.length)));\n     requestHeaders.add(new AbfsHttpHeader(CONTENT_TYPE, APPLICATION_XML));\n     requestHeaders.add(new AbfsHttpHeader(IF_MATCH, eTag));\n+    String md5Hash = null;\n     if (leaseId != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_LEASE_ID, leaseId));\n     }\n-    if (blobMd5 != null) {\n+    if (isFullBlobChecksumValidationEnabled() && blobMd5 != null) {\n       requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, blobMd5));\n+    } else {\n+      md5Hash = computeMD5Hash(buffer, 0, buffer.length);\n+      requestHeaders.add(new AbfsHttpHeader(X_MS_BLOB_CONTENT_MD5, md5Hash));\n\nReview Comment:\n   +1\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-20T13:23:34.842+0000", "updated": "2025-08-20T13:23:34.842+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18015188", "id": "18015188", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3206424078\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  42m  8s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  8s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 18s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7853 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 4daaeef5eb5d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4fd400be439b8f72113bcc8a01a8fe59d6bd4d0c |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/7/testReport/ |\r\n   | Max. process+thread count | 586 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-20T13:49:04.231+0000", "updated": "2025-08-20T13:49:04.231+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18015207", "id": "18015207", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3206874305\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  0s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  46m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 50s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  43m 12s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 36s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 12s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 54s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 35s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7853 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 89697861cd89 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c5f32be28205b5294fc2e94fe13256359a6afd52 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/8/testReport/ |\r\n   | Max. process+thread count | 590 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-20T15:23:40.485+0000", "updated": "2025-08-20T15:23:40.485+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18015225", "id": "18015225", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3207120584\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  42m 20s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  3s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 53s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 25s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7853 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a820f33a31ee 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f9a3529b2a13215a8607f50a59fb6cd95be7eef8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/9/testReport/ |\r\n   | Max. process+thread count | 550 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/9/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-20T16:32:45.300+0000", "updated": "2025-08-20T16:32:45.300+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18017400", "id": "18017400", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3241752731\n\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 815, Failures: 0, Errors: 0, Skipped: 165\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 818, Failures: 0, Errors: 0, Skipped: 117\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10\r\n    \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 657, Failures: 0, Errors: 0, Skipped: 231\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n    \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 815, Failures: 0, Errors: 0, Skipped: 176\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 12\r\n   [ERROR] Tests run: 660, Failures: 0, Errors: 0, Skipped: 171\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 654, Failures: 0, Errors: 0, Skipped: 233\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================ \r\n   \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 12\r\n   [ERROR] Tests run: 660, Failures: 0, Errors: 0, Skipped: 171\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 12\r\n   [WARNING] Tests run: 656, Failures: 0, Errors: 0, Skipped: 191\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 689, Failures: 0, Errors: 0, Skipped: 172\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 178, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 654, Failures: 0, Errors: 0, Skipped: 230\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-01T10:11:29.305+0000", "updated": "2025-09-01T10:11:29.305+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18017435", "id": "18017435", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#issuecomment-3242192837\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   1m  0s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  44m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 43s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m  5s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  42m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 52s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 144m 37s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7853 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a36b8cb29bfe 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 4e5efa2412f26325759d877638df79a4b4ed57e9 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/10/testReport/ |\r\n   | Max. process+thread count | 585 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7853/10/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-01T12:34:09.951+0000", "updated": "2025-09-01T12:34:09.951+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18017531", "id": "18017531", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-01T18:22:50.802+0000", "updated": "2025-09-01T18:22:50.802+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18017890", "id": "18017890", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 opened a new pull request, #7927:\nURL: https://github.com/apache/hadoop/pull/7927\n\n   This PR adds a configuration flag to control the use of full blob MD5 validation during the PutBlockList (flush) operation. The functionality to validate the MD5 hash of the entire blob already existed, but it could not be toggled. With this change, the feature is now configurable and is disabled by default. When the config is set to false, the system uses the default block ID hash for integrity checks. When set to true, it performs full blob MD5 validation. This config has been introduced because full blob MD5 computation can lead to increased latency and higher CPU usage, especially for large blobs.\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-03T12:09:27.875+0000", "updated": "2025-09-03T12:09:27.875+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18018232", "id": "18018232", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "slfan1989 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2323736334\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -25,7 +25,8 @@\n import java.util.UUID;\n \n import org.assertj.core.api.Assertions;\n-import org.junit.jupiter.api.Test;\n+import org.junit.Assume;\n\nReview Comment:\n   @anujmodi2021 I noticed a small issue: this PR has introduced JUnit4 unit tests again. \r\n   \r\n   cc: @bhattmanish98 @manika137 @anmolanmol1234 \r\n   \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-04T23:21:09.947+0000", "updated": "2025-09-04T23:21:09.947+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18018267", "id": "18018267", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7853:\nURL: https://github.com/apache/hadoop/pull/7853#discussion_r2323960907\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java:\n##########\n@@ -25,7 +25,8 @@\n import java.util.UUID;\n \n import org.assertj.core.api.Assertions;\n-import org.junit.jupiter.api.Test;\n+import org.junit.Assume;\n\nReview Comment:\n   > @anujmodi2021 I noticed a small issue: this PR has introduced JUnit4 unit tests again.\r\n   > \r\n   > cc: @bhattmanish98 @manika137 @anmolanmol1234\r\n   \r\n   Thanks for pointing this out @slfan1989. Looks like we overlooked this during review process.\r\n   We will fix this in separate PR ASAP.\r\n   \r\n   For the upcoming PRs we will take care of this aspect.\r\n   Thanks\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-05T02:37:02.647+0000", "updated": "2025-09-05T02:37:02.647+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18018367", "id": "18018367", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 opened a new pull request, #7933:\nURL: https://github.com/apache/hadoop/pull/7933\n\n   Revert junit 4 changes to junit5\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-05T12:18:02.026+0000", "updated": "2025-09-05T12:18:02.026+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18018419", "id": "18018419", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7933:\nURL: https://github.com/apache/hadoop/pull/7933#issuecomment-3258697392\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  25m  4s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 35s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  6s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 167m 53s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7933/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7933 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux aef4c7572d5f 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7085036652698e55316cf680fb89f93811535199 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7933/1/testReport/ |\r\n   | Max. process+thread count | 588 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7933/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-05T15:07:13.713+0000", "updated": "2025-09-05T15:07:13.713+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18018513", "id": "18018513", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "slfan1989 commented on PR #7933:\nURL: https://github.com/apache/hadoop/pull/7933#issuecomment-3260062365\n\n   @anmolanmol1234 Thanks for the contribution! LGTM.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-05T23:40:40.709+0000", "updated": "2025-09-05T23:40:40.709+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18018729", "id": "18018729", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "slfan1989 commented on PR #7933:\nURL: https://github.com/apache/hadoop/pull/7933#issuecomment-3264707540\n\n   @anujmodi2021 Could you please help review this PR? Thank you very much!\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-08T05:50:46.728+0000", "updated": "2025-09-08T05:50:46.728+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622233/comment/18018819", "id": "18018819", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7933:\nURL: https://github.com/apache/hadoop/pull/7933\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-08T12:24:22.948+0000", "updated": "2025-09-08T12:24:22.948+0000"}], "maxResults": 117, "total": 117, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/1", "description": "The issue is open and ready for the assignee to start work on it.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png", "name": "Open", "id": "1", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2", "id": 2, "key": "new", "colorName": "blue-gray", "name": "To Do"}}}}