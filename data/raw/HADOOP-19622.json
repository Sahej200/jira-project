{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13623416", "self": "https://issues.apache.org/jira/rest/api/2/issue/13623416", "key": "HADOOP-19622", "fields": {"summary": "ABFS: [ReadAheadV2] Implement Read Buffer Manager V2 with improved aggressiveness", "description": null, "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anujmodi", "name": "anujmodi", "key": "JIRAUSER307456", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Anuj Modi", "active": true, "timeZone": "Asia/Kolkata"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18010599", "id": "18010599", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 opened a new pull request, #7832:\nURL: https://github.com/apache/hadoop/pull/7832\n\n   ### Description of PR\r\n   JIRA: https://issues.apache.org/jira/browse/HADOOP-19622\r\n   \r\n   Implementing ReadBufferManagerV2 as per the new design document.\r\n   Following capabilities are added to ReadBufferManager:\r\n   1. Configurable minimum and maximum number of prefetch threads.\r\n   2. Configurable minimum and maximum size of cached buffer pool\r\n   3. Dynamically adjusting thread pool size and buffer pool size based on workload requirement and resource utilization but within the limits defined by user.\r\n   4. Mapping prefetched data to file ETag so that multiple streams reading same file can share the cache and save TPS.\r\n   \r\n   For more details on design doc please refer to the design doc attached to parent JIRA: https://issues.apache.org/jira/browse/HADOOP-19596\r\n   \r\n   ### How was this patch tested?\r\n   TBA\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-29T07:44:49.335+0000", "updated": "2025-07-29T07:44:49.335+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18010612", "id": "18010612", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3131423891\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 59s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 7 new + 3 unchanged - 9 fixed = 10 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 48s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 22s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 17s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 26s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  81m 35s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 06ad120d8f97 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 28cb97fde1eda2fa096401499e9ee1dbccbdef2b |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-29T09:07:28.936+0000", "updated": "2025-07-29T09:07:28.936+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18011465", "id": "18011465", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3145123984\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  5s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 5 new + 3 unchanged - 9 fixed = 8 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 26s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  79m 28s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   |  |  Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time  Unsynchronized access at ReadBufferManagerV2.java:81% of time  Unsynchronized access at ReadBufferManagerV2.java:[line 617] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c66f9b95f564 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7670846f293f7f629cacdd968ccc1cbd9e70ff25 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/testReport/ |\r\n   | Max. process+thread count | 709 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-01T16:26:49.551+0000", "updated": "2025-08-01T16:26:49.551+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18011681", "id": "18011681", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3148434367\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 10s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 51s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 7 new + 3 unchanged - 9 fixed = 10 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/3/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/3/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 34s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 22s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 24s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/3/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  84m 42s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   |  |  Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.bufferPool; locked 66% of time  Unsynchronized access at ReadBufferManagerV2.java:66% of time  Unsynchronized access at ReadBufferManagerV2.java:[line 361] |\r\n   |  |  Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time  Unsynchronized access at ReadBufferManagerV2.java:81% of time  Unsynchronized access at ReadBufferManagerV2.java:[line 634] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f39a03e570dd 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6a686866e701665a2cb897afdf6470fe9b636ef5 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/3/testReport/ |\r\n   | Max. process+thread count | 561 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-03T13:45:54.518+0000", "updated": "2025-08-03T13:45:54.518+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18011695", "id": "18011695", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3148480004\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 24s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 8 new + 3 unchanged - 9 fixed = 11 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 20s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/4/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  spotbugs  |   0m 44s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/4/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 37s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 21s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 25s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/4/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  76m 28s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   |  |  Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.bufferPool; locked 66% of time  Unsynchronized access at ReadBufferManagerV2.java:66% of time  Unsynchronized access at ReadBufferManagerV2.java:[line 359] |\r\n   |  |  Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time  Unsynchronized access at ReadBufferManagerV2.java:81% of time  Unsynchronized access at ReadBufferManagerV2.java:[line 632] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 6003c20c0979 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9552072e0e9e4addbb5f7ccb6f9109332fa47f2c |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/4/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-03T15:00:22.121+0000", "updated": "2025-08-03T15:00:22.121+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18011879", "id": "18011879", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3150755978\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/5/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 3 unchanged - 9 fixed = 13 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/5/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/5/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  spotbugs  |   1m 11s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/5/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 47s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 39s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/5/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   | 124m 32s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   |  |  Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time  Unsynchronized access at ReadBufferManagerV2.java:81% of time  Unsynchronized access at ReadBufferManagerV2.java:[line 635] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 4fd3a0fe88be 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0c368d836c420693f2467e8aeb52f25a5f53e1bd |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/5/testReport/ |\r\n   | Max. process+thread count | 703 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-04T13:36:43.533+0000", "updated": "2025-08-04T13:36:43.533+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18012101", "id": "18012101", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3154838752\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  30m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  0s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 19s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 18s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 18s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 16s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 16s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 3 unchanged - 9 fixed = 13 total (was 12)  |\r\n   | -1 :x: |  mvnsite  |   0m 18s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  spotbugs  |   0m 19s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 33s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 22s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  asflicense  |   0m 26s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   |  79m 59s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 9480f2b5a408 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a894d06c0b1b783fa1228997c65f5b0544285662 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-05T11:34:35.034+0000", "updated": "2025-08-05T11:34:35.034+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18012367", "id": "18012367", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3159934862\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 11 new + 3 unchanged - 9 fixed = 14 total (was 12)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 30s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/8/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  javadoc  |   0m 27s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/8/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 1 new + 10 unchanged - 0 fixed = 11 total (was 10)  |\r\n   | -1 :x: |  spotbugs  |   1m 10s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/8/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 5 new + 0 unchanged - 0 fixed = 5 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  7s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 47s |  |  hadoop-azure in the patch passed.  |\r\n   | -1 :x: |  asflicense  |   0m 38s | [/results-asflicense.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/8/artifact/out/results-asflicense.txt) |  The patch generated 1 ASF License warnings.  |\r\n   |  |   | 124m 55s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   |  |  Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.numberOfActiveBuffers; locked 81% of time  Unsynchronized access at ReadBufferManagerV2.java:81% of time  Unsynchronized access at ReadBufferManagerV2.java:[line 640] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.cpuMonitorThread from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.init()  At ReadBufferManagerV2.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.init()  At ReadBufferManagerV2.java:[line 179] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.memoryMonitorThread from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.init()  At ReadBufferManagerV2.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.init()  At ReadBufferManagerV2.java:[line 157] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux f0bb66ee2300 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b4d81832a01395287563a3cba721b500dccfafa5 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/8/testReport/ |\r\n   | Max. process+thread count | 709 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-06T12:19:35.254+0000", "updated": "2025-08-06T12:19:35.254+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18012589", "id": "18012589", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3164166890\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 9 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  28m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  25m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 15s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 12s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/9/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 8 new + 5 unchanged - 9 fixed = 13 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 42s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/9/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | -1 :x: |  shadedclient  |  25m  8s |  |  patch has errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 24s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/9/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 25s |  |  ASF License check generated no output?  |\r\n   |  |   |  87m 38s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux ae06a4383cc7 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d258d90f06489e5e4f2316a000b9d985cd572577 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/9/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/9/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-07T13:17:11.341+0000", "updated": "2025-08-07T13:17:11.341+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18012873", "id": "18012873", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3168134181\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  39m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 59s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/10/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 9 new + 5 unchanged - 9 fixed = 14 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   1m 10s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/10/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 51s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 125m  1s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux f6be6af2509d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2444f92a353dacf5cced826c51e181b696e424d7 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/10/testReport/ |\r\n   | Max. process+thread count | 708 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/10/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-08T14:24:50.488+0000", "updated": "2025-08-08T14:24:50.488+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18014518", "id": "18014518", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3195330138\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 59s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m  0s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/11/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 5 unchanged - 9 fixed = 15 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 43s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/11/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 33s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  86m 15s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/11/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5e04ca0b1e01 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a6064efe52a45fe05fe92bae1e97c5131085f74f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/11/testReport/ |\r\n   | Max. process+thread count | 567 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/11/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-18T06:42:06.039+0000", "updated": "2025-08-18T06:42:06.039+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18014942", "id": "18014942", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3201235322\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 55s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m  8s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/12/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 5 unchanged - 9 fixed = 15 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 48s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/12/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 30s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  79m 57s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/12/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ed73b0a40bbd 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 12d56c451aa41ac9ed1c6ec87e4c185f753ad956 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/12/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/12/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-19T15:30:21.139+0000", "updated": "2025-08-19T15:30:21.139+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18015970", "id": "18015970", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3219871750\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 38s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 28s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  23m 42s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/13/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 5 unchanged - 9 fixed = 15 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 48s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/13/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 27s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  80m 12s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   |  |  Unread field:field be static?  At ReadBufferManagerV2.java:[line 66] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/13/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f29ebc7d545e 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0376544b9ac0dc8a4741f6b6350b197b66d3e25f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/13/testReport/ |\r\n   | Max. process+thread count | 561 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/13/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-25T11:19:09.517+0000", "updated": "2025-08-25T11:19:09.517+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18015991", "id": "18015991", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3220266799\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  28m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  5s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  22m 17s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 12s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/14/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 12 new + 5 unchanged - 9 fixed = 17 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 43s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/14/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 24s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  82m 52s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   |  |  Unread field:field be static?  At ReadBufferManagerV2.java:[line 66] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/14/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3532fd079953 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a203e3d3ed6df6cdcca26db4f21b012fe7511bb8 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/14/testReport/ |\r\n   | Max. process+thread count | 561 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/14/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-25T13:24:50.834+0000", "updated": "2025-08-25T13:24:50.834+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18015996", "id": "18015996", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3220297099\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  29m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 41s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  22m 53s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 12s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/15/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 12 new + 5 unchanged - 9 fixed = 17 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 43s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/15/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 16s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 29s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  84m  6s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   |  |  Unread field:field be static?  At ReadBufferManagerV2.java:[line 66] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/15/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 235d7a28a991 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 08ea09b7d98df7e1b50b99b364b64d8958822cde |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/15/testReport/ |\r\n   | Max. process+thread count | 556 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/15/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-25T13:31:39.955+0000", "updated": "2025-08-25T13:31:39.955+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18017359", "id": "18017359", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3241001764\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 23s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 9 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 19s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/16/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 5 unchanged - 9 fixed = 15 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 49s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/16/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 58s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 30s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  86m 33s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   |  |  Unread field:field be static?  At ReadBufferManagerV2.java:[line 66] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/16/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 445fc7cc9c6d 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 28a2c82ca44ef89a13a7ba7a1745a03a8c458950 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/16/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/16/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-01T06:21:16.329+0000", "updated": "2025-09-01T06:21:16.329+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18017416", "id": "18017416", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3242030746\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 9 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  28m 52s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  8s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 21s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/17/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 34 new + 5 unchanged - 9 fixed = 39 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 48s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/17/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 45s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 29s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  81m 32s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/17/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 48c5c02b07a5 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / cd8a2b9fa68545870c4b362191ab3999ecd28047 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/17/testReport/ |\r\n   | Max. process+thread count | 630 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/17/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-01T11:36:08.977+0000", "updated": "2025-09-01T11:36:08.977+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18017541", "id": "18017541", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3243169253\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  5s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 18s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/18/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 18 new + 5 unchanged - 9 fixed = 23 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/18/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 30s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  87m 35s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/18/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 094ad1010077 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a0f8021e0edaa0e5b62fbf27727a06c26e9cdc78 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/18/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/18/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-01T19:50:25.617+0000", "updated": "2025-09-01T19:50:25.617+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18017610", "id": "18017610", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3244159256\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 14s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 40s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/19/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 18 new + 5 unchanged - 9 fixed = 23 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 44s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/19/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 30s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 23s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  86m 48s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.getBuffer() may expose internal representation by returning ReadBuffer.buffer  At ReadBuffer.java:by returning ReadBuffer.buffer  At ReadBuffer.java:[line 111] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBuffer(byte[]) may expose internal representation by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:by storing an externally mutable object into ReadBuffer.buffer  At ReadBuffer.java:[line 115] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/19/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d86dbba0e97b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7990ab04361b202bf6b731baf7194599beaa9101 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/19/testReport/ |\r\n   | Max. process+thread count | 567 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/19/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-02T07:39:43.382+0000", "updated": "2025-09-02T07:39:43.382+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18017675", "id": "18017675", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3245348841\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  8s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 20s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/20/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 2 new + 5 unchanged - 9 fixed = 7 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 31s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 24s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 24s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  78m 58s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/20/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 51751dc46640 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 98902b0ff5a75dccb704b866cdd12f40e5af7929 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/20/testReport/ |\r\n   | Max. process+thread count | 626 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/20/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-02T13:26:47.662+0000", "updated": "2025-09-02T13:26:47.662+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18017719", "id": "18017719", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3246049986\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  22m 38s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/21/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 6 new + 5 unchanged - 9 fixed = 11 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 29s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 23s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  82m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/21/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2dca3625cc19 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c3acf9680516650709856c239f6c4a28d01ccd9e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/21/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/21/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-02T16:36:16.922+0000", "updated": "2025-09-02T16:36:16.922+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18022050", "id": "18022050", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3322556049\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  49m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  35m 59s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/22/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 6 new + 5 unchanged - 9 fixed = 11 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 57s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  0s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 34s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 138m  2s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/22/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b9c041f528b7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a214133ad464435be643778dce6afff7f96b4d64 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/22/testReport/ |\r\n   | Max. process+thread count | 707 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/22/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-23T06:08:52.479+0000", "updated": "2025-09-23T06:08:52.479+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032722", "id": "18032722", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459585691\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1469,6 +1504,18 @@ public boolean isReadAheadEnabled() {\n     return this.enabledReadAhead;\n   }\n \n+  /**\n+   * Checks if the read-ahead v2 feature is enabled by user.\n+   * @return true if read-ahead v2 is enabled, false otherwise.\n+   */\n+  public boolean isReadAheadV2Enabled() {\n+    return this.isReadAheadV2Enabled;\n\nReview Comment:\n   nit: this. not needed\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T09:49:37.974+0000", "updated": "2025-10-24T09:49:37.974+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032723", "id": "18032723", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459591012\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -128,13 +128,20 @@ public final class FileSystemConfigurations {\n   public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120;\n \n   public static final boolean DEFAULT_ENABLE_READAHEAD = true;\n-  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = false;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = true;\n\nReview Comment:\n   should be made false as discussed for now\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T09:51:08.760+0000", "updated": "2025-10-24T09:51:08.760+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032724", "id": "18032724", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459592071\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -128,13 +128,20 @@ public final class FileSystemConfigurations {\n   public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120;\n \n   public static final boolean DEFAULT_ENABLE_READAHEAD = true;\n-  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = false;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = true;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2_DYNAMIC_SCALING = true;\n\nReview Comment:\n   same for this config\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T09:51:28.270+0000", "updated": "2025-10-24T09:51:28.270+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032725", "id": "18032725", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459660348\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -87,17 +111,53 @@ static ReadBufferManagerV2 getBufferManager() {\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Set the ReadBufferManagerV2 configurations based on the provided before singleton initialization.\n+   * @param readAheadBlockSize the read-ahead block size to set for the ReadBufferManagerV2.\n+   * @param abfsConfiguration the configuration to set for the ReadBufferManagerV2.\n+   */\n+  public static void setReadBufferManagerConfigs(final int readAheadBlockSize,\n+      final AbfsConfiguration abfsConfiguration) {\n+    // Set Configs only before initializations.\n+    if (bufferManager == null && !isConfigured) {\n+      minThreadPoolSize = abfsConfiguration.getMinReadAheadV2ThreadPoolSize();\n+      maxThreadPoolSize = abfsConfiguration.getMaxReadAheadV2ThreadPoolSize();\n+      cpuMonitoringIntervalInMilliSec = abfsConfiguration.getReadAheadV2CpuMonitoringIntervalMillis();\n+      cpuThreshold = abfsConfiguration.getReadAheadV2CpuUsageThresholdPercent()/ ONE_HUNDRED;\n+      threadPoolUpscalePercentage = abfsConfiguration.getReadAheadV2ThreadPoolUpscalePercentage();\n+      threadPoolDownscalePercentage = abfsConfiguration.getReadAheadV2ThreadPoolDownscalePercentage();\n+      executorServiceKeepAliveTimeInMilliSec = abfsConfiguration.getReadAheadExecutorServiceTTLInMillis();\n+\n+      minBufferPoolSize = abfsConfiguration.getMinReadAheadV2BufferPoolSize();\n\nReview Comment:\n   As discussed earlier this should start with 8 threads which is the default in trunk today and can scale further \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:11:43.289+0000", "updated": "2025-10-24T10:11:43.289+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032726", "id": "18032726", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459676934\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n+  }\n+\n+  private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) {\n+    for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) {\n+      ReadBuffer readBuffer = it.next();\n+      if (readBuffer.getStream() == stream) {\n+        it.remove();\n+        // As failed ReadBuffers (bufferIndex = -1) are already pushed to free\n+        // list in doneReading method, we will skip adding those here again.\n+        if (readBuffer.getBufferindex() != -1) {\n+          pushToFreeList(readBuffer.getBufferindex());\n+        }\n+      }\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Test method that can clean up the current state of readAhead buffers and\n+   * the lists. Will also trigger a fresh init.\n    */\n   @VisibleForTesting\n   @Override\n   public void testResetReadBufferManager() {\n-    // TODO: To be implemented\n+    synchronized (this) {\n+      ArrayList<ReadBuffer> completedBuffers = new ArrayList<>();\n+      for (ReadBuffer buf : getCompletedReadList()) {\n+        if (buf != null) {\n+          completedBuffers.add(buf);\n+        }\n+      }\n+\n+      for (ReadBuffer buf : completedBuffers) {\n+        manualEviction(buf);\n+      }\n+\n+      getReadAheadQueue().clear();\n+      getInProgressList().clear();\n+      getCompletedReadList().clear();\n+      getFreeList().clear();\n+      for (int i = 0; i < maxBufferPoolSize; i++) {\n+        bufferPool[i] = null;\n+      }\n+      bufferPool = null;\n+      cpuMonitorThread.shutdownNow();\n+      memoryMonitorThread.shutdownNow();\n+      workerPool.shutdownNow();\n+      resetBufferManager();\n+    }\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n   @VisibleForTesting\n   @Override\n-  public void testResetReadBufferManager(final int readAheadBlockSize,\n-      final int thresholdAgeMilliseconds) {\n-    // TODO: To be implemented\n+  public void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds) {\n+    setReadAheadBlockSize(readAheadBlockSize);\n+    setThresholdAgeMilliseconds(thresholdAgeMilliseconds);\n+    testResetReadBufferManager();\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void testMimicFullUseAndAddFailedBuffer(final ReadBuffer buf) {\n-    // TODO: To be implemented\n+  @VisibleForTesting\n+  public void callTryEvict() {\n+    tryEvict();\n   }\n \n-  private final ThreadFactory namedThreadFactory = new ThreadFactory() {\n-    private int count = 0;\n-    @Override\n-    public Thread newThread(Runnable r) {\n-      return new Thread(r, \"ReadAheadV2-Thread-\" + count++);\n+  @VisibleForTesting\n+  public int getNumBuffers() {\n+    LOCK.lock();\n+    try {\n+      return numberOfActiveBuffers;\n+    } finally {\n+      LOCK.unlock();\n     }\n-  };\n+  }\n \n   @Override\n   void resetBufferManager() {\n     setBufferManager(null); // reset the singleton instance\n+    setIsConfigured(false);\n   }\n \n   private static void setBufferManager(ReadBufferManagerV2 manager) {\n     bufferManager = manager;\n   }\n+\n+  private static void setIsConfigured(boolean configured) {\n+    isConfigured = configured;\n+  }\n+\n+  private final ThreadFactory workerThreadFactory = new ThreadFactory() {\n+    private int count = 0;\n+    @Override\n+    public Thread newThread(Runnable r) {\n+      Thread t = new Thread(r, \"ReadAheadV2-WorkerThread-\" + count++);\n+      t.setDaemon(true);\n+      return t;\n+    }\n+  };\n+\n+  private void printTraceLog(String message, Object... args) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(message, args);\n+    }\n+  }\n+\n+  private void printDebugLog(String message, Object... args) {\n+    LOGGER.debug(message, args);\n+  }\n+\n+  @VisibleForTesting\n+  double getMemoryLoad() {\n+    MemoryMXBean osBean = ManagementFactory.getMemoryMXBean();\n+    MemoryUsage memoryUsage = osBean.getHeapMemoryUsage();\n+    return (double) memoryUsage.getUsed() / memoryUsage.getMax();\n+  }\n+\n+  @VisibleForTesting\n+  public double getCpuLoad() {\n+    OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean(\n+        OperatingSystemMXBean.class);\n+    return osBean.getSystemCpuLoad();\n\nReview Comment:\n   can return negative values as well, should return 0 when nrgative\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:17:08.678+0000", "updated": "2025-10-24T10:17:08.678+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032727", "id": "18032727", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459687572\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n+  }\n+\n+  private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) {\n+    for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) {\n+      ReadBuffer readBuffer = it.next();\n+      if (readBuffer.getStream() == stream) {\n+        it.remove();\n+        // As failed ReadBuffers (bufferIndex = -1) are already pushed to free\n+        // list in doneReading method, we will skip adding those here again.\n+        if (readBuffer.getBufferindex() != -1) {\n+          pushToFreeList(readBuffer.getBufferindex());\n+        }\n+      }\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Test method that can clean up the current state of readAhead buffers and\n+   * the lists. Will also trigger a fresh init.\n    */\n   @VisibleForTesting\n   @Override\n   public void testResetReadBufferManager() {\n-    // TODO: To be implemented\n+    synchronized (this) {\n+      ArrayList<ReadBuffer> completedBuffers = new ArrayList<>();\n+      for (ReadBuffer buf : getCompletedReadList()) {\n+        if (buf != null) {\n+          completedBuffers.add(buf);\n+        }\n+      }\n+\n+      for (ReadBuffer buf : completedBuffers) {\n+        manualEviction(buf);\n+      }\n+\n+      getReadAheadQueue().clear();\n+      getInProgressList().clear();\n+      getCompletedReadList().clear();\n+      getFreeList().clear();\n+      for (int i = 0; i < maxBufferPoolSize; i++) {\n+        bufferPool[i] = null;\n+      }\n+      bufferPool = null;\n+      cpuMonitorThread.shutdownNow();\n+      memoryMonitorThread.shutdownNow();\n+      workerPool.shutdownNow();\n+      resetBufferManager();\n+    }\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n   @VisibleForTesting\n   @Override\n-  public void testResetReadBufferManager(final int readAheadBlockSize,\n-      final int thresholdAgeMilliseconds) {\n-    // TODO: To be implemented\n+  public void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds) {\n+    setReadAheadBlockSize(readAheadBlockSize);\n+    setThresholdAgeMilliseconds(thresholdAgeMilliseconds);\n+    testResetReadBufferManager();\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void testMimicFullUseAndAddFailedBuffer(final ReadBuffer buf) {\n-    // TODO: To be implemented\n+  @VisibleForTesting\n+  public void callTryEvict() {\n+    tryEvict();\n   }\n \n-  private final ThreadFactory namedThreadFactory = new ThreadFactory() {\n-    private int count = 0;\n-    @Override\n-    public Thread newThread(Runnable r) {\n-      return new Thread(r, \"ReadAheadV2-Thread-\" + count++);\n+  @VisibleForTesting\n+  public int getNumBuffers() {\n+    LOCK.lock();\n+    try {\n+      return numberOfActiveBuffers;\n+    } finally {\n+      LOCK.unlock();\n     }\n-  };\n+  }\n \n   @Override\n   void resetBufferManager() {\n     setBufferManager(null); // reset the singleton instance\n+    setIsConfigured(false);\n   }\n \n   private static void setBufferManager(ReadBufferManagerV2 manager) {\n     bufferManager = manager;\n   }\n+\n+  private static void setIsConfigured(boolean configured) {\n+    isConfigured = configured;\n+  }\n+\n+  private final ThreadFactory workerThreadFactory = new ThreadFactory() {\n+    private int count = 0;\n+    @Override\n+    public Thread newThread(Runnable r) {\n+      Thread t = new Thread(r, \"ReadAheadV2-WorkerThread-\" + count++);\n+      t.setDaemon(true);\n+      return t;\n+    }\n+  };\n+\n+  private void printTraceLog(String message, Object... args) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(message, args);\n+    }\n+  }\n+\n+  private void printDebugLog(String message, Object... args) {\n+    LOGGER.debug(message, args);\n+  }\n+\n+  @VisibleForTesting\n+  double getMemoryLoad() {\n+    MemoryMXBean osBean = ManagementFactory.getMemoryMXBean();\n+    MemoryUsage memoryUsage = osBean.getHeapMemoryUsage();\n+    return (double) memoryUsage.getUsed() / memoryUsage.getMax();\n+  }\n+\n+  @VisibleForTesting\n+  public double getCpuLoad() {\n+    OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean(\n+        OperatingSystemMXBean.class);\n+    return osBean.getSystemCpuLoad();\n+  }\n+\n+  @VisibleForTesting\n+  public static ReadBufferManagerV2 getInstance() {\n+    return bufferManager;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMinBufferPoolSize() {\n+    return minBufferPoolSize;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMaxBufferPoolSize() {\n+    return maxBufferPoolSize;\n+  }\n+\n+  @VisibleForTesting\n+  public int getCurrentThreadPoolSize() {\n+    return workerRefs.size();\n+  }\n+\n+  @VisibleForTesting\n+  public int getCpuMonitoringIntervalInMilliSec() {\n+    return cpuMonitoringIntervalInMilliSec;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMemoryMonitoringIntervalInMilliSec() {\n+    return memoryMonitoringIntervalInMilliSec;\n+  }\n+\n+  @VisibleForTesting\n+  public ScheduledExecutorService getCpuMonitoringThread() {\n+    return cpuMonitorThread;\n+  }\n+\n+  public int getRequiredThreadPoolSize() {\n+    return (int) Math.ceil(THREAD_POOL_REQUIREMENT_BUFFER\n+        * (getReadAheadQueue().size() + getInProgressList().size())); // 20% more for buffer\n+  }\n+\n+  private boolean isFreeListEmpty() {\n+    LOCK.lock();\n\nReview Comment:\n   At some places we are using synchronized and at others making use of LOCK, can we use a single synchronization strategy across ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:20:27.567+0000", "updated": "2025-10-24T10:20:27.567+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032728", "id": "18032728", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459676934\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n+  }\n+\n+  private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) {\n+    for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) {\n+      ReadBuffer readBuffer = it.next();\n+      if (readBuffer.getStream() == stream) {\n+        it.remove();\n+        // As failed ReadBuffers (bufferIndex = -1) are already pushed to free\n+        // list in doneReading method, we will skip adding those here again.\n+        if (readBuffer.getBufferindex() != -1) {\n+          pushToFreeList(readBuffer.getBufferindex());\n+        }\n+      }\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Test method that can clean up the current state of readAhead buffers and\n+   * the lists. Will also trigger a fresh init.\n    */\n   @VisibleForTesting\n   @Override\n   public void testResetReadBufferManager() {\n-    // TODO: To be implemented\n+    synchronized (this) {\n+      ArrayList<ReadBuffer> completedBuffers = new ArrayList<>();\n+      for (ReadBuffer buf : getCompletedReadList()) {\n+        if (buf != null) {\n+          completedBuffers.add(buf);\n+        }\n+      }\n+\n+      for (ReadBuffer buf : completedBuffers) {\n+        manualEviction(buf);\n+      }\n+\n+      getReadAheadQueue().clear();\n+      getInProgressList().clear();\n+      getCompletedReadList().clear();\n+      getFreeList().clear();\n+      for (int i = 0; i < maxBufferPoolSize; i++) {\n+        bufferPool[i] = null;\n+      }\n+      bufferPool = null;\n+      cpuMonitorThread.shutdownNow();\n+      memoryMonitorThread.shutdownNow();\n+      workerPool.shutdownNow();\n+      resetBufferManager();\n+    }\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n   @VisibleForTesting\n   @Override\n-  public void testResetReadBufferManager(final int readAheadBlockSize,\n-      final int thresholdAgeMilliseconds) {\n-    // TODO: To be implemented\n+  public void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds) {\n+    setReadAheadBlockSize(readAheadBlockSize);\n+    setThresholdAgeMilliseconds(thresholdAgeMilliseconds);\n+    testResetReadBufferManager();\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void testMimicFullUseAndAddFailedBuffer(final ReadBuffer buf) {\n-    // TODO: To be implemented\n+  @VisibleForTesting\n+  public void callTryEvict() {\n+    tryEvict();\n   }\n \n-  private final ThreadFactory namedThreadFactory = new ThreadFactory() {\n-    private int count = 0;\n-    @Override\n-    public Thread newThread(Runnable r) {\n-      return new Thread(r, \"ReadAheadV2-Thread-\" + count++);\n+  @VisibleForTesting\n+  public int getNumBuffers() {\n+    LOCK.lock();\n+    try {\n+      return numberOfActiveBuffers;\n+    } finally {\n+      LOCK.unlock();\n     }\n-  };\n+  }\n \n   @Override\n   void resetBufferManager() {\n     setBufferManager(null); // reset the singleton instance\n+    setIsConfigured(false);\n   }\n \n   private static void setBufferManager(ReadBufferManagerV2 manager) {\n     bufferManager = manager;\n   }\n+\n+  private static void setIsConfigured(boolean configured) {\n+    isConfigured = configured;\n+  }\n+\n+  private final ThreadFactory workerThreadFactory = new ThreadFactory() {\n+    private int count = 0;\n+    @Override\n+    public Thread newThread(Runnable r) {\n+      Thread t = new Thread(r, \"ReadAheadV2-WorkerThread-\" + count++);\n+      t.setDaemon(true);\n+      return t;\n+    }\n+  };\n+\n+  private void printTraceLog(String message, Object... args) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(message, args);\n+    }\n+  }\n+\n+  private void printDebugLog(String message, Object... args) {\n+    LOGGER.debug(message, args);\n+  }\n+\n+  @VisibleForTesting\n+  double getMemoryLoad() {\n+    MemoryMXBean osBean = ManagementFactory.getMemoryMXBean();\n+    MemoryUsage memoryUsage = osBean.getHeapMemoryUsage();\n+    return (double) memoryUsage.getUsed() / memoryUsage.getMax();\n+  }\n+\n+  @VisibleForTesting\n+  public double getCpuLoad() {\n+    OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean(\n+        OperatingSystemMXBean.class);\n+    return osBean.getSystemCpuLoad();\n\nReview Comment:\n   can return negative values as well, should return 0 when negative\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:21:09.265+0000", "updated": "2025-10-24T10:21:09.265+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032729", "id": "18032729", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459698158\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -17,67 +17,91 @@\n  */\n package org.apache.hadoop.fs.azurebfs.services;\n \n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import com.sun.management.OperatingSystemMXBean;\n+\n import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.MemoryMXBean;\n+import java.lang.management.MemoryUsage;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.SynchronousQueue;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.classification.VisibleForTesting;\n-import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n \n-final class ReadBufferManagerV2 extends ReadBufferManager {\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_HUNDRED;\n+\n+/**\n+ * The Improved Read Buffer Manager for Rest AbfsClient.\n+ */\n+public class ReadBufferManagerV2 extends ReadBufferManager {\n+  // Internal constants\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n \n   // Thread Pool Configurations\n   private static int minThreadPoolSize;\n   private static int maxThreadPoolSize;\n+  private static int cpuMonitoringIntervalInMilliSec;\n+  private static double cpuThreshold;\n+  private static int threadPoolUpscalePercentage;\n+  private static int threadPoolDownscalePercentage;\n   private static int executorServiceKeepAliveTimeInMilliSec;\n+  private static final double THREAD_POOL_REQUIREMENT_BUFFER = 1.2; // 20% more threads than the queue size\n+  private static boolean isDynamicScalingEnabled;\n+\n+  private ScheduledExecutorService cpuMonitorThread;\n   private ThreadPoolExecutor workerPool;\n+  private final List<ReadBufferWorker> workerRefs = new ArrayList<>();\n\nReview Comment:\n   Shouldn't this be synchronized as there can be concurrent modifications to the list ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:23:39.101+0000", "updated": "2025-10-24T10:23:39.101+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032730", "id": "18032730", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459704141\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n+  }\n+\n+  private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) {\n+    for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) {\n+      ReadBuffer readBuffer = it.next();\n+      if (readBuffer.getStream() == stream) {\n+        it.remove();\n+        // As failed ReadBuffers (bufferIndex = -1) are already pushed to free\n+        // list in doneReading method, we will skip adding those here again.\n+        if (readBuffer.getBufferindex() != -1) {\n+          pushToFreeList(readBuffer.getBufferindex());\n+        }\n+      }\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Test method that can clean up the current state of readAhead buffers and\n+   * the lists. Will also trigger a fresh init.\n    */\n   @VisibleForTesting\n   @Override\n   public void testResetReadBufferManager() {\n-    // TODO: To be implemented\n+    synchronized (this) {\n+      ArrayList<ReadBuffer> completedBuffers = new ArrayList<>();\n+      for (ReadBuffer buf : getCompletedReadList()) {\n+        if (buf != null) {\n+          completedBuffers.add(buf);\n+        }\n+      }\n+\n+      for (ReadBuffer buf : completedBuffers) {\n+        manualEviction(buf);\n+      }\n+\n+      getReadAheadQueue().clear();\n+      getInProgressList().clear();\n+      getCompletedReadList().clear();\n+      getFreeList().clear();\n+      for (int i = 0; i < maxBufferPoolSize; i++) {\n+        bufferPool[i] = null;\n+      }\n+      bufferPool = null;\n+      cpuMonitorThread.shutdownNow();\n\nReview Comment:\n   will be null if dynamic scaling is not enabled and can lead to null pointer\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:25:20.770+0000", "updated": "2025-10-24T10:25:20.770+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032732", "id": "18032732", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459731120\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n\nReview Comment:\n   Use constants \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:34:39.834+0000", "updated": "2025-10-24T10:34:39.834+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032733", "id": "18032733", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459736675\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n\nReview Comment:\n   add javadocs for the newly added functions\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:36:34.982+0000", "updated": "2025-10-24T10:36:34.982+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032734", "id": "18032734", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459731120\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n\nReview Comment:\n   Use constants or better way could be TimeUnit.NANOSECONDS.toMillis(System.nanoTime());\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:38:40.167+0000", "updated": "2025-10-24T10:38:40.167+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032736", "id": "18032736", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459753340\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n\n\nReview Comment:\n   A lot of code is repeated between V1 and V2, would it be better to put the common code in super class ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:42:45.440+0000", "updated": "2025-10-24T10:42:45.440+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032737", "id": "18032737", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459755892\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n\nReview Comment:\n   While adjusting thread pool size, should we not consider the memory threshold as well ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:43:32.514+0000", "updated": "2025-10-24T10:43:32.514+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032738", "id": "18032738", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459777164\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n\nReview Comment:\n   So we are evicting based on memory only and scaling and descaling based on CPU, IMO there should be a common function which considers both the params\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:50:01.002+0000", "updated": "2025-10-24T10:50:01.002+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18032739", "id": "18032739", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2459788986\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n\nReview Comment:\n   javadocs missing for many functions\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-24T10:53:01.203+0000", "updated": "2025-10-24T10:53:01.203+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033132", "id": "18033132", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2464764507\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestReadBufferManagerV2.java:\n##########\n@@ -0,0 +1,258 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.junit.jupiter.api.Test;\n+import org.mockito.Mockito;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_READAHEAD_V2;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CACHED_BUFFER_TTL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CPU_MONITORING_INTERVAL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CPU_USAGE_THRESHOLD_PERCENT;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MAX_THREAD_POOL_SIZE;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MEMORY_MONITORING_INTERVAL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MIN_THREAD_POOL_SIZE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_KB;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+/**\n+ * Unit Tests around different components of Read Buffer Manager V2\n+ */\n+public class TestReadBufferManagerV2 extends AbstractAbfsIntegrationTest {\n+  private volatile boolean running = true;\n+  private final List<byte[]> allocations = new ArrayList<>();\n+\n+\n+  public TestReadBufferManagerV2() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * Test to verify init of ReadBufferManagerV2\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testReadBufferManagerV2Init() throws Exception {\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(getConfiguration().getReadAheadBlockSize(), getConfiguration());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    assertThat(ReadBufferManagerV2.getInstance())\n+        .as(\"ReadBufferManager should be uninitialized\").isNull();\n+    intercept(IllegalStateException.class, \"ReadBufferManagerV2 is not configured.\", () -> {\n+      ReadBufferManagerV2.getBufferManager();\n+    });\n+    // verify that multiple invocations of getBufferManager returns same instance.\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(getConfiguration().getReadAheadBlockSize(), getConfiguration());\n+    ReadBufferManagerV2 bufferManager = ReadBufferManagerV2.getBufferManager();\n+    ReadBufferManagerV2 bufferManager2 = ReadBufferManagerV2.getBufferManager();\n+    ReadBufferManagerV2 bufferManager3 = ReadBufferManagerV2.getInstance();\n+    assertThat(bufferManager).isNotNull();\n+    assertThat(bufferManager2).isNotNull();\n+    assertThat(bufferManager).isSameAs(bufferManager2);\n+    assertThat(bufferManager3).isNotNull();\n+    assertThat(bufferManager3).isSameAs(bufferManager);\n+\n+    // Verify default values are not invalid.\n+    assertThat(bufferManager.getMinBufferPoolSize()).isGreaterThan(0);\n+    assertThat(bufferManager.getMaxBufferPoolSize()).isGreaterThan(0);\n+  }\n+\n+  /**\n+   * Test to verify that cpu monitor thread is not active if disabled.\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testDynamicScalingSwitchingOnAndOff() throws Exception {\n+    Configuration conf = new Configuration(getRawConfiguration());\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2, true);\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING, true);\n+    try(AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(getFileSystem().getUri(), conf)) {\n+      AbfsConfiguration abfsConfiguration = fs.getAbfsStore().getAbfsConfiguration();\n+      ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfiguration.getReadAheadBlockSize(), abfsConfiguration);\n+      ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+      assertThat(bufferManagerV2.getCpuMonitoringThread())\n+          .as(\"CPU Monitor thread should be initialized\").isNotNull();\n+      bufferManagerV2.resetBufferManager();\n+    }\n+\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING, false);\n+    try(AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(getFileSystem().getUri(), conf)) {\n+      AbfsConfiguration abfsConfiguration = fs.getAbfsStore().getAbfsConfiguration();\n+      ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfiguration.getReadAheadBlockSize(), abfsConfiguration);\n+      ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+      assertThat(bufferManagerV2.getCpuMonitoringThread())\n+          .as(\"CPU Monitor thread should not be initialized\").isNull();\n+      bufferManagerV2.resetBufferManager();\n+    }\n+  }\n+\n+  @Test\n+  public void testThreadPoolDynamicScaling() throws Exception {\n+    TestAbfsInputStream testAbfsInputStream = new TestAbfsInputStream();\n\nReview Comment:\n   Always reset running = true at the start of the test to prevent state leakage if multiple tests run.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T08:18:22.170+0000", "updated": "2025-10-27T08:18:22.170+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033134", "id": "18033134", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2464780671\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestReadBufferManagerV2.java:\n##########\n@@ -0,0 +1,258 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.junit.jupiter.api.Test;\n+import org.mockito.Mockito;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_READAHEAD_V2;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CACHED_BUFFER_TTL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CPU_MONITORING_INTERVAL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CPU_USAGE_THRESHOLD_PERCENT;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MAX_THREAD_POOL_SIZE;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MEMORY_MONITORING_INTERVAL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MIN_THREAD_POOL_SIZE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_KB;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+/**\n+ * Unit Tests around different components of Read Buffer Manager V2\n+ */\n+public class TestReadBufferManagerV2 extends AbstractAbfsIntegrationTest {\n+  private volatile boolean running = true;\n+  private final List<byte[]> allocations = new ArrayList<>();\n+\n+\n+  public TestReadBufferManagerV2() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * Test to verify init of ReadBufferManagerV2\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testReadBufferManagerV2Init() throws Exception {\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(getConfiguration().getReadAheadBlockSize(), getConfiguration());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    assertThat(ReadBufferManagerV2.getInstance())\n+        .as(\"ReadBufferManager should be uninitialized\").isNull();\n+    intercept(IllegalStateException.class, \"ReadBufferManagerV2 is not configured.\", () -> {\n+      ReadBufferManagerV2.getBufferManager();\n+    });\n+    // verify that multiple invocations of getBufferManager returns same instance.\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(getConfiguration().getReadAheadBlockSize(), getConfiguration());\n+    ReadBufferManagerV2 bufferManager = ReadBufferManagerV2.getBufferManager();\n+    ReadBufferManagerV2 bufferManager2 = ReadBufferManagerV2.getBufferManager();\n+    ReadBufferManagerV2 bufferManager3 = ReadBufferManagerV2.getInstance();\n+    assertThat(bufferManager).isNotNull();\n+    assertThat(bufferManager2).isNotNull();\n+    assertThat(bufferManager).isSameAs(bufferManager2);\n+    assertThat(bufferManager3).isNotNull();\n+    assertThat(bufferManager3).isSameAs(bufferManager);\n+\n+    // Verify default values are not invalid.\n+    assertThat(bufferManager.getMinBufferPoolSize()).isGreaterThan(0);\n+    assertThat(bufferManager.getMaxBufferPoolSize()).isGreaterThan(0);\n+  }\n+\n+  /**\n+   * Test to verify that cpu monitor thread is not active if disabled.\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testDynamicScalingSwitchingOnAndOff() throws Exception {\n+    Configuration conf = new Configuration(getRawConfiguration());\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2, true);\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING, true);\n+    try(AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(getFileSystem().getUri(), conf)) {\n+      AbfsConfiguration abfsConfiguration = fs.getAbfsStore().getAbfsConfiguration();\n+      ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfiguration.getReadAheadBlockSize(), abfsConfiguration);\n+      ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+      assertThat(bufferManagerV2.getCpuMonitoringThread())\n+          .as(\"CPU Monitor thread should be initialized\").isNotNull();\n+      bufferManagerV2.resetBufferManager();\n+    }\n+\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING, false);\n+    try(AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(getFileSystem().getUri(), conf)) {\n+      AbfsConfiguration abfsConfiguration = fs.getAbfsStore().getAbfsConfiguration();\n+      ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfiguration.getReadAheadBlockSize(), abfsConfiguration);\n+      ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+      assertThat(bufferManagerV2.getCpuMonitoringThread())\n+          .as(\"CPU Monitor thread should not be initialized\").isNull();\n+      bufferManagerV2.resetBufferManager();\n+    }\n+  }\n+\n+  @Test\n+  public void testThreadPoolDynamicScaling() throws Exception {\n+    TestAbfsInputStream testAbfsInputStream = new TestAbfsInputStream();\n+    AbfsClient client = testAbfsInputStream.getMockAbfsClient();\n+    AbfsInputStream inputStream = testAbfsInputStream.getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+    Configuration configuration = getReabAheadV2Configuration();\n+    AbfsConfiguration abfsConfig = new AbfsConfiguration(configuration,\n+        getAccountName());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfig.getReadAheadBlockSize(), abfsConfig);\n+    ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+    assertThat(bufferManagerV2.getCurrentThreadPoolSize()).isEqualTo(2);\n+    int[] reqOffset = {0};\n+    int reqLength = 1;\n+    Thread t = new Thread(() -> {\n+      while (running) {\n+        bufferManagerV2.queueReadAhead(inputStream, reqOffset[0], reqLength,\n+            inputStream.getTracingContext());\n+        reqOffset[0] += reqLength;\n+      }\n+    });\n+    t.start();\n+    Thread.sleep(2L * bufferManagerV2.getCpuMonitoringIntervalInMilliSec());\n+    assertThat(bufferManagerV2.getCurrentThreadPoolSize()).isEqualTo(4);\n+    running = false;\n+    t.join();\n+    Thread.sleep(4L * bufferManagerV2.getCpuMonitoringIntervalInMilliSec());\n+    assertThat(bufferManagerV2.getCurrentThreadPoolSize()).isLessThan(4);\n+  }\n+\n+  @Test\n+  public void testScheduledEviction() throws Exception {\n+    TestAbfsInputStream testAbfsInputStream = new TestAbfsInputStream();\n+    AbfsClient client = testAbfsInputStream.getMockAbfsClient();\n+    AbfsInputStream inputStream = testAbfsInputStream.getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+    Configuration configuration = getReabAheadV2Configuration();\n+    AbfsConfiguration abfsConfig = new AbfsConfiguration(configuration,\n+        getAccountName());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfig.getReadAheadBlockSize(), abfsConfig);\n+    ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+    // Add a failed buffer to completed queue and set to no free buffers to read ahead.\n+    ReadBuffer buff = new ReadBuffer();\n+    buff.setStatus(ReadBufferStatus.READ_FAILED);\n+    buff.setStream(inputStream);\n+    bufferManagerV2.testMimicFullUseAndAddFailedBuffer(buff);\n+    bufferManagerV2.testMimicFullUseAndAddFailedBuffer(buff);\n+    assertThat(bufferManagerV2.getCompletedReadListSize()).isEqualTo(2);\n+    Thread.sleep(2L * bufferManagerV2.getMemoryMonitoringIntervalInMilliSec());\n+    assertThat(bufferManagerV2.getCompletedReadListSize()).isEqualTo(0);\n+  }\n+\n+  @Test\n+  public void testMemoryUpscaleNotAllowedIfMemoryAboveThreshold() throws Exception {\n+    TestAbfsInputStream testAbfsInputStream = new TestAbfsInputStream();\n+    AbfsClient client = testAbfsInputStream.getMockAbfsClient();\n+    AbfsInputStream inputStream = testAbfsInputStream.getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+    Configuration configuration = getReabAheadV2Configuration();\n+    AbfsConfiguration abfsConfig = new AbfsConfiguration(configuration,\n+        getAccountName());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfig.getReadAheadBlockSize(), abfsConfig);\n+    ReadBufferManagerV2 bufferManagerV2 = Mockito.spy(ReadBufferManagerV2.getBufferManager());\n+    Mockito.doReturn(0.6).when(bufferManagerV2).getMemoryLoad();\n+    // Add a failed buffer to completed queue and set to no free buffers to read ahead.\n+    ReadBuffer buff = new ReadBuffer();\n+    buff.setStatus(ReadBufferStatus.READ_FAILED);\n+    buff.setStream(inputStream);\n+    bufferManagerV2.testMimicFullUseAndAddFailedBuffer(buff);\n+    assertThat(bufferManagerV2.getNumBuffers()).isEqualTo(bufferManagerV2.getMinBufferPoolSize());\n+    bufferManagerV2.queueReadAhead(inputStream, 0, ONE_KB,\n+        inputStream.getTracingContext());\n+    assertThat(bufferManagerV2.getNumBuffers()).isEqualTo(bufferManagerV2.getMinBufferPoolSize());\n+  }\n+\n+  @Test\n+  public void testMemoryUpscaleIfMemoryBelowThreshold() throws Exception {\n+    TestAbfsInputStream testAbfsInputStream = new TestAbfsInputStream();\n+    AbfsClient client = testAbfsInputStream.getMockAbfsClient();\n+    AbfsInputStream inputStream = testAbfsInputStream.getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+    Configuration configuration = getReabAheadV2Configuration();\n+    AbfsConfiguration abfsConfig = new AbfsConfiguration(configuration,\n+        getAccountName());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfig.getReadAheadBlockSize(), abfsConfig);\n+    ReadBufferManagerV2 bufferManagerV2 = Mockito.spy(ReadBufferManagerV2.getBufferManager());\n+    Mockito.doReturn(0.4).when(bufferManagerV2).getMemoryLoad();\n+    // Add a failed buffer to completed queue and set to no free buffers to read ahead.\n+    ReadBuffer buff = new ReadBuffer();\n+    buff.setStatus(ReadBufferStatus.READ_FAILED);\n+    buff.setStream(inputStream);\n+    bufferManagerV2.testMimicFullUseAndAddFailedBuffer(buff);\n+    assertThat(bufferManagerV2.getNumBuffers()).isEqualTo(bufferManagerV2.getMinBufferPoolSize());\n+    bufferManagerV2.queueReadAhead(inputStream, 0, ONE_KB,\n+        inputStream.getTracingContext());\n+    assertThat(bufferManagerV2.getNumBuffers()).isEqualTo(bufferManagerV2.getMinBufferPoolSize() + 1);\n+  }\n+\n+  @Test\n+  public void testMemoryDownscaleIfMemoryAboveThreshold() throws Exception {\n\nReview Comment:\n   Similar tests can be added for Cpu Upscale downscale with threshold\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T08:25:23.637+0000", "updated": "2025-10-27T08:25:23.637+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033271", "id": "18033271", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2465910989\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -128,13 +128,20 @@ public final class FileSystemConfigurations {\n   public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120;\n \n   public static final boolean DEFAULT_ENABLE_READAHEAD = true;\n-  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = false;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = true;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2_DYNAMIC_SCALING = true;\n   public static final int DEFAULT_READAHEAD_V2_MIN_THREAD_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MAX_THREAD_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MIN_BUFFER_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MAX_BUFFER_POOL_SIZE = -1;\n-  public static final int DEFAULT_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLIS = 3_000;\n+  public static final int DEFAULT_READAHEAD_V2_CPU_MONITORING_INTERVAL_MILLIS = 6_000;\n+  public static final int DEFAULT_READAHEAD_V2_THREAD_POOL_UPSCALE_PERCENTAGE = 20;\n+  public static final int DEFAULT_READAHEAD_V2_THREAD_POOL_DOWNSCALE_PERCENTAGE = 30;\n\nReview Comment:\n   Same as above\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -128,13 +128,20 @@ public final class FileSystemConfigurations {\n   public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120;\n \n   public static final boolean DEFAULT_ENABLE_READAHEAD = true;\n-  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = false;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = true;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2_DYNAMIC_SCALING = true;\n   public static final int DEFAULT_READAHEAD_V2_MIN_THREAD_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MAX_THREAD_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MIN_BUFFER_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MAX_BUFFER_POOL_SIZE = -1;\n-  public static final int DEFAULT_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLIS = 3_000;\n+  public static final int DEFAULT_READAHEAD_V2_CPU_MONITORING_INTERVAL_MILLIS = 6_000;\n+  public static final int DEFAULT_READAHEAD_V2_THREAD_POOL_UPSCALE_PERCENTAGE = 20;\n\nReview Comment:\n   For some variable you have used Persentage and for percent, we should keep it consistent across all places.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -17,67 +17,91 @@\n  */\n package org.apache.hadoop.fs.azurebfs.services;\n \n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import com.sun.management.OperatingSystemMXBean;\n+\n import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.MemoryMXBean;\n+import java.lang.management.MemoryUsage;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.SynchronousQueue;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.classification.VisibleForTesting;\n-import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n \n-final class ReadBufferManagerV2 extends ReadBufferManager {\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_HUNDRED;\n+\n+/**\n+ * The Improved Read Buffer Manager for Rest AbfsClient.\n+ */\n+public class ReadBufferManagerV2 extends ReadBufferManager {\n+  // Internal constants\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n \n   // Thread Pool Configurations\n   private static int minThreadPoolSize;\n   private static int maxThreadPoolSize;\n+  private static int cpuMonitoringIntervalInMilliSec;\n+  private static double cpuThreshold;\n+  private static int threadPoolUpscalePercentage;\n+  private static int threadPoolDownscalePercentage;\n   private static int executorServiceKeepAliveTimeInMilliSec;\n+  private static final double THREAD_POOL_REQUIREMENT_BUFFER = 1.2; // 20% more threads than the queue size\n+  private static boolean isDynamicScalingEnabled;\n+\n+  private ScheduledExecutorService cpuMonitorThread;\n   private ThreadPoolExecutor workerPool;\n+  private final List<ReadBufferWorker> workerRefs = new ArrayList<>();\n \n   // Buffer Pool Configurations\n   private static int minBufferPoolSize;\n   private static int maxBufferPoolSize;\n+  private static int memoryMonitoringIntervalInMilliSec;\n+  private static double memoryThreshold;\n+\n   private int numberOfActiveBuffers = 0;\n   private byte[][] bufferPool;\n+  private Stack<Integer> removedBufferList = new Stack<>();\n+  private ScheduledExecutorService memoryMonitorThread;\n \n+  // Buffer Manager Structures\n   private static ReadBufferManagerV2 bufferManager;\n-\n-  // hide instance constructor\n-  private ReadBufferManagerV2() {\n-    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n-  }\n+  private static boolean isConfigured = false;\n \n   /**\n-   * Sets the read buffer manager configurations.\n-   * @param readAheadBlockSize the size of the read-ahead block in bytes\n-   * @param abfsConfiguration the AbfsConfiguration instance for other configurations\n+   * Private constructor to prevent instantiation as this needs to be singleton.\n    */\n-  static void setReadBufferManagerConfigs(int readAheadBlockSize, AbfsConfiguration abfsConfiguration) {\n-    if (bufferManager == null) {\n-      minThreadPoolSize = abfsConfiguration.getMinReadAheadV2ThreadPoolSize();\n-      maxThreadPoolSize = abfsConfiguration.getMaxReadAheadV2ThreadPoolSize();\n-      executorServiceKeepAliveTimeInMilliSec = abfsConfiguration.getReadAheadExecutorServiceTTLInMillis();\n-\n-      minBufferPoolSize = abfsConfiguration.getMinReadAheadV2BufferPoolSize();\n-      maxBufferPoolSize = abfsConfiguration.getMaxReadAheadV2BufferPoolSize();\n-      setThresholdAgeMilliseconds(abfsConfiguration.getReadAheadV2CachedBufferTTLMillis());\n-      setReadAheadBlockSize(readAheadBlockSize);\n-    }\n+  private ReadBufferManagerV2() {\n+    printTraceLog(\"Creating Read Buffer Manager V2 with HADOOP-18546 patch\");\n\nReview Comment:\n   We should use LOG.trace instead of printTraceLOG.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n\nReview Comment:\n   Same as above, please change it whereever you have used it.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -17,67 +17,91 @@\n  */\n package org.apache.hadoop.fs.azurebfs.services;\n \n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import com.sun.management.OperatingSystemMXBean;\n+\n import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.MemoryMXBean;\n+import java.lang.management.MemoryUsage;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.SynchronousQueue;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.classification.VisibleForTesting;\n-import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n \n-final class ReadBufferManagerV2 extends ReadBufferManager {\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_HUNDRED;\n+\n+/**\n+ * The Improved Read Buffer Manager for Rest AbfsClient.\n+ */\n+public class ReadBufferManagerV2 extends ReadBufferManager {\n+  // Internal constants\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n \n   // Thread Pool Configurations\n   private static int minThreadPoolSize;\n   private static int maxThreadPoolSize;\n+  private static int cpuMonitoringIntervalInMilliSec;\n+  private static double cpuThreshold;\n+  private static int threadPoolUpscalePercentage;\n+  private static int threadPoolDownscalePercentage;\n   private static int executorServiceKeepAliveTimeInMilliSec;\n+  private static final double THREAD_POOL_REQUIREMENT_BUFFER = 1.2; // 20% more threads than the queue size\n\nReview Comment:\n   Is this configurable? or we have fixed this number based on POC data? Is so can we explain about it little more for future understanding.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T15:21:20.373+0000", "updated": "2025-10-27T15:21:20.373+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033289", "id": "18033289", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2466433816\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java:\n##########\n@@ -44,17 +47,34 @@ class ReadBuffer {\n   private boolean isFirstByteConsumed = false;\n   private boolean isLastByteConsumed = false;\n   private boolean isAnyByteConsumed = false;\n+  private AtomicInteger refCount = new AtomicInteger(0);\n \n   private IOException errException = null;\n \n   public AbfsInputStream getStream() {\n     return stream;\n   }\n \n+  public String getETag() {\n\nReview Comment:\n   can this return null?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T17:06:04.784+0000", "updated": "2025-10-27T17:06:04.784+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033294", "id": "18033294", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2466445973\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n\nReview Comment:\n   In the catch block- we're interrupting the thread but not informing the caller. Is it expected?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T17:10:28.968+0000", "updated": "2025-10-27T17:10:28.968+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033296", "id": "18033296", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2466464367\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n\nReview Comment:\n   Nit: can spelling\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T17:17:28.717+0000", "updated": "2025-10-27T17:17:28.717+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033300", "id": "18033300", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2466498387\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n\nReview Comment:\n   we can skip it for cases when lengthToCopy = 0\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T17:30:21.129+0000", "updated": "2025-10-27T17:30:21.129+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033418", "id": "18033418", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467825825\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n+  }\n+\n+  private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) {\n+    for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) {\n+      ReadBuffer readBuffer = it.next();\n+      if (readBuffer.getStream() == stream) {\n+        it.remove();\n+        // As failed ReadBuffers (bufferIndex = -1) are already pushed to free\n+        // list in doneReading method, we will skip adding those here again.\n+        if (readBuffer.getBufferindex() != -1) {\n+          pushToFreeList(readBuffer.getBufferindex());\n+        }\n+      }\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Test method that can clean up the current state of readAhead buffers and\n+   * the lists. Will also trigger a fresh init.\n    */\n   @VisibleForTesting\n   @Override\n   public void testResetReadBufferManager() {\n-    // TODO: To be implemented\n+    synchronized (this) {\n+      ArrayList<ReadBuffer> completedBuffers = new ArrayList<>();\n+      for (ReadBuffer buf : getCompletedReadList()) {\n+        if (buf != null) {\n+          completedBuffers.add(buf);\n+        }\n+      }\n+\n+      for (ReadBuffer buf : completedBuffers) {\n+        manualEviction(buf);\n+      }\n+\n+      getReadAheadQueue().clear();\n+      getInProgressList().clear();\n+      getCompletedReadList().clear();\n+      getFreeList().clear();\n+      for (int i = 0; i < maxBufferPoolSize; i++) {\n+        bufferPool[i] = null;\n+      }\n+      bufferPool = null;\n+      cpuMonitorThread.shutdownNow();\n+      memoryMonitorThread.shutdownNow();\n+      workerPool.shutdownNow();\n+      resetBufferManager();\n+    }\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n   @VisibleForTesting\n   @Override\n-  public void testResetReadBufferManager(final int readAheadBlockSize,\n-      final int thresholdAgeMilliseconds) {\n-    // TODO: To be implemented\n+  public void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds) {\n+    setReadAheadBlockSize(readAheadBlockSize);\n+    setThresholdAgeMilliseconds(thresholdAgeMilliseconds);\n+    testResetReadBufferManager();\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void testMimicFullUseAndAddFailedBuffer(final ReadBuffer buf) {\n-    // TODO: To be implemented\n+  @VisibleForTesting\n+  public void callTryEvict() {\n+    tryEvict();\n   }\n \n-  private final ThreadFactory namedThreadFactory = new ThreadFactory() {\n-    private int count = 0;\n-    @Override\n-    public Thread newThread(Runnable r) {\n-      return new Thread(r, \"ReadAheadV2-Thread-\" + count++);\n+  @VisibleForTesting\n+  public int getNumBuffers() {\n+    LOCK.lock();\n+    try {\n+      return numberOfActiveBuffers;\n+    } finally {\n+      LOCK.unlock();\n     }\n-  };\n+  }\n \n   @Override\n   void resetBufferManager() {\n     setBufferManager(null); // reset the singleton instance\n+    setIsConfigured(false);\n   }\n \n   private static void setBufferManager(ReadBufferManagerV2 manager) {\n     bufferManager = manager;\n   }\n+\n+  private static void setIsConfigured(boolean configured) {\n+    isConfigured = configured;\n+  }\n+\n+  private final ThreadFactory workerThreadFactory = new ThreadFactory() {\n+    private int count = 0;\n+    @Override\n+    public Thread newThread(Runnable r) {\n+      Thread t = new Thread(r, \"ReadAheadV2-WorkerThread-\" + count++);\n+      t.setDaemon(true);\n+      return t;\n+    }\n+  };\n+\n+  private void printTraceLog(String message, Object... args) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(message, args);\n+    }\n+  }\n+\n+  private void printDebugLog(String message, Object... args) {\n+    LOGGER.debug(message, args);\n+  }\n+\n+  @VisibleForTesting\n+  double getMemoryLoad() {\n+    MemoryMXBean osBean = ManagementFactory.getMemoryMXBean();\n+    MemoryUsage memoryUsage = osBean.getHeapMemoryUsage();\n+    return (double) memoryUsage.getUsed() / memoryUsage.getMax();\n+  }\n+\n+  @VisibleForTesting\n+  public double getCpuLoad() {\n+    OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean(\n+        OperatingSystemMXBean.class);\n+    return osBean.getSystemCpuLoad();\n+  }\n+\n+  @VisibleForTesting\n+  public static ReadBufferManagerV2 getInstance() {\n+    return bufferManager;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMinBufferPoolSize() {\n+    return minBufferPoolSize;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMaxBufferPoolSize() {\n+    return maxBufferPoolSize;\n+  }\n+\n+  @VisibleForTesting\n+  public int getCurrentThreadPoolSize() {\n+    return workerRefs.size();\n+  }\n+\n+  @VisibleForTesting\n+  public int getCpuMonitoringIntervalInMilliSec() {\n+    return cpuMonitoringIntervalInMilliSec;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMemoryMonitoringIntervalInMilliSec() {\n+    return memoryMonitoringIntervalInMilliSec;\n+  }\n+\n+  @VisibleForTesting\n+  public ScheduledExecutorService getCpuMonitoringThread() {\n+    return cpuMonitorThread;\n+  }\n+\n+  public int getRequiredThreadPoolSize() {\n+    return (int) Math.ceil(THREAD_POOL_REQUIREMENT_BUFFER\n+        * (getReadAheadQueue().size() + getInProgressList().size())); // 20% more for buffer\n+  }\n+\n+  private boolean isFreeListEmpty() {\n+    LOCK.lock();\n\nReview Comment:\n   We are only using Lock where freeList is getting updated. This is because there are multiple places where freeList can be updated. Synchronized block IMO just prevent parallel access to a single block of code. Here we want to prevent parallel access across multiple blocks of code. \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:17:57.252+0000", "updated": "2025-10-28T04:17:57.252+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033419", "id": "18033419", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467825825\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n+  }\n+\n+  private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) {\n+    for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) {\n+      ReadBuffer readBuffer = it.next();\n+      if (readBuffer.getStream() == stream) {\n+        it.remove();\n+        // As failed ReadBuffers (bufferIndex = -1) are already pushed to free\n+        // list in doneReading method, we will skip adding those here again.\n+        if (readBuffer.getBufferindex() != -1) {\n+          pushToFreeList(readBuffer.getBufferindex());\n+        }\n+      }\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Test method that can clean up the current state of readAhead buffers and\n+   * the lists. Will also trigger a fresh init.\n    */\n   @VisibleForTesting\n   @Override\n   public void testResetReadBufferManager() {\n-    // TODO: To be implemented\n+    synchronized (this) {\n+      ArrayList<ReadBuffer> completedBuffers = new ArrayList<>();\n+      for (ReadBuffer buf : getCompletedReadList()) {\n+        if (buf != null) {\n+          completedBuffers.add(buf);\n+        }\n+      }\n+\n+      for (ReadBuffer buf : completedBuffers) {\n+        manualEviction(buf);\n+      }\n+\n+      getReadAheadQueue().clear();\n+      getInProgressList().clear();\n+      getCompletedReadList().clear();\n+      getFreeList().clear();\n+      for (int i = 0; i < maxBufferPoolSize; i++) {\n+        bufferPool[i] = null;\n+      }\n+      bufferPool = null;\n+      cpuMonitorThread.shutdownNow();\n+      memoryMonitorThread.shutdownNow();\n+      workerPool.shutdownNow();\n+      resetBufferManager();\n+    }\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n   @VisibleForTesting\n   @Override\n-  public void testResetReadBufferManager(final int readAheadBlockSize,\n-      final int thresholdAgeMilliseconds) {\n-    // TODO: To be implemented\n+  public void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds) {\n+    setReadAheadBlockSize(readAheadBlockSize);\n+    setThresholdAgeMilliseconds(thresholdAgeMilliseconds);\n+    testResetReadBufferManager();\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void testMimicFullUseAndAddFailedBuffer(final ReadBuffer buf) {\n-    // TODO: To be implemented\n+  @VisibleForTesting\n+  public void callTryEvict() {\n+    tryEvict();\n   }\n \n-  private final ThreadFactory namedThreadFactory = new ThreadFactory() {\n-    private int count = 0;\n-    @Override\n-    public Thread newThread(Runnable r) {\n-      return new Thread(r, \"ReadAheadV2-Thread-\" + count++);\n+  @VisibleForTesting\n+  public int getNumBuffers() {\n+    LOCK.lock();\n+    try {\n+      return numberOfActiveBuffers;\n+    } finally {\n+      LOCK.unlock();\n     }\n-  };\n+  }\n \n   @Override\n   void resetBufferManager() {\n     setBufferManager(null); // reset the singleton instance\n+    setIsConfigured(false);\n   }\n \n   private static void setBufferManager(ReadBufferManagerV2 manager) {\n     bufferManager = manager;\n   }\n+\n+  private static void setIsConfigured(boolean configured) {\n+    isConfigured = configured;\n+  }\n+\n+  private final ThreadFactory workerThreadFactory = new ThreadFactory() {\n+    private int count = 0;\n+    @Override\n+    public Thread newThread(Runnable r) {\n+      Thread t = new Thread(r, \"ReadAheadV2-WorkerThread-\" + count++);\n+      t.setDaemon(true);\n+      return t;\n+    }\n+  };\n+\n+  private void printTraceLog(String message, Object... args) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(message, args);\n+    }\n+  }\n+\n+  private void printDebugLog(String message, Object... args) {\n+    LOGGER.debug(message, args);\n+  }\n+\n+  @VisibleForTesting\n+  double getMemoryLoad() {\n+    MemoryMXBean osBean = ManagementFactory.getMemoryMXBean();\n+    MemoryUsage memoryUsage = osBean.getHeapMemoryUsage();\n+    return (double) memoryUsage.getUsed() / memoryUsage.getMax();\n+  }\n+\n+  @VisibleForTesting\n+  public double getCpuLoad() {\n+    OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean(\n+        OperatingSystemMXBean.class);\n+    return osBean.getSystemCpuLoad();\n+  }\n+\n+  @VisibleForTesting\n+  public static ReadBufferManagerV2 getInstance() {\n+    return bufferManager;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMinBufferPoolSize() {\n+    return minBufferPoolSize;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMaxBufferPoolSize() {\n+    return maxBufferPoolSize;\n+  }\n+\n+  @VisibleForTesting\n+  public int getCurrentThreadPoolSize() {\n+    return workerRefs.size();\n+  }\n+\n+  @VisibleForTesting\n+  public int getCpuMonitoringIntervalInMilliSec() {\n+    return cpuMonitoringIntervalInMilliSec;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMemoryMonitoringIntervalInMilliSec() {\n+    return memoryMonitoringIntervalInMilliSec;\n+  }\n+\n+  @VisibleForTesting\n+  public ScheduledExecutorService getCpuMonitoringThread() {\n+    return cpuMonitorThread;\n+  }\n+\n+  public int getRequiredThreadPoolSize() {\n+    return (int) Math.ceil(THREAD_POOL_REQUIREMENT_BUFFER\n+        * (getReadAheadQueue().size() + getInProgressList().size())); // 20% more for buffer\n+  }\n+\n+  private boolean isFreeListEmpty() {\n+    LOCK.lock();\n\nReview Comment:\n   We are only using Lock where freeList is getting updated. This is because there are multiple places where freeList can be updated. Synchronized block IMO just prevent parallel access to a single block of code. Here we want to prevent parallel access across multiple blocks of code. \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:18:09.992+0000", "updated": "2025-10-28T04:18:09.992+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033420", "id": "18033420", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3454478483\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |   7m 51s |  |  Docker failed to build run-specific yetus/hadoop:tp-15334}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/23/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:19:22.759+0000", "updated": "2025-10-28T04:19:22.759+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033421", "id": "18033421", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467828924\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -17,67 +17,91 @@\n  */\n package org.apache.hadoop.fs.azurebfs.services;\n \n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import com.sun.management.OperatingSystemMXBean;\n+\n import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.MemoryMXBean;\n+import java.lang.management.MemoryUsage;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.SynchronousQueue;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.classification.VisibleForTesting;\n-import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n \n-final class ReadBufferManagerV2 extends ReadBufferManager {\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_HUNDRED;\n+\n+/**\n+ * The Improved Read Buffer Manager for Rest AbfsClient.\n+ */\n+public class ReadBufferManagerV2 extends ReadBufferManager {\n+  // Internal constants\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n \n   // Thread Pool Configurations\n   private static int minThreadPoolSize;\n   private static int maxThreadPoolSize;\n+  private static int cpuMonitoringIntervalInMilliSec;\n+  private static double cpuThreshold;\n+  private static int threadPoolUpscalePercentage;\n+  private static int threadPoolDownscalePercentage;\n   private static int executorServiceKeepAliveTimeInMilliSec;\n+  private static final double THREAD_POOL_REQUIREMENT_BUFFER = 1.2; // 20% more threads than the queue size\n+  private static boolean isDynamicScalingEnabled;\n+\n+  private ScheduledExecutorService cpuMonitorThread;\n   private ThreadPoolExecutor workerPool;\n+  private final List<ReadBufferWorker> workerRefs = new ArrayList<>();\n\nReview Comment:\n   This is modified at two places where parallel threads cannot arrive.\r\n   1. While init: Its singleton so only one Input stream will be able to init.\r\n   2. CpuMonitorThread. There can be only one cpu monitor thread in one JVM.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:20:43.379+0000", "updated": "2025-10-28T04:20:43.379+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033422", "id": "18033422", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467828924\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -17,67 +17,91 @@\n  */\n package org.apache.hadoop.fs.azurebfs.services;\n \n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import com.sun.management.OperatingSystemMXBean;\n+\n import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.MemoryMXBean;\n+import java.lang.management.MemoryUsage;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.SynchronousQueue;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.classification.VisibleForTesting;\n-import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n \n-final class ReadBufferManagerV2 extends ReadBufferManager {\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_HUNDRED;\n+\n+/**\n+ * The Improved Read Buffer Manager for Rest AbfsClient.\n+ */\n+public class ReadBufferManagerV2 extends ReadBufferManager {\n+  // Internal constants\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n \n   // Thread Pool Configurations\n   private static int minThreadPoolSize;\n   private static int maxThreadPoolSize;\n+  private static int cpuMonitoringIntervalInMilliSec;\n+  private static double cpuThreshold;\n+  private static int threadPoolUpscalePercentage;\n+  private static int threadPoolDownscalePercentage;\n   private static int executorServiceKeepAliveTimeInMilliSec;\n+  private static final double THREAD_POOL_REQUIREMENT_BUFFER = 1.2; // 20% more threads than the queue size\n+  private static boolean isDynamicScalingEnabled;\n+\n+  private ScheduledExecutorService cpuMonitorThread;\n   private ThreadPoolExecutor workerPool;\n+  private final List<ReadBufferWorker> workerRefs = new ArrayList<>();\n\nReview Comment:\n   This is modified at two places where parallel threads cannot arrive.\r\n   1. While init: Its singleton so only one Input stream will be able to init.\r\n   2. CpuMonitorThread. There can be only one cpu monitor thread in one JVM.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:21:32.905+0000", "updated": "2025-10-28T04:21:32.905+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033427", "id": "18033427", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467896218\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n\nReview Comment:\n   For length 0 we won't even read.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n\nReview Comment:\n   It will be thrown to caller.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:53:36.347+0000", "updated": "2025-10-28T04:53:36.347+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033428", "id": "18033428", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467896769\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java:\n##########\n@@ -44,17 +47,34 @@ class ReadBuffer {\n   private boolean isFirstByteConsumed = false;\n   private boolean isLastByteConsumed = false;\n   private boolean isAnyByteConsumed = false;\n+  private AtomicInteger refCount = new AtomicInteger(0);\n \n   private IOException errException = null;\n \n   public AbfsInputStream getStream() {\n     return stream;\n   }\n \n+  public String getETag() {\n\nReview Comment:\n   Read Buffers are created only during queueing . There we are setting it.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:53:53.217+0000", "updated": "2025-10-28T04:53:53.217+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033429", "id": "18033429", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467897734\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n\nReview Comment:\n   Its a intentional method created to avoid code redundancy\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -17,67 +17,91 @@\n  */\n package org.apache.hadoop.fs.azurebfs.services;\n \n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import com.sun.management.OperatingSystemMXBean;\n+\n import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.MemoryMXBean;\n+import java.lang.management.MemoryUsage;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.SynchronousQueue;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.classification.VisibleForTesting;\n-import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n \n-final class ReadBufferManagerV2 extends ReadBufferManager {\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_HUNDRED;\n+\n+/**\n+ * The Improved Read Buffer Manager for Rest AbfsClient.\n+ */\n+public class ReadBufferManagerV2 extends ReadBufferManager {\n+  // Internal constants\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n \n   // Thread Pool Configurations\n   private static int minThreadPoolSize;\n   private static int maxThreadPoolSize;\n+  private static int cpuMonitoringIntervalInMilliSec;\n+  private static double cpuThreshold;\n+  private static int threadPoolUpscalePercentage;\n+  private static int threadPoolDownscalePercentage;\n   private static int executorServiceKeepAliveTimeInMilliSec;\n+  private static final double THREAD_POOL_REQUIREMENT_BUFFER = 1.2; // 20% more threads than the queue size\n+  private static boolean isDynamicScalingEnabled;\n+\n+  private ScheduledExecutorService cpuMonitorThread;\n   private ThreadPoolExecutor workerPool;\n+  private final List<ReadBufferWorker> workerRefs = new ArrayList<>();\n \n   // Buffer Pool Configurations\n   private static int minBufferPoolSize;\n   private static int maxBufferPoolSize;\n+  private static int memoryMonitoringIntervalInMilliSec;\n+  private static double memoryThreshold;\n+\n   private int numberOfActiveBuffers = 0;\n   private byte[][] bufferPool;\n+  private Stack<Integer> removedBufferList = new Stack<>();\n+  private ScheduledExecutorService memoryMonitorThread;\n \n+  // Buffer Manager Structures\n   private static ReadBufferManagerV2 bufferManager;\n-\n-  // hide instance constructor\n-  private ReadBufferManagerV2() {\n-    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n-  }\n+  private static boolean isConfigured = false;\n \n   /**\n-   * Sets the read buffer manager configurations.\n-   * @param readAheadBlockSize the size of the read-ahead block in bytes\n-   * @param abfsConfiguration the AbfsConfiguration instance for other configurations\n+   * Private constructor to prevent instantiation as this needs to be singleton.\n    */\n-  static void setReadBufferManagerConfigs(int readAheadBlockSize, AbfsConfiguration abfsConfiguration) {\n-    if (bufferManager == null) {\n-      minThreadPoolSize = abfsConfiguration.getMinReadAheadV2ThreadPoolSize();\n-      maxThreadPoolSize = abfsConfiguration.getMaxReadAheadV2ThreadPoolSize();\n-      executorServiceKeepAliveTimeInMilliSec = abfsConfiguration.getReadAheadExecutorServiceTTLInMillis();\n-\n-      minBufferPoolSize = abfsConfiguration.getMinReadAheadV2BufferPoolSize();\n-      maxBufferPoolSize = abfsConfiguration.getMaxReadAheadV2BufferPoolSize();\n-      setThresholdAgeMilliseconds(abfsConfiguration.getReadAheadV2CachedBufferTTLMillis());\n-      setReadAheadBlockSize(readAheadBlockSize);\n-    }\n+  private ReadBufferManagerV2() {\n+    printTraceLog(\"Creating Read Buffer Manager V2 with HADOOP-18546 patch\");\n\nReview Comment:\n   Its same only. Just a method call to avoid redundant checks on log\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:54:11.358+0000", "updated": "2025-10-28T04:54:11.358+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033430", "id": "18033430", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467898863\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -17,67 +17,91 @@\n  */\n package org.apache.hadoop.fs.azurebfs.services;\n \n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import com.sun.management.OperatingSystemMXBean;\n+\n import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.MemoryMXBean;\n+import java.lang.management.MemoryUsage;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.SynchronousQueue;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.classification.VisibleForTesting;\n-import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n \n-final class ReadBufferManagerV2 extends ReadBufferManager {\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_HUNDRED;\n+\n+/**\n+ * The Improved Read Buffer Manager for Rest AbfsClient.\n+ */\n+public class ReadBufferManagerV2 extends ReadBufferManager {\n+  // Internal constants\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n \n   // Thread Pool Configurations\n   private static int minThreadPoolSize;\n   private static int maxThreadPoolSize;\n+  private static int cpuMonitoringIntervalInMilliSec;\n+  private static double cpuThreshold;\n+  private static int threadPoolUpscalePercentage;\n+  private static int threadPoolDownscalePercentage;\n   private static int executorServiceKeepAliveTimeInMilliSec;\n+  private static final double THREAD_POOL_REQUIREMENT_BUFFER = 1.2; // 20% more threads than the queue size\n\nReview Comment:\n   This will only be used while upscaling. This is to make sure we have sufficient threads in thread pool to cater to all the queued buffer. This is just to understand if we need to upscale further or not. If we don't have enough queued requests  we won't upscale even if cpu is much below threshhold.\r\n   \r\n   The new thread pool size is still computed using configured vaues only.\r\n   \n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -128,13 +128,20 @@ public final class FileSystemConfigurations {\n   public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120;\n \n   public static final boolean DEFAULT_ENABLE_READAHEAD = true;\n-  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = false;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = true;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2_DYNAMIC_SCALING = true;\n   public static final int DEFAULT_READAHEAD_V2_MIN_THREAD_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MAX_THREAD_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MIN_BUFFER_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MAX_BUFFER_POOL_SIZE = -1;\n-  public static final int DEFAULT_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLIS = 3_000;\n+  public static final int DEFAULT_READAHEAD_V2_CPU_MONITORING_INTERVAL_MILLIS = 6_000;\n+  public static final int DEFAULT_READAHEAD_V2_THREAD_POOL_UPSCALE_PERCENTAGE = 20;\n+  public static final int DEFAULT_READAHEAD_V2_THREAD_POOL_DOWNSCALE_PERCENTAGE = 30;\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:54:36.211+0000", "updated": "2025-10-28T04:54:36.211+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033431", "id": "18033431", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467899487\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -128,13 +128,20 @@ public final class FileSystemConfigurations {\n   public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120;\n \n   public static final boolean DEFAULT_ENABLE_READAHEAD = true;\n-  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = false;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = true;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2_DYNAMIC_SCALING = true;\n   public static final int DEFAULT_READAHEAD_V2_MIN_THREAD_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MAX_THREAD_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MIN_BUFFER_POOL_SIZE = -1;\n   public static final int DEFAULT_READAHEAD_V2_MAX_BUFFER_POOL_SIZE = -1;\n-  public static final int DEFAULT_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLIS = 3_000;\n+  public static final int DEFAULT_READAHEAD_V2_CPU_MONITORING_INTERVAL_MILLIS = 6_000;\n+  public static final int DEFAULT_READAHEAD_V2_THREAD_POOL_UPSCALE_PERCENTAGE = 20;\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:54:51.231+0000", "updated": "2025-10-28T04:54:51.231+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033432", "id": "18033432", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467900691\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestReadBufferManagerV2.java:\n##########\n@@ -0,0 +1,258 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.junit.jupiter.api.Test;\n+import org.mockito.Mockito;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_READAHEAD_V2;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CACHED_BUFFER_TTL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CPU_MONITORING_INTERVAL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CPU_USAGE_THRESHOLD_PERCENT;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MAX_THREAD_POOL_SIZE;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MEMORY_MONITORING_INTERVAL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MIN_THREAD_POOL_SIZE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_KB;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+/**\n+ * Unit Tests around different components of Read Buffer Manager V2\n+ */\n+public class TestReadBufferManagerV2 extends AbstractAbfsIntegrationTest {\n+  private volatile boolean running = true;\n+  private final List<byte[]> allocations = new ArrayList<>();\n+\n+\n+  public TestReadBufferManagerV2() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * Test to verify init of ReadBufferManagerV2\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testReadBufferManagerV2Init() throws Exception {\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(getConfiguration().getReadAheadBlockSize(), getConfiguration());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    assertThat(ReadBufferManagerV2.getInstance())\n+        .as(\"ReadBufferManager should be uninitialized\").isNull();\n+    intercept(IllegalStateException.class, \"ReadBufferManagerV2 is not configured.\", () -> {\n+      ReadBufferManagerV2.getBufferManager();\n+    });\n+    // verify that multiple invocations of getBufferManager returns same instance.\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(getConfiguration().getReadAheadBlockSize(), getConfiguration());\n+    ReadBufferManagerV2 bufferManager = ReadBufferManagerV2.getBufferManager();\n+    ReadBufferManagerV2 bufferManager2 = ReadBufferManagerV2.getBufferManager();\n+    ReadBufferManagerV2 bufferManager3 = ReadBufferManagerV2.getInstance();\n+    assertThat(bufferManager).isNotNull();\n+    assertThat(bufferManager2).isNotNull();\n+    assertThat(bufferManager).isSameAs(bufferManager2);\n+    assertThat(bufferManager3).isNotNull();\n+    assertThat(bufferManager3).isSameAs(bufferManager);\n+\n+    // Verify default values are not invalid.\n+    assertThat(bufferManager.getMinBufferPoolSize()).isGreaterThan(0);\n+    assertThat(bufferManager.getMaxBufferPoolSize()).isGreaterThan(0);\n+  }\n+\n+  /**\n+   * Test to verify that cpu monitor thread is not active if disabled.\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testDynamicScalingSwitchingOnAndOff() throws Exception {\n+    Configuration conf = new Configuration(getRawConfiguration());\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2, true);\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING, true);\n+    try(AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(getFileSystem().getUri(), conf)) {\n+      AbfsConfiguration abfsConfiguration = fs.getAbfsStore().getAbfsConfiguration();\n+      ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfiguration.getReadAheadBlockSize(), abfsConfiguration);\n+      ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+      assertThat(bufferManagerV2.getCpuMonitoringThread())\n+          .as(\"CPU Monitor thread should be initialized\").isNotNull();\n+      bufferManagerV2.resetBufferManager();\n+    }\n+\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING, false);\n+    try(AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(getFileSystem().getUri(), conf)) {\n+      AbfsConfiguration abfsConfiguration = fs.getAbfsStore().getAbfsConfiguration();\n+      ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfiguration.getReadAheadBlockSize(), abfsConfiguration);\n+      ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+      assertThat(bufferManagerV2.getCpuMonitoringThread())\n+          .as(\"CPU Monitor thread should not be initialized\").isNull();\n+      bufferManagerV2.resetBufferManager();\n+    }\n+  }\n+\n+  @Test\n+  public void testThreadPoolDynamicScaling() throws Exception {\n+    TestAbfsInputStream testAbfsInputStream = new TestAbfsInputStream();\n\nReview Comment:\n   Nice catch\r\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n\nReview Comment:\n   Added\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:55:16.418+0000", "updated": "2025-10-28T04:55:16.418+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033433", "id": "18033433", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467905140\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n\nReview Comment:\n   During design phase we discussed this aspect and decided to keep both separate. \r\n   Still there is indirect connection between cpu and memory.\r\n   \r\n   If we don't have enough memory, we won't queue. If we don't have enough queued, we won't upscale CPU\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:56:51.557+0000", "updated": "2025-10-28T04:56:51.557+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033434", "id": "18033434", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467906251\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n\nReview Comment:\n   We only upscale when we have enough queued requests and we only queue if we have enough memory\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:57:21.596+0000", "updated": "2025-10-28T04:57:21.596+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033436", "id": "18033436", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467910587\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n\n\nReview Comment:\n   Yeah I also tried that. Though they sound similar but they are now different\r\n   For eg, in V1 we have all the checks on stream and in V2 we have on eTag.\r\n   \r\n   Also, I wanted to keep V1 untouched so that any issue in V2 can be easily mitigated.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:59:07.015+0000", "updated": "2025-10-28T04:59:07.015+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033437", "id": "18033437", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467911147\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n\nReview Comment:\n   Nice suggestion.\r\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n+  }\n+\n+  private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) {\n+    for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) {\n+      ReadBuffer readBuffer = it.next();\n+      if (readBuffer.getStream() == stream) {\n+        it.remove();\n+        // As failed ReadBuffers (bufferIndex = -1) are already pushed to free\n+        // list in doneReading method, we will skip adding those here again.\n+        if (readBuffer.getBufferindex() != -1) {\n+          pushToFreeList(readBuffer.getBufferindex());\n+        }\n+      }\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Test method that can clean up the current state of readAhead buffers and\n+   * the lists. Will also trigger a fresh init.\n    */\n   @VisibleForTesting\n   @Override\n   public void testResetReadBufferManager() {\n-    // TODO: To be implemented\n+    synchronized (this) {\n+      ArrayList<ReadBuffer> completedBuffers = new ArrayList<>();\n+      for (ReadBuffer buf : getCompletedReadList()) {\n+        if (buf != null) {\n+          completedBuffers.add(buf);\n+        }\n+      }\n+\n+      for (ReadBuffer buf : completedBuffers) {\n+        manualEviction(buf);\n+      }\n+\n+      getReadAheadQueue().clear();\n+      getInProgressList().clear();\n+      getCompletedReadList().clear();\n+      getFreeList().clear();\n+      for (int i = 0; i < maxBufferPoolSize; i++) {\n+        bufferPool[i] = null;\n+      }\n+      bufferPool = null;\n+      cpuMonitorThread.shutdownNow();\n\nReview Comment:\n   Added check\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -17,67 +17,91 @@\n  */\n package org.apache.hadoop.fs.azurebfs.services;\n \n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import com.sun.management.OperatingSystemMXBean;\n+\n import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.MemoryMXBean;\n+import java.lang.management.MemoryUsage;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.SynchronousQueue;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.classification.VisibleForTesting;\n-import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n \n-final class ReadBufferManagerV2 extends ReadBufferManager {\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_HUNDRED;\n+\n+/**\n+ * The Improved Read Buffer Manager for Rest AbfsClient.\n+ */\n+public class ReadBufferManagerV2 extends ReadBufferManager {\n+  // Internal constants\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n \n   // Thread Pool Configurations\n   private static int minThreadPoolSize;\n   private static int maxThreadPoolSize;\n+  private static int cpuMonitoringIntervalInMilliSec;\n+  private static double cpuThreshold;\n+  private static int threadPoolUpscalePercentage;\n+  private static int threadPoolDownscalePercentage;\n   private static int executorServiceKeepAliveTimeInMilliSec;\n+  private static final double THREAD_POOL_REQUIREMENT_BUFFER = 1.2; // 20% more threads than the queue size\n+  private static boolean isDynamicScalingEnabled;\n+\n+  private ScheduledExecutorService cpuMonitorThread;\n   private ThreadPoolExecutor workerPool;\n+  private final List<ReadBufferWorker> workerRefs = new ArrayList<>();\n\nReview Comment:\n   This is modified at two places where parallel threads cannot arrive.\r\n   1. While init: Its singleton so only one Input stream will be able to init.\r\n   2. CpuMonitorThread. There can be only one cpu monitor thread in one JVM.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n+  }\n+\n+  private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) {\n+    for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) {\n+      ReadBuffer readBuffer = it.next();\n+      if (readBuffer.getStream() == stream) {\n+        it.remove();\n+        // As failed ReadBuffers (bufferIndex = -1) are already pushed to free\n+        // list in doneReading method, we will skip adding those here again.\n+        if (readBuffer.getBufferindex() != -1) {\n+          pushToFreeList(readBuffer.getBufferindex());\n+        }\n+      }\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Test method that can clean up the current state of readAhead buffers and\n+   * the lists. Will also trigger a fresh init.\n    */\n   @VisibleForTesting\n   @Override\n   public void testResetReadBufferManager() {\n-    // TODO: To be implemented\n+    synchronized (this) {\n+      ArrayList<ReadBuffer> completedBuffers = new ArrayList<>();\n+      for (ReadBuffer buf : getCompletedReadList()) {\n+        if (buf != null) {\n+          completedBuffers.add(buf);\n+        }\n+      }\n+\n+      for (ReadBuffer buf : completedBuffers) {\n+        manualEviction(buf);\n+      }\n+\n+      getReadAheadQueue().clear();\n+      getInProgressList().clear();\n+      getCompletedReadList().clear();\n+      getFreeList().clear();\n+      for (int i = 0; i < maxBufferPoolSize; i++) {\n+        bufferPool[i] = null;\n+      }\n+      bufferPool = null;\n+      cpuMonitorThread.shutdownNow();\n+      memoryMonitorThread.shutdownNow();\n+      workerPool.shutdownNow();\n+      resetBufferManager();\n+    }\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n   @VisibleForTesting\n   @Override\n-  public void testResetReadBufferManager(final int readAheadBlockSize,\n-      final int thresholdAgeMilliseconds) {\n-    // TODO: To be implemented\n+  public void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds) {\n+    setReadAheadBlockSize(readAheadBlockSize);\n+    setThresholdAgeMilliseconds(thresholdAgeMilliseconds);\n+    testResetReadBufferManager();\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void testMimicFullUseAndAddFailedBuffer(final ReadBuffer buf) {\n-    // TODO: To be implemented\n+  @VisibleForTesting\n+  public void callTryEvict() {\n+    tryEvict();\n   }\n \n-  private final ThreadFactory namedThreadFactory = new ThreadFactory() {\n-    private int count = 0;\n-    @Override\n-    public Thread newThread(Runnable r) {\n-      return new Thread(r, \"ReadAheadV2-Thread-\" + count++);\n+  @VisibleForTesting\n+  public int getNumBuffers() {\n+    LOCK.lock();\n+    try {\n+      return numberOfActiveBuffers;\n+    } finally {\n+      LOCK.unlock();\n     }\n-  };\n+  }\n \n   @Override\n   void resetBufferManager() {\n     setBufferManager(null); // reset the singleton instance\n+    setIsConfigured(false);\n   }\n \n   private static void setBufferManager(ReadBufferManagerV2 manager) {\n     bufferManager = manager;\n   }\n+\n+  private static void setIsConfigured(boolean configured) {\n+    isConfigured = configured;\n+  }\n+\n+  private final ThreadFactory workerThreadFactory = new ThreadFactory() {\n+    private int count = 0;\n+    @Override\n+    public Thread newThread(Runnable r) {\n+      Thread t = new Thread(r, \"ReadAheadV2-WorkerThread-\" + count++);\n+      t.setDaemon(true);\n+      return t;\n+    }\n+  };\n+\n+  private void printTraceLog(String message, Object... args) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(message, args);\n+    }\n+  }\n+\n+  private void printDebugLog(String message, Object... args) {\n+    LOGGER.debug(message, args);\n+  }\n+\n+  @VisibleForTesting\n+  double getMemoryLoad() {\n+    MemoryMXBean osBean = ManagementFactory.getMemoryMXBean();\n+    MemoryUsage memoryUsage = osBean.getHeapMemoryUsage();\n+    return (double) memoryUsage.getUsed() / memoryUsage.getMax();\n+  }\n+\n+  @VisibleForTesting\n+  public double getCpuLoad() {\n+    OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean(\n+        OperatingSystemMXBean.class);\n+    return osBean.getSystemCpuLoad();\n+  }\n+\n+  @VisibleForTesting\n+  public static ReadBufferManagerV2 getInstance() {\n+    return bufferManager;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMinBufferPoolSize() {\n+    return minBufferPoolSize;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMaxBufferPoolSize() {\n+    return maxBufferPoolSize;\n+  }\n+\n+  @VisibleForTesting\n+  public int getCurrentThreadPoolSize() {\n+    return workerRefs.size();\n+  }\n+\n+  @VisibleForTesting\n+  public int getCpuMonitoringIntervalInMilliSec() {\n+    return cpuMonitoringIntervalInMilliSec;\n+  }\n+\n+  @VisibleForTesting\n+  public int getMemoryMonitoringIntervalInMilliSec() {\n+    return memoryMonitoringIntervalInMilliSec;\n+  }\n+\n+  @VisibleForTesting\n+  public ScheduledExecutorService getCpuMonitoringThread() {\n+    return cpuMonitorThread;\n+  }\n+\n+  public int getRequiredThreadPoolSize() {\n+    return (int) Math.ceil(THREAD_POOL_REQUIREMENT_BUFFER\n+        * (getReadAheadQueue().size() + getInProgressList().size())); // 20% more for buffer\n+  }\n+\n+  private boolean isFreeListEmpty() {\n+    LOCK.lock();\n\nReview Comment:\n   We are only using Lock where freeList is getting updated. This is because there are multiple places where freeList can be updated. Synchronized block IMO just prevent parallel access to a single block of code. Here we want to prevent parallel access across multiple blocks of code.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:59:22.035+0000", "updated": "2025-10-28T04:59:22.035+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033438", "id": "18033438", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467911950\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -106,123 +166,731 @@ void init() {\n         executorServiceKeepAliveTimeInMilliSec,\n         TimeUnit.MILLISECONDS,\n         new SynchronousQueue<>(),\n-        namedThreadFactory);\n+        workerThreadFactory);\n     workerPool.allowCoreThreadTimeOut(true);\n     for (int i = 0; i < minThreadPoolSize; i++) {\n-      ReadBufferWorker worker = new ReadBufferWorker(i, this);\n+      ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+      workerRefs.add(worker);\n       workerPool.submit(worker);\n     }\n     ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+\n+    if (isDynamicScalingEnabled) {\n+      cpuMonitorThread = Executors.newSingleThreadScheduledExecutor(runnable -> {\n+        Thread t = new Thread(runnable, \"ReadAheadV2-CPU-Monitor\");\n+        t.setDaemon(true);\n+        return t;\n+      });\n+      cpuMonitorThread.scheduleAtFixedRate(this::adjustThreadPool,\n+          getCpuMonitoringIntervalInMilliSec(), getCpuMonitoringIntervalInMilliSec(),\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    printTraceLog(\"ReadBufferManagerV2 initialized with {} buffers and {} worker threads\",\n+        numberOfActiveBuffers, workerRefs.size());\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method to queueing read-ahead.\n+   * @param stream which read-ahead is requested from.\n+   * @param requestedOffset The offset in the file which should be read.\n+   * @param requestedLength The length to read.\n    */\n   @Override\n-  public void queueReadAhead(final AbfsInputStream stream,\n-      final long requestedOffset,\n-      final int requestedLength,\n-      final TracingContext tracingContext) {\n-    // TODO: To be implemented\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext) {\n+    printTraceLog(\"Start Queueing readAhead for file: {}, with eTag: {}, offset: {}, length: {}, triggered by stream: {}\",\n+        stream.getPath(), stream.getETag(), requestedOffset, requestedLength, stream.hashCode());\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream.getETag(), requestedOffset)) {\n+        // Already queued for this offset, so skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as it is already queued\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+      if (isFreeListEmpty() && !tryMemoryUpscale() && !tryEvict()) {\n+        // No buffers are available and more buffers cannot be created. Skip queuing.\n+        printTraceLog(\"Skipping queuing readAhead for file: {}, with eTag: {}, offset: {}, triggered by stream: {} as no buffers are available\",\n+            stream.getPath(), stream.getETag(), requestedOffset, stream.hashCode());\n+        return;\n+      }\n+\n+      // Create a new ReadBuffer to keep the prefetched data and queue.\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream); // To map buffer with stream that requested it\n+      buffer.setETag(stream.getETag()); // To map buffer with file it belongs to\n+      buffer.setPath(stream.getPath());\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      if (isFreeListEmpty()) {\n+        /*\n+         * By now there should be at least one buffer available.\n+         * This is to double sure that after upscaling or eviction,\n+         * we still have free buffer available. If not, we skip queueing.\n+         */\n+        return;\n+      }\n+      Integer bufferIndex = popFromFreeList();\n+      buffer.setBuffer(bufferPool[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      getReadAheadQueue().add(buffer);\n+      notifyAll();\n+      printTraceLog(\"Done q-ing readAhead for file: {}, with eTag:{}, offset: {}, buffer idx: {}, triggered by stream: {}\",\n+          stream.getPath(), stream.getETag(), requestedOffset, buffer.getBufferindex(), stream.hashCode());\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do its own\n+   * read to get the data faster (compared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream of the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n    */\n   @Override\n-  public int getBlock(final AbfsInputStream stream,\n-      final long position,\n-      final int length,\n-      final byte[] buffer) throws IOException {\n-    // TODO: To be implemented\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    printTraceLog(\"getBlock request for file: {}, with eTag: {}, for position: {} for length: {} received from stream: {}\",\n+        stream.getPath(), stream.getETag(), position, length, stream.hashCode());\n+\n+    String requestedETag = stream.getETag();\n+    boolean isFirstRead = stream.isFirstRead();\n+\n+    // Wait for any in-progress read to complete.\n+    waitForProcess(requestedETag, position, isFirstRead);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(requestedETag, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      printTraceLog(\"Done read from Cache for the file with eTag: {}, position: {}, length: {}, requested by stream: {}\",\n+          requestedETag, position, bytesRead, stream.hashCode());\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n     return 0;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this to get the next buffer that it should work on.\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n    */\n   @Override\n   public ReadBuffer getNextBlockToRead() throws InterruptedException {\n-    // TODO: To be implemented\n-    return null;\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      // Blocking Call to wait for prefetch to be queued.\n+      while (getReadAheadQueue().size() == 0) {\n+        wait();\n+      }\n+\n+      buffer = getReadAheadQueue().remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      getInProgressList().add(buffer);\n+    }\n+    printTraceLog(\"ReadBufferWorker picked file: {}, with eTag: {}, for offset: {}, queued by stream: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode());\n+    return buffer;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * {@link ReadBufferWorker} thread calls this method to post completion.   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n    */\n   @Override\n-  public void doneReading(final ReadBuffer buffer,\n-      final ReadBufferStatus result,\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n       final int bytesActuallyRead) {\n-    // TODO: To be implemented\n+    printTraceLog(\"ReadBufferWorker completed prefetch for file: {} with eTag: {}, for offset: {}, queued by stream: {}, with status: {} and bytes read: {}\",\n+        buffer.getPath(), buffer.getETag(), buffer.getOffset(), buffer.getStream().hashCode(), result, bytesActuallyRead);\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (getInProgressList().contains(buffer)) {\n+        getInProgressList().remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          // Successful read, so update the buffer status and length\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          // Failed read, reuse buffer for next read, this buffer will be\n+          // evicted later based on eviction policy.\n+          pushToFreeList(buffer.getBufferindex());\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        getCompletedReadList().add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV2} when stream is closed.\n+   * @param stream input stream.\n    */\n-  @Override\n-  public void purgeBuffersForStream(final AbfsInputStream stream) {\n-    // TODO: To be implemented\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    printDebugLog(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    getReadAheadQueue().removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, getCompletedReadList());\n+  }\n+\n+  private boolean isAlreadyQueued(final String eTag, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(getReadAheadQueue(), eTag, requestedOffset)\n+        || isInList(getInProgressList(), eTag, requestedOffset)\n+        || isInList(getCompletedReadList(), eTag, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    return (getFromList(list, eTag, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final String eTag,\n+      final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (eTag.equals(buffer.getETag())) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset\n+            < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * If any buffer in the completed list can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * @return whether the eviction succeeded - i.e., were we able to free up one buffer\n    */\n-  @VisibleForTesting\n-  @Override\n-  public int getNumBuffers() {\n-    return numberOfActiveBuffers;\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (getCompletedReadList().size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isFullyConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (maybe a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > getThresholdAgeMilliseconds()) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      manualEviction(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > getThresholdAgeMilliseconds()) && (nodeToEvict != null)) {\n+      return manualEviction(nodeToEvict);\n+    }\n+\n+    printTraceLog(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    if (buf.getRefCount() > 0) {\n+      // If the buffer is still being read, then we cannot evict it.\n+      printTraceLog(\n+          \"Cannot evict buffer with index: {}, file: {}, with eTag: {}, offset: {} as it is still being read by some input stream\",\n+          buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset());\n+      return false;\n+    }\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in getCompletedReadList(),\n+    // avoid adding it to availableBufferList.\n+    if (buf.getBufferindex() != -1) {\n+      pushToFreeList(buf.getBufferindex());\n+    }\n+    getCompletedReadList().remove(buf);\n+    buf.setTracingContext(null);\n+    printTraceLog(\n+        \"Eviction of Buffer Completed for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, is fully consumed: {}, is partially consumed: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(),\n+        buf.isFullyConsumed(), buf.isAnyByteConsumed());\n+    return true;\n+  }\n+\n+  private void waitForProcess(final String eTag, final long position, boolean isFirstRead) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      readBuf = clearFromReadAheadQueue(eTag, position, isFirstRead);\n+      if (readBuf == null) {\n+        readBuf = getFromList(getInProgressList(), eTag, position);\n+      }\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        printTraceLog(\"A relevant read buffer for file: {}, with eTag: {}, offset: {}, queued by stream: {}, having buffer idx: {} is being prefetched, waiting for latch\",\n+            readBuf.getPath(), readBuf.getETag(), readBuf.getOffset(), readBuf.getStream().hashCode(), readBuf.getBufferindex());\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of getInProgressList() only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // getInProgressList(). So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever getInProgressList() is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      printTraceLog(\"Latch done for file: {}, with eTag: {}, for offset: {}, \"\n+          + \"buffer index: {} queued by stream: {}\", readBuf.getPath(), readBuf.getETag(),\n+          readBuf.getOffset(), readBuf.getBufferindex(), readBuf.getStream().hashCode());\n+    }\n+  }\n+\n+  private ReadBuffer clearFromReadAheadQueue(final String eTag, final long requestedOffset, boolean isFirstRead) {\n+    ReadBuffer buffer = getFromList(getReadAheadQueue(), eTag, requestedOffset);\n+    /*\n+     * If this prefetch was triggered by first read of this input stream,\n+     * we should not remove it from queue and let it complete by backend threads.\n+     */\n+    if (buffer != null && isFirstRead) {\n+      return buffer;\n+    }\n+    if (buffer != null) {\n+      getReadAheadQueue().remove(buffer);\n+      notifyAll();   // lock is held in calling method\n+      pushToFreeList(buffer.getBufferindex());\n+    }\n+    return null;\n   }\n+\n+  private int getBlockFromCompletedQueue(final String eTag, final long position,\n+      final int length, final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(eTag, position);\n+\n+    if (buf == null) {\n+      return 0;\n+    }\n+\n+    buf.startReading(); // atomic increment of refCount.\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n+      // return exception only from buffers that failed within last getThresholdAgeMilliseconds()\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < getThresholdAgeMilliseconds())) {\n+        throw buf.getErrException();\n+      } else {\n+        return 0;\n+      }\n+    }\n+\n+    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n+        || (position >= buf.getOffset() + buf.getLength())) {\n+      return 0;\n+    }\n+\n+    int cursor = (int) (position - buf.getOffset());\n+    int availableLengthInBuffer = buf.getLength() - cursor;\n+    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n+    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n+    if (cursor == 0) {\n+      buf.setFirstByteConsumed(true);\n+    }\n+    if (cursor + lengthToCopy == buf.getLength()) {\n+      buf.setLastByteConsumed(true);\n+    }\n+    buf.setAnyByteConsumed(true);\n+\n+    buf.endReading(); // atomic decrement of refCount\n+    return lengthToCopy;\n+  }\n+\n+  private ReadBuffer getBufferFromCompletedQueue(final String eTag, final long requestedOffset) {\n+    for (ReadBuffer buffer : getCompletedReadList()) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if (eTag.equals(buffer.getETag())\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n+          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n+        return buffer;\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private synchronized boolean tryMemoryUpscale() {\n+    if (!isDynamicScalingEnabled) {\n+      printTraceLog(\"Dynamic scaling is disabled, skipping memory upscale\");\n+      return false; // Dynamic scaling is disabled, so no upscaling.\n+    }\n+    double memoryLoad = getMemoryLoad();\n+    if (memoryLoad < memoryThreshold && getNumBuffers() < maxBufferPoolSize) {\n+      // Create and Add more buffers in getFreeList().\n+      if (removedBufferList.isEmpty()) {\n+        bufferPool[getNumBuffers()] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(getNumBuffers());\n+      } else {\n+        // Reuse a removed buffer index.\n+        int freeIndex = removedBufferList.pop();\n+        if (freeIndex >= bufferPool.length) {\n+          printTraceLog(\"Invalid free index: {}. Current buffer pool size: {}\",\n+              freeIndex, bufferPool.length);\n+          return false;\n+        }\n+        bufferPool[freeIndex] = new byte[getReadAheadBlockSize()];\n+        pushToFreeList(freeIndex);\n+      }\n+      incrementActiveBufferCount();\n+      printTraceLog(\"Current Memory Load: {}. Incrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      return true;\n+    }\n+    printTraceLog(\"Could not Upscale memory. Total buffers: {} Memory Load: {}\",\n+        getNumBuffers(), memoryLoad);\n+    return false;\n+  }\n+\n+  private void scheduledEviction() {\n+    for (ReadBuffer buf : getCompletedReadList()) {\n+      if (currentTimeMillis() - buf.getTimeStamp() > getThresholdAgeMilliseconds()) {\n+        // If the buffer is older than thresholdAge, evict it.\n+        printTraceLog(\"Scheduled Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, length: {}, queued by stream: {}\",\n+            buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getLength(), buf.getStream().hashCode());\n+        evict(buf);\n+      }\n+    }\n+\n+    double memoryLoad = getMemoryLoad();\n+    if (isDynamicScalingEnabled && memoryLoad > memoryThreshold) {\n+      synchronized (this) {\n+        if (isFreeListEmpty()) {\n+          printTraceLog(\"No free buffers available. Skipping downscale of buffer pool\");\n+          return; // No free buffers available, so cannot downscale.\n+        }\n+        int freeIndex = popFromFreeList();\n+        bufferPool[freeIndex] = null;\n+        removedBufferList.add(freeIndex);\n+        decrementActiveBufferCount();\n+        printTraceLog(\"Current Memory Load: {}. Decrementing buffer pool size to {}\", memoryLoad, getNumBuffers());\n+      }\n+    }\n+  }\n+\n+  private boolean manualEviction(final ReadBuffer buf) {\n+    printTraceLog(\"Manual Eviction of Buffer Triggered for BufferIndex: {}, file: {}, with eTag: {}, offset: {}, queued by stream: {}\",\n+        buf.getBufferindex(), buf.getPath(), buf.getETag(), buf.getOffset(), buf.getStream().hashCode());\n+    return evict(buf);\n+  }\n+\n+  private void adjustThreadPool() {\n+    int currentPoolSize = workerRefs.size();\n+    double cpuLoad = getCpuLoad();\n+    int requiredPoolSize = getRequiredThreadPoolSize();\n+    int newThreadPoolSize;\n+    printTraceLog(\"Current CPU load: {}, Current worker pool size: {}, Current queue size: {}\", cpuLoad, currentPoolSize, requiredPoolSize);\n+    if (currentPoolSize < requiredPoolSize && cpuLoad < cpuThreshold) {\n+      // Submit more background tasks.\n+      newThreadPoolSize = Math.min(maxThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED + threadPoolUpscalePercentage))/ONE_HUNDRED));\n+      // Create new Worker Threads\n+      for (int i = currentPoolSize; i < newThreadPoolSize; i++) {\n+        ReadBufferWorker worker = new ReadBufferWorker(i, getBufferManager());\n+        workerRefs.add(worker);\n+        workerPool.submit(worker);\n+      }\n+      printTraceLog(\"Increased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else if (cpuLoad > cpuThreshold || currentPoolSize > requiredPoolSize) {\n+      newThreadPoolSize = Math.max(minThreadPoolSize,\n+          (int) Math.ceil((currentPoolSize * (ONE_HUNDRED - threadPoolDownscalePercentage))/ONE_HUNDRED));\n+      // Signal the extra workers to stop\n+      while (workerRefs.size() > newThreadPoolSize) {\n+        ReadBufferWorker worker = workerRefs.remove(workerRefs.size() - 1);\n+        worker.stop();\n+      }\n+      printTraceLog(\"Decreased worker pool size from {} to {}\", currentPoolSize, newThreadPoolSize);\n+    } else {\n+      printTraceLog(\"No change in worker pool size. CPU load: {} Pool size: {}\", cpuLoad, currentPoolSize);\n+    }\n+  }\n+\n   /**\n-   * {@inheritDoc}\n+   * Similar to System.currentTimeMillis, except implemented with System.nanoTime().\n+   * System.currentTimeMillis can go backwards when system clock is changed (e.g., with NTP time synchronization),\n+   * making it unsuitable for measuring time intervals. nanotime is strictly monotonically increasing per CPU core.\n+   * Note: it is not monotonic across Sockets, and even within a CPU, its only the\n+   * more recent parts which share a clock across all cores.\n+   *\n+   * @return current time in milliseconds\n    */\n-  @VisibleForTesting\n-  @Override\n-  public void callTryEvict() {\n-    // TODO: To be implemented\n+  private long currentTimeMillis() {\n+    return System.nanoTime() / 1000 / 1000;\n+  }\n+\n+  private void purgeList(AbfsInputStream stream, LinkedList<ReadBuffer> list) {\n+    for (Iterator<ReadBuffer> it = list.iterator(); it.hasNext();) {\n+      ReadBuffer readBuffer = it.next();\n+      if (readBuffer.getStream() == stream) {\n+        it.remove();\n+        // As failed ReadBuffers (bufferIndex = -1) are already pushed to free\n+        // list in doneReading method, we will skip adding those here again.\n+        if (readBuffer.getBufferindex() != -1) {\n+          pushToFreeList(readBuffer.getBufferindex());\n+        }\n+      }\n+    }\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Test method that can clean up the current state of readAhead buffers and\n+   * the lists. Will also trigger a fresh init.\n    */\n   @VisibleForTesting\n   @Override\n   public void testResetReadBufferManager() {\n-    // TODO: To be implemented\n+    synchronized (this) {\n+      ArrayList<ReadBuffer> completedBuffers = new ArrayList<>();\n+      for (ReadBuffer buf : getCompletedReadList()) {\n+        if (buf != null) {\n+          completedBuffers.add(buf);\n+        }\n+      }\n+\n+      for (ReadBuffer buf : completedBuffers) {\n+        manualEviction(buf);\n+      }\n+\n+      getReadAheadQueue().clear();\n+      getInProgressList().clear();\n+      getCompletedReadList().clear();\n+      getFreeList().clear();\n+      for (int i = 0; i < maxBufferPoolSize; i++) {\n+        bufferPool[i] = null;\n+      }\n+      bufferPool = null;\n+      cpuMonitorThread.shutdownNow();\n+      memoryMonitorThread.shutdownNow();\n+      workerPool.shutdownNow();\n+      resetBufferManager();\n+    }\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n   @VisibleForTesting\n   @Override\n-  public void testResetReadBufferManager(final int readAheadBlockSize,\n-      final int thresholdAgeMilliseconds) {\n-    // TODO: To be implemented\n+  public void testResetReadBufferManager(int readAheadBlockSize, int thresholdAgeMilliseconds) {\n+    setReadAheadBlockSize(readAheadBlockSize);\n+    setThresholdAgeMilliseconds(thresholdAgeMilliseconds);\n+    testResetReadBufferManager();\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void testMimicFullUseAndAddFailedBuffer(final ReadBuffer buf) {\n-    // TODO: To be implemented\n+  @VisibleForTesting\n+  public void callTryEvict() {\n+    tryEvict();\n   }\n \n-  private final ThreadFactory namedThreadFactory = new ThreadFactory() {\n-    private int count = 0;\n-    @Override\n-    public Thread newThread(Runnable r) {\n-      return new Thread(r, \"ReadAheadV2-Thread-\" + count++);\n+  @VisibleForTesting\n+  public int getNumBuffers() {\n+    LOCK.lock();\n+    try {\n+      return numberOfActiveBuffers;\n+    } finally {\n+      LOCK.unlock();\n     }\n-  };\n+  }\n \n   @Override\n   void resetBufferManager() {\n     setBufferManager(null); // reset the singleton instance\n+    setIsConfigured(false);\n   }\n \n   private static void setBufferManager(ReadBufferManagerV2 manager) {\n     bufferManager = manager;\n   }\n+\n+  private static void setIsConfigured(boolean configured) {\n+    isConfigured = configured;\n+  }\n+\n+  private final ThreadFactory workerThreadFactory = new ThreadFactory() {\n+    private int count = 0;\n+    @Override\n+    public Thread newThread(Runnable r) {\n+      Thread t = new Thread(r, \"ReadAheadV2-WorkerThread-\" + count++);\n+      t.setDaemon(true);\n+      return t;\n+    }\n+  };\n+\n+  private void printTraceLog(String message, Object... args) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(message, args);\n+    }\n+  }\n+\n+  private void printDebugLog(String message, Object... args) {\n+    LOGGER.debug(message, args);\n+  }\n+\n+  @VisibleForTesting\n+  double getMemoryLoad() {\n+    MemoryMXBean osBean = ManagementFactory.getMemoryMXBean();\n+    MemoryUsage memoryUsage = osBean.getHeapMemoryUsage();\n+    return (double) memoryUsage.getUsed() / memoryUsage.getMax();\n+  }\n+\n+  @VisibleForTesting\n+  public double getCpuLoad() {\n+    OperatingSystemMXBean osBean = ManagementFactory.getPlatformMXBean(\n+        OperatingSystemMXBean.class);\n+    return osBean.getSystemCpuLoad();\n\nReview Comment:\n   Nice catch\r\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -87,17 +111,53 @@ static ReadBufferManagerV2 getBufferManager() {\n   }\n \n   /**\n-   * {@inheritDoc}\n+   * Set the ReadBufferManagerV2 configurations based on the provided before singleton initialization.\n+   * @param readAheadBlockSize the read-ahead block size to set for the ReadBufferManagerV2.\n+   * @param abfsConfiguration the configuration to set for the ReadBufferManagerV2.\n+   */\n+  public static void setReadBufferManagerConfigs(final int readAheadBlockSize,\n+      final AbfsConfiguration abfsConfiguration) {\n+    // Set Configs only before initializations.\n+    if (bufferManager == null && !isConfigured) {\n+      minThreadPoolSize = abfsConfiguration.getMinReadAheadV2ThreadPoolSize();\n+      maxThreadPoolSize = abfsConfiguration.getMaxReadAheadV2ThreadPoolSize();\n+      cpuMonitoringIntervalInMilliSec = abfsConfiguration.getReadAheadV2CpuMonitoringIntervalMillis();\n+      cpuThreshold = abfsConfiguration.getReadAheadV2CpuUsageThresholdPercent()/ ONE_HUNDRED;\n+      threadPoolUpscalePercentage = abfsConfiguration.getReadAheadV2ThreadPoolUpscalePercentage();\n+      threadPoolDownscalePercentage = abfsConfiguration.getReadAheadV2ThreadPoolDownscalePercentage();\n+      executorServiceKeepAliveTimeInMilliSec = abfsConfiguration.getReadAheadExecutorServiceTTLInMillis();\n+\n+      minBufferPoolSize = abfsConfiguration.getMinReadAheadV2BufferPoolSize();\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -128,13 +128,20 @@ public final class FileSystemConfigurations {\n   public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120;\n \n   public static final boolean DEFAULT_ENABLE_READAHEAD = true;\n-  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = false;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = true;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2_DYNAMIC_SCALING = true;\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:59:41.847+0000", "updated": "2025-10-28T04:59:41.847+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033439", "id": "18033439", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467912661\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -128,13 +128,20 @@ public final class FileSystemConfigurations {\n   public static final long DEFAULT_SAS_TOKEN_RENEW_PERIOD_FOR_STREAMS_IN_SECONDS = 120;\n \n   public static final boolean DEFAULT_ENABLE_READAHEAD = true;\n-  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = false;\n+  public static final boolean DEFAULT_ENABLE_READAHEAD_V2 = true;\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T04:59:56.963+0000", "updated": "2025-10-28T04:59:56.963+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033440", "id": "18033440", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2467913226\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1469,6 +1504,18 @@ public boolean isReadAheadEnabled() {\n     return this.enabledReadAhead;\n   }\n \n+  /**\n+   * Checks if the read-ahead v2 feature is enabled by user.\n+   * @return true if read-ahead v2 is enabled, false otherwise.\n+   */\n+  public boolean isReadAheadV2Enabled() {\n+    return this.isReadAheadV2Enabled;\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T05:00:16.774+0000", "updated": "2025-10-28T05:00:16.774+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033441", "id": "18033441", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3454626712\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |  24m 42s |  |  Docker failed to build run-specific yetus/hadoop:tp-18459}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/24/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T05:19:56.346+0000", "updated": "2025-10-28T05:19:56.346+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033555", "id": "18033555", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3456424095\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m  7s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/25/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  1s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 15s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/25/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 8 new + 5 unchanged - 9 fixed = 13 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/25/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 57 new + 1472 unchanged - 0 fixed = 1529 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/25/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 25 new + 1413 unchanged - 0 fixed = 1438 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/25/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 17 new + 177 unchanged - 1 fixed = 194 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 16s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  67m 24s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:[line 91] |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:[line 172] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:[line 196] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:[line 180] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:[line 188] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:[line 99] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:[line 107] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:[line 141] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.getBufferManager() may expose internal representation by returning ReadBufferManagerV1.bufferManager  At ReadBufferManagerV1.java:internal representation by returning ReadBufferManagerV1.bufferManager  At ReadBufferManagerV1.java:[line 81] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setIsConfigured(boolean)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setIsConfigured(boolean)  At ReadBufferManagerV2.java:[line 866] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 124] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 135] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 132] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getBufferManager() may expose internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:[line 110] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance() may expose internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:[line 918] |\r\n   |  |  Unknown bug pattern SING_SINGLETON_GETTER_NOT_SYNCHRONIZED in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance()  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance()  At ReadBufferManagerV2.java:[line 918] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/25/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 9ceda3b430ab 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5e03a9bd9c72d8d520c7375cbe4d945bdf02d341 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/25/testReport/ |\r\n   | Max. process+thread count | 645 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/25/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T13:17:38.680+0000", "updated": "2025-10-28T13:17:38.680+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033654", "id": "18033654", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3457952724\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 54s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/26/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 34s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 46s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/26/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 5 unchanged - 9 fixed = 6 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 19s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/26/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 57 new + 1472 unchanged - 0 fixed = 1529 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/26/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 25 new + 1413 unchanged - 0 fixed = 1438 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/26/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 17 new + 177 unchanged - 1 fixed = 194 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 15s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  61m 31s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:[line 91] |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:[line 172] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:[line 196] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:[line 180] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:[line 188] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:[line 99] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:[line 107] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:[line 141] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.getBufferManager() may expose internal representation by returning ReadBufferManagerV1.bufferManager  At ReadBufferManagerV1.java:internal representation by returning ReadBufferManagerV1.bufferManager  At ReadBufferManagerV1.java:[line 81] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setIsConfigured(boolean)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setIsConfigured(boolean)  At ReadBufferManagerV2.java:[line 969] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 144] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 164] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 157] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getBufferManager() may expose internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:[line 129] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance() may expose internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:[line 1022] |\r\n   |  |  Unknown bug pattern SING_SINGLETON_GETTER_NOT_SYNCHRONIZED in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance()  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance()  At ReadBufferManagerV2.java:[line 1022] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/26/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 4d4d787adb2d 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1ba53200e68cf042498ab6c29695559899dd05b6 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/26/testReport/ |\r\n   | Max. process+thread count | 642 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/26/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T18:35:33.898+0000", "updated": "2025-10-28T18:35:33.898+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033800", "id": "18033800", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2472635112\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -530,7 +530,7 @@ private int readInternal(final long position, final byte[] b, final int offset,\n       while (numReadAheads > 0 && nextOffset < contentLength) {\n         LOG.debug(\"issuing read ahead requestedOffset = {} requested size {}\",\n             nextOffset, nextSize);\n-        readBufferManager.queueReadAhead(this, nextOffset, (int) nextSize,\n+        getReadBufferManager().queueReadAhead(this, nextOffset, (int) nextSize,\n\nReview Comment:\n   Needs changes in constructor to return an instance of readbuffermanager v2 if enabled\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T11:27:41.863+0000", "updated": "2025-10-29T11:27:41.863+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18033813", "id": "18033813", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3461333187\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/27/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 26s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 38s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/27/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 5 unchanged - 9 fixed = 6 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/27/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 57 new + 1472 unchanged - 0 fixed = 1529 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 15s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/27/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 25 new + 1413 unchanged - 0 fixed = 1438 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 44s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/27/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 17 new + 177 unchanged - 1 fixed = 194 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 10s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 15s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  61m 27s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:[line 91] |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:[line 172] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:[line 196] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:[line 180] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:[line 188] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:[line 99] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:[line 107] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:[line 141] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.getBufferManager() may expose internal representation by returning ReadBufferManagerV1.bufferManager  At ReadBufferManagerV1.java:internal representation by returning ReadBufferManagerV1.bufferManager  At ReadBufferManagerV1.java:[line 81] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setIsConfigured(boolean)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setIsConfigured(boolean)  At ReadBufferManagerV2.java:[line 969] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 144] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 164] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 157] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getBufferManager() may expose internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:[line 129] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance() may expose internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:[line 1022] |\r\n   |  |  Unknown bug pattern SING_SINGLETON_GETTER_NOT_SYNCHRONIZED in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance()  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance()  At ReadBufferManagerV2.java:[line 1022] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/27/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b976b294b91c 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 56f97b353198841354143086c9ffd63111784eeb |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/27/testReport/ |\r\n   | Max. process+thread count | 765 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/27/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T12:46:25.239+0000", "updated": "2025-10-29T12:46:25.239+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18034098", "id": "18034098", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3466195427\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/28/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 23s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/28/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 5 unchanged - 9 fixed = 6 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/28/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 57 new + 1472 unchanged - 0 fixed = 1529 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 15s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/28/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 25 new + 1413 unchanged - 0 fixed = 1438 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/28/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 17 new + 177 unchanged - 1 fixed = 194 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 46s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 14s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  61m 16s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:[line 91] |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:[line 172] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:[line 196] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:[line 180] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:[line 188] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:[line 99] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:[line 107] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:[line 141] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.getBufferManager() may expose internal representation by returning ReadBufferManagerV1.bufferManager  At ReadBufferManagerV1.java:internal representation by returning ReadBufferManagerV1.bufferManager  At ReadBufferManagerV1.java:[line 81] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setIsConfigured(boolean)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setIsConfigured(boolean)  At ReadBufferManagerV2.java:[line 969] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 144] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 164] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.setReadBufferManagerConfigs(int, AbfsConfiguration)  At ReadBufferManagerV2.java:[line 157] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getBufferManager() may expose internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:[line 129] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance() may expose internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:[line 1022] |\r\n   |  |  Unknown bug pattern SING_SINGLETON_GETTER_NOT_SYNCHRONIZED in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance()  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance()  At ReadBufferManagerV2.java:[line 1022] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/28/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2d1253aff5cf 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / da3661abefc75fd7aa292becc2186a16c5ffc065 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/28/testReport/ |\r\n   | Max. process+thread count | 611 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/28/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T05:33:23.836+0000", "updated": "2025-10-30T05:33:23.836+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18034102", "id": "18034102", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2476624648\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -530,7 +530,7 @@ private int readInternal(final long position, final byte[] b, final int offset,\n       while (numReadAheads > 0 && nextOffset < contentLength) {\n         LOG.debug(\"issuing read ahead requestedOffset = {} requested size {}\",\n             nextOffset, nextSize);\n-        readBufferManager.queueReadAhead(this, nextOffset, (int) nextSize,\n+        getReadBufferManager().queueReadAhead(this, nextOffset, (int) nextSize,\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T06:56:44.265+0000", "updated": "2025-10-30T06:56:44.265+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18034103", "id": "18034103", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#discussion_r2476640605\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestReadBufferManagerV2.java:\n##########\n@@ -0,0 +1,258 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.junit.jupiter.api.Test;\n+import org.mockito.Mockito;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_READAHEAD_V2;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CACHED_BUFFER_TTL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CPU_MONITORING_INTERVAL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_CPU_USAGE_THRESHOLD_PERCENT;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MAX_THREAD_POOL_SIZE;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MEMORY_MONITORING_INTERVAL_MILLIS;\n+import static org.apache.hadoop.fs.azurebfs.constants.ConfigurationKeys.FS_AZURE_READAHEAD_V2_MIN_THREAD_POOL_SIZE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ONE_KB;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+/**\n+ * Unit Tests around different components of Read Buffer Manager V2\n+ */\n+public class TestReadBufferManagerV2 extends AbstractAbfsIntegrationTest {\n+  private volatile boolean running = true;\n+  private final List<byte[]> allocations = new ArrayList<>();\n+\n+\n+  public TestReadBufferManagerV2() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * Test to verify init of ReadBufferManagerV2\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testReadBufferManagerV2Init() throws Exception {\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(getConfiguration().getReadAheadBlockSize(), getConfiguration());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    assertThat(ReadBufferManagerV2.getInstance())\n+        .as(\"ReadBufferManager should be uninitialized\").isNull();\n+    intercept(IllegalStateException.class, \"ReadBufferManagerV2 is not configured.\", () -> {\n+      ReadBufferManagerV2.getBufferManager();\n+    });\n+    // verify that multiple invocations of getBufferManager returns same instance.\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(getConfiguration().getReadAheadBlockSize(), getConfiguration());\n+    ReadBufferManagerV2 bufferManager = ReadBufferManagerV2.getBufferManager();\n+    ReadBufferManagerV2 bufferManager2 = ReadBufferManagerV2.getBufferManager();\n+    ReadBufferManagerV2 bufferManager3 = ReadBufferManagerV2.getInstance();\n+    assertThat(bufferManager).isNotNull();\n+    assertThat(bufferManager2).isNotNull();\n+    assertThat(bufferManager).isSameAs(bufferManager2);\n+    assertThat(bufferManager3).isNotNull();\n+    assertThat(bufferManager3).isSameAs(bufferManager);\n+\n+    // Verify default values are not invalid.\n+    assertThat(bufferManager.getMinBufferPoolSize()).isGreaterThan(0);\n+    assertThat(bufferManager.getMaxBufferPoolSize()).isGreaterThan(0);\n+  }\n+\n+  /**\n+   * Test to verify that cpu monitor thread is not active if disabled.\n+   * @throws Exception if test fails\n+   */\n+  @Test\n+  public void testDynamicScalingSwitchingOnAndOff() throws Exception {\n+    Configuration conf = new Configuration(getRawConfiguration());\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2, true);\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING, true);\n+    try(AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(getFileSystem().getUri(), conf)) {\n+      AbfsConfiguration abfsConfiguration = fs.getAbfsStore().getAbfsConfiguration();\n+      ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfiguration.getReadAheadBlockSize(), abfsConfiguration);\n+      ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+      assertThat(bufferManagerV2.getCpuMonitoringThread())\n+          .as(\"CPU Monitor thread should be initialized\").isNotNull();\n+      bufferManagerV2.resetBufferManager();\n+    }\n+\n+    conf.setBoolean(FS_AZURE_ENABLE_READAHEAD_V2_DYNAMIC_SCALING, false);\n+    try(AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(getFileSystem().getUri(), conf)) {\n+      AbfsConfiguration abfsConfiguration = fs.getAbfsStore().getAbfsConfiguration();\n+      ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfiguration.getReadAheadBlockSize(), abfsConfiguration);\n+      ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+      assertThat(bufferManagerV2.getCpuMonitoringThread())\n+          .as(\"CPU Monitor thread should not be initialized\").isNull();\n+      bufferManagerV2.resetBufferManager();\n+    }\n+  }\n+\n+  @Test\n+  public void testThreadPoolDynamicScaling() throws Exception {\n+    TestAbfsInputStream testAbfsInputStream = new TestAbfsInputStream();\n+    AbfsClient client = testAbfsInputStream.getMockAbfsClient();\n+    AbfsInputStream inputStream = testAbfsInputStream.getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+    Configuration configuration = getReabAheadV2Configuration();\n+    AbfsConfiguration abfsConfig = new AbfsConfiguration(configuration,\n+        getAccountName());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfig.getReadAheadBlockSize(), abfsConfig);\n+    ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+    assertThat(bufferManagerV2.getCurrentThreadPoolSize()).isEqualTo(2);\n+    int[] reqOffset = {0};\n+    int reqLength = 1;\n+    Thread t = new Thread(() -> {\n+      while (running) {\n+        bufferManagerV2.queueReadAhead(inputStream, reqOffset[0], reqLength,\n+            inputStream.getTracingContext());\n+        reqOffset[0] += reqLength;\n+      }\n+    });\n+    t.start();\n+    Thread.sleep(2L * bufferManagerV2.getCpuMonitoringIntervalInMilliSec());\n+    assertThat(bufferManagerV2.getCurrentThreadPoolSize()).isEqualTo(4);\n+    running = false;\n+    t.join();\n+    Thread.sleep(4L * bufferManagerV2.getCpuMonitoringIntervalInMilliSec());\n+    assertThat(bufferManagerV2.getCurrentThreadPoolSize()).isLessThan(4);\n+  }\n+\n+  @Test\n+  public void testScheduledEviction() throws Exception {\n+    TestAbfsInputStream testAbfsInputStream = new TestAbfsInputStream();\n+    AbfsClient client = testAbfsInputStream.getMockAbfsClient();\n+    AbfsInputStream inputStream = testAbfsInputStream.getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+    Configuration configuration = getReabAheadV2Configuration();\n+    AbfsConfiguration abfsConfig = new AbfsConfiguration(configuration,\n+        getAccountName());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfig.getReadAheadBlockSize(), abfsConfig);\n+    ReadBufferManagerV2 bufferManagerV2 = ReadBufferManagerV2.getBufferManager();\n+    // Add a failed buffer to completed queue and set to no free buffers to read ahead.\n+    ReadBuffer buff = new ReadBuffer();\n+    buff.setStatus(ReadBufferStatus.READ_FAILED);\n+    buff.setStream(inputStream);\n+    bufferManagerV2.testMimicFullUseAndAddFailedBuffer(buff);\n+    bufferManagerV2.testMimicFullUseAndAddFailedBuffer(buff);\n+    assertThat(bufferManagerV2.getCompletedReadListSize()).isEqualTo(2);\n+    Thread.sleep(2L * bufferManagerV2.getMemoryMonitoringIntervalInMilliSec());\n+    assertThat(bufferManagerV2.getCompletedReadListSize()).isEqualTo(0);\n+  }\n+\n+  @Test\n+  public void testMemoryUpscaleNotAllowedIfMemoryAboveThreshold() throws Exception {\n+    TestAbfsInputStream testAbfsInputStream = new TestAbfsInputStream();\n+    AbfsClient client = testAbfsInputStream.getMockAbfsClient();\n+    AbfsInputStream inputStream = testAbfsInputStream.getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+    Configuration configuration = getReabAheadV2Configuration();\n+    AbfsConfiguration abfsConfig = new AbfsConfiguration(configuration,\n+        getAccountName());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfig.getReadAheadBlockSize(), abfsConfig);\n+    ReadBufferManagerV2 bufferManagerV2 = Mockito.spy(ReadBufferManagerV2.getBufferManager());\n+    Mockito.doReturn(0.6).when(bufferManagerV2).getMemoryLoad();\n+    // Add a failed buffer to completed queue and set to no free buffers to read ahead.\n+    ReadBuffer buff = new ReadBuffer();\n+    buff.setStatus(ReadBufferStatus.READ_FAILED);\n+    buff.setStream(inputStream);\n+    bufferManagerV2.testMimicFullUseAndAddFailedBuffer(buff);\n+    assertThat(bufferManagerV2.getNumBuffers()).isEqualTo(bufferManagerV2.getMinBufferPoolSize());\n+    bufferManagerV2.queueReadAhead(inputStream, 0, ONE_KB,\n+        inputStream.getTracingContext());\n+    assertThat(bufferManagerV2.getNumBuffers()).isEqualTo(bufferManagerV2.getMinBufferPoolSize());\n+  }\n+\n+  @Test\n+  public void testMemoryUpscaleIfMemoryBelowThreshold() throws Exception {\n+    TestAbfsInputStream testAbfsInputStream = new TestAbfsInputStream();\n+    AbfsClient client = testAbfsInputStream.getMockAbfsClient();\n+    AbfsInputStream inputStream = testAbfsInputStream.getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+    Configuration configuration = getReabAheadV2Configuration();\n+    AbfsConfiguration abfsConfig = new AbfsConfiguration(configuration,\n+        getAccountName());\n+    ReadBufferManagerV2.getBufferManager().testResetReadBufferManager();\n+    ReadBufferManagerV2.setReadBufferManagerConfigs(abfsConfig.getReadAheadBlockSize(), abfsConfig);\n+    ReadBufferManagerV2 bufferManagerV2 = Mockito.spy(ReadBufferManagerV2.getBufferManager());\n+    Mockito.doReturn(0.4).when(bufferManagerV2).getMemoryLoad();\n+    // Add a failed buffer to completed queue and set to no free buffers to read ahead.\n+    ReadBuffer buff = new ReadBuffer();\n+    buff.setStatus(ReadBufferStatus.READ_FAILED);\n+    buff.setStream(inputStream);\n+    bufferManagerV2.testMimicFullUseAndAddFailedBuffer(buff);\n+    assertThat(bufferManagerV2.getNumBuffers()).isEqualTo(bufferManagerV2.getMinBufferPoolSize());\n+    bufferManagerV2.queueReadAhead(inputStream, 0, ONE_KB,\n+        inputStream.getTracingContext());\n+    assertThat(bufferManagerV2.getNumBuffers()).isEqualTo(bufferManagerV2.getMinBufferPoolSize() + 1);\n+  }\n+\n+  @Test\n+  public void testMemoryDownscaleIfMemoryAboveThreshold() throws Exception {\n\nReview Comment:\n   Added\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T07:04:56.670+0000", "updated": "2025-10-30T07:04:56.670+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18034115", "id": "18034115", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3466650627\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 44s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/29/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 26s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 39s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 11s |  |  hadoop-tools/hadoop-azure: The patch generated 0 new + 5 unchanged - 9 fixed = 5 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/29/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 57 new + 1472 unchanged - 0 fixed = 1529 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/29/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 25 new + 1413 unchanged - 0 fixed = 1438 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/29/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 13 new + 168 unchanged - 10 fixed = 181 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 15s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 14s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 23s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  59m 35s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:[line 91] |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:[line 172] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:[line 196] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:[line 180] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:[line 188] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:[line 99] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:[line 107] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:[line 141] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.getBufferManager() may expose internal representation by returning ReadBufferManagerV1.bufferManager  At ReadBufferManagerV1.java:internal representation by returning ReadBufferManagerV1.bufferManager  At ReadBufferManagerV1.java:[line 81] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getBufferManager() may expose internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:[line 131] |\r\n   |  |  Public static org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance() may expose internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:internal representation by returning ReadBufferManagerV2.bufferManager  At ReadBufferManagerV2.java:[line 1026] |\r\n   |  |  Unknown bug pattern SING_SINGLETON_GETTER_NOT_SYNCHRONIZED in org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance()  At ReadBufferManagerV2.java:org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.getInstance()  At ReadBufferManagerV2.java:[line 1026] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/29/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ecab2c043952 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1667de1df7968c09d5097c63dae3904ef185a69e |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/29/testReport/ |\r\n   | Max. process+thread count | 615 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/29/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T08:32:26.088+0000", "updated": "2025-10-30T08:32:26.088+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18034156", "id": "18034156", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3467547352\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 202, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 214\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 202, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 877, Failures: 0, Errors: 0, Skipped: 166\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 202, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 716, Failures: 0, Errors: 0, Skipped: 279\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 202, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 874, Failures: 0, Errors: 0, Skipped: 225\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 202, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 723, Failures: 0, Errors: 0, Skipped: 137\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 202, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 713, Failures: 0, Errors: 0, Skipped: 281\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 202, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 720, Failures: 0, Errors: 0, Skipped: 149\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 202, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 715, Failures: 0, Errors: 0, Skipped: 195\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 202, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 748, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 202, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 713, Failures: 0, Errors: 0, Skipped: 278\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T11:33:04.861+0000", "updated": "2025-10-30T11:33:04.861+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18034163", "id": "18034163", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3467707768\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/30/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 23s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/30/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 5 unchanged - 9 fixed = 6 total (was 14)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/30/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 54 new + 1464 unchanged - 0 fixed = 1518 total (was 1464)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/30/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 22 new + 1390 unchanged - 0 fixed = 1412 total (was 1390)  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/30/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 9 new + 168 unchanged - 10 fixed = 177 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 17s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  59m 45s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setOffset(long)  At ReadBuffer.java:[line 91] |\r\n   |  |  Unknown bug pattern AT_NONATOMIC_64BIT_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setTimeStamp(long)  At ReadBuffer.java:[line 172] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setAnyByteConsumed(boolean)  At ReadBuffer.java:[line 196] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setBufferindex(int)  At ReadBuffer.java:[line 123] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setFirstByteConsumed(boolean)  At ReadBuffer.java:[line 180] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLastByteConsumed(boolean)  At ReadBuffer.java:[line 188] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setLength(int)  At ReadBuffer.java:[line 99] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setRequestedLength(int)  At ReadBuffer.java:[line 107] |\r\n   |  |  Unknown bug pattern AT_STALE_THREAD_WRITE_OF_PRIMITIVE in org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:org.apache.hadoop.fs.azurebfs.services.ReadBuffer.setStatus(ReadBufferStatus)  At ReadBuffer.java:[line 141] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/30/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7832 |\r\n   | JIRA Issue | HADOOP-19622 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c80b7679a995 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c7f986e38ea5338b02c63b80e37e98de86559047 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/30/testReport/ |\r\n   | Max. process+thread count | 636 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7832/30/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T12:15:33.368+0000", "updated": "2025-10-30T12:15:33.368+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18034327", "id": "18034327", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832#issuecomment-3471543143\n\n   Thanks everyone for reviewing this.\r\n   \r\n   Spotbugs and Javadocs warnings are due to https://issues.apache.org/jira/browse/HADOOP-19731\r\n   Rest all looks good. Going ahead with the merge\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T06:50:10.257+0000", "updated": "2025-10-31T06:50:10.257+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13623416/comment/18034328", "id": "18034328", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7832:\nURL: https://github.com/apache/hadoop/pull/7832\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-31T06:50:28.277+0000", "updated": "2025-10-31T06:50:28.277+0000"}], "maxResults": 76, "total": 76, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/3", "description": "This issue is being actively worked on at the moment by the assignee.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/inprogress.png", "name": "In Progress", "id": "3", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/4", "id": 4, "key": "indeterminate", "colorName": "yellow", "name": "In Progress"}}}}