{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13616976", "self": "https://issues.apache.org/jira/rest/api/2/issue/13616976", "key": "HADOOP-19559", "fields": {"summary": "S3A: Analytics accelerator for S3 to be enabled by default", "description": "Make \"analytics\" the default input stream in S3A.\u00a0\r\n\r\nGoals\r\n* Parquet performance through applications running queries over the data (spark etc)\r\n* Performance for other formats good as/better than today. Examples: avro manifests in iceberg, ORC in hive/spark\r\n* Performance for other uses as good as today (whole-file/sequential reads of parquet data in distcp etc)\r\n* better resilience to bad uses (incomplete reads not retaining http streams, buffer allocations on long-retained data)\r\n* efficient on applications like Impala, which caches parquet footers itself, and uses unbuffer() to discard all stream-side resources. Maybe just throw alway all state on unbuffer() and stop trying to be sophisticated, or support some new openFile flag which can be used to disable footer parsing\r\n", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=ahmar", "name": "ahmar", "key": "JIRAUSER283484", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Ahmar Suhail", "active": true, "timeZone": "Etc/UTC"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17987375", "id": "17987375", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "ahmarsuhail opened a new pull request, #7776:\nURL: https://github.com/apache/hadoop/pull/7776\n\n   <!--\r\n     Thanks for sending a pull request!\r\n       1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n       2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n   -->\r\n   \r\n   ### Description of PR\r\n   \r\n   Draft PR to make turn AAL default ON. \r\n   \r\n   Prerequisite PR's: \r\n   \r\n   * readVectored(): https://github.com/apache/hadoop/pull/7720\r\n   * auditing: https://github.com/apache/hadoop/pull/7723\r\n   * SSE-C: https://github.com/apache/hadoop/pull/7738\r\n   * IoStats: https://github.com/apache/hadoop/pull/7763 [WiP] \r\n   \r\n   \r\n   ### How was this patch tested?\r\n   \r\n   \r\n   ### For code changes:\r\n   \r\n   - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n   - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n   - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n   - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-02T08:46:42.026+0000", "updated": "2025-07-02T08:46:42.026+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17987858", "id": "17987858", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7776:\nURL: https://github.com/apache/hadoop/pull/7776#issuecomment-3027248675\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 5 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  2s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 11s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-mvninstall-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 13s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 13s | [/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-aws in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 11s | [/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-compile-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-aws in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 12s | [/results-checkstyle-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-aws.txt) |  hadoop-tools/hadoop-aws: The patch generated 18 new + 7 unchanged - 0 fixed = 25 total (was 7)  |\r\n   | -1 :x: |  mvnsite  |   0m 15s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-mvnsite-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 12s | [/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-tools_hadoop-aws-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 generated 9 new + 0 unchanged - 0 fixed = 9 total (was 0)  |\r\n   | -1 :x: |  spotbugs  |   0m 12s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 27s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 15s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  75m 38s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7776 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2e0d17a78cf4 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 198da21540a93c15c514e3fc013ebb843caf56c9 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/testReport/ |\r\n   | Max. process+thread count | 557 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-02T10:03:23.820+0000", "updated": "2025-07-02T10:03:23.820+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17987875", "id": "17987875", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7776:\nURL: https://github.com/apache/hadoop/pull/7776#issuecomment-3027514574\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 23s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 6 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 44s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  20m 55s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 40s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 42s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 59s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  5s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m  1s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 32s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 24s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   9m  2s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   9m  2s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 55s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/2/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 18 new + 7 unchanged - 0 fixed = 25 total (was 7)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m  1s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 59s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 28s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 28s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 26s |  |  hadoop-project in the patch passed.  |\r\n   | -1 :x: |  unit  |   2m 58s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/2/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 126m  1s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | Failed junit tests | hadoop.fs.s3a.impl.streams.TestStreamFactories |\r\n   |   | hadoop.fs.s3a.TestS3AUnbuffer |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7776 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux d304096c3ebc 5.15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 051f6c4c35bf399b5a66a78763e60f22b2091bad |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/2/testReport/ |\r\n   | Max. process+thread count | 547 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7776/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-02T11:28:06.630+0000", "updated": "2025-07-02T11:28:06.630+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17993072", "id": "17993072", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "fuatbasik commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2182634430\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/statistics/ITestS3AFileSystemStatistic.java:\n##########\n@@ -44,10 +44,6 @@ public class ITestS3AFileSystemStatistic extends AbstractS3ATestBase {\n    */\n   @Test\n   public void testBytesReadWithStream() throws IOException {\n-    // Analytics accelerator currently does not support IOStatistics, this will be added as\n\nReview Comment:\n   similar to previous class, can you please confirm all these tests are passing now?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T12:15:18.135+0000", "updated": "2025-07-03T12:15:18.135+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17993073", "id": "17993073", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "fuatbasik commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2182638702\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestS3AOpenCost.java:\n##########\n@@ -261,10 +263,13 @@ public void testOpenFileLongerLengthReadFully() throws Throwable {\n       }\n     },\n         always(),\n-        // two GET calls were made, one for readFully,\n-        // the second on the read() past the EOF\n-        // the operation has got as far as S3\n-        probe(!prefetching(), STREAM_READ_OPENED, 1 + 1));\n+        // Analytics stream: 1 open (persistent connection)\n+        // S3AInputStream: 2 opens (reopen on EOF)\n+            // two GET calls were made, one for readFully,\n+            // the second on the read() past the EOF\n+            // the operation has got as far as S3\n+        probe(!prefetching() && !isAnalyticsStream(), STREAM_READ_OPENED, 2),\n\nReview Comment:\n   nit: i think 1+1 instead of 2 as the 3rd parameter was intentional. We might want to revert back to 1+1 this line. \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T12:17:29.273+0000", "updated": "2025-07-03T12:17:29.273+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17993075", "id": "17993075", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "fuatbasik commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2182645332\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +223,96 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  /**\n+   *\n+   * TXT files are classified as SEQUENTIAL format and use SequentialPrefetcher(requests the entire 10MB file)\n+   * RangeOptimiser splits ranges larger than maxRangeSizeBytes (8MB) using partSizeBytes (8MB)\n+   * The 10MB range gets split into: [0-8MB) and [8MB-10MB)\n+   * Each split range becomes a separate Block, resulting in 2 GET requests:\n+   */\n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = path(\"large-test-file.txt\");\n+    byte[] data = dataset(10 * S_1M, 256, 255);\n+    writeDataset(getFileSystem(), dest, data, 10 * S_1M, 1024, true);\n+\n+    byte[] buffer = new byte[S_1M * 10];\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(buffer);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 2);\n\nReview Comment:\n   Well, the number of requests here would highly depend on how AAL implements request splitting. If we change the default from 8MB to 16Mb tomorrow, these tests will start failing. I guess this is OK for now but we might want to explicitly set AAL request size configuration to ensure underlying changes does not cause test failures here. \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T12:20:50.518+0000", "updated": "2025-07-03T12:20:50.518+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17993076", "id": "17993076", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "fuatbasik commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2182648632\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +223,96 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  /**\n+   *\n+   * TXT files are classified as SEQUENTIAL format and use SequentialPrefetcher(requests the entire 10MB file)\n+   * RangeOptimiser splits ranges larger than maxRangeSizeBytes (8MB) using partSizeBytes (8MB)\n+   * The 10MB range gets split into: [0-8MB) and [8MB-10MB)\n+   * Each split range becomes a separate Block, resulting in 2 GET requests:\n+   */\n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = path(\"large-test-file.txt\");\n+    byte[] data = dataset(10 * S_1M, 256, 255);\n+    writeDataset(getFileSystem(), dest, data, 10 * S_1M, 1024, true);\n+\n+    byte[] buffer = new byte[S_1M * 10];\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(buffer);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 2);\n+      // Because S3A passes in the meta-data(content length) on file open, we expect AAL to make no HEAD requests\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_HEAD_REQUESTS, 0);\n+    }\n+  }\n+\n+  @Test\n+  public void testSmallFileSingleGet() throws Throwable {\n+    describe(\"Small file should trigger only one GET request\");\n+\n+    Path dest = path(\"small-test-file.txt\");\n+    byte[] data = dataset(S_1M, 256, 255);\n+    writeDataset(getFileSystem(), dest, data, S_1M, 1024, true);\n+\n+    byte[] buffer = new byte[S_1M];\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(buffer);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+      // Because S3A passes in the meta-data(content length) on file open, we expect AAL to make no HEAD requests\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_HEAD_REQUESTS, 0);\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testRandomSeekPatternGets() throws Throwable {\n+    describe(\"Random seek pattern should optimize GET requests\");\n+\n+    Path dest = path(\"seek-test.txt\");\n+    byte[] data = dataset(5 * S_1M, 256, 255);\n+    writeDataset(getFileSystem(), dest, data, 5 * S_1M, 1024, true);\n+\n+    byte[] buffer = new byte[S_1M];\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+\n+      inputStream.read(buffer);\n+      inputStream.seek(2 * S_1M);\n+      inputStream.read(new byte[512 * S_1K]);\n+      inputStream.seek(3 * S_1M);\n+      inputStream.read(new byte[512 * S_1K]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_HEAD_REQUESTS, 0);\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testSequentialStreamsNoDuplicateGets() throws Throwable {\n\nReview Comment:\n   we probably want another test where we open a stream and close it, without reading from. If thats a Parquet file or a small object there will be some reads on AAL. \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T12:22:29.766+0000", "updated": "2025-07-03T12:22:29.766+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17993078", "id": "17993078", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "fuatbasik commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2182650460\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsStream.java:\n##########\n@@ -48,13 +49,17 @@ public class AnalyticsStream extends ObjectInputStream implements StreamCapabili\n   private S3SeekableInputStream inputStream;\n   private long lastReadCurrentPos = 0;\n   private volatile boolean closed;\n+  private final long contentLength;\n+  private final long lengthLimit;\n \n   public static final Logger LOG = LoggerFactory.getLogger(AnalyticsStream.class);\n \n   public AnalyticsStream(final ObjectReadParameters parameters,\n       final S3SeekableInputStreamFactory s3SeekableInputStreamFactory) throws IOException {\n     super(InputStreamType.Analytics, parameters);\n     S3ObjectAttributes s3Attributes = parameters.getObjectAttributes();\n+    this.contentLength = s3Attributes.getLen();\n\nReview Comment:\n   +1\r\n   \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T12:23:30.838+0000", "updated": "2025-07-03T12:23:30.838+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17993079", "id": "17993079", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "vaibhav5140 commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2180287554\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -67,7 +70,7 @@ public class ITestS3AAnalyticsAcceleratorStreamReading extends AbstractS3ATestBa\n \n   private Path externalTestFile;\n \n-  @Before\n+  @BeforeEach\n\nReview Comment:\n   Addressed\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -96,14 +99,20 @@ public void testConnectorFrameWorkIntegration() throws Throwable {\n             .build().get()) {\n       ioStats = inputStream.getIOStatistics();\n       inputStream.seek(5);\n-      inputStream.read(buffer, 0, 500);\n+      int bytesRead = inputStream.read(buffer, 0, 500);\n \n       final InputStream wrappedStream = inputStream.getWrappedStream();\n       ObjectInputStream objectInputStream = (ObjectInputStream) wrappedStream;\n \n       Assertions.assertThat(objectInputStream.streamType()).isEqualTo(InputStreamType.Analytics);\n       Assertions.assertThat(objectInputStream.getInputPolicy())\n           .isEqualTo(S3AInputPolicy.Sequential);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_BYTES, bytesRead);\n\nReview Comment:\n   Addressed in new revision\n\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/statistics/StreamStatisticNames.java:\n##########\n@@ -132,6 +132,12 @@ public final class StreamStatisticNames {\n   public static final String STREAM_READ_OPERATIONS =\n       \"stream_read_operations\";\n \n+  /** Analytics GET requests made by stream. */\n+  public static final String STREAM_READ_ANALYTICS_GET_REQUESTS = \"stream_read_analytics_get_requests\";\n\nReview Comment:\n   Actually, new analytics specific statistics provide isolated tracking. In case if both S3A and Analytics streams are used simultaneously, separate metrics provide precise tracking\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsRequestCallback.java:\n##########\n@@ -0,0 +1,30 @@\n+package org.apache.hadoop.fs.s3a.impl.streams;\n\nReview Comment:\n   License error is fixed but javadoc error is because AnalyticsRequestCallback interface is currently not available in the released version of AAL\r\n   \r\n   \n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +215,127 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = writeThenReadFile(\"large-test-file.txt\", 10 * 1024 * 1024); // 10MB\n+\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 2);\n+    }\n+  }\n+\n+  @Test\n+  public void testSmallFileSingleGet() throws Throwable {\n+    describe(\"Small file should trigger only one GET request\");\n+\n+    Path dest = writeThenReadFile(\"small-test-file.txt\", 1 * 1024 * 1024); // 1KB\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testRandomSeekPatternGets() throws Throwable {\n+    describe(\"Random seek pattern should optimize GET requests\");\n+\n+    Path dest = writeThenReadFile(\"seek-test.txt\", 100 * 1024);\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+\n+      inputStream.seek(1000);\n+      inputStream.read(new byte[100]);\n+\n+      inputStream.seek(50000);\n+      inputStream.read(new byte[100]);\n+\n+      inputStream.seek(90000);\n+      inputStream.read(new byte[100]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+    }\n+}\n+\n+  @Test\n+  public void testAALNeverMakesHeadRequests() throws Throwable {\n+    describe(\"Prove AAL never makes HEAD requests - S3A provides all metadata\");\n+\n+    Path dest = writeThenReadFile(\"no-head-test.txt\", 1024 * 1024); // 1MB\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.read(new byte[1024]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_HEAD_REQUESTS, 0);\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_OPENED, 1);\n+\n+      ObjectInputStream objectInputStream = (ObjectInputStream) inputStream.getWrappedStream();\n+      Assertions.assertThat(objectInputStream.streamType()).isEqualTo(InputStreamType.Analytics);\n+\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testParquetReadingNoHeadRequests() throws Throwable {\n\nReview Comment:\n   Addressed in new revision\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/statistics/ITestS3AContractStreamIOStatistics.java:\n##########\n@@ -78,13 +77,4 @@ public List<String> outputStreamStatisticKeys() {\n         STREAM_WRITE_BLOCK_UPLOADS,\n         STREAM_WRITE_EXCEPTIONS);\n   }\n-\n-  @Override\n\nReview Comment:\n   Yes, they are passing now\r\n   \n\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsStream.java:\n##########\n@@ -48,13 +49,17 @@ public class AnalyticsStream extends ObjectInputStream implements StreamCapabili\n   private S3SeekableInputStream inputStream;\n   private long lastReadCurrentPos = 0;\n   private volatile boolean closed;\n+  private final long contentLength;\n+  private final long lengthLimit;\n \n   public static final Logger LOG = LoggerFactory.getLogger(AnalyticsStream.class);\n \n   public AnalyticsStream(final ObjectReadParameters parameters,\n       final S3SeekableInputStreamFactory s3SeekableInputStreamFactory) throws IOException {\n     super(InputStreamType.Analytics, parameters);\n     S3ObjectAttributes s3Attributes = parameters.getObjectAttributes();\n+    this.contentLength = s3Attributes.getLen();\n\nReview Comment:\n   getLen() is needed for length limiting, it ensures AnalyticsStream respects the declared file length from openFile() options rather than reading the entire S3 object\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestS3AOpenCost.java:\n##########\n@@ -177,6 +174,11 @@ public void testStreamIsNotChecksummed() throws Throwable {\n \n     // if prefetching is enabled, skip this test\n     assumeNoPrefetching();\n+    // Skip for Analytics streams - checksum validation only exists in S3AInputStream.\n+    // AnalyticsStream handles data integrity through AWS Analytics Accelerator internally.\n+    if (isAnalyticsStream()) {\n+      skip(\"Analytics stream doesn't use checksums\");\n\nReview Comment:\n   Skip because S3AInputStream uses raw http with optional checksum validation that can be tested, but analytics stream uses AAL which has built-in data integrity and has no exposed checksum controls which can be verified\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +215,127 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = writeThenReadFile(\"large-test-file.txt\", 10 * 1024 * 1024); // 10MB\n+\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 2);\n+    }\n+  }\n+\n+  @Test\n+  public void testSmallFileSingleGet() throws Throwable {\n+    describe(\"Small file should trigger only one GET request\");\n+\n+    Path dest = writeThenReadFile(\"small-test-file.txt\", 1 * 1024 * 1024); // 1KB\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testRandomSeekPatternGets() throws Throwable {\n+    describe(\"Random seek pattern should optimize GET requests\");\n+\n+    Path dest = writeThenReadFile(\"seek-test.txt\", 100 * 1024);\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+\n+      inputStream.seek(1000);\n+      inputStream.read(new byte[100]);\n+\n+      inputStream.seek(50000);\n+      inputStream.read(new byte[100]);\n+\n+      inputStream.seek(90000);\n+      inputStream.read(new byte[100]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+    }\n+}\n+\n+  @Test\n+  public void testAALNeverMakesHeadRequests() throws Throwable {\n\nReview Comment:\n   Addressed in new revision\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -129,11 +140,17 @@ public void testMalformedParquetFooter() throws IOException {\n \n     byte[] buffer = new byte[500];\n     IOStatistics ioStats;\n+    int bytesRead;\n \n     try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n       ioStats = inputStream.getIOStatistics();\n       inputStream.seek(5);\n-      inputStream.read(buffer, 0, 500);\n+      bytesRead = inputStream.read(buffer, 0, 500);\n\nReview Comment:\n   Addressed in new revision\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +215,127 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = writeThenReadFile(\"large-test-file.txt\", 10 * 1024 * 1024); // 10MB\n+\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 2);\n\nReview Comment:\n   Addressed in new revision\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +215,127 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = writeThenReadFile(\"large-test-file.txt\", 10 * 1024 * 1024); // 10MB\n+\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 2);\n+    }\n+  }\n+\n+  @Test\n+  public void testSmallFileSingleGet() throws Throwable {\n+    describe(\"Small file should trigger only one GET request\");\n+\n+    Path dest = writeThenReadFile(\"small-test-file.txt\", 1 * 1024 * 1024); // 1KB\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testRandomSeekPatternGets() throws Throwable {\n+    describe(\"Random seek pattern should optimize GET requests\");\n+\n+    Path dest = writeThenReadFile(\"seek-test.txt\", 100 * 1024);\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+\n+      inputStream.seek(1000);\n+      inputStream.read(new byte[100]);\n\nReview Comment:\n   Addressed in new revision\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +215,127 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = writeThenReadFile(\"large-test-file.txt\", 10 * 1024 * 1024); // 10MB\n\nReview Comment:\n   Addressed in new revision\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -166,15 +183,19 @@ public void testMultiRowGroupParquet() throws Throwable {\n     }\n \n     verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_OPENED, 1);\n-\n+    verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n     try (FSDataInputStream inputStream = getFileSystem().openFile(dest)\n         .must(FS_OPTION_OPENFILE_READ_POLICY, FS_OPTION_OPENFILE_READ_POLICY_PARQUET)\n         .build().get()) {\n       ioStats = inputStream.getIOStatistics();\n       inputStream.readFully(buffer, 0, (int) fileStatus.getLen());\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_BYTES, (int) fileStatus.getLen());\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_OPERATIONS, 1);\n     }\n \n     verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_OPENED, 1);\n+    verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_HEAD_REQUESTS, 0);\n\nReview Comment:\n   Addressed in new revision\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +215,127 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = writeThenReadFile(\"large-test-file.txt\", 10 * 1024 * 1024); // 10MB\n+\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n\nReview Comment:\n   Addressed in new revision\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsRequestCallback.java:\n##########\n@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl.streams;\n+\n+import org.apache.hadoop.fs.s3a.statistics.S3AInputStreamStatistics;\n+import org.apache.hadoop.fs.statistics.DurationTracker;\n+import software.amazon.s3.analyticsaccelerator.util.RequestCallback;\n+\n+/**\n+ * Implementation of AAL's RequestCallback interface that tracks analytics operations.\n+ */\n+public class AnalyticsRequestCallback implements RequestCallback {\n+    private final S3AInputStreamStatistics statistics;\n+\n+    /**\n+     * Create a new callback instance.\n+     * @param statistics the statistics to update\n+     */\n+    public AnalyticsRequestCallback(S3AInputStreamStatistics statistics) {\n+        this.statistics = statistics;\n+    }\n+\n+    @Override\n+    public void onGetRequest() {\n+        statistics.incrementAnalyticsGetRequests();\n+        // Update ACTION_HTTP_GET_REQUEST statistic\n+        DurationTracker tracker = statistics.initiateGetRequest();\n\nReview Comment:\n   way to increment the ACTION_HTTP_GET_REQUEST statistic. The statistics.initiateGetRequest() call increments the counter, and tracker.close() completes the measurement\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +215,127 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = writeThenReadFile(\"large-test-file.txt\", 10 * 1024 * 1024); // 10MB\n+\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 2);\n+    }\n+  }\n+\n+  @Test\n+  public void testSmallFileSingleGet() throws Throwable {\n+    describe(\"Small file should trigger only one GET request\");\n+\n+    Path dest = writeThenReadFile(\"small-test-file.txt\", 1 * 1024 * 1024); // 1KB\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testRandomSeekPatternGets() throws Throwable {\n+    describe(\"Random seek pattern should optimize GET requests\");\n+\n+    Path dest = writeThenReadFile(\"seek-test.txt\", 100 * 1024);\n\nReview Comment:\n   Addressed in new revision\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +215,127 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = writeThenReadFile(\"large-test-file.txt\", 10 * 1024 * 1024); // 10MB\n+\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 2);\n+    }\n+  }\n+\n+  @Test\n+  public void testSmallFileSingleGet() throws Throwable {\n+    describe(\"Small file should trigger only one GET request\");\n+\n+    Path dest = writeThenReadFile(\"small-test-file.txt\", 1 * 1024 * 1024); // 1KB\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(new byte[(int) getFileSystem().getFileStatus(dest).getLen()]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testRandomSeekPatternGets() throws Throwable {\n+    describe(\"Random seek pattern should optimize GET requests\");\n+\n+    Path dest = writeThenReadFile(\"seek-test.txt\", 100 * 1024);\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+\n+      inputStream.seek(1000);\n+      inputStream.read(new byte[100]);\n+\n+      inputStream.seek(50000);\n+      inputStream.read(new byte[100]);\n+\n+      inputStream.seek(90000);\n+      inputStream.read(new byte[100]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+    }\n+}\n+\n+  @Test\n+  public void testAALNeverMakesHeadRequests() throws Throwable {\n+    describe(\"Prove AAL never makes HEAD requests - S3A provides all metadata\");\n+\n+    Path dest = writeThenReadFile(\"no-head-test.txt\", 1024 * 1024); // 1MB\n+\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.read(new byte[1024]);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_HEAD_REQUESTS, 0);\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_OPENED, 1);\n+\n+      ObjectInputStream objectInputStream = (ObjectInputStream) inputStream.getWrappedStream();\n+      Assertions.assertThat(objectInputStream.streamType()).isEqualTo(InputStreamType.Analytics);\n+\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testParquetReadingNoHeadRequests() throws Throwable {\n+    describe(\"Parquet-optimized reading should not trigger AAL HEAD requests\");\n+\n+    Path dest = path(\"parquet-head-test.parquet\");\n+    File file = new File(\"src/test/resources/multi_row_group.parquet\");\n+    Path sourcePath = new Path(file.toURI().getPath());\n+    getFileSystem().copyFromLocalFile(false, true, sourcePath, dest);\n+\n+    try (FSDataInputStream stream = getFileSystem().openFile(dest)\n+            .must(FS_OPTION_OPENFILE_READ_POLICY, FS_OPTION_OPENFILE_READ_POLICY_PARQUET)\n+            .build().get()) {\n+\n+      FileStatus fileStatus = getFileSystem().getFileStatus(dest);\n+      stream.readFully(new byte[(int) fileStatus.getLen()]);\n+\n+      IOStatistics stats = stream.getIOStatistics();\n+\n+      verifyStatisticCounterValue(stats, STREAM_READ_ANALYTICS_HEAD_REQUESTS, 0);\n+      verifyStatisticCounterValue(stats, STREAM_READ_ANALYTICS_OPENED, 1);\n+\n+      verifyStatisticCounterValue(stats, STREAM_READ_ANALYTICS_GET_REQUESTS, 1);\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testConcurrentStreamsNoDuplicateGets() throws Throwable {\n+    describe(\"Concurrent streams reading same object should not duplicate GETs\");\n+\n+    Path dest = writeThenReadFile(\"concurrent-test.txt\", 1 * 1024 * 1024);\n+\n+    try (FSDataInputStream stream1 = getFileSystem().open(dest);\n\nReview Comment:\n   Addressed in new revision\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T12:27:02.308+0000", "updated": "2025-07-03T12:27:02.308+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17993080", "id": "17993080", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "fuatbasik commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2182658893\n\n\n##########\nhadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/statistics/StreamStatisticNames.java:\n##########\n@@ -132,6 +132,12 @@ public final class StreamStatisticNames {\n   public static final String STREAM_READ_OPERATIONS =\n       \"stream_read_operations\";\n \n+  /** Analytics GET requests made by stream. */\n+  public static final String STREAM_READ_ANALYTICS_GET_REQUESTS = \"stream_read_analytics_get_requests\";\n\nReview Comment:\n   why do we need isolated tracking? Can we ever use mix and match streams?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T12:27:51.243+0000", "updated": "2025-07-03T12:27:51.243+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/17993081", "id": "17993081", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "fuatbasik commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2182650460\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsStream.java:\n##########\n@@ -48,13 +49,17 @@ public class AnalyticsStream extends ObjectInputStream implements StreamCapabili\n   private S3SeekableInputStream inputStream;\n   private long lastReadCurrentPos = 0;\n   private volatile boolean closed;\n+  private final long contentLength;\n+  private final long lengthLimit;\n \n   public static final Logger LOG = LoggerFactory.getLogger(AnalyticsStream.class);\n \n   public AnalyticsStream(final ObjectReadParameters parameters,\n       final S3SeekableInputStreamFactory s3SeekableInputStreamFactory) throws IOException {\n     super(InputStreamType.Analytics, parameters);\n     S3ObjectAttributes s3Attributes = parameters.getObjectAttributes();\n+    this.contentLength = s3Attributes.getLen();\n\nReview Comment:\n   +1\r\n   \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-03T12:29:41.426+0000", "updated": "2025-07-03T12:29:41.426+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/18003834", "id": "18003834", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#issuecomment-3049428576\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 8 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   5m 41s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  19m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 30s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 59s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 32s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 32s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 24s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 58s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   7m 58s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  1s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   8m  1s |  |  the patch passed  |\r\n   | -1 :x: |  blanks  |   0m  0s | [/blanks-eol.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7763/7/artifact/out/blanks-eol.txt) |  The patch has 1 line(s) that end in blanks. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply  |\r\n   | -0 :warning: |  checkstyle  |   2m 26s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7763/7/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 27 new + 0 unchanged - 0 fixed = 27 total (was 0)  |\r\n   | +1 :green_heart: |  mvnsite  |   1m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 16s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 24s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  18m 58s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 55s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 44s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 149m 10s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7763/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7763 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux 1085cbb4805c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 048d8aa5692bc9624a8777a1c7500f86c56b9eff |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7763/7/testReport/ |\r\n   | Max. process+thread count | 3151 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7763/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-08T15:35:06.394+0000", "updated": "2025-07-08T15:35:06.394+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/18003871", "id": "18003871", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#issuecomment-3049994314\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 8 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   6m  1s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  19m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 58s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 49s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 32s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 32s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 57s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 24s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m  4s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  5s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   1m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   2m  1s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   1m 41s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   1m 36s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 26s |  |  hadoop-project has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 12s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 25s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  18m 46s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 57s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 42s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 149m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7763/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7763 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n   | uname | Linux 8121b4c5477e 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 08a5f4261cdd33e27997d4702f517237d4158c3e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7763/8/testReport/ |\r\n   | Max. process+thread count | 3151 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7763/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-08T18:58:40.794+0000", "updated": "2025-07-08T18:58:40.794+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/18004148", "id": "18004148", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "fuatbasik commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2195000463\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsRequestCallback.java:\n##########\n@@ -0,0 +1,30 @@\n+package org.apache.hadoop.fs.s3a.impl.streams;\n\nReview Comment:\n   we probably can now resolve this. \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T13:11:40.572+0000", "updated": "2025-07-09T13:11:40.572+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/18004157", "id": "18004157", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "fuatbasik commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2195042590\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Statistic.java:\n##########\n@@ -459,6 +459,14 @@ public enum Statistic {\n       \"Gauge of active memory in use\",\n       TYPE_GAUGE),\n \n+  ANALYTICS_GET_REQUESTS(\n+      StreamStatisticNames.STREAM_READ_ANALYTICS_GET_REQUESTS,\n+      \"GET requests made by analytics streams\",\n\nReview Comment:\n   This is nit but i believe an important one. We have to make sure we are giving right name, description as well as doing it in the right place.\r\n   1/  why are these are here at these lines? This is neither lexicographic nor following a logical separation. We probably need to move these up to Object IO section. \r\n   \r\n   2/ Next is the name for Stream read we had `STREAM_READ_OPENED` and we added `STREAM_READ_ANALYTICS_OPENED` so following the same schema we probably need to re-name this to `OBJECT_GET_ANALYTICS_REQUESTS` (there is no GET equivalent currently because stream read opens = gets. Likewise for HEAD, we have `OBJECT_METADATA_REQUESTS`, which we can add `OBJECT_METADATA_ANALYTICS_REQUESTS`. \r\n   \r\n   3/ we should make sure the messages are similar to others. For example, we should call this `Count of object get requests made by analytics stream`. and for head `Count of requests for object metadata made by analytics stream`\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsStream.java:\n##########\n@@ -63,13 +68,23 @@ public AnalyticsStream(final ObjectReadParameters parameters,\n   @Override\n   public int read() throws IOException {\n     throwIfClosed();\n+    if (getPos() >= lengthLimit) {\n+      return -1; // EOF reached due to length limit\n\nReview Comment:\n   do we need to close underlying stream here before returning? We might check this from other Stream implementations. \n\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsRequestCallback.java:\n##########\n@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl.streams;\n+\n+import org.apache.hadoop.fs.s3a.statistics.S3AInputStreamStatistics;\n+import org.apache.hadoop.fs.statistics.DurationTracker;\n+import software.amazon.s3.analyticsaccelerator.util.RequestCallback;\n+\n+/**\n+ * Implementation of AAL's RequestCallback interface that tracks analytics operations.\n+ */\n+public class AnalyticsRequestCallback implements RequestCallback {\n+  private final S3AInputStreamStatistics statistics;\n+\n+    /**\n+     * Create a new callback instance.\n+     * @param statistics the statistics to update\n+     */\n+  public AnalyticsRequestCallback(S3AInputStreamStatistics statistics) {\n+    this.statistics = statistics;\n+  }\n+\n+  @Override\n+    public void onGetRequest() {\n+    statistics.incrementAnalyticsGetRequests();\n+    // Update ACTION_HTTP_GET_REQUEST statistic\n+    DurationTracker tracker = statistics.initiateGetRequest();\n\nReview Comment:\n   this duration will be always very close to 0 right? Why do we need to start and close tracker?\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/ITestCommitOperationCost.java:\n##########\n@@ -174,10 +173,7 @@ private void abortActiveStream() throws IOException {\n   public void testCostOfCreatingMagicFile() throws Throwable {\n     describe(\"Files created under magic paths skip existence checks and marker deletes\");\n \n-    // Analytics accelerator currently does not support IOStatistics, this will be added as\n-    // part of https://issues.apache.org/jira/browse/HADOOP-19364\n-    skipIfAnalyticsAcceleratorEnabled(getConfiguration(),\n-        \"Analytics Accelerator currently does not support stream statistics\");\n+\n\nReview Comment:\n   nit: extra whitespace\n\n\n\n##########\nhadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/streams/AnalyticsRequestCallback.java:\n##########\n@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl.streams;\n+\n+import org.apache.hadoop.fs.s3a.statistics.S3AInputStreamStatistics;\n+import org.apache.hadoop.fs.statistics.DurationTracker;\n+import software.amazon.s3.analyticsaccelerator.util.RequestCallback;\n+\n+/**\n+ * Implementation of AAL's RequestCallback interface that tracks analytics operations.\n+ */\n+public class AnalyticsRequestCallback implements RequestCallback {\n+  private final S3AInputStreamStatistics statistics;\n+\n+    /**\n+     * Create a new callback instance.\n+     * @param statistics the statistics to update\n+     */\n+  public AnalyticsRequestCallback(S3AInputStreamStatistics statistics) {\n+    this.statistics = statistics;\n+  }\n+\n+  @Override\n+    public void onGetRequest() {\n+    statistics.incrementAnalyticsGetRequests();\n+    // Update ACTION_HTTP_GET_REQUEST statistic\n+    DurationTracker tracker = statistics.initiateGetRequest();\n+    tracker.close();\n+  }\n+\n+  @Override\n+    public void onHeadRequest() {\n+    statistics.incrementAnalyticsHeadRequests();\n\nReview Comment:\n   now on GET you are updating both Analytics request counter and ACTION_HTTP_GET_REQUEST (with tracker). Why are you not updating OBJECT_METADATA_REQUESTS here?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-09T13:40:02.731+0000", "updated": "2025-07-09T13:40:02.731+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/18024115", "id": "18024115", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "steveloughran commented on code in PR #7763:\nURL: https://github.com/apache/hadoop/pull/7763#discussion_r2395468392\n\n\n##########\nhadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAnalyticsAcceleratorStreamReading.java:\n##########\n@@ -194,4 +223,96 @@ public void testInvalidConfigurationThrows() throws Exception {\n         () -> S3SeekableInputStreamConfiguration.fromConfiguration(connectorConfiguration));\n   }\n \n+  /**\n+   *\n+   * TXT files are classified as SEQUENTIAL format and use SequentialPrefetcher(requests the entire 10MB file)\n+   * RangeOptimiser splits ranges larger than maxRangeSizeBytes (8MB) using partSizeBytes (8MB)\n+   * The 10MB range gets split into: [0-8MB) and [8MB-10MB)\n+   * Each split range becomes a separate Block, resulting in 2 GET requests:\n+   */\n+  @Test\n+  public void testLargeFileMultipleGets() throws Throwable {\n+    describe(\"Large file should trigger multiple GET requests\");\n+\n+    Path dest = path(\"large-test-file.txt\");\n+    byte[] data = dataset(10 * S_1M, 256, 255);\n+    writeDataset(getFileSystem(), dest, data, 10 * S_1M, 1024, true);\n+\n+    byte[] buffer = new byte[S_1M * 10];\n+    try (FSDataInputStream inputStream = getFileSystem().open(dest)) {\n+      IOStatistics ioStats = inputStream.getIOStatistics();\n+      inputStream.readFully(buffer);\n+\n+      verifyStatisticCounterValue(ioStats, STREAM_READ_ANALYTICS_GET_REQUESTS, 2);\n\nReview Comment:\n   +1 on fixing the request size, and use removeBaseAndBucketOverrides() to clear that setting from the test bucket in case it's been set by a developer\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-01T18:28:29.280+0000", "updated": "2025-10-01T18:28:29.280+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/18029119", "id": "18029119", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "github-actions[bot] commented on PR #7776:\nURL: https://github.com/apache/hadoop/pull/7776#issuecomment-3392652505\n\n   We're closing this stale PR because it has been open for 100 days with no activity. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.\n   If you feel like this was a mistake, or you would like to continue working on it, please feel free to re-open it and ask for a committer to remove the stale tag and review again.\n   Thanks all for your contribution.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-11T00:20:50.817+0000", "updated": "2025-10-11T00:20:50.817+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13616976/comment/18029245", "id": "18029245", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "github-actions[bot] closed pull request #7776: HADOOP-19559. Make AAL the default input stream. [DRAFT]\nURL: https://github.com/apache/hadoop/pull/7776\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-12T00:22:54.190+0000", "updated": "2025-10-12T00:22:54.190+0000"}], "maxResults": 18, "total": 18, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/1", "description": "The issue is open and ready for the assignee to start work on it.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png", "name": "Open", "id": "1", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2", "id": 2, "key": "new", "colorName": "blue-gray", "name": "To Do"}}}}