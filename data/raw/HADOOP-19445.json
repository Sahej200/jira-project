{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13607268", "self": "https://issues.apache.org/jira/rest/api/2/issue/13607268", "key": "HADOOP-19445", "fields": {"summary": "ABFS: [FnsOverBlob][Tests] Add Tests For Negative Scenarios Identified for Rename Operation", "description": "We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint.\r\n\r\nAttached file shows the the scenarios identified for Rename File and Rename directory operations on blob Endpoint\r\n\r\nThis Jira tracks implementing these tests.", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anujmodi", "name": "anujmodi", "key": "JIRAUSER307456", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Anuj Modi", "active": true, "timeZone": "Asia/Kolkata"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17926905", "id": "17926905", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 opened a new pull request, #7386:\nURL: https://github.com/apache/hadoop/pull/7386\n\n   We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint. So, we have added test cases for all those scenarios in this PR.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-13T19:03:58.551+0000", "updated": "2025-02-13T19:03:58.551+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17926934", "id": "17926934", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2657734081\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 9 new + 2 unchanged - 0 fixed = 11 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 58s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 134m 13s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7386 |\r\n   | JIRA Issue | HADOOP-19445 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f1c6768f1b54 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0bfe00728dc19c14378d0a2aa58842330a1b5f4a |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/testReport/ |\r\n   | Max. process+thread count | 604 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-13T21:20:24.079+0000", "updated": "2025-02-13T21:20:24.079+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17928482", "id": "17928482", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2669092678\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 9 new + 7 unchanged - 0 fixed = 16 total (was 7)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  75m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7386 |\r\n   | JIRA Issue | HADOOP-19445 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3e831a046d9d 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 35b31f95f614c22e990cddad3450bf1f7d448fda |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/2/testReport/ |\r\n   | Max. process+thread count | 738 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-19T16:05:51.058+0000", "updated": "2025-02-19T16:05:51.058+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17928486", "id": "17928486", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2669103301\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  4s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 55s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 26s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  75m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.47 ServerAPI=1.47 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7386 |\r\n   | JIRA Issue | HADOOP-19445 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ba460f92d49c 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6f5a3a37baa5788f77e57331c413ec49f2bfcbfe |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/3/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-19T16:09:32.085+0000", "updated": "2025-02-19T16:09:32.085+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17928657", "id": "17928657", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2670576270\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 780, Failures: 0, Errors: 0, Skipped: 150\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 783, Failures: 0, Errors: 0, Skipped: 105\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 622, Failures: 0, Errors: 0, Skipped: 206\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 780, Failures: 0, Errors: 0, Skipped: 155\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 2\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 625, Failures: 0, Errors: 0, Skipped: 134\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 619, Failures: 0, Errors: 0, Skipped: 207\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 622, Failures: 0, Errors: 0, Skipped: 135\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 620, Failures: 0, Errors: 0, Skipped: 152\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 654, Failures: 0, Errors: 0, Skipped: 153\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 619, Failures: 0, Errors: 0, Skipped: 205\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 24\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-20T06:26:33.612+0000", "updated": "2025-02-20T06:26:33.612+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17928699", "id": "17928699", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2670855433\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  18m 25s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 40s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 58s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 152m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7386 |\r\n   | JIRA Issue | HADOOP-19445 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 418d4cfa6bcf 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 47eccd5f533f62f524235e42deb368bcabfe39f6 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/4/testReport/ |\r\n   | Max. process+thread count | 610 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-20T09:00:14.386+0000", "updated": "2025-02-20T09:00:14.386+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17929147", "id": "17929147", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1965400041\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -444,15 +443,14 @@ public void createNonRecursivePreCheck(Path parentPath,\n       }\n       getPathStatus(parentPath.toUri().getPath(), false,\n           tracingContext, null);\n+      incrementAbfsGetPathStatus();\n     } catch (AbfsRestOperationException ex) {\n       if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\n         throw new FileNotFoundException(\"Cannot create file \"\n             + parentPath.toUri().getPath()\n             + \" because parent folder does not exist.\");\n       }\n       throw ex;\n\nReview Comment:\n   The increase will not get implemented if getPathStatus call fails\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-21T12:28:11.890+0000", "updated": "2025-02-21T12:28:11.890+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17929149", "id": "17929149", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1965401619\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -993,8 +993,6 @@ public Void call() throws Exception {\n             delete(fs.getPath(), fs.isDirectory());\n             if (fs.isDirectory()) {\n               statIncrement(DIRECTORIES_DELETED);\n-            } else {\n\nReview Comment:\n   should call incrementAbfsDeleteFile ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-21T12:29:36.970+0000", "updated": "2025-02-21T12:29:36.970+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17929150", "id": "17929150", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1965403210\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -693,6 +697,34 @@ protected void incrementAbfsRenamePath() {\n     abfsCounters.incrementCounter(RENAME_PATH_ATTEMPTS, 1);\n   }\n \n+  /**\n+   * Increments AbfsCounters for get path status by 1.\n+   */\n+  protected void incrementAbfsGetPathStatus() {\n\nReview Comment:\n   counter increment should not be in abfsclient class in my opinion\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-21T12:31:07.089+0000", "updated": "2025-02-21T12:31:07.089+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17930178", "id": "17930178", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1969269565\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1751,6 +1749,7 @@ public void takeGetPathStatusAtomicRenameKeyAction(final Path path,\n       pendingJsonFileStatus = getPathStatus(\n           pendingJsonPath.toUri().getPath(), tracingContext,\n           null, false);\n+      incrementAbfsGetPathStatus();\n\nReview Comment:\n   At some places we are incrementing before the operation and at some places after the operation is executed, we must keep it consitent\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-25T08:39:56.082+0000", "updated": "2025-02-25T08:39:56.082+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17930179", "id": "17930179", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1969271210\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/BlobDeleteHandler.java:\n##########\n@@ -90,6 +90,7 @@ int getMaxConsumptionParallelism() {\n   private boolean deleteInternal(final Path path)\n       throws AzureBlobFileSystemException {\n     getAbfsClient().deleteBlobPath(path, null, tracingContext);\n+    getAbfsClient().incrementAbfsDeleteFile();\n\nReview Comment:\n   same as above\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-02-25T08:40:54.948+0000", "updated": "2025-02-25T08:40:54.948+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17931912", "id": "17931912", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1977149452\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -444,15 +443,14 @@ public void createNonRecursivePreCheck(Path parentPath,\n       }\n       getPathStatus(parentPath.toUri().getPath(), false,\n           tracingContext, null);\n+      incrementAbfsGetPathStatus();\n     } catch (AbfsRestOperationException ex) {\n       if (ex.getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\n         throw new FileNotFoundException(\"Cannot create file \"\n             + parentPath.toUri().getPath()\n             + \" because parent folder does not exist.\");\n       }\n       throw ex;\n\nReview Comment:\n   Moved it to finally block\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/BlobDeleteHandler.java:\n##########\n@@ -90,6 +90,7 @@ int getMaxConsumptionParallelism() {\n   private boolean deleteInternal(final Path path)\n       throws AzureBlobFileSystemException {\n     getAbfsClient().deleteBlobPath(path, null, tracingContext);\n+    getAbfsClient().incrementAbfsDeleteFile();\n\nReview Comment:\n   Moved to finally block\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java:\n##########\n@@ -993,8 +993,6 @@ public Void call() throws Exception {\n             delete(fs.getPath(), fs.isDirectory());\n             if (fs.isDirectory()) {\n               statIncrement(DIRECTORIES_DELETED);\n-            } else {\n\nReview Comment:\n   We are incrementing Delete Files in Client.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java:\n##########\n@@ -693,6 +697,34 @@ protected void incrementAbfsRenamePath() {\n     abfsCounters.incrementCounter(RENAME_PATH_ATTEMPTS, 1);\n   }\n \n+  /**\n+   * Increments AbfsCounters for get path status by 1.\n+   */\n+  protected void incrementAbfsGetPathStatus() {\n\nReview Comment:\n   As discussed, this is just a placeholder so that we can use the same method in both DFS and Blob client.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-03T09:25:37.751+0000", "updated": "2025-03-03T09:25:37.751+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17931913", "id": "17931913", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1977152795\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsBlobClient.java:\n##########\n@@ -1751,6 +1749,7 @@ public void takeGetPathStatusAtomicRenameKeyAction(final Path path,\n       pendingJsonFileStatus = getPathStatus(\n           pendingJsonPath.toUri().getPath(), tracingContext,\n           null, false);\n+      incrementAbfsGetPathStatus();\n\nReview Comment:\n   Moved this call in finally block everywhere.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-03T09:26:15.946+0000", "updated": "2025-03-03T09:26:15.946+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17931930", "id": "17931930", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1977284407\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -462,8 +461,6 @@ public void createNonRecursivePreCheck(Path parentPath,\n             + \" because parent folder does not exist.\");\n       }\n       throw ex;\n-    } finally {\n-      getAbfsCounters().incrementCounter(CALL_GET_FILE_STATUS, 1);\n\nReview Comment:\n   Call Get File Status is HDFS API call, therefore removed it from client.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-03T10:41:28.827+0000", "updated": "2025-03-03T10:41:28.827+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17931948", "id": "17931948", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1977284407\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -462,8 +461,6 @@ public void createNonRecursivePreCheck(Path parentPath,\n             + \" because parent folder does not exist.\");\n       }\n       throw ex;\n-    } finally {\n-      getAbfsCounters().incrementCounter(CALL_GET_FILE_STATUS, 1);\n\nReview Comment:\n   Call Get File Status is HDFS API call, therefore removed it from client.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-03T12:01:56.150+0000", "updated": "2025-03-03T12:01:56.150+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17932047", "id": "17932047", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2695236283\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  18m  1s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  36m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 32s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  35m 48s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 53s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 139m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7386 |\r\n   | JIRA Issue | HADOOP-19445 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b185d1443f6f 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7c71345e4cdc3645007238aaeabf24b43740b80a |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/5/testReport/ |\r\n   | Max. process+thread count | 558 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-03T18:34:47.339+0000", "updated": "2025-03-03T18:34:47.339+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17932220", "id": "17932220", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1978955779\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -304,12 +324,67 @@ public void testRenameNotFoundBlobToEmptyRoot() throws Exception {\n    *\n    * @throws Exception if an error occurs during test execution\n    */\n-  @Test(expected = IOException.class)\n-  public void testRenameBlobToDstWithColonInPath() throws Exception {\n+  @Test\n+  public void testRenameBlobToDstWithColonInSourcePath() throws Exception {\n     AzureBlobFileSystem fs = getFileSystem();\n     assumeBlobServiceType();\n+    fs.create(new Path(\"/src:/file\"));\n+    Assertions.assertThat(\n\nReview Comment:\n   Add description for assert\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -304,12 +324,67 @@ public void testRenameNotFoundBlobToEmptyRoot() throws Exception {\n    *\n    * @throws Exception if an error occurs during test execution\n    */\n-  @Test(expected = IOException.class)\n-  public void testRenameBlobToDstWithColonInPath() throws Exception {\n+  @Test\n+  public void testRenameBlobToDstWithColonInSourcePath() throws Exception {\n     AzureBlobFileSystem fs = getFileSystem();\n     assumeBlobServiceType();\n+    fs.create(new Path(\"/src:/file\"));\n+    Assertions.assertThat(\n+        fs.rename(new Path(\"/src:\"),\n+            new Path(\"/dst\"))\n+    ).isTrue();\n+  }\n+\n+  /**\n+   * Tests renaming a source path to a destination path that contains a colon in the path.\n+   * This verifies that the rename operation handles paths with special characters like a colon.\n+   *\n+   * The test creates a source directory and renames it to a destination path that includes a colon,\n+   * ensuring that the operation succeeds without errors.\n+   *\n+   * @throws Exception if an error occurs during test execution\n+   */\n+  @Test\n+  public void testRenameWithColonInDestinationPath() throws Exception {\n+    AzureBlobFileSystem fs = getFileSystem();\n     fs.create(new Path(\"/src\"));\n-    fs.rename(new Path(\"/src\"), new Path(\"/dst:file\"));\n+    Assertions.assertThat(\n\nReview Comment:\n   Add assert description here and everywhere in file\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -304,12 +324,67 @@ public void testRenameNotFoundBlobToEmptyRoot() throws Exception {\n    *\n    * @throws Exception if an error occurs during test execution\n    */\n-  @Test(expected = IOException.class)\n-  public void testRenameBlobToDstWithColonInPath() throws Exception {\n+  @Test\n+  public void testRenameBlobToDstWithColonInSourcePath() throws Exception {\n     AzureBlobFileSystem fs = getFileSystem();\n     assumeBlobServiceType();\n\nReview Comment:\n   Can we make this run for both DFS and Blob?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -1655,6 +1730,827 @@ public void testRenameSrcDirDeleteEmitDeletionCountInClientRequestId()\n     fs.rename(new Path(dirPathStr), new Path(\"/dst/\"));\n   }\n \n+  /**\n+   * Helper method to configure the AzureBlobFileSystem and rename directories.\n+   *\n+   * @param currentFs The current AzureBlobFileSystem to use for renaming.\n+   * @param producerQueueSize Maximum size of the producer queue.\n+   * @param consumerMaxLag Maximum lag allowed for the consumer.\n+   * @param maxThread Maximum threads for the rename operation.\n+   * @param src The source path of the directory to rename.\n+   * @param dst The destination path of the renamed directory.\n+   * @throws IOException If an I/O error occurs during the operation.\n+   */\n+  private void renameDir(AzureBlobFileSystem currentFs, String producerQueueSize,\n+      String consumerMaxLag, String maxThread, Path src, Path dst)\n+      throws IOException {\n+    Configuration config = createConfig(producerQueueSize, consumerMaxLag, maxThread);\n+    try (AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config)) {\n+      fs.rename(src, dst);\n+      validateRename(fs, src, dst, false, true, false);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to create the configuration for the AzureBlobFileSystem.\n+   *\n+   * @param producerQueueSize Maximum size of the producer queue.\n+   * @param consumerMaxLag Maximum lag allowed for the consumer.\n+   * @param maxThread Maximum threads for the rename operation.\n+   * @return The configuration object.\n+   */\n+  private Configuration createConfig(String producerQueueSize, String consumerMaxLag, String maxThread) {\n+    Configuration config = new Configuration(this.getRawConfiguration());\n+    config.set(FS_AZURE_PRODUCER_QUEUE_MAX_SIZE, producerQueueSize);\n+    config.set(FS_AZURE_CONSUMER_MAX_LAG, consumerMaxLag);\n+    config.set(FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD, maxThread);\n+    return config;\n+  }\n+\n+  /**\n+   * Helper method to validate that the rename was successful and that the destination exists.\n+   *\n+   * @param fs The AzureBlobFileSystem instance to check the existence on.\n+   * @param dst The destination path.\n+   * @param src The source path.\n+   * @throws IOException If an I/O error occurs during the validation.\n+   */\n+  private void validateRename(AzureBlobFileSystem fs, Path src, Path dst,\n+      boolean isSrcExist, boolean isDstExist, boolean isJsonExist)\n+      throws IOException {\n+    Assertions.assertThat(fs.exists(dst))\n+        .describedAs(\"Renamed Destination directory should exist.\")\n+        .isEqualTo(isDstExist);\n+    Assertions.assertThat(fs.exists(new Path(src.getParent(), src.getName() + SUFFIX)))\n+        .describedAs(\"Renamed Pending Json file should exist.\")\n+        .isEqualTo(isJsonExist);\n+    Assertions.assertThat(fs.exists(src))\n+        .describedAs(\"Renamed Destination directory should exist.\")\n+        .isEqualTo(isSrcExist);\n+  }\n+\n+  /**\n+   * Test the renaming of a directory with different parallelism configurations.\n+   */\n+  @Test\n+  public void testRenameDirWithDifferentParallelism() throws Exception {\n\nReview Comment:\n   Test name should also say its different parallelism configuration not just different parallelsm\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ListActionTaker.java:\n##########\n@@ -261,7 +269,7 @@ protected String listAndEnqueue(final ListBlobQueue listBlobQueue,\n   protected void addPaths(final List<Path> paths,\n       final ListResultSchema retrievedSchema) {\n     for (ListResultEntrySchema entry : retrievedSchema.paths()) {\n-      Path entryPath = new Path(ROOT_PATH, entry.name());\n+      Path entryPath = new Path(ROOT_PATH + entry.name());\n\nReview Comment:\n   Why this change?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -1655,6 +1730,827 @@ public void testRenameSrcDirDeleteEmitDeletionCountInClientRequestId()\n     fs.rename(new Path(dirPathStr), new Path(\"/dst/\"));\n   }\n \n+  /**\n+   * Helper method to configure the AzureBlobFileSystem and rename directories.\n+   *\n+   * @param currentFs The current AzureBlobFileSystem to use for renaming.\n+   * @param producerQueueSize Maximum size of the producer queue.\n+   * @param consumerMaxLag Maximum lag allowed for the consumer.\n+   * @param maxThread Maximum threads for the rename operation.\n+   * @param src The source path of the directory to rename.\n+   * @param dst The destination path of the renamed directory.\n+   * @throws IOException If an I/O error occurs during the operation.\n+   */\n+  private void renameDir(AzureBlobFileSystem currentFs, String producerQueueSize,\n+      String consumerMaxLag, String maxThread, Path src, Path dst)\n+      throws IOException {\n+    Configuration config = createConfig(producerQueueSize, consumerMaxLag, maxThread);\n+    try (AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config)) {\n+      fs.rename(src, dst);\n+      validateRename(fs, src, dst, false, true, false);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to create the configuration for the AzureBlobFileSystem.\n+   *\n+   * @param producerQueueSize Maximum size of the producer queue.\n+   * @param consumerMaxLag Maximum lag allowed for the consumer.\n+   * @param maxThread Maximum threads for the rename operation.\n+   * @return The configuration object.\n+   */\n+  private Configuration createConfig(String producerQueueSize, String consumerMaxLag, String maxThread) {\n+    Configuration config = new Configuration(this.getRawConfiguration());\n+    config.set(FS_AZURE_PRODUCER_QUEUE_MAX_SIZE, producerQueueSize);\n+    config.set(FS_AZURE_CONSUMER_MAX_LAG, consumerMaxLag);\n+    config.set(FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD, maxThread);\n+    return config;\n+  }\n+\n+  /**\n+   * Helper method to validate that the rename was successful and that the destination exists.\n+   *\n+   * @param fs The AzureBlobFileSystem instance to check the existence on.\n+   * @param dst The destination path.\n+   * @param src The source path.\n+   * @throws IOException If an I/O error occurs during the validation.\n+   */\n+  private void validateRename(AzureBlobFileSystem fs, Path src, Path dst,\n+      boolean isSrcExist, boolean isDstExist, boolean isJsonExist)\n+      throws IOException {\n+    Assertions.assertThat(fs.exists(dst))\n+        .describedAs(\"Renamed Destination directory should exist.\")\n+        .isEqualTo(isDstExist);\n+    Assertions.assertThat(fs.exists(new Path(src.getParent(), src.getName() + SUFFIX)))\n+        .describedAs(\"Renamed Pending Json file should exist.\")\n+        .isEqualTo(isJsonExist);\n+    Assertions.assertThat(fs.exists(src))\n+        .describedAs(\"Renamed Destination directory should exist.\")\n+        .isEqualTo(isSrcExist);\n+  }\n+\n+  /**\n+   * Test the renaming of a directory with different parallelism configurations.\n+   */\n+  @Test\n+  public void testRenameDirWithDifferentParallelism() throws Exception {\n+    try (AzureBlobFileSystem currentFs = getFileSystem()) {\n+      assumeBlobServiceType();\n+      Path src = new Path(\"/hbase/A1/A2\");\n+      Path dst = new Path(\"/hbase/A1/A3\");\n+\n+      // Create sample files in the source directory\n+      createFiles(currentFs, src, TOTAL_FILES);\n+\n+      // Test renaming with different configurations\n+      renameDir(currentFs, \"10\", \"5\", \"2\", src, dst);\n+      renameDir(currentFs, \"100\", \"5\", \"2\", dst, src);\n+\n+      String errorMessage = intercept(PathIOException.class,\n+          () -> renameDir(currentFs, \"50\", \"50\", \"5\", src, dst))\n+          .getMessage();\n+\n+      // Validate error message for invalid configuration\n+      Assertions.assertThat(errorMessage)\n+          .describedAs(\"maxConsumptionLag should be lesser than maxSize\")\n+          .contains(\n+              \"Invalid configuration value detected for \\\"fs.azure.blob.dir.list.consumer.max.lag\\\". \"\n+                  + \"maxConsumptionLag should be lesser than maxSize\");\n+    }\n+  }\n+\n+  /**\n+   * Helper method to create files in the given directory.\n+   *\n+   * @param fs The AzureBlobFileSystem instance to use for file creation.\n+   * @param src The source path (directory).\n+   * @param numFiles The number of files to create.\n+   * @throws ExecutionException, InterruptedException If an error occurs during file creation.\n+   */\n+  private void createFiles(AzureBlobFileSystem fs, Path src, int numFiles)\n+      throws ExecutionException, InterruptedException {\n+    ExecutorService executorService = Executors.newFixedThreadPool(TOTAL_THREADS_IN_POOL);\n+    List<Future> futures = new ArrayList<>();\n+    for (int i = 0; i < numFiles; i++) {\n+      final int iter = i;\n+      Future future = executorService.submit(() ->\n+          fs.create(new Path(src, \"file\" + iter + \".txt\")));\n+      futures.add(future);\n+    }\n+    for (Future future : futures) {\n+      future.get();\n+    }\n+    executorService.shutdown();\n+  }\n+\n+  /**\n+   * Tests renaming a directory with a failure during the copy operation.\n+   * Simulates an error when copying on the 6th call.\n+   */\n+  @Test\n+  public void testRenameCopyFailureInBetween() throws Exception {\n+    try (AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem(\n+        createConfig(\"5\", \"3\", \"2\")))) {\n+      assumeBlobServiceType();\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+      fs.getAbfsStore().setClient(client);\n+      Path src = new Path(\"/hbase/A1/A2\");\n+      Path dst = new Path(\"/hbase/A1/A3\");\n+\n+      // Create sample files in the source directory\n+      createFiles(fs, src, TOTAL_FILES);\n+\n+      // Track the number of copy operations\n+      AtomicInteger copyCall = new AtomicInteger(0);\n+      Mockito.doAnswer(copyRequest -> {\n+        if (copyCall.get() == FAILED_CALL) {\n+          throw new AbfsRestOperationException(\n+              BLOB_ALREADY_EXISTS.getStatusCode(),\n+              BLOB_ALREADY_EXISTS.getErrorCode(),\n+              BLOB_ALREADY_EXISTS.getErrorMessage(),\n+              new Exception());\n+        }\n+        copyCall.incrementAndGet();\n+        return copyRequest.callRealMethod();\n+      }).when(client).copyBlob(Mockito.any(Path.class),\n+          Mockito.any(Path.class), Mockito.nullable(String.class),\n+          Mockito.any(TracingContext.class));\n+\n+      fs.rename(src, dst);\n+      // Validate copy operation count\n+      Assertions.assertThat(copyCall.get())\n+          .describedAs(\"Copy operation count should be less than 10.\")\n+          .isLessThan(TOTAL_FILES);\n+\n+      // Validate that rename redo operation was triggered\n+      copyCall.set(0);\n+\n+      // Assertions to validate renamed destination and source\n+      validateRename(fs, src, dst, false, true, true);\n+\n+      Assertions.assertThat(copyCall.get())\n+          .describedAs(\"Copy operation count should be greater than 0.\")\n+          .isGreaterThan(0);\n+\n+      // Validate final state of destination and source\n+      validateRename(fs, src, dst, false, true, false);\n+    }\n+  }\n+\n+  /**\n+   * Tests renaming a directory with a failure during the delete operation.\n+   * Simulates an error on the 6th delete operation and verifies the behavior.\n+   */\n+  @Test\n+  public void testRenameDeleteFailureInBetween() throws Exception {\n+    try (AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem(\n+        createConfig(\"5\", \"3\", \"2\")))) {\n+      assumeBlobServiceType();\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+      fs.getAbfsStore().setClient(client);\n+      Path src = new Path(\"/hbase/A1/A2\");\n+      Path dst = new Path(\"/hbase/A1/A3\");\n+\n+      // Create sample files in the source directory\n+      createFiles(fs, src, TOTAL_FILES);\n+\n+      // Track the number of delete operations\n+      AtomicInteger deleteCall = new AtomicInteger(0);\n+      Mockito.doAnswer(deleteRequest -> {\n+        if (deleteCall.get() == FAILED_CALL) {\n+          throw new AbfsRestOperationException(\n+              BLOB_PATH_NOT_FOUND.getStatusCode(),\n+              BLOB_PATH_NOT_FOUND.getErrorCode(),\n+              BLOB_PATH_NOT_FOUND.getErrorMessage(),\n+              new Exception());\n+        }\n+        deleteCall.incrementAndGet();\n+        return deleteRequest.callRealMethod();\n+      }).when(client).deleteBlobPath(Mockito.any(Path.class),\n+          Mockito.anyString(), Mockito.any(TracingContext.class));\n+\n+      fs.rename(src, dst);\n+\n+      // Validate delete operation count\n+      Assertions.assertThat(deleteCall.get())\n+          .describedAs(\"Delete operation count should be less than 10.\")\n+          .isLessThan(TOTAL_FILES);\n+\n+      // Validate that delete redo operation was triggered\n+      deleteCall.set(0);\n+      // Assertions to validate renamed destination and source\n+      validateRename(fs, src, dst, false, true, true);\n+\n+      Assertions.assertThat(deleteCall.get())\n+          .describedAs(\"Delete operation count should be greater than 0.\")\n+          .isGreaterThan(0);\n+\n+      // Validate final state of destination and source\n+      // Validate that delete redo operation was triggered\n+      validateRename(fs, src, dst, false, true, false);\n+    }\n+  }\n+\n+  /**\n+   * Tests renaming a file or directory when the destination path contains\n+   * a colon (\":\"). The test ensures that:\n+   * - The source directory exists before the rename.\n+   * - The file is successfully renamed to the destination path.\n+   * - The old source directory no longer exists after the rename.\n+   * - The new destination directory exists after the rename.\n+   *\n+   * @throws Exception if an error occurs during file system operations\n+   */\n+  @Test\n+  public void testRenameWhenDestinationPathContainsColon() throws Exception {\n+    AzureBlobFileSystem fs = getFileSystem();\n+    fs.setWorkingDirectory(new Path(ROOT_PATH));\n+    String fileName = \"file\";\n+    Path src = new Path(\"/test1/\");\n+    Path dst = new Path(\"/test1:/\");\n+\n+    // Create the file\n+    fs.create(new Path(src, fileName));\n+\n+    // Perform the rename operation and validate the results\n+    performRenameAndValidate(fs, src, dst, fileName);\n+  }\n+\n+  /**\n+   * Performs the rename operation and validates the existence of the directories and files.\n+   *\n+   * @param fs the AzureBlobFileSystem instance\n+   * @param src the source path to be renamed\n+   * @param dst the destination path for the rename\n+   * @param fileName the name of the file to be renamed\n+   */\n+  private void performRenameAndValidate(AzureBlobFileSystem fs, Path src, Path dst, String fileName)\n+      throws IOException {\n+    // Assert the source directory exists\n+    Assertions.assertThat(fs.exists(src))\n+        .describedAs(\"Old directory should exist before rename\")\n+        .isTrue();\n+\n+    // Perform rename\n+    fs.rename(src, dst);\n+\n+    // Assert the destination directory and file exist after rename\n+    Assertions.assertThat(fs.exists(new Path(dst, fileName)))\n+        .describedAs(\"Rename should be successful\")\n+        .isTrue();\n+\n+    // Assert the source directory no longer exists\n+    Assertions.assertThat(fs.exists(src))\n+        .describedAs(\"Old directory should not exist\")\n+        .isFalse();\n+\n+    // Assert the new destination directory exists\n+    Assertions.assertThat(fs.exists(dst))\n+        .describedAs(\"New directory should exist\")\n+        .isTrue();\n+  }\n+\n+  /**\n+   * Tests the behavior of the atomic rename key for the root folder\n+   * in Azure Blob File System. The test verifies that the atomic rename key\n+   * returns false for the root folder path.\n+   *\n+   * @throws Exception if an error occurs during the atomic rename key check\n+   */\n+  @Test\n+  public void testGetAtomicRenameKeyForRootFolder() throws Exception {\n+    AzureBlobFileSystem fs = getFileSystem();\n+    assumeBlobServiceType();\n+    AbfsBlobClient abfsBlobClient = (AbfsBlobClient) fs.getAbfsClient();\n+    Assertions.assertThat(abfsBlobClient.isAtomicRenameKey(\"/hbase\"))\n+        .describedAs(\"Atomic rename key should return false for Root folder\")\n+        .isFalse();\n+  }\n+\n+  /**\n+   * Tests the behavior of the atomic rename key for non-root folders\n+   * in Azure Blob File System. The test verifies that the atomic rename key\n+   * works for specific folders as defined in the configuration.\n+   * It checks the atomic rename key for various paths,\n+   * ensuring it returns true for matching paths and false for others.\n+   *\n+   * @throws Exception if an error occurs during the atomic rename key check\n+   */\n+  @Test\n+  public void testGetAtomicRenameKeyForNonRootFolder() throws Exception {\n+    final AzureBlobFileSystem currentFs = getFileSystem();\n+    Configuration config = new Configuration(this.getRawConfiguration());\n+    config.set(FS_AZURE_ATOMIC_RENAME_KEY, \"/hbase,/a,/b\");\n+\n+    final AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config);\n+    assumeBlobServiceType();\n+    AbfsBlobClient abfsBlobClient = (AbfsBlobClient) fs.getAbfsClient();\n+\n+    // Test for various paths\n+    validateAtomicRenameKey(abfsBlobClient, \"/hbase1/test\", false);\n+    validateAtomicRenameKey(abfsBlobClient, \"/hbase/test\", true);\n+    validateAtomicRenameKey(abfsBlobClient, \"/a/b/c\", true);\n+    validateAtomicRenameKey(abfsBlobClient, \"/test/a\", false);\n+  }\n+\n+  /**\n+   * Validates the atomic rename key for a specific path.\n+   *\n+   * @param abfsBlobClient the AbfsBlobClient instance\n+   * @param path the path to check for atomic rename key\n+   * @param expected the expected value (true or false)\n+   */\n+  private void validateAtomicRenameKey(AbfsBlobClient abfsBlobClient, String path, boolean expected) {\n+    Assertions.assertThat(abfsBlobClient.isAtomicRenameKey(path))\n+        .describedAs(\"Atomic rename key check for path: \" + path)\n+        .isEqualTo(expected);\n+  }\n+\n+  /**\n+   * Helper method to create a json file.\n+   * @param path parent path\n+   * @param renameJson rename json path\n+   * @return file system\n+   * @throws IOException in case of failure\n+   */\n+  public AzureBlobFileSystem createJsonFile(Path path, Path renameJson) throws IOException {\n+    final AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem());\n+    assumeBlobServiceType();\n+    AzureBlobFileSystemStore store = Mockito.spy(fs.getAbfsStore());\n+    Mockito.doReturn(store).when(fs).getAbfsStore();\n+    AbfsClient client = Mockito.spy(store.getClient());\n+    Mockito.doReturn(client).when(store).getClient();\n+\n+    fs.setWorkingDirectory(new Path(ROOT_PATH));\n+    fs.create(new Path(path, \"file.txt\"));\n+\n+    AzureBlobFileSystemStore.VersionedFileStatus fileStatus = (AzureBlobFileSystemStore.VersionedFileStatus) fs.getFileStatus(path);\n+\n+    new RenameAtomicity(path, new Path(\"/hbase/test4\"), renameJson, getTestTracingContext(fs, true), fileStatus.getEtag(), client)\n+        .preRename();\n+\n+    Assertions.assertThat(fs.exists(renameJson))\n+        .describedAs(\"Rename Pending Json file should exist.\")\n+        .isTrue();\n+\n+    return fs;\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery with a single child folder.\n+   *\n+   * This test simulates a scenario where a pending rename JSON file exists for a single child folder\n+   * under the parent directory. It ensures that when listing the files in the parent directory,\n+   * only the child folder (with the pending rename JSON file) is returned, and no additional files are listed.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithSingleChildFolder() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+\n+      FileStatus[] fileStatuses = fs.listStatus(new Path(\"/hbase/A1\"));\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 1 file\")\n+          .isEqualTo(1);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery with multiple child folders.\n+   *\n+   * This test simulates a scenario where a pending rename JSON file exists, and multiple files are\n+   * created in the parent directory. It ensures that when listing the files in the parent directory,\n+   * the correct number of files is returned, including the pending rename JSON file.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithMultipleChildFolder() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+\n+      fs.create(new Path(\"/hbase/A1/file1.txt\"));\n+      fs.create(new Path(\"/hbase/A1/file2.txt\"));\n+\n+      FileStatus[] fileStatuses = fs.listStatus(new Path(\"/hbase/A1\"));\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 3 files\")\n+          .isEqualTo(3);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery with a pending rename JSON file.\n+   *\n+   * This test simulates a scenario where a pending rename JSON file exists in the parent directory,\n+   * and it ensures that after the deletion of the target directory and creation of new files,\n+   * the listing operation correctly returns the remaining files without considering the pending rename.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithPendingJsonFile() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+\n+      fs.delete(path, true);\n+      fs.create(new Path(\"/hbase/A1/file1.txt\"));\n+      fs.create(new Path(\"/hbase/A1/file2.txt\"));\n+\n+      FileStatus[] fileStatuses = fs.listStatus(path.getParent());\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 2 files\")\n+          .isEqualTo(2);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery when no pending rename JSON file exists.\n+   *\n+   * This test simulates a scenario where there is no pending rename JSON file in the directory.\n+   * It ensures that the listing operation correctly returns all files in the parent directory, including\n+   * those created after the rename JSON file is deleted.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithoutAnyPendingJsonFile() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+\n+      fs.delete(renameJson, true);\n+      fs.create(new Path(\"/hbase/A1/file1.txt\"));\n+      fs.create(new Path(\"/hbase/A1/file2.txt\"));\n+\n+      FileStatus[] fileStatuses = fs.listStatus(path.getParent());\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 3 files\")\n+          .isEqualTo(3);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery when a pending rename JSON directory exists.\n+   *\n+   * This test simulates a scenario where a pending rename JSON directory exists, ensuring that the\n+   * listing operation correctly returns all files in the parent directory without triggering a redo\n+   * rename operation. It also checks that the directory with the suffix \"-RenamePending.json\" exists.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithPendingJsonDir() throws Exception {\n+    try (AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem())) {\n+      assumeBlobServiceType();\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs.mkdirs(renameJson);\n+\n+      fs.create(new Path(path.getParent(), \"file1.txt\"));\n+      fs.create(new Path(path, \"file2.txt\"));\n+\n+      AtomicInteger redoRenameCall = new AtomicInteger(0);\n+      Mockito.doAnswer(answer -> {\n+        redoRenameCall.incrementAndGet();\n+        return answer.callRealMethod();\n+      }).when(client).getRedoRenameAtomicity(Mockito.any(Path.class),\n+          Mockito.anyInt(), Mockito.any(TracingContext.class));\n+\n+      FileStatus[] fileStatuses = fs.listStatus(path.getParent());\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 3 files\")\n+          .isEqualTo(3);\n+\n+      Assertions.assertThat(redoRenameCall.get())\n+          .describedAs(\"No redo rename call should be made\")\n+          .isEqualTo(0);\n+\n+      Assertions.assertThat(\n+              Arrays.stream(fileStatuses)\n+                  .anyMatch(status -> renameJson.toUri().getPath().equals(status.getPath().toUri().getPath())))\n+          .describedAs(\"Directory with suffix -RenamePending.json should exist.\")\n+          .isTrue();\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery during listing with multiple pending rename JSON files.\n+   *\n+   * This test simulates a scenario where multiple pending rename JSON files exist, ensuring that\n+   * crash recovery properly handles the situation. It verifies that two redo rename calls are made\n+   * and that the list operation returns the correct number of paths.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithMultipleJsonFile() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+\n+      // 1st Json file\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+\n+      // 2nd Json file\n+      Path path2 = new Path(\"/hbase/A1/A3\");\n+      fs.create(new Path(path2, \"file3.txt\"));\n+\n+      Path renameJson2 = new Path(path2.getParent(), path2.getName() + SUFFIX);\n+      AzureBlobFileSystemStore.VersionedFileStatus fileStatus = (AzureBlobFileSystemStore.VersionedFileStatus) fs.getFileStatus(path2);\n+\n+      new RenameAtomicity(path2, new Path(\"/hbase/test4\"), renameJson2, getTestTracingContext(fs, true), fileStatus.getEtag(), client).preRename();\n+\n+      fs.create(new Path(path, \"file2.txt\"));\n+\n+      AtomicInteger redoRenameCall = new AtomicInteger(0);\n+      Mockito.doAnswer(answer -> {\n+        redoRenameCall.incrementAndGet();\n+        return answer.callRealMethod();\n+      }).when(client).getRedoRenameAtomicity(Mockito.any(Path.class),\n+          Mockito.anyInt(), Mockito.any(TracingContext.class));\n+\n+      FileStatus[] fileStatuses = fs.listStatus(path.getParent());\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 2 paths\")\n+          .isEqualTo(2);\n+\n+      Assertions.assertThat(redoRenameCall.get())\n+          .describedAs(\"2 redo rename calls should be made\")\n+          .isEqualTo(2);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify path status when a pending rename JSON file exists.\n+   *\n+   * This test simulates a scenario where a rename operation was pending, and ensures that\n+   * the path status retrieval triggers a redo rename operation. The test also checks that\n+   * the correct error code (`PATH_NOT_FOUND`) is returned.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void getPathStatusWithPendingJsonFile() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+\n+      fs.create(new Path(\"/hbase/A1/file1.txt\"));\n+      fs.create(new Path(\"/hbase/A1/file2.txt\"));\n+\n+      AbfsConfiguration conf = fs.getAbfsStore().getAbfsConfiguration();\n+\n+      AtomicInteger redoRenameCall = new AtomicInteger(0);\n+      Mockito.doAnswer(answer -> {\n+        redoRenameCall.incrementAndGet();\n+        return answer.callRealMethod();\n+      }).when(client).getRedoRenameAtomicity(Mockito.any(Path.class),\n+          Mockito.anyInt(), Mockito.any(TracingContext.class));\n+\n+      TracingContext tracingContext = new TracingContext(\n+          conf.getClientCorrelationId(), fs.getFileSystemId(),\n+          FSOperationType.GET_FILESTATUS, TracingHeaderFormat.ALL_ID_FORMAT, null);\n+\n+      AzureServiceErrorCode azureServiceErrorCode = intercept(\n+          AbfsRestOperationException.class, () -> client.getPathStatus(\n+              path.toUri().getPath(), true,\n+              tracingContext, null)).getErrorCode();\n+\n+      Assertions.assertThat(azureServiceErrorCode.getErrorCode())\n+          .describedAs(\"Path had to be recovered from atomic rename operation.\")\n+          .isEqualTo(PATH_NOT_FOUND.getErrorCode());\n+\n+      Assertions.assertThat(redoRenameCall.get())\n+          .describedAs(\"There should be one redo rename call\")\n+          .isEqualTo(1);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify path status when there is no pending rename JSON file.\n+   *\n+   * This test ensures that when no rename pending JSON file is present, the path status is\n+   * successfully retrieved, the ETag is present, and no redo rename operation is triggered.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void getPathStatusWithoutPendingJsonFile() throws Exception {\n+    try (AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem())) {\n+      assumeBlobServiceType();\n+\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+\n+      fs.create(new Path(path, \"file1.txt\"));\n+      fs.create(new Path(path, \"file2.txt\"));\n+\n+      AbfsConfiguration conf = fs.getAbfsStore().getAbfsConfiguration();\n+\n+      AtomicInteger redoRenameCall = new AtomicInteger(0);\n+      Mockito.doAnswer(answer -> {\n+        redoRenameCall.incrementAndGet();\n+        return answer.callRealMethod();\n+      }).when(client).getRedoRenameAtomicity(\n+          Mockito.any(Path.class), Mockito.anyInt(),\n+          Mockito.any(TracingContext.class));\n+\n+      TracingContext tracingContext = new TracingContext(\n+          conf.getClientCorrelationId(), fs.getFileSystemId(),\n+          FSOperationType.GET_FILESTATUS, TracingHeaderFormat.ALL_ID_FORMAT,\n+          null);\n+\n+      AbfsHttpOperation abfsHttpOperation = client.getPathStatus(\n+          path.toUri().getPath(), true,\n+          tracingContext, null).getResult();\n+\n+      Assertions.assertThat(abfsHttpOperation.getStatusCode())\n+          .describedAs(\"Path should be found.\")\n+          .isEqualTo(HTTP_OK);\n+\n+      Assertions.assertThat(extractEtagHeader(abfsHttpOperation))\n+          .describedAs(\"Etag should be present.\")\n+          .isNotNull();\n+\n+      Assertions.assertThat(redoRenameCall.get())\n+          .describedAs(\"There should be no redo rename call.\")\n+          .isEqualTo(0);\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify path status when there is a pending rename JSON directory.\n+   *\n+   * This test simulates the scenario where a directory is created with a rename pending JSON\n+   * file (indicated by a specific suffix). It ensures that the path is found, the ETag is present,\n+   * and no redo rename operation is triggered. It also verifies that the rename pending directory\n+   * exists.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void getPathStatusWithPendingJsonDir() throws Exception {\n+    try (AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem())) {\n+      assumeBlobServiceType();\n+\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+\n+      fs.create(new Path(path, \"file1.txt\"));\n+      fs.create(new Path(path, \"file2.txt\"));\n+\n+      fs.mkdirs(new Path(path.getParent(), path.getName() + SUFFIX));\n+\n+      AbfsConfiguration conf = fs.getAbfsStore().getAbfsConfiguration();\n+\n+      AtomicInteger redoRenameCall = new AtomicInteger(0);\n+      Mockito.doAnswer(answer -> {\n+        redoRenameCall.incrementAndGet();\n+        return answer.callRealMethod();\n+      }).when(client).getRedoRenameAtomicity(Mockito.any(Path.class),\n+          Mockito.anyInt(), Mockito.any(TracingContext.class));\n+\n+      TracingContext tracingContext = new TracingContext(\n+          conf.getClientCorrelationId(), fs.getFileSystemId(),\n+          FSOperationType.GET_FILESTATUS, TracingHeaderFormat.ALL_ID_FORMAT, null);\n+\n+      AbfsHttpOperation abfsHttpOperation = client.getPathStatus(path.toUri().getPath(), true, tracingContext, null).getResult();\n+\n+      Assertions.assertThat(abfsHttpOperation.getStatusCode())\n+          .describedAs(\"Path should be found.\")\n+          .isEqualTo(HTTP_OK);\n+\n+      Assertions.assertThat(extractEtagHeader(abfsHttpOperation))\n+          .describedAs(\"Etag should be present.\")\n+          .isNotNull();\n+\n+      Assertions.assertThat(redoRenameCall.get())\n+          .describedAs(\"There should be no redo rename call.\")\n+          .isEqualTo(0);\n+\n+      Assertions.assertThat(fs.exists(new Path(path.getParent(), path.getName() + SUFFIX)))\n+          .describedAs(\"Directory with suffix -RenamePending.json should exist.\")\n+          .isTrue();\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify the behavior when the ETag of a file changes during a rename operation.\n+   *\n+   * This test simulates a scenario where the ETag of a file changes after the creation of a\n+   * rename pending JSON file. The steps include:\n+   * - Creating a rename pending JSON file with an old ETag.\n+   * - Deleting the original directory for an ETag change.\n+   * - Creating new files in the directory.\n+   * - Verifying that the copy blob call is not triggered.\n+   * - Verifying that the rename atomicity operation is called once.\n+   *\n+   * The test ensures that the system correctly handles the ETag change during the rename process.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void eTagChangedDuringRename() throws Exception {\n\nReview Comment:\n   test name should starte with 'test' here and everywhere\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-04T09:34:03.880+0000", "updated": "2025-03-04T09:34:03.880+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17932231", "id": "17932231", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1979065621\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -1655,6 +1730,827 @@ public void testRenameSrcDirDeleteEmitDeletionCountInClientRequestId()\n     fs.rename(new Path(dirPathStr), new Path(\"/dst/\"));\n   }\n \n+  /**\n+   * Helper method to configure the AzureBlobFileSystem and rename directories.\n+   *\n+   * @param currentFs The current AzureBlobFileSystem to use for renaming.\n+   * @param producerQueueSize Maximum size of the producer queue.\n+   * @param consumerMaxLag Maximum lag allowed for the consumer.\n+   * @param maxThread Maximum threads for the rename operation.\n+   * @param src The source path of the directory to rename.\n+   * @param dst The destination path of the renamed directory.\n+   * @throws IOException If an I/O error occurs during the operation.\n+   */\n+  private void renameDir(AzureBlobFileSystem currentFs, String producerQueueSize,\n+      String consumerMaxLag, String maxThread, Path src, Path dst)\n+      throws IOException {\n+    Configuration config = createConfig(producerQueueSize, consumerMaxLag, maxThread);\n+    try (AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config)) {\n+      fs.rename(src, dst);\n+      validateRename(fs, src, dst, false, true, false);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to create the configuration for the AzureBlobFileSystem.\n+   *\n+   * @param producerQueueSize Maximum size of the producer queue.\n+   * @param consumerMaxLag Maximum lag allowed for the consumer.\n+   * @param maxThread Maximum threads for the rename operation.\n+   * @return The configuration object.\n+   */\n+  private Configuration createConfig(String producerQueueSize, String consumerMaxLag, String maxThread) {\n+    Configuration config = new Configuration(this.getRawConfiguration());\n+    config.set(FS_AZURE_PRODUCER_QUEUE_MAX_SIZE, producerQueueSize);\n+    config.set(FS_AZURE_CONSUMER_MAX_LAG, consumerMaxLag);\n+    config.set(FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD, maxThread);\n+    return config;\n+  }\n+\n+  /**\n+   * Helper method to validate that the rename was successful and that the destination exists.\n+   *\n+   * @param fs The AzureBlobFileSystem instance to check the existence on.\n+   * @param dst The destination path.\n+   * @param src The source path.\n+   * @throws IOException If an I/O error occurs during the validation.\n+   */\n+  private void validateRename(AzureBlobFileSystem fs, Path src, Path dst,\n+      boolean isSrcExist, boolean isDstExist, boolean isJsonExist)\n+      throws IOException {\n+    Assertions.assertThat(fs.exists(dst))\n+        .describedAs(\"Renamed Destination directory should exist.\")\n+        .isEqualTo(isDstExist);\n+    Assertions.assertThat(fs.exists(new Path(src.getParent(), src.getName() + SUFFIX)))\n+        .describedAs(\"Renamed Pending Json file should exist.\")\n+        .isEqualTo(isJsonExist);\n+    Assertions.assertThat(fs.exists(src))\n+        .describedAs(\"Renamed Destination directory should exist.\")\n+        .isEqualTo(isSrcExist);\n+  }\n+\n+  /**\n+   * Test the renaming of a directory with different parallelism configurations.\n+   */\n+  @Test\n+  public void testRenameDirWithDifferentParallelism() throws Exception {\n+    try (AzureBlobFileSystem currentFs = getFileSystem()) {\n+      assumeBlobServiceType();\n+      Path src = new Path(\"/hbase/A1/A2\");\n+      Path dst = new Path(\"/hbase/A1/A3\");\n+\n+      // Create sample files in the source directory\n+      createFiles(currentFs, src, TOTAL_FILES);\n+\n+      // Test renaming with different configurations\n+      renameDir(currentFs, \"10\", \"5\", \"2\", src, dst);\n+      renameDir(currentFs, \"100\", \"5\", \"2\", dst, src);\n+\n+      String errorMessage = intercept(PathIOException.class,\n+          () -> renameDir(currentFs, \"50\", \"50\", \"5\", src, dst))\n+          .getMessage();\n+\n+      // Validate error message for invalid configuration\n+      Assertions.assertThat(errorMessage)\n+          .describedAs(\"maxConsumptionLag should be lesser than maxSize\")\n+          .contains(\n+              \"Invalid configuration value detected for \\\"fs.azure.blob.dir.list.consumer.max.lag\\\". \"\n+                  + \"maxConsumptionLag should be lesser than maxSize\");\n+    }\n+  }\n+\n+  /**\n+   * Helper method to create files in the given directory.\n+   *\n+   * @param fs The AzureBlobFileSystem instance to use for file creation.\n+   * @param src The source path (directory).\n+   * @param numFiles The number of files to create.\n+   * @throws ExecutionException, InterruptedException If an error occurs during file creation.\n+   */\n+  private void createFiles(AzureBlobFileSystem fs, Path src, int numFiles)\n+      throws ExecutionException, InterruptedException {\n+    ExecutorService executorService = Executors.newFixedThreadPool(TOTAL_THREADS_IN_POOL);\n+    List<Future> futures = new ArrayList<>();\n+    for (int i = 0; i < numFiles; i++) {\n+      final int iter = i;\n+      Future future = executorService.submit(() ->\n+          fs.create(new Path(src, \"file\" + iter + \".txt\")));\n+      futures.add(future);\n+    }\n+    for (Future future : futures) {\n+      future.get();\n+    }\n+    executorService.shutdown();\n+  }\n+\n+  /**\n+   * Tests renaming a directory with a failure during the copy operation.\n+   * Simulates an error when copying on the 6th call.\n+   */\n+  @Test\n+  public void testRenameCopyFailureInBetween() throws Exception {\n+    try (AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem(\n+        createConfig(\"5\", \"3\", \"2\")))) {\n+      assumeBlobServiceType();\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+      fs.getAbfsStore().setClient(client);\n+      Path src = new Path(\"/hbase/A1/A2\");\n+      Path dst = new Path(\"/hbase/A1/A3\");\n+\n+      // Create sample files in the source directory\n+      createFiles(fs, src, TOTAL_FILES);\n+\n+      // Track the number of copy operations\n+      AtomicInteger copyCall = new AtomicInteger(0);\n+      Mockito.doAnswer(copyRequest -> {\n+        if (copyCall.get() == FAILED_CALL) {\n+          throw new AbfsRestOperationException(\n+              BLOB_ALREADY_EXISTS.getStatusCode(),\n+              BLOB_ALREADY_EXISTS.getErrorCode(),\n+              BLOB_ALREADY_EXISTS.getErrorMessage(),\n+              new Exception());\n+        }\n+        copyCall.incrementAndGet();\n+        return copyRequest.callRealMethod();\n+      }).when(client).copyBlob(Mockito.any(Path.class),\n+          Mockito.any(Path.class), Mockito.nullable(String.class),\n+          Mockito.any(TracingContext.class));\n+\n+      fs.rename(src, dst);\n+      // Validate copy operation count\n+      Assertions.assertThat(copyCall.get())\n+          .describedAs(\"Copy operation count should be less than 10.\")\n+          .isLessThan(TOTAL_FILES);\n+\n+      // Validate that rename redo operation was triggered\n+      copyCall.set(0);\n+\n+      // Assertions to validate renamed destination and source\n+      validateRename(fs, src, dst, false, true, true);\n+\n+      Assertions.assertThat(copyCall.get())\n+          .describedAs(\"Copy operation count should be greater than 0.\")\n+          .isGreaterThan(0);\n+\n+      // Validate final state of destination and source\n+      validateRename(fs, src, dst, false, true, false);\n+    }\n+  }\n+\n+  /**\n+   * Tests renaming a directory with a failure during the delete operation.\n+   * Simulates an error on the 6th delete operation and verifies the behavior.\n+   */\n+  @Test\n+  public void testRenameDeleteFailureInBetween() throws Exception {\n+    try (AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem(\n+        createConfig(\"5\", \"3\", \"2\")))) {\n+      assumeBlobServiceType();\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+      fs.getAbfsStore().setClient(client);\n+      Path src = new Path(\"/hbase/A1/A2\");\n+      Path dst = new Path(\"/hbase/A1/A3\");\n+\n+      // Create sample files in the source directory\n+      createFiles(fs, src, TOTAL_FILES);\n+\n+      // Track the number of delete operations\n+      AtomicInteger deleteCall = new AtomicInteger(0);\n+      Mockito.doAnswer(deleteRequest -> {\n+        if (deleteCall.get() == FAILED_CALL) {\n+          throw new AbfsRestOperationException(\n+              BLOB_PATH_NOT_FOUND.getStatusCode(),\n+              BLOB_PATH_NOT_FOUND.getErrorCode(),\n+              BLOB_PATH_NOT_FOUND.getErrorMessage(),\n+              new Exception());\n+        }\n+        deleteCall.incrementAndGet();\n+        return deleteRequest.callRealMethod();\n+      }).when(client).deleteBlobPath(Mockito.any(Path.class),\n+          Mockito.anyString(), Mockito.any(TracingContext.class));\n+\n+      fs.rename(src, dst);\n+\n+      // Validate delete operation count\n+      Assertions.assertThat(deleteCall.get())\n+          .describedAs(\"Delete operation count should be less than 10.\")\n+          .isLessThan(TOTAL_FILES);\n+\n+      // Validate that delete redo operation was triggered\n+      deleteCall.set(0);\n+      // Assertions to validate renamed destination and source\n+      validateRename(fs, src, dst, false, true, true);\n+\n+      Assertions.assertThat(deleteCall.get())\n+          .describedAs(\"Delete operation count should be greater than 0.\")\n+          .isGreaterThan(0);\n+\n+      // Validate final state of destination and source\n+      // Validate that delete redo operation was triggered\n+      validateRename(fs, src, dst, false, true, false);\n+    }\n+  }\n+\n+  /**\n+   * Tests renaming a file or directory when the destination path contains\n+   * a colon (\":\"). The test ensures that:\n+   * - The source directory exists before the rename.\n+   * - The file is successfully renamed to the destination path.\n+   * - The old source directory no longer exists after the rename.\n+   * - The new destination directory exists after the rename.\n+   *\n+   * @throws Exception if an error occurs during file system operations\n+   */\n+  @Test\n+  public void testRenameWhenDestinationPathContainsColon() throws Exception {\n+    AzureBlobFileSystem fs = getFileSystem();\n+    fs.setWorkingDirectory(new Path(ROOT_PATH));\n+    String fileName = \"file\";\n+    Path src = new Path(\"/test1/\");\n+    Path dst = new Path(\"/test1:/\");\n+\n+    // Create the file\n+    fs.create(new Path(src, fileName));\n+\n+    // Perform the rename operation and validate the results\n+    performRenameAndValidate(fs, src, dst, fileName);\n+  }\n+\n+  /**\n+   * Performs the rename operation and validates the existence of the directories and files.\n+   *\n+   * @param fs the AzureBlobFileSystem instance\n+   * @param src the source path to be renamed\n+   * @param dst the destination path for the rename\n+   * @param fileName the name of the file to be renamed\n+   */\n+  private void performRenameAndValidate(AzureBlobFileSystem fs, Path src, Path dst, String fileName)\n+      throws IOException {\n+    // Assert the source directory exists\n+    Assertions.assertThat(fs.exists(src))\n+        .describedAs(\"Old directory should exist before rename\")\n+        .isTrue();\n+\n+    // Perform rename\n+    fs.rename(src, dst);\n+\n+    // Assert the destination directory and file exist after rename\n+    Assertions.assertThat(fs.exists(new Path(dst, fileName)))\n+        .describedAs(\"Rename should be successful\")\n+        .isTrue();\n+\n+    // Assert the source directory no longer exists\n+    Assertions.assertThat(fs.exists(src))\n+        .describedAs(\"Old directory should not exist\")\n+        .isFalse();\n+\n+    // Assert the new destination directory exists\n+    Assertions.assertThat(fs.exists(dst))\n+        .describedAs(\"New directory should exist\")\n+        .isTrue();\n+  }\n+\n+  /**\n+   * Tests the behavior of the atomic rename key for the root folder\n+   * in Azure Blob File System. The test verifies that the atomic rename key\n+   * returns false for the root folder path.\n+   *\n+   * @throws Exception if an error occurs during the atomic rename key check\n+   */\n+  @Test\n+  public void testGetAtomicRenameKeyForRootFolder() throws Exception {\n+    AzureBlobFileSystem fs = getFileSystem();\n+    assumeBlobServiceType();\n+    AbfsBlobClient abfsBlobClient = (AbfsBlobClient) fs.getAbfsClient();\n+    Assertions.assertThat(abfsBlobClient.isAtomicRenameKey(\"/hbase\"))\n+        .describedAs(\"Atomic rename key should return false for Root folder\")\n+        .isFalse();\n+  }\n+\n+  /**\n+   * Tests the behavior of the atomic rename key for non-root folders\n+   * in Azure Blob File System. The test verifies that the atomic rename key\n+   * works for specific folders as defined in the configuration.\n+   * It checks the atomic rename key for various paths,\n+   * ensuring it returns true for matching paths and false for others.\n+   *\n+   * @throws Exception if an error occurs during the atomic rename key check\n+   */\n+  @Test\n+  public void testGetAtomicRenameKeyForNonRootFolder() throws Exception {\n+    final AzureBlobFileSystem currentFs = getFileSystem();\n+    Configuration config = new Configuration(this.getRawConfiguration());\n+    config.set(FS_AZURE_ATOMIC_RENAME_KEY, \"/hbase,/a,/b\");\n+\n+    final AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config);\n+    assumeBlobServiceType();\n+    AbfsBlobClient abfsBlobClient = (AbfsBlobClient) fs.getAbfsClient();\n+\n+    // Test for various paths\n+    validateAtomicRenameKey(abfsBlobClient, \"/hbase1/test\", false);\n+    validateAtomicRenameKey(abfsBlobClient, \"/hbase/test\", true);\n+    validateAtomicRenameKey(abfsBlobClient, \"/a/b/c\", true);\n+    validateAtomicRenameKey(abfsBlobClient, \"/test/a\", false);\n+  }\n+\n+  /**\n+   * Validates the atomic rename key for a specific path.\n+   *\n+   * @param abfsBlobClient the AbfsBlobClient instance\n+   * @param path the path to check for atomic rename key\n+   * @param expected the expected value (true or false)\n+   */\n+  private void validateAtomicRenameKey(AbfsBlobClient abfsBlobClient, String path, boolean expected) {\n+    Assertions.assertThat(abfsBlobClient.isAtomicRenameKey(path))\n+        .describedAs(\"Atomic rename key check for path: \" + path)\n+        .isEqualTo(expected);\n+  }\n+\n+  /**\n+   * Helper method to create a json file.\n+   * @param path parent path\n+   * @param renameJson rename json path\n+   * @return file system\n+   * @throws IOException in case of failure\n+   */\n+  public AzureBlobFileSystem createJsonFile(Path path, Path renameJson) throws IOException {\n+    final AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem());\n+    assumeBlobServiceType();\n+    AzureBlobFileSystemStore store = Mockito.spy(fs.getAbfsStore());\n+    Mockito.doReturn(store).when(fs).getAbfsStore();\n+    AbfsClient client = Mockito.spy(store.getClient());\n+    Mockito.doReturn(client).when(store).getClient();\n+\n+    fs.setWorkingDirectory(new Path(ROOT_PATH));\n+    fs.create(new Path(path, \"file.txt\"));\n+\n+    AzureBlobFileSystemStore.VersionedFileStatus fileStatus = (AzureBlobFileSystemStore.VersionedFileStatus) fs.getFileStatus(path);\n+\n+    new RenameAtomicity(path, new Path(\"/hbase/test4\"), renameJson, getTestTracingContext(fs, true), fileStatus.getEtag(), client)\n+        .preRename();\n+\n+    Assertions.assertThat(fs.exists(renameJson))\n+        .describedAs(\"Rename Pending Json file should exist.\")\n+        .isTrue();\n+\n+    return fs;\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery with a single child folder.\n+   *\n+   * This test simulates a scenario where a pending rename JSON file exists for a single child folder\n+   * under the parent directory. It ensures that when listing the files in the parent directory,\n+   * only the child folder (with the pending rename JSON file) is returned, and no additional files are listed.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithSingleChildFolder() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+\n+      FileStatus[] fileStatuses = fs.listStatus(new Path(\"/hbase/A1\"));\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 1 file\")\n+          .isEqualTo(1);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery with multiple child folders.\n+   *\n+   * This test simulates a scenario where a pending rename JSON file exists, and multiple files are\n+   * created in the parent directory. It ensures that when listing the files in the parent directory,\n+   * the correct number of files is returned, including the pending rename JSON file.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithMultipleChildFolder() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+\n+      fs.create(new Path(\"/hbase/A1/file1.txt\"));\n+      fs.create(new Path(\"/hbase/A1/file2.txt\"));\n+\n+      FileStatus[] fileStatuses = fs.listStatus(new Path(\"/hbase/A1\"));\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 3 files\")\n+          .isEqualTo(3);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery with a pending rename JSON file.\n+   *\n+   * This test simulates a scenario where a pending rename JSON file exists in the parent directory,\n+   * and it ensures that after the deletion of the target directory and creation of new files,\n+   * the listing operation correctly returns the remaining files without considering the pending rename.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithPendingJsonFile() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+\n+      fs.delete(path, true);\n+      fs.create(new Path(\"/hbase/A1/file1.txt\"));\n+      fs.create(new Path(\"/hbase/A1/file2.txt\"));\n+\n+      FileStatus[] fileStatuses = fs.listStatus(path.getParent());\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 2 files\")\n+          .isEqualTo(2);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery when no pending rename JSON file exists.\n+   *\n+   * This test simulates a scenario where there is no pending rename JSON file in the directory.\n+   * It ensures that the listing operation correctly returns all files in the parent directory, including\n+   * those created after the rename JSON file is deleted.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithoutAnyPendingJsonFile() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+\n+      fs.delete(renameJson, true);\n+      fs.create(new Path(\"/hbase/A1/file1.txt\"));\n+      fs.create(new Path(\"/hbase/A1/file2.txt\"));\n+\n+      FileStatus[] fileStatuses = fs.listStatus(path.getParent());\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 3 files\")\n+          .isEqualTo(3);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery when a pending rename JSON directory exists.\n+   *\n+   * This test simulates a scenario where a pending rename JSON directory exists, ensuring that the\n+   * listing operation correctly returns all files in the parent directory without triggering a redo\n+   * rename operation. It also checks that the directory with the suffix \"-RenamePending.json\" exists.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithPendingJsonDir() throws Exception {\n+    try (AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem())) {\n+      assumeBlobServiceType();\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs.mkdirs(renameJson);\n+\n+      fs.create(new Path(path.getParent(), \"file1.txt\"));\n+      fs.create(new Path(path, \"file2.txt\"));\n+\n+      AtomicInteger redoRenameCall = new AtomicInteger(0);\n+      Mockito.doAnswer(answer -> {\n+        redoRenameCall.incrementAndGet();\n+        return answer.callRealMethod();\n+      }).when(client).getRedoRenameAtomicity(Mockito.any(Path.class),\n+          Mockito.anyInt(), Mockito.any(TracingContext.class));\n+\n+      FileStatus[] fileStatuses = fs.listStatus(path.getParent());\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 3 files\")\n+          .isEqualTo(3);\n+\n+      Assertions.assertThat(redoRenameCall.get())\n+          .describedAs(\"No redo rename call should be made\")\n+          .isEqualTo(0);\n+\n+      Assertions.assertThat(\n+              Arrays.stream(fileStatuses)\n+                  .anyMatch(status -> renameJson.toUri().getPath().equals(status.getPath().toUri().getPath())))\n+          .describedAs(\"Directory with suffix -RenamePending.json should exist.\")\n+          .isTrue();\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify crash recovery during listing with multiple pending rename JSON files.\n+   *\n+   * This test simulates a scenario where multiple pending rename JSON files exist, ensuring that\n+   * crash recovery properly handles the situation. It verifies that two redo rename calls are made\n+   * and that the list operation returns the correct number of paths.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void listCrashRecoveryWithMultipleJsonFile() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+\n+      // 1st Json file\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+\n+      // 2nd Json file\n+      Path path2 = new Path(\"/hbase/A1/A3\");\n+      fs.create(new Path(path2, \"file3.txt\"));\n+\n+      Path renameJson2 = new Path(path2.getParent(), path2.getName() + SUFFIX);\n+      AzureBlobFileSystemStore.VersionedFileStatus fileStatus = (AzureBlobFileSystemStore.VersionedFileStatus) fs.getFileStatus(path2);\n+\n+      new RenameAtomicity(path2, new Path(\"/hbase/test4\"), renameJson2, getTestTracingContext(fs, true), fileStatus.getEtag(), client).preRename();\n+\n+      fs.create(new Path(path, \"file2.txt\"));\n+\n+      AtomicInteger redoRenameCall = new AtomicInteger(0);\n+      Mockito.doAnswer(answer -> {\n+        redoRenameCall.incrementAndGet();\n+        return answer.callRealMethod();\n+      }).when(client).getRedoRenameAtomicity(Mockito.any(Path.class),\n+          Mockito.anyInt(), Mockito.any(TracingContext.class));\n+\n+      FileStatus[] fileStatuses = fs.listStatus(path.getParent());\n+\n+      Assertions.assertThat(fileStatuses.length)\n+          .describedAs(\"List should return 2 paths\")\n+          .isEqualTo(2);\n+\n+      Assertions.assertThat(redoRenameCall.get())\n+          .describedAs(\"2 redo rename calls should be made\")\n+          .isEqualTo(2);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify path status when a pending rename JSON file exists.\n+   *\n+   * This test simulates a scenario where a rename operation was pending, and ensures that\n+   * the path status retrieval triggers a redo rename operation. The test also checks that\n+   * the correct error code (`PATH_NOT_FOUND`) is returned.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void getPathStatusWithPendingJsonFile() throws Exception {\n+    AzureBlobFileSystem fs = null;\n+    try {\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      Path renameJson = new Path(path.getParent(), path.getName() + SUFFIX);\n+      fs = createJsonFile(path, renameJson);\n+\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+\n+      fs.create(new Path(\"/hbase/A1/file1.txt\"));\n+      fs.create(new Path(\"/hbase/A1/file2.txt\"));\n+\n+      AbfsConfiguration conf = fs.getAbfsStore().getAbfsConfiguration();\n+\n+      AtomicInteger redoRenameCall = new AtomicInteger(0);\n+      Mockito.doAnswer(answer -> {\n+        redoRenameCall.incrementAndGet();\n+        return answer.callRealMethod();\n+      }).when(client).getRedoRenameAtomicity(Mockito.any(Path.class),\n+          Mockito.anyInt(), Mockito.any(TracingContext.class));\n+\n+      TracingContext tracingContext = new TracingContext(\n+          conf.getClientCorrelationId(), fs.getFileSystemId(),\n+          FSOperationType.GET_FILESTATUS, TracingHeaderFormat.ALL_ID_FORMAT, null);\n+\n+      AzureServiceErrorCode azureServiceErrorCode = intercept(\n+          AbfsRestOperationException.class, () -> client.getPathStatus(\n+              path.toUri().getPath(), true,\n+              tracingContext, null)).getErrorCode();\n+\n+      Assertions.assertThat(azureServiceErrorCode.getErrorCode())\n+          .describedAs(\"Path had to be recovered from atomic rename operation.\")\n+          .isEqualTo(PATH_NOT_FOUND.getErrorCode());\n+\n+      Assertions.assertThat(redoRenameCall.get())\n+          .describedAs(\"There should be one redo rename call\")\n+          .isEqualTo(1);\n+    } finally {\n+      if (fs != null) {\n+        fs.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify path status when there is no pending rename JSON file.\n+   *\n+   * This test ensures that when no rename pending JSON file is present, the path status is\n+   * successfully retrieved, the ETag is present, and no redo rename operation is triggered.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void getPathStatusWithoutPendingJsonFile() throws Exception {\n+    try (AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem())) {\n+      assumeBlobServiceType();\n+\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+\n+      fs.create(new Path(path, \"file1.txt\"));\n+      fs.create(new Path(path, \"file2.txt\"));\n+\n+      AbfsConfiguration conf = fs.getAbfsStore().getAbfsConfiguration();\n+\n+      AtomicInteger redoRenameCall = new AtomicInteger(0);\n+      Mockito.doAnswer(answer -> {\n+        redoRenameCall.incrementAndGet();\n+        return answer.callRealMethod();\n+      }).when(client).getRedoRenameAtomicity(\n+          Mockito.any(Path.class), Mockito.anyInt(),\n+          Mockito.any(TracingContext.class));\n+\n+      TracingContext tracingContext = new TracingContext(\n+          conf.getClientCorrelationId(), fs.getFileSystemId(),\n+          FSOperationType.GET_FILESTATUS, TracingHeaderFormat.ALL_ID_FORMAT,\n+          null);\n+\n+      AbfsHttpOperation abfsHttpOperation = client.getPathStatus(\n+          path.toUri().getPath(), true,\n+          tracingContext, null).getResult();\n+\n+      Assertions.assertThat(abfsHttpOperation.getStatusCode())\n+          .describedAs(\"Path should be found.\")\n+          .isEqualTo(HTTP_OK);\n+\n+      Assertions.assertThat(extractEtagHeader(abfsHttpOperation))\n+          .describedAs(\"Etag should be present.\")\n+          .isNotNull();\n+\n+      Assertions.assertThat(redoRenameCall.get())\n+          .describedAs(\"There should be no redo rename call.\")\n+          .isEqualTo(0);\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify path status when there is a pending rename JSON directory.\n+   *\n+   * This test simulates the scenario where a directory is created with a rename pending JSON\n+   * file (indicated by a specific suffix). It ensures that the path is found, the ETag is present,\n+   * and no redo rename operation is triggered. It also verifies that the rename pending directory\n+   * exists.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void getPathStatusWithPendingJsonDir() throws Exception {\n+    try (AzureBlobFileSystem fs = Mockito.spy(this.getFileSystem())) {\n+      assumeBlobServiceType();\n+\n+      Path path = new Path(\"/hbase/A1/A2\");\n+      AbfsBlobClient client = (AbfsBlobClient) addSpyHooksOnClient(fs);\n+\n+      fs.create(new Path(path, \"file1.txt\"));\n+      fs.create(new Path(path, \"file2.txt\"));\n+\n+      fs.mkdirs(new Path(path.getParent(), path.getName() + SUFFIX));\n+\n+      AbfsConfiguration conf = fs.getAbfsStore().getAbfsConfiguration();\n+\n+      AtomicInteger redoRenameCall = new AtomicInteger(0);\n+      Mockito.doAnswer(answer -> {\n+        redoRenameCall.incrementAndGet();\n+        return answer.callRealMethod();\n+      }).when(client).getRedoRenameAtomicity(Mockito.any(Path.class),\n+          Mockito.anyInt(), Mockito.any(TracingContext.class));\n+\n+      TracingContext tracingContext = new TracingContext(\n+          conf.getClientCorrelationId(), fs.getFileSystemId(),\n+          FSOperationType.GET_FILESTATUS, TracingHeaderFormat.ALL_ID_FORMAT, null);\n+\n+      AbfsHttpOperation abfsHttpOperation = client.getPathStatus(path.toUri().getPath(), true, tracingContext, null).getResult();\n+\n+      Assertions.assertThat(abfsHttpOperation.getStatusCode())\n+          .describedAs(\"Path should be found.\")\n+          .isEqualTo(HTTP_OK);\n+\n+      Assertions.assertThat(extractEtagHeader(abfsHttpOperation))\n+          .describedAs(\"Etag should be present.\")\n+          .isNotNull();\n+\n+      Assertions.assertThat(redoRenameCall.get())\n+          .describedAs(\"There should be no redo rename call.\")\n+          .isEqualTo(0);\n+\n+      Assertions.assertThat(fs.exists(new Path(path.getParent(), path.getName() + SUFFIX)))\n+          .describedAs(\"Directory with suffix -RenamePending.json should exist.\")\n+          .isTrue();\n+    }\n+  }\n+\n+  /**\n+   * Test case to verify the behavior when the ETag of a file changes during a rename operation.\n+   *\n+   * This test simulates a scenario where the ETag of a file changes after the creation of a\n+   * rename pending JSON file. The steps include:\n+   * - Creating a rename pending JSON file with an old ETag.\n+   * - Deleting the original directory for an ETag change.\n+   * - Creating new files in the directory.\n+   * - Verifying that the copy blob call is not triggered.\n+   * - Verifying that the rename atomicity operation is called once.\n+   *\n+   * The test ensures that the system correctly handles the ETag change during the rename process.\n+   *\n+   * @throws Exception if any error occurs during the test execution\n+   */\n+  @Test\n+  public void eTagChangedDuringRename() throws Exception {\n\nReview Comment:\n   Added test wherever required.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -1655,6 +1730,827 @@ public void testRenameSrcDirDeleteEmitDeletionCountInClientRequestId()\n     fs.rename(new Path(dirPathStr), new Path(\"/dst/\"));\n   }\n \n+  /**\n+   * Helper method to configure the AzureBlobFileSystem and rename directories.\n+   *\n+   * @param currentFs The current AzureBlobFileSystem to use for renaming.\n+   * @param producerQueueSize Maximum size of the producer queue.\n+   * @param consumerMaxLag Maximum lag allowed for the consumer.\n+   * @param maxThread Maximum threads for the rename operation.\n+   * @param src The source path of the directory to rename.\n+   * @param dst The destination path of the renamed directory.\n+   * @throws IOException If an I/O error occurs during the operation.\n+   */\n+  private void renameDir(AzureBlobFileSystem currentFs, String producerQueueSize,\n+      String consumerMaxLag, String maxThread, Path src, Path dst)\n+      throws IOException {\n+    Configuration config = createConfig(producerQueueSize, consumerMaxLag, maxThread);\n+    try (AzureBlobFileSystem fs = (AzureBlobFileSystem) FileSystem.newInstance(currentFs.getUri(), config)) {\n+      fs.rename(src, dst);\n+      validateRename(fs, src, dst, false, true, false);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to create the configuration for the AzureBlobFileSystem.\n+   *\n+   * @param producerQueueSize Maximum size of the producer queue.\n+   * @param consumerMaxLag Maximum lag allowed for the consumer.\n+   * @param maxThread Maximum threads for the rename operation.\n+   * @return The configuration object.\n+   */\n+  private Configuration createConfig(String producerQueueSize, String consumerMaxLag, String maxThread) {\n+    Configuration config = new Configuration(this.getRawConfiguration());\n+    config.set(FS_AZURE_PRODUCER_QUEUE_MAX_SIZE, producerQueueSize);\n+    config.set(FS_AZURE_CONSUMER_MAX_LAG, consumerMaxLag);\n+    config.set(FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD, maxThread);\n+    return config;\n+  }\n+\n+  /**\n+   * Helper method to validate that the rename was successful and that the destination exists.\n+   *\n+   * @param fs The AzureBlobFileSystem instance to check the existence on.\n+   * @param dst The destination path.\n+   * @param src The source path.\n+   * @throws IOException If an I/O error occurs during the validation.\n+   */\n+  private void validateRename(AzureBlobFileSystem fs, Path src, Path dst,\n+      boolean isSrcExist, boolean isDstExist, boolean isJsonExist)\n+      throws IOException {\n+    Assertions.assertThat(fs.exists(dst))\n+        .describedAs(\"Renamed Destination directory should exist.\")\n+        .isEqualTo(isDstExist);\n+    Assertions.assertThat(fs.exists(new Path(src.getParent(), src.getName() + SUFFIX)))\n+        .describedAs(\"Renamed Pending Json file should exist.\")\n+        .isEqualTo(isJsonExist);\n+    Assertions.assertThat(fs.exists(src))\n+        .describedAs(\"Renamed Destination directory should exist.\")\n+        .isEqualTo(isSrcExist);\n+  }\n+\n+  /**\n+   * Test the renaming of a directory with different parallelism configurations.\n+   */\n+  @Test\n+  public void testRenameDirWithDifferentParallelism() throws Exception {\n\nReview Comment:\n   Taken!\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-04T10:00:21.896+0000", "updated": "2025-03-04T10:00:21.896+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17932233", "id": "17932233", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1979066104\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -304,12 +324,67 @@ public void testRenameNotFoundBlobToEmptyRoot() throws Exception {\n    *\n    * @throws Exception if an error occurs during test execution\n    */\n-  @Test(expected = IOException.class)\n-  public void testRenameBlobToDstWithColonInPath() throws Exception {\n+  @Test\n+  public void testRenameBlobToDstWithColonInSourcePath() throws Exception {\n     AzureBlobFileSystem fs = getFileSystem();\n     assumeBlobServiceType();\n+    fs.create(new Path(\"/src:/file\"));\n+    Assertions.assertThat(\n+        fs.rename(new Path(\"/src:\"),\n+            new Path(\"/dst\"))\n+    ).isTrue();\n+  }\n+\n+  /**\n+   * Tests renaming a source path to a destination path that contains a colon in the path.\n+   * This verifies that the rename operation handles paths with special characters like a colon.\n+   *\n+   * The test creates a source directory and renames it to a destination path that includes a colon,\n+   * ensuring that the operation succeeds without errors.\n+   *\n+   * @throws Exception if an error occurs during test execution\n+   */\n+  @Test\n+  public void testRenameWithColonInDestinationPath() throws Exception {\n+    AzureBlobFileSystem fs = getFileSystem();\n     fs.create(new Path(\"/src\"));\n-    fs.rename(new Path(\"/src\"), new Path(\"/dst:file\"));\n+    Assertions.assertThat(\n\nReview Comment:\n   Done!\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -304,12 +324,67 @@ public void testRenameNotFoundBlobToEmptyRoot() throws Exception {\n    *\n    * @throws Exception if an error occurs during test execution\n    */\n-  @Test(expected = IOException.class)\n-  public void testRenameBlobToDstWithColonInPath() throws Exception {\n+  @Test\n+  public void testRenameBlobToDstWithColonInSourcePath() throws Exception {\n     AzureBlobFileSystem fs = getFileSystem();\n     assumeBlobServiceType();\n+    fs.create(new Path(\"/src:/file\"));\n+    Assertions.assertThat(\n\nReview Comment:\n   Done\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-04T10:00:41.443+0000", "updated": "2025-03-04T10:00:41.443+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17932234", "id": "17932234", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1979066765\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java:\n##########\n@@ -304,12 +324,67 @@ public void testRenameNotFoundBlobToEmptyRoot() throws Exception {\n    *\n    * @throws Exception if an error occurs during test execution\n    */\n-  @Test(expected = IOException.class)\n-  public void testRenameBlobToDstWithColonInPath() throws Exception {\n+  @Test\n+  public void testRenameBlobToDstWithColonInSourcePath() throws Exception {\n     AzureBlobFileSystem fs = getFileSystem();\n     assumeBlobServiceType();\n\nReview Comment:\n   Make sense, removed this assumption.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-04T10:01:06.406+0000", "updated": "2025-03-04T10:01:06.406+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17932390", "id": "17932390", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#discussion_r1979924494\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ListActionTaker.java:\n##########\n@@ -261,7 +269,7 @@ protected String listAndEnqueue(final ListBlobQueue listBlobQueue,\n   protected void addPaths(final List<Path> paths,\n       final ListResultSchema retrievedSchema) {\n     for (ListResultEntrySchema entry : retrievedSchema.paths()) {\n-      Path entryPath = new Path(ROOT_PATH, entry.name());\n+      Path entryPath = new Path(ROOT_PATH + entry.name());\n\nReview Comment:\n   As discussed offline, this change is needed because of new Path() behavior in hadoop common.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-04T17:39:09.584+0000", "updated": "2025-03-04T17:39:09.584+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17932417", "id": "17932417", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2698755333\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  41m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m  2s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  39m 24s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 45s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 59s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 36s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 133m 46s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7386 |\r\n   | JIRA Issue | HADOOP-19445 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 229d2656b14a 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / da81cc8aa43136a11e55781e70449b6864591275 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/6/testReport/ |\r\n   | Max. process+thread count | 532 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7386/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-04T19:53:00.798+0000", "updated": "2025-03-04T19:53:00.798+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17932500", "id": "17932500", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386#issuecomment-2699762218\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 788, Failures: 0, Errors: 0, Skipped: 157\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 791, Failures: 0, Errors: 0, Skipped: 111\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 630, Failures: 0, Errors: 0, Skipped: 213\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 788, Failures: 0, Errors: 0, Skipped: 162\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 2\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 633, Failures: 0, Errors: 0, Skipped: 142\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 627, Failures: 0, Errors: 0, Skipped: 214\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 630, Failures: 0, Errors: 0, Skipped: 143\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 628, Failures: 0, Errors: 0, Skipped: 160\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 662, Failures: 0, Errors: 0, Skipped: 160\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 627, Failures: 0, Errors: 0, Skipped: 212\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 264, Failures: 0, Errors: 0, Skipped: 24\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-05T04:13:46.251+0000", "updated": "2025-03-05T04:13:46.251+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17932520", "id": "17932520", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7386:\nURL: https://github.com/apache/hadoop/pull/7386\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-05T06:08:43.845+0000", "updated": "2025-03-05T06:08:43.845+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17937053", "id": "17937053", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 opened a new pull request, #7525:\nURL: https://github.com/apache/hadoop/pull/7525\n\n   Jira: https://issues.apache.org/jira/browse/HADOOP-19445\r\n   \r\n   We have identified a few scenarios worth adding integration or mocked behavior tests for while implementing FNS Support over Blob Endpoint. So, we have added test cases for all those scenarios in this PR.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-20T08:51:10.838+0000", "updated": "2025-03-20T08:51:10.838+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17937089", "id": "17937089", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7525:\nURL: https://github.com/apache/hadoop/pull/7525#issuecomment-2740038349\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  18m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 21s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |   0m 40s |  |  branch-3.4 passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  branch-3.4 passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  branch-3.4 passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  branch-3.4 passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 12s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  6s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m  5s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 23s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 146m 55s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7525/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7525 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 82242d618577 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / 51ba347563440a09cb0e0ffb76d217816e1e7aa2 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7525/1/testReport/ |\r\n   | Max. process+thread count | 596 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7525/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-20T11:19:27.375+0000", "updated": "2025-03-20T11:19:27.375+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17937103", "id": "17937103", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7525:\nURL: https://github.com/apache/hadoop/pull/7525\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-20T12:01:32.135+0000", "updated": "2025-03-20T12:01:32.135+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13607268/comment/17937129", "id": "17937129", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on PR #7525:\nURL: https://github.com/apache/hadoop/pull/7525#issuecomment-2740657264\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 800, Failures: 0, Errors: 0, Skipped: 169\r\n   [WARNING] Tests run: 171, Failures: 0, Errors: 0, Skipped: 25\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 800, Failures: 0, Errors: 0, Skipped: 120\r\n   [WARNING] Tests run: 171, Failures: 0, Errors: 0, Skipped: 25\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 784, Failures: 0, Errors: 0, Skipped: 367\r\n   [WARNING] Tests run: 171, Failures: 0, Errors: 0, Skipped: 27\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 800, Failures: 0, Errors: 0, Skipped: 174\r\n   [WARNING] Tests run: 171, Failures: 0, Errors: 0, Skipped: 49\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 784, Failures: 0, Errors: 0, Skipped: 293\r\n   [WARNING] Tests run: 171, Failures: 0, Errors: 0, Skipped: 28\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 784, Failures: 0, Errors: 0, Skipped: 371\r\n   [WARNING] Tests run: 171, Failures: 0, Errors: 0, Skipped: 27\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 784, Failures: 0, Errors: 0, Skipped: 297\r\n   [WARNING] Tests run: 171, Failures: 0, Errors: 0, Skipped: 28\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 784, Failures: 0, Errors: 0, Skipped: 316\r\n   [WARNING] Tests run: 171, Failures: 0, Errors: 0, Skipped: 52\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 800, Failures: 0, Errors: 0, Skipped: 298\r\n   [WARNING] Tests run: 171, Failures: 0, Errors: 0, Skipped: 25\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 784, Failures: 0, Errors: 0, Skipped: 369\r\n   [WARNING] Tests run: 171, Failures: 0, Errors: 0, Skipped: 27\r\n   [WARNING] Tests run: 262, Failures: 0, Errors: 0, Skipped: 24\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-20T14:29:57.142+0000", "updated": "2025-03-20T14:29:57.142+0000"}], "maxResults": 28, "total": 28, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/5", "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png", "name": "Resolved", "id": "5", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3", "id": 3, "key": "done", "colorName": "green", "name": "Done"}}}}