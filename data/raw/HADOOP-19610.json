{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13622811", "self": "https://issues.apache.org/jira/rest/api/2/issue/13622811", "key": "HADOOP-19610", "fields": {"summary": "S3A: ITests to run under JUnit5", "description": "hadoop-aws tests which need to be parameterized on a class level\r\nare configured to do so through the @ParameterizedClass tag.\r\nFilesystem contract test suites in hadoop-common have\r\nalso been parameterized as appropriate.\r\n\r\nThere are custom JUnit tags declared in org.apache.hadoop.test.tags,\r\nwhich add tag strings to test suites/cases declaring them.\r\nThey can be used on the command line and in IDEs to control\r\nwhich tests are/are not executed.\r\n\r\n@FlakyTest \"flaky\"\r\n@LoadTest \"load\"\r\n@RootFilesystemTest \"rootfilesystem\"\r\n@ScaleTest \"scale\"\r\n\r\nFor anyone migrating tests to JUnit 5\r\n* Methods which subclass an existing test case MUST declare the @Test\r\n  tag again -it is no longer inherited.\r\n* All overridden setup/teardown methods MUST be located and\r\n  @BeforeEach/@AfterEach attribute added respectively\r\n* Subclasses of a parameterized test suite MUST redeclare themselves\r\n  as a @ParameterizedClass, and the binding mechanism again.\r\n* Parameterized test suites SHOULD declare a pattern to generate an\r\n  informative parameter value string for logs, IDEs and stack traces, e.g.\r\n  @ParameterizedClass(name=\"performance-{0}\")\r\n* Test suites SHOULD add a org.apache.hadoop.test.tags tag to\r\n  declare what kind of test it is. These tags are inherited, so it\r\n  may be that only shared superclasses of test suites need to be tagged.\r\n  The abstract filesystem contract tests are NOT declared as integration\r\n  tests -implementations MUST do so if they are integration tests.\r\n", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org", "name": "stevel@apache.org", "key": "stevel@apache.org", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"}, "displayName": "Steve Loughran", "active": true, "timeZone": "Europe/London"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18003777", "id": "18003777", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org", "name": "stevel@apache.org", "key": "stevel@apache.org", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"}, "displayName": "Steve Loughran", "active": true, "timeZone": "Europe/London"}, "body": "Looks mostly test setup; some parameterization.\r\n\r\nPlan:\r\n* move to ParameterizedClass\r\n* fix tests which fail\r\n\r\n{code}\r\n[ERROR] Failures: \r\n[ERROR]   ITestS3AContractBulkDelete>AbstractContractBulkDeleteTest.testBulkDeleteParentDirectoryWithDirectories:241 [Parent non empty directory should not be deleted] \r\nExpected size:<1> but was:<0> in:\r\n<[]>\r\n[ERROR]   ITestS3AContractBulkDelete.testBulkDeleteZeroPageSizePrecondition:134 Expected a java.lang.IllegalArgumentException to be thrown, but got the result: : org.apache.hadoop.fs.s3a.impl.BulkDeleteOperation@6d3a56ea\r\n[ERROR]   ITestS3ACopyFromLocalFile.testOptionPropagation:88 [path capability of fs.s3a.optimized.copy.from.local.enabled] \r\nExpecting:\r\n <false>\r\nto be equal to:\r\n <true>\r\nbut was not.\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testDeleteNonExistentFile:755 Doesn't exist ==> expected: <false> but was: <true>\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGetFileStatusThrowsExceptionForNonExistentFile:267 Should throw FileNotFoundException\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGlobStatusFilterWithEmptyPathResults:506 expected: <0> but was: <3>\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGlobStatusFilterWithSomePathMatchesAndTrivialFilter:528 expected: <3> but was: <5>\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGlobStatusWithMultipleWildCardMatches:457 expected: <4> but was: <9>\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGlobStatusWithNoMatchesInPath:412 expected: <0> but was: <3>\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testListStatus:318 expected: <1> but was: <4>\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testListStatusFilterWithNoMatches:353 expected: <0> but was: <2>\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testListStatusThrowsExceptionForNonExistentFile:278 Should throw FileNotFoundException\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testMkdirs:210 expected: <false> but was: <true>\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testMkdirsFailsForSubdirectoryOfExistingFile:236 expected: <false> but was: <true>\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testWriteInNonExistentDirectory:744 Parent doesn't exist ==> expected: <false> but was: <true>\r\n[ERROR]   ITestS3AMiscOperationCost.testGetContentMissingPath:167->AbstractS3ACostTest.verifyMetricsIntercepting:299->Assertions.assertEquals:664 operation returning java.io.FileNotFoundException: No such file or directory: s3a://stevel-london/job-00-fork-0001/test/testGetContentMissingPath: audit_span_creation ==> expected: <1> but was: <0>\r\n[ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryDir:145->AbstractS3ACostTest.verifyMetrics:276->Assertions.assertEquals:664 operation returning         none             inf            none             inf            2            1                  0 : audit_span_creation ==> expected: <1> but was: <0>\r\n[ERROR]   ITestS3AMiscOperationCost.testMkdirOverDir:112->AbstractS3ACostTest.verifyMetrics:276->Assertions.assertEquals:664 operation returning true: audit_span_creation ==> expected: <1> but was: <0>\r\n[ERROR] Errors: \r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractSeek>AbstractContractSeekTest.setup:59->AbstractFSContractTestBase.setup:180->createConfiguration:127 \u00bb IllegalArgument The value of property fs.s3a.experimental.input.fadvise must not be null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testAllRangesMergedIntoOne:271->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testAllRangesMergedIntoOne:271->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testConsecutiveRanges:399->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testConsecutiveRanges:399->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testDisjointRanges:251->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testDisjointRanges:251->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testEOFRanges:435->AbstractFSContractTestBase.isSupported:144 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testEOFRanges:435->AbstractFSContractTestBase.isSupported:144 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testEOFRanges416Handling:118->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testEOFRanges416Handling:118->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testEmptyRanges:411->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testEmptyRanges:411->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMinSeekAndMaxSizeConfigsPropagation:160 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMinSeekAndMaxSizeConfigsPropagation:160 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMinSeekAndMaxSizeDefaultValues:186 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMinSeekAndMaxSizeDefaultValues:186 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMultiVectoredReadStatsCollection:371->getTestFileSystemWithReadAheadDisabled:439 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testMultiVectoredReadStatsCollection:371->getTestFileSystemWithReadAheadDisabled:439 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testMultipleVectoredReads:541->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testMultipleVectoredReads:541->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNegativeLengthRange:478->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNegativeLengthRange:478->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNegativeOffsetRange:485->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNegativeOffsetRange:485->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNormalReadAfterVectoredRead:504->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNormalReadAfterVectoredRead:504->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testNormalReadVsVectoredReadStatsCollection:250->getTestFileSystemWithReadAheadDisabled:439 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testNormalReadVsVectoredReadStatsCollection:250->getTestFileSystemWithReadAheadDisabled:439 NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.getConf()\" because the return value of \"org.apache.hadoop.fs.contract.s3a.ITestS3AContractVectoredRead.getFileSystem()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullRange:358->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullRange:358->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullRangeList:369->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullRangeList:369->AbstractContractVectoredReadTest.verifyExceptionalVectoredRead:655->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullReleaseOperation:493->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testNullReleaseOperation:493->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testOverlappingRanges:313->AbstractFSContractTestBase.isSupported:144 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testOverlappingRanges:313->AbstractFSContractTestBase.isSupported:144 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSameRanges:334->AbstractFSContractTestBase.isSupported:144 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSameRanges:334->AbstractFSContractTestBase.isSupported:144 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSomeRandomNonOverlappingRanges:383->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSomeRandomNonOverlappingRanges:383->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSomeRangesMergedSomeUnmerged:293->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testSomeRangesMergedSomeUnmerged:293->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testStopVectoredIoOperationsCloseStream:207->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testStopVectoredIoOperationsCloseStream:207->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testStopVectoredIoOperationsUnbuffer:230->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead.testStopVectoredIoOperationsUnbuffer:230->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredIOEndToEnd:570->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredIOEndToEnd:570->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAfterNormalRead:522->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAfterNormalRead:522->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:210->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadAndReadFully:210->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:189->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadMultipleRanges:189->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadWholeFile:229->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadWholeFile:229->AbstractContractVectoredReadTest.openVectorFile:162->AbstractContractVectoredReadTest.openVectorFile:173 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.FileSystem.openFile(org.apache.hadoop.fs.Path)\" because \"fs\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadWholeFilePlusOne:451->AbstractFSContractTestBase.isSupported:144 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AContractVectoredRead>AbstractContractVectoredReadTest.testVectoredReadWholeFilePlusOne:451->AbstractFSContractTestBase.isSupported:144 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.isSupported(String, boolean)\" because \"this.contract\" is null\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testDeleteEmptyDirectory:793 \u00bb PathIsNotEmptyDirectory `s3a://stevel-london/job-00-fork-0005/test/test/hadoop': Directory is not empty\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testDeleteRecursively:765->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testGetWrappedInputStream:1120->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testInputStreamClosedTwice:1098->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameDirectoryAsNonEmptyDirectory:1048->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/dir/file1 already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameDirectoryAsNonExistentDirectory:990->FSMainOperationsBaseTest.doTestRenameDirectoryAsNonExistentDirectory:1007->FSMainOperationsBaseTest.rename:1159 \u00bb FileAlreadyExists rename destination s3a://stevel-london/job-00-fork-0005/test/test/new/newdir already exists.\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameFileAsExistingDirectory:920->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameFileAsExistingFile:899->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameFileToDestinationWithParentFile:846->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameFileToExistingParent:868->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testRenameFileToItself:878->FSMainOperationsBaseTest.createFile:1152->FileSystemTestHelper.createFile:158->FileSystemTestHelper.createFile:130->FileSystemTestHelper.createFile:137 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testWriteReadAndDeleteEmptyFile:660->FSMainOperationsBaseTest.writeReadAndDelete:691 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testWriteReadAndDeleteHalfABlock:665->FSMainOperationsBaseTest.writeReadAndDelete:691 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3AFSMainOperations>FSMainOperationsBaseTest.testWriteReadAndDeleteOneBlock:670->FSMainOperationsBaseTest.writeReadAndDelete:691 \u00bb FileAlreadyExists s3a://stevel-london/job-00-fork-0005/test/test/hadoop/file already exists\r\n[ERROR]   ITestS3APrefetchingLruEviction>AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:89 \u00bb NumberFormat Cannot parse null string\r\n[ERROR]   ITestS3APrefetchingLruEviction>AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:89 \u00bb NumberFormat Cannot parse null string\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3AStorageClass>AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:77 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestUploadRecovery.setup:159->AbstractS3ACostTest.setup:105->AbstractS3ATestBase.setup:109->AbstractFSContractTestBase.setup:180->createConfiguration:136 \u00bb IllegalArgument The value of property fs.s3a.fast.upload.buffer must not be null\r\n[ERROR]   ITestS3ACommitterMRJob.test_200_execute:282 \u00bb NullPointer Cannot read the array length because \"blkLocations\" is null\r\n[ERROR]   ITestS3ACommitterMRJob.test_200_execute:282 \u00bb NullPointer Cannot read the array length because \"blkLocations\" is null\r\n[ERROR]   ITestS3ACommitterMRJob.test_200_execute:282 \u00bb NullPointer Cannot read the array length because \"blkLocations\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testAMWorkflow:1475->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testAbortJobNoWorkDone:1188->AbstractITCommitProtocol.executeWork:608->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testAbortJobNotTask:1289->AbstractITCommitProtocol.executeWork:608->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testAbortTaskNoWorkDone:1181->AbstractITCommitProtocol.executeWork:608->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testAbortTaskThenJob:1213->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitJobButNotTask:1195->AbstractITCommitProtocol.executeWork:608->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitLifecycle:820->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitWithStorageClassConfig:864->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol.testCommitterCleanup:224->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitterWithDuplicatedCommit:898->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitterWithFailure:1003->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testCommitterWithNoOutputs:1074->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testConcurrentCommitTaskWithSubDir:1318->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testFailAbort:1256->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testMapFileOutputCommitter:1095->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testOutputFormatIntegration:1409->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testParallelJobsToAdjacentPaths:1504->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testParallelJobsToSameDestination:1586->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testRecoveryAndCleanup:640->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testRequirePropagatedUUID:1784->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testS3ACommitterFactoryBinding:1829->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testSelfGeneratedUUID:1730->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestMagicCommitProtocol>AbstractITCommitProtocol.testTwoTaskAttemptsCommit:936->AbstractITCommitProtocol.startJob:480->AbstractITCommitProtocol.startJob:498->AbstractITCommitProtocol.newJob:444->AbstractS3ATestBase.getConfiguration:154 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getConf()\" because the return value of \"org.apache.hadoop.fs.s3a.AbstractS3ATestBase.getContract()\" is null\r\n[ERROR]   ITestS3AConditionalCreateBehavior.testConditionalWrite \u00bb ParameterResolution No ParameterResolver registered for parameter [boolean arg0] in constructor [public org.apache.hadoop.fs.s3a.impl.ITestS3AConditionalCreateBehavior(boolean)].\r\n[ERROR]   ITestS3AConditionalCreateBehavior.testWriteWithEtag \u00bb ParameterResolution No ParameterResolver registered for parameter [boolean arg0] in constructor [public org.apache.hadoop.fs.s3a.impl.ITestS3AConditionalCreateBehavior(boolean)].\r\n[ERROR]   ITestS3AConditionalCreateBehavior.testWriteWithPerformanceFlagAndOverwriteFalse \u00bb ParameterResolution No ParameterResolver registered for parameter [boolean arg0] in constructor [public org.apache.hadoop.fs.s3a.impl.ITestS3AConditionalCreateBehavior(boolean)].\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testConditionalCreateWhenPerformanceFlagEnabledAndOverwriteDisabled:637->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteDeletedFileWithEtag:484->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteFileWithEmptyEtag:508->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteWithCorrectEtag:432->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteWithOutdatedEtag:460->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchTwoMultipartUploadsRaceConditionOneClosesFirst:522->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchConflictOnMultipartUpload:312->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchConflictOnOverwrite:288->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchMultipartUploadWithRaceCondition:333->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchOverwriteEmptyFileWithFile:397->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchOverwriteEmptyWithEmptyFile:414->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchOverwriteWithEmptyFile:379->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n[ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfNoneMatchTwoConcurrentMultipartUploads:356->AbstractFSContractTestBase.methodPath:242->AbstractFSContractTestBase.path:233 \u00bb NullPointer Cannot invoke \"org.apache.hadoop.fs.contract.AbstractFSContract.getTestPath()\" because the return value of \"org.apache.hadoop.fs.contract.AbstractFSContractTestBase.getContract()\" is null\r\n{code}\r\n\r\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org", "name": "stevel@apache.org", "key": "stevel@apache.org", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"}, "displayName": "Steve Loughran", "active": true, "timeZone": "Europe/London"}, "created": "2025-07-08T13:10:14.226+0000", "updated": "2025-07-08T13:10:14.226+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18008098", "id": "18008098", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3089822111\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 16s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 10 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  44m  7s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  20m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 46s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 33s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 26s |  |  branch/hadoop-client-modules/hadoop-client-integration-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 33s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 30s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   1m 37s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m 11s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 46s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   1m 55s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   3m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 42s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   2m 35s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  hadoop-client-modules/hadoop-client-integration-tests has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 20s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 18s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  18m 32s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 28s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 26s |  |  hadoop-client-integration-tests in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 41s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 211m 27s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7814 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 63d1f6ad482c 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / f6e890ab053f9169cc683ac5ac5046f4bc4be111 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/testReport/ |\r\n   | Max. process+thread count | 1509 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-client-modules/hadoop-client-integration-tests U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-18T15:16:20.766+0000", "updated": "2025-07-18T15:16:20.766+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18008845", "id": "18008845", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3100023345\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 32 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 47s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  19m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   8m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   7m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   4m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   3m 36s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 52s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 33s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 22s |  |  branch/hadoop-client-modules/hadoop-client-integration-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 28s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   2m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   8m  7s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   8m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   7m 43s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   7m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   2m  2s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/4/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 13 new + 18 unchanged - 9 fixed = 31 total (was 27)  |\r\n   | +1 :green_heart: |  mvnsite  |   3m 59s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   3m 26s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 45s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 27s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 25s |  |  hadoop-client-modules/hadoop-client-integration-tests has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 27s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  18m 25s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  90m 23s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 59s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   2m 33s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 33s |  |  hadoop-client-integration-tests in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 46s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 265m 19s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7814 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux e5cbce971dc1 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b2d347f58b50e511ffc02a66625fe9ae7dbceafe |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/4/testReport/ |\r\n   | Max. process+thread count | 3309 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-client-modules/hadoop-client-integration-tests U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-22T00:11:09.776+0000", "updated": "2025-07-22T00:11:09.776+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18008875", "id": "18008875", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3100591778\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m 41s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 25 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |  10m  0s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  40m 13s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 54s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 28s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   6m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 42s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 39s |  |  branch/hadoop-client-modules/hadoop-client-integration-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 52s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 38s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   3m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 11s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 26s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m 36s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/3/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 8 new + 10 unchanged - 5 fixed = 18 total (was 15)  |\r\n   | +1 :green_heart: |  mvnsite  |   6m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 34s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 37s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 38s |  |  hadoop-client-modules/hadoop-client-integration-tests has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 38s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  22m 47s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 135m 48s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   4m 14s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m  4s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 48s |  |  hadoop-client-integration-tests in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 11s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 465m 57s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7814 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux ab824a3be5e1 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2444ec7d853650d3152a4ed784baefae7e067113 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/3/testReport/ |\r\n   | Max. process+thread count | 3136 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-client-modules/hadoop-client-integration-tests U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-22T02:53:19.535+0000", "updated": "2025-07-22T02:53:19.535+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18008899", "id": "18008899", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "shameersss1 commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3101270194\n\n   @steveloughran  Thanks for picking this up. This was blocking dev work on S3A.\r\n   I see we have a green run now. There are minor checkstyle issues\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-22T06:33:54.261+0000", "updated": "2025-07-22T06:33:54.261+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18009117", "id": "18009117", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3105225336\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 56s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  1s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 64 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 55s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  38m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  17m 56s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 34s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   6m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 39s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 42s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 39s |  |  branch/hadoop-client-modules/hadoop-client-integration-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 56s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 40s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   3m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  18m 50s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  18m 50s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m 25s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  17m 25s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m 54s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/5/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 18 new + 76 unchanged - 19 fixed = 94 total (was 95)  |\r\n   | +1 :green_heart: |  mvnsite  |   6m  7s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 24s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 34s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 34s |  |  hadoop-client-modules/hadoop-client-integration-tests has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  42m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 34s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  25m 14s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 140m 56s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   4m 35s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 45s |  |  hadoop-client-integration-tests in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 454m 27s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7814 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux 52bd0126de14 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8249bb8f1d09833d9421d940ddf5bf7fab4688db |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/5/testReport/ |\r\n   | Max. process+thread count | 2002 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-client-modules/hadoop-client-integration-tests U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T00:22:49.331+0000", "updated": "2025-07-23T00:22:49.331+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18009332", "id": "18009332", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "steveloughran commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3109457647\n\n   I'm doing another local run on this with the analytics profile, after which it is ready to merge. That is even though every mini yarn cluster refuses to come up with queue problems.\r\n   \r\n   I'm not convinced that is related as there have been recent changes near the failing yarn code. And we need this in so urgently that reducing the number of tests down to a limited few is still a success\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T17:09:46.691+0000", "updated": "2025-07-23T17:09:46.691+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18009335", "id": "18009335", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "steveloughran commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3109474963\n\n   one failure being fixed elsewhere\r\n   ```\r\n   [ERROR] org.apache.hadoop.fs.s3a.impl.ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteWithOutdatedEtag ", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T17:16:12.148+0000", "updated": "2025-07-23T17:16:12.148+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18009376", "id": "18009376", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3110001500\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 67 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   9m 16s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  20m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  10m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   9m  2s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   2m  5s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   4m  4s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   2m 58s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   3m 10s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 23s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 18s |  |  branch/hadoop-client-modules/hadoop-client-integration-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | -1 :x: |  shadedclient  |  33m 22s |  |  branch has errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  33m 38s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 30s |  |  Maven dependency ordering for patch  |\r\n   | -1 :x: |  mvninstall  |   0m 30s | [/patch-mvninstall-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvninstall-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 20s | [/patch-mvninstall-hadoop-hdfs-project_hadoop-hdfs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvninstall-hadoop-hdfs-project_hadoop-hdfs.txt) |  hadoop-hdfs in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 22s | [/patch-mvninstall-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvninstall-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 22s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  mvninstall  |   0m 23s | [/patch-mvninstall-hadoop-client-modules_hadoop-client-integration-tests.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvninstall-hadoop-client-modules_hadoop-client-integration-tests.txt) |  hadoop-client-integration-tests in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 22s | [/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-compile-root-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  root in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 21s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 21s | [/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-compile-root-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  root in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 19s | [/buildtool-patch-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/buildtool-patch-checkstyle-root.txt) |  The patch fails to run checkstyle in root  |\r\n   | -1 :x: |  mvnsite  |   0m 23s | [/patch-mvnsite-hadoop-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvnsite-hadoop-project.txt) |  hadoop-project in the patch failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 21s | [/patch-mvnsite-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvnsite-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 22s | [/patch-mvnsite-hadoop-hdfs-project_hadoop-hdfs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvnsite-hadoop-hdfs-project_hadoop-hdfs.txt) |  hadoop-hdfs in the patch failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 22s | [/patch-mvnsite-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvnsite-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 22s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  mvnsite  |   0m 23s | [/patch-mvnsite-hadoop-client-modules_hadoop-client-integration-tests.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-mvnsite-hadoop-client-modules_hadoop-client-integration-tests.txt) |  hadoop-client-integration-tests in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-project-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-project-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-project in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-common in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-hdfs-project_hadoop-hdfs-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-hdfs-project_hadoop-hdfs-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-hdfs in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 23s | [/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-tools_hadoop-aws-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-aws in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-client-modules_hadoop-client-integration-tests-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-client-modules_hadoop-client-integration-tests-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-client-integration-tests in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-project-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-project-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-project in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-common-project_hadoop-common-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-common-project_hadoop-common-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-common in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-hdfs-project_hadoop-hdfs-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-hdfs-project_hadoop-hdfs-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-hdfs in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javadoc  |   0m 20s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javadoc  |   0m 23s | [/patch-javadoc-hadoop-client-modules_hadoop-client-integration-tests-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-javadoc-hadoop-client-modules_hadoop-client-integration-tests-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-client-integration-tests in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  spotbugs  |   0m 22s | [/patch-spotbugs-hadoop-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-spotbugs-hadoop-project.txt) |  hadoop-project in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 23s | [/patch-spotbugs-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-spotbugs-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 22s | [/patch-spotbugs-hadoop-hdfs-project_hadoop-hdfs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-spotbugs-hadoop-hdfs-project_hadoop-hdfs.txt) |  hadoop-hdfs in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 22s | [/patch-spotbugs-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-spotbugs-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 23s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  spotbugs  |   0m 22s | [/patch-spotbugs-hadoop-client-modules_hadoop-client-integration-tests.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-spotbugs-hadoop-client-modules_hadoop-client-integration-tests.txt) |  hadoop-client-integration-tests in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  13m 19s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 22s | [/patch-unit-hadoop-project.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-unit-hadoop-project.txt) |  hadoop-project in the patch failed.  |\r\n   | -1 :x: |  unit  |   0m 23s | [/patch-unit-hadoop-common-project_hadoop-common.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt) |  hadoop-common in the patch failed.  |\r\n   | -1 :x: |  unit  |   0m 22s | [/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt) |  hadoop-hdfs in the patch failed.  |\r\n   | -1 :x: |  unit  |   0m 23s | [/patch-unit-hadoop-tools_hadoop-aws.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-unit-hadoop-tools_hadoop-aws.txt) |  hadoop-aws in the patch failed.  |\r\n   | -1 :x: |  unit  |   0m 22s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  unit  |   0m 22s | [/patch-unit-hadoop-client-modules_hadoop-client-integration-tests.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/patch-unit-hadoop-client-modules_hadoop-client-integration-tests.txt) |  hadoop-client-integration-tests in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 21s |  |  ASF License check generated no output?  |\r\n   |  |   | 120m 10s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7814 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux b97e3b21bf6d 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 9fea0cb4b718cad8baf22fe35e153d1f15f2ab24 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/testReport/ |\r\n   | Max. process+thread count | 260 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-client-modules/hadoop-client-integration-tests U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T20:12:48.115+0000", "updated": "2025-07-23T20:12:48.115+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18009414", "id": "18009414", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3111576861\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 50s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  1s |  |  The patch appears to include 64 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 58s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  39m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  18m 34s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  15m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   6m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 39s |  |  branch/hadoop-project no spotbugs output file (spotbugsXml.xml)  |\r\n   | +0 :ok: |  spotbugs  |   0m 38s |  |  branch/hadoop-client-modules/hadoop-client-integration-tests no spotbugs output file (spotbugsXml.xml)  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  43m 45s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 41s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   3m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  17m  8s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  17m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 23s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m 37s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/6/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 14 new + 68 unchanged - 19 fixed = 82 total (was 87)  |\r\n   | +1 :green_heart: |  mvnsite  |   6m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   5m 35s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   5m 38s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +0 :ok: |  spotbugs  |   0m 38s |  |  hadoop-project has no data from spotbugs  |\r\n   | +0 :ok: |  spotbugs  |   0m 38s |  |  hadoop-client-modules/hadoop-client-integration-tests has no data from spotbugs  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  7s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   0m 36s |  |  hadoop-project in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |  22m 42s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 131m 56s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 56s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m  7s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   0m 40s |  |  hadoop-client-integration-tests in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 11s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 440m 24s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7814 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient codespell detsecrets xmllint spotbugs checkstyle |\r\n   | uname | Linux aad058f7c5fe 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d0753c923ace2e2f668d1b66a6943a48aed231c4 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/6/testReport/ |\r\n   | Max. process+thread count | 3136 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-project hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure hadoop-client-modules/hadoop-client-integration-tests U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T00:31:41.149+0000", "updated": "2025-07-24T00:31:41.149+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18009582", "id": "18009582", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "steveloughran commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3113482749\n\n   rebased and with core tests working, just some minor failures. Also new tag `@IntegrationTest` to declare that a suite is an integration test, adds test runner tag \"integration\".\r\n   \r\n   Failures now yarn minicluster and multipart.\r\n   ```\r\n   [ERROR] Errors: \r\n   [ERROR]   ITestS3AContractMultipartUploader.testConcurrentUploads \u00bb AWSStatus500 Completing multipart upload on job-00-fork-0007/test/testConcurrentUploads: software.amazon.awssdk.services.s3.model.S3Exception: We encountered an internal error. Please try again. (Service: S3, Status Code: 500, Request ID: A4Z5WRV9V3W74C5V, Extended Request ID: 6m/CFanxXEWMLy8l+qBBQr+OMt6g9goM86k0WiHL66DRBZ0SkMhoaamAtR9tX+UVE8fyD2ddpCk=):InternalError: We encountered an internal error. Please try again. (Service: S3, Status Code: 500, Request ID: A4Z5WRV9V3W74C5V, Extended Request ID: 6m/CFanxXEWMLy8l+qBBQr+OMt6g9goM86k0WiHL66DRBZ0SkMhoaamAtR9tX+UVE8fyD2ddpCk=)                                                                                                                                 \r\n   [ERROR]   ITestS3AContractMultipartUploader.testMultipartUploadReverseOrderNonContiguousPartNumbers \u00bb AWSStatus500 Completing multipart upload on job-00-fork-0007/test/testMultipartUploadReverseOrderNonContiguousPartNumbers: software.amazon.awssdk.services.s3.model.S3Exception: We encountered an internal error. Please try again. (Service: S3, Status Code: 500, Request ID: DA6VS34DM5F80MBW, Extended Request ID: ovgd95kBya/OUd3NRGcN81Ls/GCUQW5D3uQ+hNz1DcKiKpIBZHWshGyeaXWv3awM4FJcSs4+5fQ=):InternalError: We encountered an internal error. Please try again. (Service: S3, Status Code: 500, Request ID: DA6VS34DM5F80MBW, Extended Request ID: ovgd95kBya/OUd3NRGcN81Ls/GCUQW5D3uQ+hNz1DcKiKpIBZHWshGyeaXWv3awM4FJcSs4+5fQ=)                                                             \r\n   [ERROR] org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob.test_200_execute(Path)\r\n   [ERROR]   Run 1: ITestS3ACommitterMRJob.test_200_execute:280 \u00bb NullPointer Cannot read the array length because \"blkLocations\" is null\r\n   [ERROR]   Run 2: ITestS3ACommitterMRJob.test_200_execute:280 \u00bb NullPointer Cannot read the array length because \"blkLocations\" is null\r\n   [ERROR]   Run 3: ITestS3ACommitterMRJob.test_200_execute:280 \u00bb NullPointer Cannot read the array length because \"blkLocations\" is null\r\n   [INFO] \r\n   ```\r\n   \r\n   I do suspect the multipart is related but I don't want it to hold up this (critical) patch.\r\n   \r\n   ITestS3ACommitterMRJob may be a test setup again as it is a sequence of tests.\r\n   \r\n   ```\r\n   [INFO] Running org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob\r\n   [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.756 s <<< FAILURE! ", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T13:27:48.954+0000", "updated": "2025-07-24T13:27:48.954+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18009662", "id": "18009662", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3114347396\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 54s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  2s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 92 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   8m 20s |  |  Maven dependency ordering for branch  |\r\n   | +1 :green_heart: |  mvninstall  |  35m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |  16m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |  13m 41s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   4m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   4m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   4m  7s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 45s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   8m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 22s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for patch  |\r\n   | +1 :green_heart: |  mvninstall  |   2m 55s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  14m 56s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |  14m 56s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |  15m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |  15m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   4m 55s | [/results-checkstyle-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/8/artifact/out/results-checkstyle-root.txt) |  root: The patch generated 14 new + 76 unchanged - 19 fixed = 90 total (was 95)  |\r\n   | +1 :green_heart: |  mvnsite  |   5m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   4m 13s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   4m 34s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   9m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  36m 20s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |  22m 44s |  |  hadoop-common in the patch passed.  |\r\n   | +1 :green_heart: |  unit  | 139m 36s |  |  hadoop-hdfs in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 43s |  |  hadoop-aws in the patch passed.  |\r\n   | +1 :green_heart: |  unit  |   3m 13s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   1m 12s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 414m 42s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7814 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 02a710867175 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 99198ef20b6947689534ecc76026b2bc5b4771f5 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/8/testReport/ |\r\n   | Max. process+thread count | 3152 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-aws hadoop-tools/hadoop-azure U: . |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7814/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T17:54:56.005+0000", "updated": "2025-07-24T17:54:56.005+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18009905", "id": "18009905", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "steveloughran commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3117076931\n\n   checkstyles are all about test numbering _100() etc, so invalid\r\n   ```\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/integration/ITestS3ACommitterMRJob.java:201:  public void test_000() throws Throwable {:15: Name 'test_000' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/integration/ITestS3ACommitterMRJob.java:206:  public void test_100() throws Throwable {:15: Name 'test_100' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/integration/ITestS3ACommitterMRJob.java:211:  public void test_200_execute(:15: Name 'test_200_execute' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/integration/ITestS3ACommitterMRJob.java:361:  public void test_500() throws Throwable {:15: Name 'test_500' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/terasort/ITestTerasortOnS3A.java:270:  public void test_100_terasort_setup() throws Throwable {:15: Name 'test_100_terasort_setup' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/terasort/ITestTerasortOnS3A.java:279:  public void test_110_teragen() throws Throwable {:15: Name 'test_110_teragen' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/terasort/ITestTerasortOnS3A.java:295:  public void test_120_terasort() throws Throwable {:15: Name 'test_120_terasort' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/terasort/ITestTerasortOnS3A.java:312:  public void test_130_teravalidate() throws Throwable {:15: Name 'test_130_teravalidate' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/terasort/ITestTerasortOnS3A.java:332:  public void test_140_teracomplete() throws Throwable {:15: Name 'test_140_teracomplete' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/terasort/ITestTerasortOnS3A.java:368:  public void test_150_teracleanup() throws Throwable {:15: Name 'test_150_teracleanup' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/terasort/ITestTerasortOnS3A.java:373:  public void test_200_directory_deletion() throws Throwable {:15: Name 'test_200_directory_deletion' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ILoadTestS3ABulkDeleteThrottling.java:190:  public void test_010_Reset() throws Throwable {:15: Name 'test_010_Reset' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ILoadTestS3ABulkDeleteThrottling.java:195:  public void test_020_DeleteThrottling() throws Throwable {:15: Name 'test_020_DeleteThrottling' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ./hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ILoadTestS3ABulkDeleteThrottling.java:208:  public void test_030_Sleep() throws Throwable {:15: Name 'test_030_Sleep' must match pattern '^[a-z][a-zA-Z0-9]*$'. [MethodName]\r\n   ```\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-25T09:33:05.136+0000", "updated": "2025-07-25T09:33:05.136+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18009975", "id": "18009975", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "steveloughran commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3118315112\n\n   I need this in ASAP as there's no way to validate any other PR: failures of their tests are meaningless.\r\n   \r\n   @slfan1989 @ahmarsuhail @mukund-thakur can I get an upvote on this?\r\n   \r\n   It isn't perfect, but it's test ony code and we can tune later\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-25T14:53:33.562+0000", "updated": "2025-07-25T14:53:33.562+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18010065", "id": "18010065", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "slfan1989 commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3120638695\n\n   > I need this in ASAP as there's no way to validate any other PR: failures of their tests are meaningless.\r\n   > \r\n   > @slfan1989 @ahmarsuhail @mukund-thakur can I get an upvote on this?\r\n   > \r\n   > It isn't perfect, but it's test ony code and we can tune later\r\n   \r\n   @steveloughran  Thanks for your contribution! I believe this PR is good enough, and we can go ahead and merge it.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-25T23:08:05.368+0000", "updated": "2025-07-25T23:08:05.368+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18010477", "id": "18010477", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "steveloughran merged PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-28T18:34:01.860+0000", "updated": "2025-07-28T18:34:01.860+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18010868", "id": "18010868", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "steveloughran commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3135501945\n\n   FYI, outstanding failures. \r\n   * multiparts may be parameterization again, though s3 isn't being helpful\r\n   * ITestS3APutIfMatchAndIfNoneMatch known\r\n   * MR stuff: yarn minicluster failure. Needs fixing, but not sure it's related to junit5\r\n   * \r\n   \r\n   ```\r\n   [ERROR] Failures: \r\n   [ERROR]   ITestS3APutIfMatchAndIfNoneMatch.testIfMatchOverwriteWithOutdatedEtag:478 Expected a org.apache.hadoop.fs.s3a.RemoteFileChangedException to be thrown, but got the result: : S3AFileStatus{path=s3a://stevel-london/job-00-fork-0005/test/testIfMatchOverwriteWithOutdatedEtag; isDirectory=false; length=1024; replication=1; blocksize=33554432; modification_time=1753293814000; access_time=0; owner=stevel; group=stevel; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=\"4340256b04f80df42c1a89c65a60d35d\" versionId=LHzL24avWKO0NNOzGMc8O9fsRzmxollO                                                                                                                                                             \r\n   [ERROR] Errors: \r\n   [ERROR]   ITestS3AContractMultipartUploader.testConcurrentUploads \u00bb AWSStatus500 Completing multipart upload on job-00-fork-0005/test/testConcurrentUploads: software.amazon.awssdk.services.s3.model.S3Exception: We encountered an internal error. Please try again. (Service: S3, Status Code: 500, Request ID: 50AFRGSS8QZC2QDX, Extended Request ID: XIg6zvO2fPb/WLdIrmATQN6/MawhpdSCmYTJ1C+FSb+4HIdosT/FTVcROikLN5WdKmUklJ9XPIK7ZWVpw+RlrsoUZ3NDkJUz):InternalError: We encountered an internal error. Please try again. (Service: S3, Status Code: 500, Request ID: 50AFRGSS8QZC2QDX, Extended Request ID: XIg6zvO2fPb/WLdIrmATQN6/MawhpdSCmYTJ1C+FSb+4HIdosT/FTVcROikLN5WdKmUklJ9XPIK7ZWVpw+RlrsoUZ3NDkJUz)                                                                                         \r\n   [ERROR]   ITestS3AContractMultipartUploader.testMultipartUploadReverseOrderNonContiguousPartNumbers \u00bb AWSStatus500 Completing multipart upload on job-00-fork-0005/test/testMultipartUploadReverseOrderNonContiguousPartNumbers: software.amazon.awssdk.services.s3.model.S3Exception: We encountered an internal error. Please try again. (Service: S3, Status Code: 500, Request ID: Q2V03E5APH6XGAV0, Extended Request ID: lDU/3cdSBAfiU9MIR4BCY+jx4sQCfrRk06Rc4Lw9nfqyDQlEwP1iekUSkkfOUGc2Rvln6b7DzrM=):InternalError: We encountered an internal error. Please try again. (Service: S3, Status Code: 500, Request ID: Q2V03E5APH6XGAV0, Extended Request ID: lDU/3cdSBAfiU9MIR4BCY+jx4sQCfrRk06Rc4Lw9nfqyDQlEwP1iekUSkkfOUGc2Rvln6b7DzrM=)                                                             \r\n   [ERROR] org.apache.hadoop.fs.s3a.commit.integration.ITestS3ACommitterMRJob.test_200_execute(Path)\r\n   [ERROR]   Run 1: ITestS3ACommitterMRJob.test_200_execute:280 \u00bb NullPointer\r\n   [ERROR]   Run 2: ITestS3ACommitterMRJob.test_200_execute:280 \u00bb NullPointer\r\n   [ERROR]   Run 3: ITestS3ACommitterMRJob.test_200_execute:280 \u00bb NullPointer\r\n   [INFO] \r\n   ```\r\n   \r\n   \r\n   ```\r\n   org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to initialize queues\r\n   \r\n   \tat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n   \tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:174)\r\n   \tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:110)\r\n   \tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:996)\r\n   \tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:165)\r\n   \tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1511)\r\n   \tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:351)\r\n   \tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:165)\r\n   \tat org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:375)\r\n   \tat org.apache.hadoop.yarn.server.MiniYARNCluster.access$200(MiniYARNCluster.java:129)\r\n   \tat org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceInit(MiniYARNCluster.java:510)\r\n   \tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:165)\r\n   \tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:110)\r\n   \tat org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:343)\r\n   \tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:165)\r\n   \tat org.apache.hadoop.fs.s3a.yarn.ITestS3AMiniYarnCluster.setup(ITestS3AMiniYarnCluster.java:86)\r\n   \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n   \tat java.util.ArrayList.forEach(ArrayList.java:1259)\r\n   \tat java.util.ArrayList.forEach(ArrayList.java:1259)\r\n   Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to initialize queues\r\n   \tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initializeQueues(CapacityScheduler.java:815)\r\n   \tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:320)\r\n   \tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:414)\r\n   \tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:165)\r\n   \t... 17 more\r\n   Caused by: java.lang.IllegalStateException: Queue configuration missing child queue names for root\r\n   \tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.validateParent(CapacitySchedulerQueueManager.java:741)\r\n   \tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.parseQueue(CapacitySchedulerQueueManager.java:255)\r\n   \tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.initializeQueues(CapacitySchedulerQueueManager.java:177)\r\n   \tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initializeQueues(CapacityScheduler.java:806)\r\n   \t... 20 more\r\n   \r\n   ```\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-30T09:23:40.529+0000", "updated": "2025-07-30T09:23:40.529+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18010871", "id": "18010871", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "slfan1989 commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3135524041\n\n   @steveloughran I'll follow up on the Yarn MiniCluster issue, but it might be later today. I've dealt with a similar issue before, so it might be helpful. The main cause is that during parallel testing, different unit tests are accessing the same configuration file, as some of the configurations are stored in directories like `/tmp/yarn/jenkins`.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-30T09:30:48.991+0000", "updated": "2025-07-30T09:30:48.991+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18011005", "id": "18011005", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "steveloughran commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3137465807\n\n   ahh, I see. I think temp dirs are set per process and a base test jobID. \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-30T18:46:48.274+0000", "updated": "2025-07-30T18:46:48.274+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622811/comment/18011009", "id": "18011009", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "steveloughran commented on PR #7814:\nURL: https://github.com/apache/hadoop/pull/7814#issuecomment-3137490642\n\n   just discovered (Iceberg code) that assertj may limit stack traces, so you need to set `    Assertions.setMaxStackTraceElementsDisplayed(Integer.MAX_VALUE)` before test runs. We could add that in AbstractHadoopTestSuite. It's a static value so only needs to be set once\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-30T18:56:55.143+0000", "updated": "2025-07-30T18:56:55.143+0000"}], "maxResults": 20, "total": 20, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/5", "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png", "name": "Resolved", "id": "5", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3", "id": 3, "key": "done", "colorName": "green", "name": "Done"}}}}