{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13632718", "self": "https://issues.apache.org/jira/rest/api/2/issue/13632718", "key": "SPARK-54071", "fields": {"summary": "Spark Structured Streaming Filesink can not generate open lineage with output details", "description": "h2. Environment details\r\nh3. OpenLineage version\r\n\r\n{quote}io.openlineage:openlineage-spark_2.13:1.39.0\r\nTechnology and package versions\r\n\r\nPython: 3.13.3\r\nScala: 2.13.16\r\nJava: OpenJDK 64-Bit Server VM, 17.0.16\r\n\r\npip freeze\r\npy4j==0.10.9.9\r\npyspark==4.0.1{quote}\r\n\r\n\r\nFor the openlineage set up, I used the default setting:\r\n\r\n\r\n{quote}$ git clone https://github.com/MarquezProject/marquez.git && cd marquez\r\n\r\n$ ./docker/up.sh{quote}\r\n\r\nh3. Spark Deployment details\r\n\r\nI used native spark on local machine. There is no managed services involved.\r\nProblem details\r\n\r\nh2. Issue details\r\nWhen using Spark structured streaming to write parquet file to file systems,\r\n\r\n*     File sink will only generate openlineage event with streaming processing type with output information as empty.\r\n*     Foreachbatch sink will generate openlineage event with both streaming processing type and batch processing type. The batch processing type will have valid output information.\r\n\r\nThe bug is that File sink in Spark structured streaming does not generate open lineage event with output details.\r\n\r\nMore details about the sample code and sample events are following.\r\nFile sink:\r\nSample code:\r\n{quote}\r\nquery = streaming_df.writeStream \\\r\n    .format('parquet') \\\r\n    .outputMode('append') \\\r\n    .partitionBy('year', 'month', 'day') \\\r\n    .option('checkpointLocation', checkpoint_path) \\\r\n    .option('path', output_path) \\\r\n    .queryName('filesink') \\\r\n    .start()\r\n{quote}\r\nSample event for \"processingType\":\"STREAMING\"\r\n{quote}\r\n25/10/29 00:49:02 DEBUG wire: http-outgoing-52 >> \"{\"eventTime\":\"2025-10-28T13:45:34.282Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b14-4e9d-7574-95a9-55182f07591d\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b10-5218-7ba3-9cf4-1ce4b1800752\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"filesink\"},\"root\":{\"run\":{\"runId\":\"019a2b10-5218-7ba3-9cf4-1ce4b1800752\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"filesink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"filesink.project\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"STREAMING\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\"{quote}\r\n\r\nforeachbatch sink\r\nsample code\r\n{quote}\r\ndef write_to_file(batch_df, batch_id):\r\n    if batch_df.count() > 0:\r\n        batch_df.write \\\r\n            .mode(\"append\") \\\r\n            .partitionBy(\"year\", \"month\", \"day\") \\\r\n            .parquet(output_path)\r\n{quote}\r\n{quote}\r\nquery = streaming_df \\\r\n    .writeStream \\\r\n    .outputMode(\"append\") \\\r\n    .foreachBatch(write_to_file) \\\r\n    .option(\"checkpointLocation\", checkpoint_path) \\\r\n    .trigger(processingTime='10 seconds') \\\r\n    .start()\r\n{quote}\r\n\r\nThe above code with generate both streaming and batch processing type event.\r\nSample streaming type event\r\n{quote}\r\n25/10/29 01:04:45 DEBUG wire: http-outgoing-1 >> \"{\"eventTime\":\"2025-10-28T14:04:43.373Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b22-b364-79ca-be87-2d173c25c16c\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink\"},\"root\":{\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"foreachbatchsink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink.project\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"STREAMING\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\"\r\n{quote}\r\nSample batch type event\r\n{quote}\r\n25/10/29 01:07:26 DEBUG wire: http-outgoing-33 >> \"{\"eventTime\":\"2025-10-28T14:07:20.711Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b25-28f6-7fa0-a4e2-2aaba4f61d7e\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink\"},\"root\":{\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"foreachbatchsink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink.execute_insert_into_hadoop_fs_relation_command.tests_output_foreachbatchsink\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"file\",\"name\":\"/Users/xxxx/venvs/tests/output_foreachbatchsink\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"file\",\"uri\":\"file\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"long\"},{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"timestamp\",\"type\":\"timestamp\"},{\"name\":\"value\",\"type\":\"double\"},{\"name\":\"year\",\"type\":\"integer\"},{\"name\":\"month\",\"type\":\"integer\"},{\"name\":\"day\",\"type\":\"integer\"}]}},\"outputFacets\":{\"outputStatistics\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-2/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet\",\"rowCount\":10,\"size\":13259,\"fileCount\":10}}}]}\"{quote}\r\n\r\nWhat you think should happen instead\r\n\r\nFile sink for spark structured streaming should create open lineage event with valid output details as the information is in the spark query logic plan.\r\n\r\nHere is the logic plan for streaming query using file sink.\r\n{quote}\r\n== Analyzed Logical Plan ==\r\nid: bigint, name: string, timestamp: timestamp, value: double, year: int, month: int, day: int\r\n~WriteToMicroBatchDataSourceV1 FileSink[file:/Users/xxx/venvs/tests/output_filesink], 753bdc9a-07cd-4788-a17d-27ff622ababc, [checkpointLocation=checkpoint_filesink, path=output_filesink, queryName=filesink], Append, 1\r\n+- ~Project [id#2L, name#3, timestamp#0, value#4, year#5, month#6, dayofmonth(cast(timestamp#0 as date)) AS day#7]\r\n   +- ~Project [id#2L, name#3, timestamp#0, value#4, year#5, month(cast(timestamp#0 as date)) AS month#6]\r\n      +- ~Project [id#2L, name#3, timestamp#0, value#4, year(cast(timestamp#0 as date)) AS year#5]\r\n         +- ~Project [(value#1L % cast(1000 as bigint)) AS id#2L, concat(user_, cast((value#1L % cast(100 as bigint)) as string)) AS name#3, timestamp#0, (rand(-1344458628259366487) * cast(100 as double)) AS value#4]\r\n            +- ~StreamingDataSourceV2ScanRelation[timestamp#0, value#1L] RateStream(rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=12)\r\n{quote}\r\nHow to reproduce\r\n\r\n*     step 1: install pyspark on your local machine https://spark.apache.org/docs/latest/api/python/getting_started/install.html\r\n*     step 2: install openlineage server on your local machine https://openlineage.io/getting-started\r\n*     step 3: refer to following spark-submit command to run file_sink.py and foreachbatch_sink.py. You will see the open lineage event in the debug logs.\r\n\r\n{quote}\r\nspark-submit   --packages io.openlineage:openlineage-spark_2.13:1.39.0   --conf \"spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener\"   --conf \"spark.openlineage.transport.type=http\"   --conf \"spark.openlineage.transport.url=http://localhost:5000\"   --conf \"spark.openlineage.namespace=spark_namespace\"   --conf \"spark.openlineage.parentJobNamespace=airflow_namespace\"   --conf \"spark.openlineage.parentJobName=airflow_dag.airflow_task\"   --conf \"spark.openlineage.parentRunId=xxxx-xxxx-xxxx-xxxx\"   [filename].py\r\n{quote}\r\n", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=yangguo", "name": "yangguo", "key": "JIRAUSER306465", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34058", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"}, "displayName": "Yang Guo", "active": true, "timeZone": "Etc/UTC"}, "comment": {"comments": [], "maxResults": 0, "total": 0, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/1", "description": "The issue is open and ready for the assignee to start work on it.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png", "name": "Open", "id": "1", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2", "id": 2, "key": "new", "colorName": "blue-gray", "name": "To Do"}}}}