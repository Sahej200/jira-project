{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13609690", "self": "https://issues.apache.org/jira/rest/api/2/issue/13609690", "key": "HADOOP-19472", "fields": {"summary": "ABFS: Enhance performance of ABFS driver for write-heavy workloads", "description": "The goal of this work item is to enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes.", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=asrani_anmol", "name": "asrani_anmol", "key": "JIRAUSER281089", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34046", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"}, "displayName": "Anmol Asrani", "active": true, "timeZone": "Etc/UTC"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/17949139", "id": "17949139", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 opened a new pull request, #7669:\nURL: https://github.com/apache/hadoop/pull/7669\n\n   Enhance the performance of ABFS Driver for write-heavy workloads by improving concurrency within writes. \r\n   \r\n   ![{05B55BCA-EF1F-496D-B1ED-17DCD394DDA1}](https://github.com/user-attachments/assets/5ebd5ad7-51db-4028-812f-ce9da9266984)\r\n   \r\n   \r\n   The proposed design advocates for a centralized `WriteThreadPoolSizeManager` class to handle the collective thread allocation required for all write operations across the system, replacing the current CachedThreadPool in AzureBlobFileSystemStore. This centralized approach ensures that the initial thread pool size is set at `4 * number of available processors` and dynamically adjusts the pool size based on the system's current CPU utilization. This adaptive scaling and descaling mechanism optimizes resource usage and responsiveness. Moreover, this shared thread pool is accessible and utilized by all output streams, streamlining resource management and promoting efficient concurrency across write operations.  \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-05-03T08:22:10.199+0000", "updated": "2025-05-03T08:22:10.199+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/17949140", "id": "17949140", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-2848521492\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 798, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 801, Failures: 0, Errors: 0, Skipped: 117\r\n   [ERROR] Tests run: 146, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 640, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 798, Failures: 0, Errors: 0, Skipped: 171\r\n   [WARNING] Tests run: 132, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 643, Failures: 0, Errors: 0, Skipped: 144\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 637, Failures: 0, Errors: 0, Skipped: 217\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [ERROR] Tests run: 640, Failures: 0, Errors: 0, Skipped: 146\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [ERROR] Tests run: 638, Failures: 0, Errors: 0, Skipped: 164\r\n   [WARNING] Tests run: 132, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [ERROR] Tests run: 672, Failures: 0, Errors: 0, Skipped: 167\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 637, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 155, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-05-03T08:35:17.220+0000", "updated": "2025-05-03T08:35:17.220+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/17949151", "id": "17949151", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-2848547561\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  26m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 19s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 31s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 14 new + 2 unchanged - 0 fixed = 16 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 19s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 23s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  77m 36s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.WriteThreadPoolSizeManager.adjustThreadPoolSizeBasedOnCPU(double) does not release lock on all exception paths  At WriteThreadPoolSizeManager.java:on all exception paths  At WriteThreadPoolSizeManager.java:[line 268] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a1000f66baec 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1ba12f6247567e3f5e9087c00fb52e741b1eb98c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-05-03T09:52:51.621+0000", "updated": "2025-05-03T09:52:51.621+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/17949940", "id": "17949940", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-2857800447\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 39s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 39s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 18s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  22m 30s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 40s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  22m  6s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 20s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  78m 44s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.WriteThreadPoolSizeManager.adjustThreadPoolSizeBasedOnCPU(double) does not release lock on all exception paths  At WriteThreadPoolSizeManager.java:on all exception paths  At WriteThreadPoolSizeManager.java:[line 263] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3b526ffc404b 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 8fe08428cb90f286b3d7408e60bd3c05bbde7ba6 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/2/testReport/ |\r\n   | Max. process+thread count | 558 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-05-07T09:15:21.506+0000", "updated": "2025-05-07T09:15:21.506+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/17949992", "id": "17949992", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-2858500737\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  23m  7s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 50s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m  2s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  20m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 17s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  74m 18s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 3ee980188ead 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / c56bdcbeec1025c521eae998c50aaf72d72458d7 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/3/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-05-07T12:59:31.510+0000", "updated": "2025-05-07T12:59:31.510+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/17954262", "id": "17954262", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-2911679089\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 25s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  23m 37s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 13s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  80m 29s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 251c9a94f809 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ca7be8805b4a932ebcfbef19749c9b8b3e0497e1 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/4/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-05-27T08:41:23.368+0000", "updated": "2025-05-27T08:41:23.368+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/17954306", "id": "17954306", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-2912198409\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 53s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  22m  5s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 15s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  3s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 17s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  76m 56s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.49 ServerAPI=1.49 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 957e28bff80b 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5e79b10358006820caa9e39d1d472571aba1f56c |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/5/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-05-27T11:40:52.735+0000", "updated": "2025-05-27T11:40:52.735+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18012597", "id": "18012597", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3164446290\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 23s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  30m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  25m 39s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  25m 52s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  25m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 20s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 25s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  91m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b233612874b4 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 73a8012c216fb9222f1be6bb639378f77091a305 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/6/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-07T14:31:26.632+0000", "updated": "2025-08-07T14:31:26.632+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18014548", "id": "18014548", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3195908154\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  25m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  25m 49s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 2 unchanged - 0 fixed = 6 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  23m 39s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 15s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 22s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  86m  0s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a58d8ee4661b 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3164050e6fccde89f26d597bee73fa42a725dc3a |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/7/testReport/ |\r\n   | Max. process+thread count | 729 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-18T09:35:38.124+0000", "updated": "2025-08-18T09:35:38.124+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18014558", "id": "18014558", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3195944600\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  27m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 25s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  26m 50s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 17s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 10s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 3 new + 2 unchanged - 0 fixed = 5 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 40s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 41s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 27s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  86m 51s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 345d7c0ea8f7 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / db7ae1fa85a844e69cbdfde57081ce77091c3799 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/8/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-18T09:46:18.409+0000", "updated": "2025-08-18T09:46:18.409+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18014654", "id": "18014654", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3197457197\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  30m 15s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  22m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  22m 44s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 16s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 16s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 13s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/9/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 32 new + 2 unchanged - 0 fixed = 34 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  7s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 25s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  84m  9s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 806146c83fa5 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 812ea4660ae805dd76ddb18bf25a92a80c70e0b0 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/9/testReport/ |\r\n   | Max. process+thread count | 556 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/9/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-08-18T15:43:41.228+0000", "updated": "2025-08-18T15:43:41.228+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18017417", "id": "18017417", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2313716838\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -321,16 +330,13 @@ public void close() throws IOException {\n     try {\n       Futures.allAsList(futures).get();\n       // shutdown the threadPool and set it to null.\n-      HadoopExecutors.shutdown(boundedThreadPool, LOG,\n-          30, TimeUnit.SECONDS);\n-      boundedThreadPool = null;\n     } catch (InterruptedException e) {\n       LOG.error(\"Interrupted freeing leases\", e);\n       Thread.currentThread().interrupt();\n     } catch (ExecutionException e) {\n       LOG.error(\"Error freeing leases\", e);\n     } finally {\n-      IOUtils.cleanupWithLogger(LOG, getClient());\n+      IOUtils.cleanupWithLogger(LOG, poolSizeManager, getClient());\n\nReview Comment:\n   For the non-dynamic pool- how are we closing the boundedThreadPool?\r\n   Would we need HadoopExecutors.shutdown(..) for it?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-01T11:38:52.355+0000", "updated": "2025-09-01T11:38:52.355+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18017420", "id": "18017420", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2313736451\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,377 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+\n+    /*  Initialize the bounded thread pool executor */\n+    this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize);\n\nReview Comment:\n   we're naming the threads in non-dynamic pool and the manager pool for dynamic write pool. Should we also name the threads for dynamic case?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-01T11:48:19.191+0000", "updated": "2025-09-01T11:48:19.191+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18017437", "id": "18017437", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3242214939\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 30s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 38s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 51s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/10/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 4 new + 2 unchanged - 0 fixed = 6 total (was 2)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m  4s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 24s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  78m 49s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2e870a45b9cb 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / cafe1e5e618e22865f26a57b6891dc57a368e658 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/10/testReport/ |\r\n   | Max. process+thread count | 555 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/10/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-01T12:41:34.036+0000", "updated": "2025-09-01T12:41:34.036+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18017583", "id": "18017583", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2314852491\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,377 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+\n+    /*  Initialize the bounded thread pool executor */\n+    this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize);\n+\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(\n+        abfsConfiguration.getWriteCorePoolSize());\n+  }\n+\n+  public AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n+   * @return Computed max thread pool size.\n+   */\n+  private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) {\n+      LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory);\n+      LOG.debug(\"The number of available processors is {} \", availableProcessors);\n+      int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors);\n+      LOG.debug(\"The max thread pool size is {} \", maxpoolSize);\n+      return maxpoolSize;\n+  }\n+\n+  /**\n+   * Calculates the available heap memory in gigabytes.\n+   * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory\n+   * allowed for the JVM and subtracts the currently used memory (total - free)\n+   * to determine how much heap memory is still available.\n+   * The result is rounded up to the nearest gigabyte.\n+   *\n+   * @return the available heap memory in gigabytes\n+   */\n+  private long getAvailableHeapMemory() {\n+    Runtime runtime = Runtime.getRuntime();\n+    long maxMemory = runtime.maxMemory();\n+    long usedMemory = runtime.totalMemory() - runtime.freeMemory();\n+    long availableHeapBytes = maxMemory - usedMemory;\n+    return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE;\n+  }\n+\n+  /**\n+   * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier.\n+   */\n+  private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) {\n+    int multiplier;\n+    if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getLowTierMemoryMultiplier();\n+    } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getMediumTierMemoryMultiplier();\n+    } else {\n+      multiplier = abfsConfiguration.getHighTierMemoryMultiplier();\n+    }\n+    return availableProcessors * multiplier;\n+  }\n+\n+  /**\n+   * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem.\n+   *\n+   * @param filesystemName the name of the filesystem.\n+   * @param abfsConfiguration the configuration for the ABFS.\n+   *\n+   * @return the singleton instance.\n+   */\n+  public static synchronized WriteThreadPoolSizeManager getInstance(\n+      String filesystemName, AbfsConfiguration abfsConfiguration) {\n+    /* Check if an instance already exists in the map for the given filesystem */\n+    WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get(\n+        filesystemName);\n+\n+    /* If an existing instance is found, return it */\n+    if (existingInstance != null && existingInstance.boundedThreadPool != null\n+        && !existingInstance.boundedThreadPool.isShutdown()) {\n+      return existingInstance;\n+    }\n+\n+    /* Otherwise, create a new instance, put it in the map, and return it */\n+    LOG.debug(\n+        \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\",\n+        filesystemName);\n+    WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager(\n+        filesystemName, abfsConfiguration);\n+    POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance);\n+    return newInstance;\n+  }\n+\n+  /**\n+   * Adjusts the thread pool size to the specified maximum pool size.\n+   *\n+   * @param newMaxPoolSize the new maximum pool size.\n+   */\n+  private void adjustThreadPoolSize(int newMaxPoolSize) {\n+    synchronized (this) {\n+      ThreadPoolExecutor threadPoolExecutor\n+          = ((ThreadPoolExecutor) boundedThreadPool);\n+      int currentCorePoolSize = threadPoolExecutor.getCorePoolSize();\n+\n+      if (newMaxPoolSize >= currentCorePoolSize) {\n+        threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize);\n+        threadPoolExecutor.setCorePoolSize(newMaxPoolSize);\n+      } else {\n+        threadPoolExecutor.setCorePoolSize(newMaxPoolSize);\n+        threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize);\n+      }\n+\n+      LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize);\n+      LOG.debug(\"The pool size is: {} \", threadPoolExecutor.getPoolSize());\n+      LOG.debug(\"The active thread count is: {}\", threadPoolExecutor.getActiveCount());\n+    }\n+  }\n+\n+  /**\n+   * Starts monitoring the CPU utilization and adjusts the thread pool size accordingly.\n+   */\n+  synchronized void startCPUMonitoring() {\n+    cpuMonitorExecutor.scheduleAtFixedRate(() -> {\n+      double cpuUtilization = getCpuUtilization();\n+      LOG.debug(\"Current CPU Utilization is this: {}\", cpuUtilization);\n+      try {\n+        adjustThreadPoolSizeBasedOnCPU(cpuUtilization);\n+      } catch (InterruptedException e) {\n+        throw new RuntimeException(String.format(\n+            \"Thread pool size adjustment interrupted for filesystem %s\",\n+            filesystemName), e);\n+      }\n+    }, 0, getAbfsConfiguration().getWriteCpuMonitoringInterval(), TimeUnit.SECONDS);\n+  }\n+\n+  /**\n+   * Gets the current CPU utilization.\n+   *\n+   * @return the CPU utilization as a percentage (0.0 to 1.0).\n+   */\n+  private double getCpuUtilization() {\n+    OperatingSystemMXBean osBean = ManagementFactory.getOperatingSystemMXBean();\n+    if (osBean instanceof com.sun.management.OperatingSystemMXBean) {\n+      com.sun.management.OperatingSystemMXBean sunOsBean\n+          = (com.sun.management.OperatingSystemMXBean) osBean;\n+      double cpuLoad = sunOsBean.getSystemCpuLoad();\n+      if (cpuLoad >= 0) {\n\nReview Comment:\n   if cpuLoad is -1.0, should we log it?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-02T03:55:19.950+0000", "updated": "2025-09-02T03:55:19.950+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18017586", "id": "18017586", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2314884087\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -438,6 +438,10 @@ public class AbfsConfiguration{\n       FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION)\n   private boolean isChecksumValidationEnabled;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey =\n\nReview Comment:\n   Nit: we can remove it (part of prev PR)\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-02T04:31:25.563+0000", "updated": "2025-09-02T04:31:25.563+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18017587", "id": "18017587", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2314884087\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -438,6 +438,10 @@ public class AbfsConfiguration{\n       FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION)\n   private boolean isChecksumValidationEnabled;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey =\n\nReview Comment:\n   Nit: we can remove it and related ones below (part of prev PR)\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-02T04:33:03.176+0000", "updated": "2025-09-02T04:33:03.176+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18018035", "id": "18018035", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3252145988\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  patch  |   0m 17s |  |  https://github.com/apache/hadoop/pull/7669 does not apply to trunk. Rebase required? Wrong Branch? See https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute for help.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/11/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-04T06:38:48.437+0000", "updated": "2025-09-04T06:38:48.437+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18018070", "id": "18018070", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3252822256\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  28m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 21s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  27m 36s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  27m 49s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 24s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 12s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 17s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 47s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  27m 14s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 27s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 27s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  93m 53s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/12/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f783f8b2200a 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6f5cd826e33f1d86cc5f4aa373268b6ef03fb4bc |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/12/testReport/ |\r\n   | Max. process+thread count | 563 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/12/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-04T09:37:37.727+0000", "updated": "2025-09-04T09:37:37.727+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18018320", "id": "18018320", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3257523521\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m  0s |  |  Docker mode activated.  |\r\n   | -1 :x: |  docker  |  16m  4s |  |  Docker failed to build run-specific yetus/hadoop:tp-31571}.  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/13/console |\r\n   | versions | git=2.34.1 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-05T08:30:39.811+0000", "updated": "2025-09-05T08:30:39.811+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18018402", "id": "18018402", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3258488704\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   9m 39s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |  33m 34s | [/branch-mvninstall-root.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/branch-mvninstall-root.txt) |  root in trunk failed.  |\r\n   | -1 :x: |  compile  |   0m 16s | [/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/branch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in trunk failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 41s |  |  trunk passed  |\r\n   | -1 :x: |  shadedclient  |  22m  8s |  |  branch has errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  22m 33s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 22s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 22s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 20s | [/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/buildtool-patch-checkstyle-hadoop-tools_hadoop-azure.txt) |  The patch fails to run checkstyle in hadoop-azure  |\r\n   | -1 :x: |  mvnsite  |   0m 22s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  javadoc  |   0m 22s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javadoc  |   0m 29s | [/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-javadoc-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  spotbugs  |   0m 21s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |   4m 17s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 22s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +0 :ok: |  asflicense  |   0m 23s |  |  ASF License check generated no output?  |\r\n   |  |   |  75m 16s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 88e24d886e71 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / ed6c78ee45d94c69e87c25c3b95b55effeed6923 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/testReport/ |\r\n   | Max. process+thread count | 556 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/14/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-05T14:07:23.238+0000", "updated": "2025-09-05T14:07:23.238+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18018794", "id": "18018794", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3265728007\n\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 819, Failures: 0, Errors: 0, Skipped: 167\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 822, Failures: 0, Errors: 0, Skipped: 119\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10\r\n    \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 661, Failures: 0, Errors: 0, Skipped: 235\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n    \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 819, Failures: 0, Errors: 0, Skipped: 178\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 12\r\n   [WARNING] Tests run: 668, Failures: 0, Errors: 0, Skipped: 161\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 658, Failures: 0, Errors: 0, Skipped: 237\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 12\r\n   [WARNING] Tests run: 665, Failures: 0, Errors: 0, Skipped: 173\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 12\r\n   [WARNING] Tests run: 660, Failures: 0, Errors: 0, Skipped: 195\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 693, Failures: 0, Errors: 0, Skipped: 176\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 189, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 658, Failures: 0, Errors: 0, Skipped: 234\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-08T10:50:59.689+0000", "updated": "2025-09-08T10:50:59.689+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18018813", "id": "18018813", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3265990736\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 32s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  21m 45s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  21m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 24s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  78m 22s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/15/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 74a5f58b2863 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 443556cdb787d2c6be17e9317a9870812950506e |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/15/testReport/ |\r\n   | Max. process+thread count | 561 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/15/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-08T12:08:02.077+0000", "updated": "2025-09-08T12:08:02.077+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18021766", "id": "18021766", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3317397225\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 24s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 35s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  24m 49s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  1s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 14s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 16s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  24m 21s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 22s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  98m  4s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/17/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c3b676228a26 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 6a0e5bcaabcdfcbbc3ba4dc1ab3cefc20362dfdf |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/17/testReport/ |\r\n   | Max. process+thread count | 561 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/17/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-09-22T07:45:21.363+0000", "updated": "2025-09-22T07:45:21.363+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18031975", "id": "18031975", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2450656923\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n+        initialPoolSize,\n+        r -> {\n+          Thread t = new Thread(r);\n+          t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement());\n+          return t;\n+        }\n+    );\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(\n+        abfsConfiguration.getWriteCorePoolSize());\n+  }\n+\n+  public AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n+   * @return Computed max thread pool size.\n+   */\n+  private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) {\n+      LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory);\n\nReview Comment:\n   ll the log lines can be combined into a single log here.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n+        initialPoolSize,\n+        r -> {\n+          Thread t = new Thread(r);\n+          t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement());\n+          return t;\n+        }\n+    );\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(\n+        abfsConfiguration.getWriteCorePoolSize());\n+  }\n+\n+  public AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n+   * @return Computed max thread pool size.\n+   */\n+  private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) {\n+      LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory);\n+      LOG.debug(\"The number of available processors is {} \", availableProcessors);\n+      int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors);\n+      LOG.debug(\"The max thread pool size is {} \", maxpoolSize);\n+      return maxpoolSize;\n+  }\n+\n+  /**\n+   * Calculates the available heap memory in gigabytes.\n+   * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory\n+   * allowed for the JVM and subtracts the currently used memory (total - free)\n+   * to determine how much heap memory is still available.\n+   * The result is rounded up to the nearest gigabyte.\n+   *\n+   * @return the available heap memory in gigabytes\n+   */\n+  private long getAvailableHeapMemory() {\n+    Runtime runtime = Runtime.getRuntime();\n+    long maxMemory = runtime.maxMemory();\n+    long usedMemory = runtime.totalMemory() - runtime.freeMemory();\n+    long availableHeapBytes = maxMemory - usedMemory;\n+    return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE;\n+  }\n+\n+  /**\n+   * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier.\n+   */\n+  private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) {\n+    int multiplier;\n+    if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getLowTierMemoryMultiplier();\n+    } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getMediumTierMemoryMultiplier();\n+    } else {\n+      multiplier = abfsConfiguration.getHighTierMemoryMultiplier();\n+    }\n+    return availableProcessors * multiplier;\n+  }\n+\n+  /**\n+   * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem.\n+   *\n+   * @param filesystemName the name of the filesystem.\n+   * @param abfsConfiguration the configuration for the ABFS.\n+   *\n+   * @return the singleton instance.\n+   */\n+  public static synchronized WriteThreadPoolSizeManager getInstance(\n+      String filesystemName, AbfsConfiguration abfsConfiguration) {\n+    /* Check if an instance already exists in the map for the given filesystem */\n+    WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get(\n+        filesystemName);\n+\n+    /* If an existing instance is found, return it */\n+    if (existingInstance != null && existingInstance.boundedThreadPool != null\n+        && !existingInstance.boundedThreadPool.isShutdown()) {\n+      return existingInstance;\n+    }\n+\n+    /* Otherwise, create a new instance, put it in the map, and return it */\n+    LOG.debug(\n+        \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\",\n+        filesystemName);\n+    WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager(\n+        filesystemName, abfsConfiguration);\n+    POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance);\n+    return newInstance;\n+  }\n+\n+  /**\n+   * Adjusts the thread pool size to the specified maximum pool size.\n+   *\n+   * @param newMaxPoolSize the new maximum pool size.\n+   */\n+  private void adjustThreadPoolSize(int newMaxPoolSize) {\n+    synchronized (this) {\n+      ThreadPoolExecutor threadPoolExecutor\n+          = ((ThreadPoolExecutor) boundedThreadPool);\n+      int currentCorePoolSize = threadPoolExecutor.getCorePoolSize();\n+\n+      if (newMaxPoolSize >= currentCorePoolSize) {\n+        threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize);\n\nReview Comment:\n   Setting both core pool size and max pool size to same value will make it a fixed size thread pool.\n   We should only set max pool size and executor will spawn new threads only when needed.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n+        initialPoolSize,\n+        r -> {\n+          Thread t = new Thread(r);\n+          t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement());\n+          return t;\n+        }\n+    );\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(\n+        abfsConfiguration.getWriteCorePoolSize());\n+  }\n+\n+  public AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n+   * @return Computed max thread pool size.\n+   */\n+  private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) {\n+      LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory);\n+      LOG.debug(\"The number of available processors is {} \", availableProcessors);\n+      int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors);\n+      LOG.debug(\"The max thread pool size is {} \", maxpoolSize);\n+      return maxpoolSize;\n+  }\n+\n+  /**\n+   * Calculates the available heap memory in gigabytes.\n+   * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory\n+   * allowed for the JVM and subtracts the currently used memory (total - free)\n+   * to determine how much heap memory is still available.\n+   * The result is rounded up to the nearest gigabyte.\n+   *\n+   * @return the available heap memory in gigabytes\n+   */\n+  private long getAvailableHeapMemory() {\n+    Runtime runtime = Runtime.getRuntime();\n+    long maxMemory = runtime.maxMemory();\n+    long usedMemory = runtime.totalMemory() - runtime.freeMemory();\n+    long availableHeapBytes = maxMemory - usedMemory;\n+    return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE;\n+  }\n+\n+  /**\n+   * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier.\n+   */\n+  private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) {\n\nReview Comment:\n   Are we computing thread pool size based on available memory?\n   Shouldn't it be available cpu?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n\nReview Comment:\n   We are using a fixed Thread Pool executor service here. with initialThreadPoolSize\n   Where are we setting the max threread pool size here?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n+        initialPoolSize,\n+        r -> {\n+          Thread t = new Thread(r);\n+          t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement());\n+          return t;\n+        }\n+    );\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(\n+        abfsConfiguration.getWriteCorePoolSize());\n+  }\n+\n+  public AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n+   * @return Computed max thread pool size.\n+   */\n+  private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) {\n+      LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory);\n+      LOG.debug(\"The number of available processors is {} \", availableProcessors);\n+      int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors);\n+      LOG.debug(\"The max thread pool size is {} \", maxpoolSize);\n+      return maxpoolSize;\n+  }\n+\n+  /**\n+   * Calculates the available heap memory in gigabytes.\n+   * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory\n+   * allowed for the JVM and subtracts the currently used memory (total - free)\n+   * to determine how much heap memory is still available.\n+   * The result is rounded up to the nearest gigabyte.\n+   *\n+   * @return the available heap memory in gigabytes\n+   */\n+  private long getAvailableHeapMemory() {\n+    Runtime runtime = Runtime.getRuntime();\n+    long maxMemory = runtime.maxMemory();\n+    long usedMemory = runtime.totalMemory() - runtime.freeMemory();\n+    long availableHeapBytes = maxMemory - usedMemory;\n+    return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE;\n+  }\n+\n+  /**\n+   * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier.\n+   */\n+  private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) {\n+    int multiplier;\n+    if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getLowTierMemoryMultiplier();\n+    } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getMediumTierMemoryMultiplier();\n+    } else {\n+      multiplier = abfsConfiguration.getHighTierMemoryMultiplier();\n+    }\n+    return availableProcessors * multiplier;\n+  }\n+\n+  /**\n+   * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem.\n+   *\n+   * @param filesystemName the name of the filesystem.\n+   * @param abfsConfiguration the configuration for the ABFS.\n+   *\n+   * @return the singleton instance.\n+   */\n+  public static synchronized WriteThreadPoolSizeManager getInstance(\n+      String filesystemName, AbfsConfiguration abfsConfiguration) {\n+    /* Check if an instance already exists in the map for the given filesystem */\n+    WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get(\n+        filesystemName);\n+\n+    /* If an existing instance is found, return it */\n+    if (existingInstance != null && existingInstance.boundedThreadPool != null\n+        && !existingInstance.boundedThreadPool.isShutdown()) {\n+      return existingInstance;\n+    }\n+\n+    /* Otherwise, create a new instance, put it in the map, and return it */\n+    LOG.debug(\n+        \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\",\n+        filesystemName);\n+    WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager(\n+        filesystemName, abfsConfiguration);\n+    POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance);\n+    return newInstance;\n+  }\n+\n+  /**\n+   * Adjusts the thread pool size to the specified maximum pool size.\n+   *\n+   * @param newMaxPoolSize the new maximum pool size.\n+   */\n+  private void adjustThreadPoolSize(int newMaxPoolSize) {\n+    synchronized (this) {\n+      ThreadPoolExecutor threadPoolExecutor\n+          = ((ThreadPoolExecutor) boundedThreadPool);\n+      int currentCorePoolSize = threadPoolExecutor.getCorePoolSize();\n+\n+      if (newMaxPoolSize >= currentCorePoolSize) {\n+        threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize);\n+        threadPoolExecutor.setCorePoolSize(newMaxPoolSize);\n+      } else {\n+        threadPoolExecutor.setCorePoolSize(newMaxPoolSize);\n+        threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize);\n+      }\n+\n+      LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize);\n\nReview Comment:\n   Have a single log line\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n\nReview Comment:\n   This seems a bit misleading. Configuration says its the max count but t is used as a inital thread pool size, may be a better variable name like `configuredMaxPoolSize`\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -478,6 +478,57 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES)\n   private int maxApacheHttpClientIoExceptionsRetries;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT,\n+      DefaultValue = DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT)\n+  private boolean dynamicWriteThreadPoolEnablement;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME,\n+      DefaultValue = DEFAULT_WRITE_THREADPOOL_KEEP_ALIVE_TIME)\n+  private int writeThreadPoolKeepAliveTime;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_CPU_MONITORING_INTERVAL,\n+      MinValue = MIN_WRITE_CPU_MONITORING_INTERVAL,\n+      MaxValue = MAX_WRITE_CPU_MONITORING_INTERVAL,\n+      DefaultValue = DEFAULT_WRITE_CPU_MONITORING_INTERVAL)\n+  private int writeCpuMonitoringInterval;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_CORE_POOL_SIZE,\n\nReview Comment:\n   Is this the minimum thread pool size?\r\n   Should we name it likewise then?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-22T07:18:00.525+0000", "updated": "2025-10-22T07:18:00.525+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032380", "id": "18032380", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2454205775\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -321,16 +330,13 @@ public void close() throws IOException {\n     try {\n       Futures.allAsList(futures).get();\n       // shutdown the threadPool and set it to null.\n-      HadoopExecutors.shutdown(boundedThreadPool, LOG,\n-          30, TimeUnit.SECONDS);\n-      boundedThreadPool = null;\n     } catch (InterruptedException e) {\n       LOG.error(\"Interrupted freeing leases\", e);\n       Thread.currentThread().interrupt();\n     } catch (ExecutionException e) {\n       LOG.error(\"Error freeing leases\", e);\n     } finally {\n-      IOUtils.cleanupWithLogger(LOG, getClient());\n+      IOUtils.cleanupWithLogger(LOG, poolSizeManager, getClient());\n\nReview Comment:\n   That is taken care when the AzureBlobFileSystemStore is shutdown\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,377 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+\n+    /*  Initialize the bounded thread pool executor */\n+    this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize);\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T07:36:53.064+0000", "updated": "2025-10-23T07:36:53.064+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032386", "id": "18032386", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2454234411\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,377 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+\n+    /*  Initialize the bounded thread pool executor */\n+    this.boundedThreadPool = Executors.newFixedThreadPool(initialPoolSize);\n+\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(\n+        abfsConfiguration.getWriteCorePoolSize());\n+  }\n+\n+  public AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n+   * @return Computed max thread pool size.\n+   */\n+  private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) {\n+      LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory);\n+      LOG.debug(\"The number of available processors is {} \", availableProcessors);\n+      int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors);\n+      LOG.debug(\"The max thread pool size is {} \", maxpoolSize);\n+      return maxpoolSize;\n+  }\n+\n+  /**\n+   * Calculates the available heap memory in gigabytes.\n+   * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory\n+   * allowed for the JVM and subtracts the currently used memory (total - free)\n+   * to determine how much heap memory is still available.\n+   * The result is rounded up to the nearest gigabyte.\n+   *\n+   * @return the available heap memory in gigabytes\n+   */\n+  private long getAvailableHeapMemory() {\n+    Runtime runtime = Runtime.getRuntime();\n+    long maxMemory = runtime.maxMemory();\n+    long usedMemory = runtime.totalMemory() - runtime.freeMemory();\n+    long availableHeapBytes = maxMemory - usedMemory;\n+    return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE;\n+  }\n+\n+  /**\n+   * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier.\n+   */\n+  private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) {\n+    int multiplier;\n+    if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getLowTierMemoryMultiplier();\n+    } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getMediumTierMemoryMultiplier();\n+    } else {\n+      multiplier = abfsConfiguration.getHighTierMemoryMultiplier();\n+    }\n+    return availableProcessors * multiplier;\n+  }\n+\n+  /**\n+   * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem.\n+   *\n+   * @param filesystemName the name of the filesystem.\n+   * @param abfsConfiguration the configuration for the ABFS.\n+   *\n+   * @return the singleton instance.\n+   */\n+  public static synchronized WriteThreadPoolSizeManager getInstance(\n+      String filesystemName, AbfsConfiguration abfsConfiguration) {\n+    /* Check if an instance already exists in the map for the given filesystem */\n+    WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get(\n+        filesystemName);\n+\n+    /* If an existing instance is found, return it */\n+    if (existingInstance != null && existingInstance.boundedThreadPool != null\n+        && !existingInstance.boundedThreadPool.isShutdown()) {\n+      return existingInstance;\n+    }\n+\n+    /* Otherwise, create a new instance, put it in the map, and return it */\n+    LOG.debug(\n+        \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\",\n+        filesystemName);\n+    WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager(\n+        filesystemName, abfsConfiguration);\n+    POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance);\n+    return newInstance;\n+  }\n+\n+  /**\n+   * Adjusts the thread pool size to the specified maximum pool size.\n+   *\n+   * @param newMaxPoolSize the new maximum pool size.\n+   */\n+  private void adjustThreadPoolSize(int newMaxPoolSize) {\n+    synchronized (this) {\n+      ThreadPoolExecutor threadPoolExecutor\n+          = ((ThreadPoolExecutor) boundedThreadPool);\n+      int currentCorePoolSize = threadPoolExecutor.getCorePoolSize();\n+\n+      if (newMaxPoolSize >= currentCorePoolSize) {\n+        threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize);\n+        threadPoolExecutor.setCorePoolSize(newMaxPoolSize);\n+      } else {\n+        threadPoolExecutor.setCorePoolSize(newMaxPoolSize);\n+        threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize);\n+      }\n+\n+      LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize);\n+      LOG.debug(\"The pool size is: {} \", threadPoolExecutor.getPoolSize());\n+      LOG.debug(\"The active thread count is: {}\", threadPoolExecutor.getActiveCount());\n+    }\n+  }\n+\n+  /**\n+   * Starts monitoring the CPU utilization and adjusts the thread pool size accordingly.\n+   */\n+  synchronized void startCPUMonitoring() {\n+    cpuMonitorExecutor.scheduleAtFixedRate(() -> {\n+      double cpuUtilization = getCpuUtilization();\n+      LOG.debug(\"Current CPU Utilization is this: {}\", cpuUtilization);\n+      try {\n+        adjustThreadPoolSizeBasedOnCPU(cpuUtilization);\n+      } catch (InterruptedException e) {\n+        throw new RuntimeException(String.format(\n+            \"Thread pool size adjustment interrupted for filesystem %s\",\n+            filesystemName), e);\n+      }\n+    }, 0, getAbfsConfiguration().getWriteCpuMonitoringInterval(), TimeUnit.SECONDS);\n+  }\n+\n+  /**\n+   * Gets the current CPU utilization.\n+   *\n+   * @return the CPU utilization as a percentage (0.0 to 1.0).\n+   */\n+  private double getCpuUtilization() {\n+    OperatingSystemMXBean osBean = ManagementFactory.getOperatingSystemMXBean();\n+    if (osBean instanceof com.sun.management.OperatingSystemMXBean) {\n+      com.sun.management.OperatingSystemMXBean sunOsBean\n+          = (com.sun.management.OperatingSystemMXBean) osBean;\n+      double cpuLoad = sunOsBean.getSystemCpuLoad();\n+      if (cpuLoad >= 0) {\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T07:49:04.744+0000", "updated": "2025-10-23T07:49:04.744+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032387", "id": "18032387", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2454238973\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -438,6 +438,10 @@ public class AbfsConfiguration{\n       FS_AZURE_ABFS_ENABLE_CHECKSUM_VALIDATION, DefaultValue = DEFAULT_ENABLE_ABFS_CHECKSUM_VALIDATION)\n   private boolean isChecksumValidationEnabled;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey =\n\nReview Comment:\n   taken care by trunk merge\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T07:50:41.003+0000", "updated": "2025-10-23T07:50:41.003+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032389", "id": "18032389", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2454257729\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -478,6 +478,57 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES)\n   private int maxApacheHttpClientIoExceptionsRetries;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT,\n+      DefaultValue = DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT)\n+  private boolean dynamicWriteThreadPoolEnablement;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME,\n+      DefaultValue = DEFAULT_WRITE_THREADPOOL_KEEP_ALIVE_TIME)\n+  private int writeThreadPoolKeepAliveTime;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_CPU_MONITORING_INTERVAL,\n+      MinValue = MIN_WRITE_CPU_MONITORING_INTERVAL,\n+      MaxValue = MAX_WRITE_CPU_MONITORING_INTERVAL,\n+      DefaultValue = DEFAULT_WRITE_CPU_MONITORING_INTERVAL)\n+  private int writeCpuMonitoringInterval;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_CORE_POOL_SIZE,\n\nReview Comment:\n   This was used to determine the no. of threads to spawn for CPU monitoring and was kept default as 1 but it would be better to always have a single thread and hence removed this config,\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T07:58:17.584+0000", "updated": "2025-10-23T07:58:17.584+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032402", "id": "18032402", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2454304102\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T08:17:18.877+0000", "updated": "2025-10-23T08:17:18.877+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032403", "id": "18032403", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2454309973\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n\nReview Comment:\n   So we are starting with the configured value of writeconcurrentrequestcount as the initial thread pool size which is also the max until it is scaled further, the logic for which is present in the adjustThreadPoolBasedOnCpu\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T08:19:42.481+0000", "updated": "2025-10-23T08:19:42.481+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032414", "id": "18032414", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3435800307\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 1 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  31m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  17m 44s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  17m 58s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 13s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 24s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 53 new + 1472 unchanged - 0 fixed = 1525 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 53 new + 1413 unchanged - 0 fixed = 1466 total (was 1413)  |\r\n   | -1 :x: |  spotbugs  |   0m 49s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 1 new + 178 unchanged - 0 fixed = 179 total (was 178)  |\r\n   | +1 :green_heart: |  shadedclient  |  18m 36s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 11s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 18s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  77m 36s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.WriteThreadPoolSizeManager.getAbfsConfiguration() may expose internal representation by returning WriteThreadPoolSizeManager.abfsConfiguration  At WriteThreadPoolSizeManager.java:by returning WriteThreadPoolSizeManager.abfsConfiguration  At WriteThreadPoolSizeManager.java:[line 127] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux e7d97cc8e101 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7708c26a556b7b84bd7edb6c70304f6f301eb0c3 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/testReport/ |\r\n   | Max. process+thread count | 642 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/18/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T08:49:58.762+0000", "updated": "2025-10-23T08:49:58.762+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032430", "id": "18032430", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2454540355\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n+        initialPoolSize,\n+        r -> {\n+          Thread t = new Thread(r);\n+          t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement());\n+          return t;\n+        }\n+    );\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(\n+        abfsConfiguration.getWriteCorePoolSize());\n+  }\n+\n+  public AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n+   * @return Computed max thread pool size.\n+   */\n+  private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) {\n+      LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory);\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T09:45:15.835+0000", "updated": "2025-10-23T09:45:15.835+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032431", "id": "18032431", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2454542830\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n+        initialPoolSize,\n+        r -> {\n+          Thread t = new Thread(r);\n+          t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement());\n+          return t;\n+        }\n+    );\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(\n+        abfsConfiguration.getWriteCorePoolSize());\n+  }\n+\n+  public AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n+   * @return Computed max thread pool size.\n+   */\n+  private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) {\n+      LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory);\n+      LOG.debug(\"The number of available processors is {} \", availableProcessors);\n+      int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors);\n+      LOG.debug(\"The max thread pool size is {} \", maxpoolSize);\n+      return maxpoolSize;\n+  }\n+\n+  /**\n+   * Calculates the available heap memory in gigabytes.\n+   * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory\n+   * allowed for the JVM and subtracts the currently used memory (total - free)\n+   * to determine how much heap memory is still available.\n+   * The result is rounded up to the nearest gigabyte.\n+   *\n+   * @return the available heap memory in gigabytes\n+   */\n+  private long getAvailableHeapMemory() {\n+    Runtime runtime = Runtime.getRuntime();\n+    long maxMemory = runtime.maxMemory();\n+    long usedMemory = runtime.totalMemory() - runtime.freeMemory();\n+    long availableHeapBytes = maxMemory - usedMemory;\n+    return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE;\n+  }\n+\n+  /**\n+   * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier.\n+   */\n+  private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) {\n\nReview Comment:\n   This is during initialization when CPU doesn't play a role, CPU is used for scaling and descaling only in the current design\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T09:46:16.037+0000", "updated": "2025-10-23T09:46:16.037+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032432", "id": "18032432", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2454547932\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n+        initialPoolSize,\n+        r -> {\n+          Thread t = new Thread(r);\n+          t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement());\n+          return t;\n+        }\n+    );\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(\n+        abfsConfiguration.getWriteCorePoolSize());\n+  }\n+\n+  public AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n+   * @return Computed max thread pool size.\n+   */\n+  private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) {\n+      LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory);\n+      LOG.debug(\"The number of available processors is {} \", availableProcessors);\n+      int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors);\n+      LOG.debug(\"The max thread pool size is {} \", maxpoolSize);\n+      return maxpoolSize;\n+  }\n+\n+  /**\n+   * Calculates the available heap memory in gigabytes.\n+   * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory\n+   * allowed for the JVM and subtracts the currently used memory (total - free)\n+   * to determine how much heap memory is still available.\n+   * The result is rounded up to the nearest gigabyte.\n+   *\n+   * @return the available heap memory in gigabytes\n+   */\n+  private long getAvailableHeapMemory() {\n+    Runtime runtime = Runtime.getRuntime();\n+    long maxMemory = runtime.maxMemory();\n+    long usedMemory = runtime.totalMemory() - runtime.freeMemory();\n+    long availableHeapBytes = maxMemory - usedMemory;\n+    return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE;\n+  }\n+\n+  /**\n+   * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier.\n+   */\n+  private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) {\n+    int multiplier;\n+    if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getLowTierMemoryMultiplier();\n+    } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getMediumTierMemoryMultiplier();\n+    } else {\n+      multiplier = abfsConfiguration.getHighTierMemoryMultiplier();\n+    }\n+    return availableProcessors * multiplier;\n+  }\n+\n+  /**\n+   * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem.\n+   *\n+   * @param filesystemName the name of the filesystem.\n+   * @param abfsConfiguration the configuration for the ABFS.\n+   *\n+   * @return the singleton instance.\n+   */\n+  public static synchronized WriteThreadPoolSizeManager getInstance(\n+      String filesystemName, AbfsConfiguration abfsConfiguration) {\n+    /* Check if an instance already exists in the map for the given filesystem */\n+    WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get(\n+        filesystemName);\n+\n+    /* If an existing instance is found, return it */\n+    if (existingInstance != null && existingInstance.boundedThreadPool != null\n+        && !existingInstance.boundedThreadPool.isShutdown()) {\n+      return existingInstance;\n+    }\n+\n+    /* Otherwise, create a new instance, put it in the map, and return it */\n+    LOG.debug(\n+        \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\",\n+        filesystemName);\n+    WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager(\n+        filesystemName, abfsConfiguration);\n+    POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance);\n+    return newInstance;\n+  }\n+\n+  /**\n+   * Adjusts the thread pool size to the specified maximum pool size.\n+   *\n+   * @param newMaxPoolSize the new maximum pool size.\n+   */\n+  private void adjustThreadPoolSize(int newMaxPoolSize) {\n+    synchronized (this) {\n+      ThreadPoolExecutor threadPoolExecutor\n+          = ((ThreadPoolExecutor) boundedThreadPool);\n+      int currentCorePoolSize = threadPoolExecutor.getCorePoolSize();\n+\n+      if (newMaxPoolSize >= currentCorePoolSize) {\n+        threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize);\n\nReview Comment:\n   If we do on need basis, it doesn't serve our purpose of being aggressive. This was tried as well and even though max was set to higher number, available threads were not getting used.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T09:47:45.980+0000", "updated": "2025-10-23T09:47:45.980+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032434", "id": "18032434", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2454550766\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,383 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.OperatingSystemMXBean;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteMaxConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n+        initialPoolSize,\n+        r -> {\n+          Thread t = new Thread(r);\n+          t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement());\n+          return t;\n+        }\n+    );\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(\n+        abfsConfiguration.getWriteCorePoolSize());\n+  }\n+\n+  public AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n+   * @return Computed max thread pool size.\n+   */\n+  private int getComputedMaxPoolSize(final int availableProcessors, long initialAvailableHeapMemory) {\n+      LOG.debug(\"The available heap space in GB {} \", initialAvailableHeapMemory);\n+      LOG.debug(\"The number of available processors is {} \", availableProcessors);\n+      int maxpoolSize = getMemoryTierMaxThreads(initialAvailableHeapMemory, availableProcessors);\n+      LOG.debug(\"The max thread pool size is {} \", maxpoolSize);\n+      return maxpoolSize;\n+  }\n+\n+  /**\n+   * Calculates the available heap memory in gigabytes.\n+   * This method uses {@link Runtime#getRuntime()} to obtain the maximum heap memory\n+   * allowed for the JVM and subtracts the currently used memory (total - free)\n+   * to determine how much heap memory is still available.\n+   * The result is rounded up to the nearest gigabyte.\n+   *\n+   * @return the available heap memory in gigabytes\n+   */\n+  private long getAvailableHeapMemory() {\n+    Runtime runtime = Runtime.getRuntime();\n+    long maxMemory = runtime.maxMemory();\n+    long usedMemory = runtime.totalMemory() - runtime.freeMemory();\n+    long availableHeapBytes = maxMemory - usedMemory;\n+    return (availableHeapBytes + BYTES_PER_GIGABYTE - 1) / BYTES_PER_GIGABYTE;\n+  }\n+\n+  /**\n+   * Returns aggressive thread count = CPU cores \u00d7 multiplier based on heap tier.\n+   */\n+  private int getMemoryTierMaxThreads(long availableHeapGB, int availableProcessors) {\n+    int multiplier;\n+    if (availableHeapGB <= LOW_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getLowTierMemoryMultiplier();\n+    } else if (availableHeapGB <= MEDIUM_HEAP_SPACE_FACTOR) {\n+      multiplier = abfsConfiguration.getMediumTierMemoryMultiplier();\n+    } else {\n+      multiplier = abfsConfiguration.getHighTierMemoryMultiplier();\n+    }\n+    return availableProcessors * multiplier;\n+  }\n+\n+  /**\n+   * Returns the singleton instance of WriteThreadPoolSizeManager for the given filesystem.\n+   *\n+   * @param filesystemName the name of the filesystem.\n+   * @param abfsConfiguration the configuration for the ABFS.\n+   *\n+   * @return the singleton instance.\n+   */\n+  public static synchronized WriteThreadPoolSizeManager getInstance(\n+      String filesystemName, AbfsConfiguration abfsConfiguration) {\n+    /* Check if an instance already exists in the map for the given filesystem */\n+    WriteThreadPoolSizeManager existingInstance = POOL_SIZE_MANAGER_MAP.get(\n+        filesystemName);\n+\n+    /* If an existing instance is found, return it */\n+    if (existingInstance != null && existingInstance.boundedThreadPool != null\n+        && !existingInstance.boundedThreadPool.isShutdown()) {\n+      return existingInstance;\n+    }\n+\n+    /* Otherwise, create a new instance, put it in the map, and return it */\n+    LOG.debug(\n+        \"Creating new WriteThreadPoolSizeManager instance for filesystem: {}\",\n+        filesystemName);\n+    WriteThreadPoolSizeManager newInstance = new WriteThreadPoolSizeManager(\n+        filesystemName, abfsConfiguration);\n+    POOL_SIZE_MANAGER_MAP.put(filesystemName, newInstance);\n+    return newInstance;\n+  }\n+\n+  /**\n+   * Adjusts the thread pool size to the specified maximum pool size.\n+   *\n+   * @param newMaxPoolSize the new maximum pool size.\n+   */\n+  private void adjustThreadPoolSize(int newMaxPoolSize) {\n+    synchronized (this) {\n+      ThreadPoolExecutor threadPoolExecutor\n+          = ((ThreadPoolExecutor) boundedThreadPool);\n+      int currentCorePoolSize = threadPoolExecutor.getCorePoolSize();\n+\n+      if (newMaxPoolSize >= currentCorePoolSize) {\n+        threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize);\n+        threadPoolExecutor.setCorePoolSize(newMaxPoolSize);\n+      } else {\n+        threadPoolExecutor.setCorePoolSize(newMaxPoolSize);\n+        threadPoolExecutor.setMaximumPoolSize(newMaxPoolSize);\n+      }\n+\n+      LOG.debug(\"The thread pool size is: {} \", newMaxPoolSize);\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T09:48:41.077+0000", "updated": "2025-10-23T09:48:41.077+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18032453", "id": "18032453", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3436426336\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 22s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  24m 50s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 44s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  16m 11s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  16m 24s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 50 new + 1472 unchanged - 0 fixed = 1522 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 50 new + 1413 unchanged - 0 fixed = 1463 total (was 1413)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  16m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 13s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 23s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  67m 45s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5c77e6ac2aef 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / fb58cf38644cae72dc0238fa354724d0af74d820 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/testReport/ |\r\n   | Max. process+thread count | 612 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/19/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-23T11:23:09.821+0000", "updated": "2025-10-23T11:23:09.821+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033173", "id": "18033173", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2464543406\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -459,10 +459,43 @@ public static String containerProperty(String property, String fsName, String ac\n   public static final String FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD = \"fs.azure.blob.dir.rename.max.thread\";\n   /**Maximum number of thread per blob-delete orchestration: {@value}*/\n   public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\";\n+  /**\n+   * Configuration key for the keep-alive time for the write thread pool.\n+   * This value specifies the amount of time that threads in the write thread pool\n+   * will remain idle before being terminated.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME = \"fs.azure.write.threadpool.keep.alive.time\";\n\nReview Comment:\n   Is this time in millis? If yes, can we put millis at the end in the name and in the config key?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -459,10 +459,43 @@ public static String containerProperty(String property, String fsName, String ac\n   public static final String FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD = \"fs.azure.blob.dir.rename.max.thread\";\n   /**Maximum number of thread per blob-delete orchestration: {@value}*/\n   public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\";\n+  /**\n+   * Configuration key for the keep-alive time for the write thread pool.\n+   * This value specifies the amount of time that threads in the write thread pool\n+   * will remain idle before being terminated.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME = \"fs.azure.write.threadpool.keep.alive.time\";\n+\n+  public static final String FS_AZURE_WRITE_CPU_MONITORING_INTERVAL = \"fs.azure.write.cpu.monitoring.interval\";\n+\n+  public static final String FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT = \"fs.azure.write.dynamic.threadpool.enablement\";\n+\n+  public static final String FS_AZURE_WRITE_HIGH_CPU_THRESHOLD = \"fs.azure.write.high.cpu.threshold\";\n+\n+  public static final String FS_AZURE_WRITE_MEDIUM_CPU_THRESHOLD = \"fs.azure.write.medium.cpu.threshold\";\n+\n+  public static final String FS_AZURE_WRITE_LOW_CPU_THRESHOLD = \"fs.azure.write.low.cpu.threshold\";\n+\n+  public static final String FS_AZURE_WRITE_LOW_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.low.tier.memory.multiplier\";\n+\n+  public static final String FS_AZURE_WRITE_MEDIUM_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.medium.tier.memory.multiplier\";\n+\n+  public static final String FS_AZURE_WRITE_HIGH_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.high.tier.memory.multiplier\";\n+\n+\n+\n   /**Flag to enable/disable sending client transactional ID during create/rename operations: {@value}*/\n   public static final String FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = \"fs.azure.enable.client.transaction.id\";\n   /**Flag to enable/disable create idempotency during create operation: {@value}*/\n   public static final String FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = \"fs.azure.enable.create.blob.idempotency\";\n \n+  /**\n\nReview Comment:\n   Since this config is related to the above newly added ones, should we group them together?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,388 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.MemoryMXBean;\n+import java.lang.management.MemoryUsage;\n+\n+import com.sun.management.OperatingSystemMXBean;\n+\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n+        initialPoolSize,\n+        r -> {\n+          Thread t = new Thread(r);\n+          t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement());\n+          return t;\n+        }\n+    );\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(1);\n+  }\n+\n+  /** Returns the internal {@link AbfsConfiguration}. */\n+  private AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n\nReview Comment:\n   @param initialAvailableHeapMemory is missing\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -277,11 +278,19 @@ public AzureBlobFileSystemStore(\n     }\n     this.blockFactory = abfsStoreBuilder.blockFactory;\n     this.blockOutputActiveBlocks = abfsStoreBuilder.blockOutputActiveBlocks;\n-    this.boundedThreadPool = BlockingThreadPoolExecutorService.newInstance(\n-        abfsConfiguration.getWriteMaxConcurrentRequestCount(),\n-        abfsConfiguration.getMaxWriteRequestsToQueue(),\n-        10L, TimeUnit.SECONDS,\n-        \"abfs-bounded\");\n+    if (abfsConfiguration.isDynamicWriteThreadPoolEnablement()) {\n+      this.poolSizeManager = WriteThreadPoolSizeManager.getInstance(\n+          getClient().getFileSystem() + \"-\" + UUID.randomUUID(),\n+          abfsConfiguration);\n+      poolSizeManager.startCPUMonitoring();\n\nReview Comment:\n   WriteThreadPoolSizeManager.getInstance(...) can return null and this line can throw null pointer exception.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T10:32:39.307+0000", "updated": "2025-10-27T10:32:39.307+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033221", "id": "18033221", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2465547159\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -478,6 +478,57 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_APACHE_HTTP_CLIENT_MAX_IO_EXCEPTION_RETRIES)\n   private int maxApacheHttpClientIoExceptionsRetries;\n \n+  @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT,\n+      DefaultValue = DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT)\n+  private boolean dynamicWriteThreadPoolEnablement;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME,\n+      DefaultValue = DEFAULT_WRITE_THREADPOOL_KEEP_ALIVE_TIME)\n+  private int writeThreadPoolKeepAliveTime;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_CPU_MONITORING_INTERVAL,\n+      MinValue = MIN_WRITE_CPU_MONITORING_INTERVAL,\n+      MaxValue = MAX_WRITE_CPU_MONITORING_INTERVAL,\n+      DefaultValue = DEFAULT_WRITE_CPU_MONITORING_INTERVAL)\n+  private int writeCpuMonitoringInterval;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_WRITE_THREADPOOL_CORE_POOL_SIZE,\n\nReview Comment:\n   Makes sense\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T12:50:51.757+0000", "updated": "2025-10-27T12:50:51.757+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033232", "id": "18033232", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2465584624\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -262,6 +281,48 @@ public final class FileSystemConfigurations {\n \n   public static final int DEFAULT_FS_AZURE_BLOB_DELETE_THREAD = DEFAULT_FS_AZURE_LISTING_ACTION_THREADS;\n \n+  public static final boolean DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT = true;\n\nReview Comment:\n   By default should be disabled\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestWriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,770 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.assertj.core.api.Assertions;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.CyclicBarrier;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicIntegerArray;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ZERO;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+class TestWriteThreadPoolSizeManager extends AbstractAbfsIntegrationTest {\n+\n+  private AbfsConfiguration mockConfig;\n+  private static final double HIGH_CPU_UTILIZATION_THRESHOLD = 0.95;\n+  private static final double LOW_CPU_UTILIZATION_THRESHOLD = 0.05;\n+  private static final int THREAD_SLEEP_DURATION_MS = 200;\n+  private static final String TEST_FILE_PATH = \"testFilePath\";\n+  private static final String TEST_DIR_PATH = \"testDirPath\";\n+  private static final int TEST_FILE_LENGTH = 1024 * 1024 * 8;\n+  private static final int CONCURRENT_REQUEST_COUNT = 15;\n+  private static final int THREAD_POOL_KEEP_ALIVE_TIME = 10;\n+  private static final int LOW_TIER_MEMORY_MULTIPLIER = 4;\n+  private static final int MEDIUM_TIER_MEMORY_MULTIPLIER = 6;\n+  private static final int HIGH_TIER_MEMORY_MULTIPLIER = 8;\n+  private static final int HIGH_CPU_THRESHOLD = 15;\n+  private static final int MEDIUM_CPU_THRESHOLD = 10;\n+  private static final int LOW_CPU_THRESHOLD = 5;\n+  private static final int CPU_MONITORING_INTERVAL = 15;\n+  private static final int WAIT_DURATION_MS = 3000;\n+  private static final int LATCH_TIMEOUT_SECONDS = 60;\n+  private static final int RESIZE_WAIT_TIME_MS = 6_000;\n+  private static final double HIGH_CPU_USAGE_RATIO = 0.95;\n+  private static final double LOW_CPU_USAGE_RATIO = 0.05;\n+  private static final int SLEEP_DURATION_MS = 150;\n+  private static final int AWAIT_TIMEOUT_SECONDS = 45;\n+  private static final int RESIZER_JOIN_TIMEOUT_MS = 2_000;\n+  private static final int WAIT_TIMEOUT_MS = 5000;\n+  private static final int SLEEP_DURATION_30S_MS = 30000;\n+  private static final int SMALL_PAUSE_MS = 50;\n+  private static final int BURST_LOAD = 50;\n+  private static final long LOAD_SLEEP_DURATION_MS = 2000;\n+\n+  TestWriteThreadPoolSizeManager() throws Exception {\n+    super.setup();\n+  }\n+\n+  /**\n+   * Common setup to prepare a mock configuration for each test.\n+   */\n+  @BeforeEach\n+  public void setUp() {\n+    mockConfig = mock(AbfsConfiguration.class);\n+    when(mockConfig.getWriteConcurrentRequestCount()).thenReturn(CONCURRENT_REQUEST_COUNT);\n+    when(mockConfig.getWriteThreadPoolKeepAliveTime()).thenReturn(THREAD_POOL_KEEP_ALIVE_TIME);\n+    when(mockConfig.getLowTierMemoryMultiplier()).thenReturn(LOW_TIER_MEMORY_MULTIPLIER);\n+    when(mockConfig.getMediumTierMemoryMultiplier()).thenReturn(MEDIUM_TIER_MEMORY_MULTIPLIER);\n+    when(mockConfig.getHighTierMemoryMultiplier()).thenReturn(HIGH_TIER_MEMORY_MULTIPLIER);\n+    when(mockConfig.getWriteHighCpuThreshold()).thenReturn(HIGH_CPU_THRESHOLD);\n+    when(mockConfig.getWriteMediumCpuThreshold()).thenReturn(MEDIUM_CPU_THRESHOLD);\n+    when(mockConfig.getWriteLowCpuThreshold()).thenReturn(LOW_CPU_THRESHOLD);\n+    when(mockConfig.getWriteCpuMonitoringInterval()).thenReturn(CPU_MONITORING_INTERVAL);\n+  }\n+\n+  /**\n+   * Ensures that {@link WriteThreadPoolSizeManager#getInstance(String, AbfsConfiguration)} returns a singleton per key.\n+   */\n+  @Test\n+  void testGetInstanceReturnsSingleton() {\n\nReview Comment:\n   Its no more a singleton right?\n   May be we don't need this test anymore.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T13:03:53.902+0000", "updated": "2025-10-27T13:03:53.902+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033239", "id": "18033239", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2465654952\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestWriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,770 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.assertj.core.api.Assertions;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.CyclicBarrier;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicIntegerArray;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.ZERO;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+class TestWriteThreadPoolSizeManager extends AbstractAbfsIntegrationTest {\n+\n+  private AbfsConfiguration mockConfig;\n+  private static final double HIGH_CPU_UTILIZATION_THRESHOLD = 0.95;\n+  private static final double LOW_CPU_UTILIZATION_THRESHOLD = 0.05;\n+  private static final int THREAD_SLEEP_DURATION_MS = 200;\n+  private static final String TEST_FILE_PATH = \"testFilePath\";\n+  private static final String TEST_DIR_PATH = \"testDirPath\";\n+  private static final int TEST_FILE_LENGTH = 1024 * 1024 * 8;\n+  private static final int CONCURRENT_REQUEST_COUNT = 15;\n+  private static final int THREAD_POOL_KEEP_ALIVE_TIME = 10;\n+  private static final int LOW_TIER_MEMORY_MULTIPLIER = 4;\n+  private static final int MEDIUM_TIER_MEMORY_MULTIPLIER = 6;\n+  private static final int HIGH_TIER_MEMORY_MULTIPLIER = 8;\n+  private static final int HIGH_CPU_THRESHOLD = 15;\n+  private static final int MEDIUM_CPU_THRESHOLD = 10;\n+  private static final int LOW_CPU_THRESHOLD = 5;\n+  private static final int CPU_MONITORING_INTERVAL = 15;\n+  private static final int WAIT_DURATION_MS = 3000;\n+  private static final int LATCH_TIMEOUT_SECONDS = 60;\n+  private static final int RESIZE_WAIT_TIME_MS = 6_000;\n+  private static final double HIGH_CPU_USAGE_RATIO = 0.95;\n+  private static final double LOW_CPU_USAGE_RATIO = 0.05;\n+  private static final int SLEEP_DURATION_MS = 150;\n+  private static final int AWAIT_TIMEOUT_SECONDS = 45;\n+  private static final int RESIZER_JOIN_TIMEOUT_MS = 2_000;\n+  private static final int WAIT_TIMEOUT_MS = 5000;\n+  private static final int SLEEP_DURATION_30S_MS = 30000;\n+  private static final int SMALL_PAUSE_MS = 50;\n+  private static final int BURST_LOAD = 50;\n+  private static final long LOAD_SLEEP_DURATION_MS = 2000;\n+\n+  TestWriteThreadPoolSizeManager() throws Exception {\n+    super.setup();\n+  }\n+\n+  /**\n+   * Common setup to prepare a mock configuration for each test.\n+   */\n+  @BeforeEach\n+  public void setUp() {\n+    mockConfig = mock(AbfsConfiguration.class);\n+    when(mockConfig.getWriteConcurrentRequestCount()).thenReturn(CONCURRENT_REQUEST_COUNT);\n+    when(mockConfig.getWriteThreadPoolKeepAliveTime()).thenReturn(THREAD_POOL_KEEP_ALIVE_TIME);\n+    when(mockConfig.getLowTierMemoryMultiplier()).thenReturn(LOW_TIER_MEMORY_MULTIPLIER);\n+    when(mockConfig.getMediumTierMemoryMultiplier()).thenReturn(MEDIUM_TIER_MEMORY_MULTIPLIER);\n+    when(mockConfig.getHighTierMemoryMultiplier()).thenReturn(HIGH_TIER_MEMORY_MULTIPLIER);\n+    when(mockConfig.getWriteHighCpuThreshold()).thenReturn(HIGH_CPU_THRESHOLD);\n+    when(mockConfig.getWriteMediumCpuThreshold()).thenReturn(MEDIUM_CPU_THRESHOLD);\n+    when(mockConfig.getWriteLowCpuThreshold()).thenReturn(LOW_CPU_THRESHOLD);\n+    when(mockConfig.getWriteCpuMonitoringInterval()).thenReturn(CPU_MONITORING_INTERVAL);\n+  }\n+\n+  /**\n+   * Ensures that {@link WriteThreadPoolSizeManager#getInstance(String, AbfsConfiguration)} returns a singleton per key.\n+   */\n+  @Test\n+  void testGetInstanceReturnsSingleton() {\n\nReview Comment:\n   Currently the design returns a singleton instance only_\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T13:22:17.836+0000", "updated": "2025-10-27T13:22:17.836+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033240", "id": "18033240", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2465656516\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java:\n##########\n@@ -262,6 +281,48 @@ public final class FileSystemConfigurations {\n \n   public static final int DEFAULT_FS_AZURE_BLOB_DELETE_THREAD = DEFAULT_FS_AZURE_LISTING_ACTION_THREADS;\n \n+  public static final boolean DEFAULT_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT = true;\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T13:22:51.887+0000", "updated": "2025-10-27T13:22:51.887+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033244", "id": "18033244", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2465671952\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/WriteThreadPoolSizeManager.java:\n##########\n@@ -0,0 +1,388 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.lang.management.ManagementFactory;\n+import java.lang.management.MemoryMXBean;\n+import java.lang.management.MemoryUsage;\n+\n+import com.sun.management.OperatingSystemMXBean;\n+\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.util.concurrent.HadoopExecutors;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.LOW_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.MEDIUM_HEAP_SPACE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.BYTES_PER_GIGABYTE;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HIGH_MEDIUM_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.HUNDRED_D;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HEAP_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_HIGH_MEMORY_DECREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.LOW_CPU_POOL_SIZE_INCREASE_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_LOW_MEMORY_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.MEDIUM_CPU_REDUCTION_FACTOR;\n+import static org.apache.hadoop.fs.azurebfs.constants.FileSystemConfigurations.THIRTY_SECONDS;\n+\n+/**\n+ * Manages a thread pool for writing operations, adjusting the pool size based on CPU utilization.\n+ */\n+public final class WriteThreadPoolSizeManager implements Closeable {\n+\n+  /* Maximum allowed size for the thread pool. */\n+  private final int maxThreadPoolSize;\n+  /* Executor for periodically monitoring CPU usage. */\n+  private final ScheduledExecutorService cpuMonitorExecutor;\n+  /* Thread pool whose size is dynamically managed. */\n+  private volatile ExecutorService boundedThreadPool;\n+  /* Lock to ensure thread-safe updates to the thread pool. */\n+  private final Lock lock = new ReentrantLock();\n+  /* New computed max size for the thread pool after adjustment. */\n+  private volatile int newMaxPoolSize;\n+  /* Logger instance for logging events from WriteThreadPoolSizeManager. */\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      WriteThreadPoolSizeManager.class);\n+  /* Map to maintain a WriteThreadPoolSizeManager instance per filesystem. */\n+  private static final ConcurrentHashMap<String, WriteThreadPoolSizeManager>\n+      POOL_SIZE_MANAGER_MAP = new ConcurrentHashMap<>();\n+  /* Name of the filesystem associated with this manager. */\n+  private final String filesystemName;\n+  /* Initial size for the thread pool when created. */\n+  private final int initialPoolSize;\n+  /* Initially available heap memory. */\n+  private final long initialAvailableHeapMemory;\n+  /* The configuration instance. */\n+  private final AbfsConfiguration abfsConfiguration;\n+\n+  /**\n+   * Private constructor to initialize the write thread pool and CPU monitor executor\n+   * based on system resources and ABFS configuration.\n+   *\n+   * @param filesystemName       Name of the ABFS filesystem.\n+   * @param abfsConfiguration    Configuration containing pool size parameters.\n+   */\n+  private WriteThreadPoolSizeManager(String filesystemName,\n+      AbfsConfiguration abfsConfiguration) {\n+    this.filesystemName = filesystemName;\n+    this.abfsConfiguration = abfsConfiguration;\n+    int availableProcessors = Runtime.getRuntime().availableProcessors();\n+    /* Get the heap space available when the instance is created */\n+    this.initialAvailableHeapMemory = getAvailableHeapMemory();\n+    /* Compute the max pool size */\n+    int computedMaxPoolSize = getComputedMaxPoolSize(availableProcessors, initialAvailableHeapMemory);\n+\n+    /* Get the initial pool size from config, fallback to at least 1 */\n+    this.initialPoolSize = Math.max(1,\n+        abfsConfiguration.getWriteConcurrentRequestCount());\n+\n+    /* Set the upper bound for the thread pool size */\n+    this.maxThreadPoolSize = Math.max(computedMaxPoolSize, initialPoolSize);\n+    AtomicInteger threadCount = new AtomicInteger(1);\n+    this.boundedThreadPool = Executors.newFixedThreadPool(\n+        initialPoolSize,\n+        r -> {\n+          Thread t = new Thread(r);\n+          t.setName(\"abfs-boundedwrite-\" + threadCount.getAndIncrement());\n+          return t;\n+        }\n+    );\n+    ThreadPoolExecutor executor = (ThreadPoolExecutor) this.boundedThreadPool;\n+    executor.setKeepAliveTime(\n+        abfsConfiguration.getWriteThreadPoolKeepAliveTime(), TimeUnit.SECONDS);\n+    executor.allowCoreThreadTimeOut(true);\n+    /* Create a scheduled executor for CPU monitoring and pool adjustment */\n+    this.cpuMonitorExecutor = Executors.newScheduledThreadPool(1);\n+  }\n+\n+  /** Returns the internal {@link AbfsConfiguration}. */\n+  private AbfsConfiguration getAbfsConfiguration() {\n+    return abfsConfiguration;\n+  }\n+\n+  /**\n+   * Calculates the max thread pool size using a multiplier based on\n+   * memory per core. Higher memory per core results in a larger multiplier.\n+   *\n+   * @param availableProcessors Number of CPU cores.\n\nReview Comment:\n   taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T13:27:33.025+0000", "updated": "2025-10-27T13:27:33.025+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033245", "id": "18033245", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2465675885\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java:\n##########\n@@ -277,11 +278,19 @@ public AzureBlobFileSystemStore(\n     }\n     this.blockFactory = abfsStoreBuilder.blockFactory;\n     this.blockOutputActiveBlocks = abfsStoreBuilder.blockOutputActiveBlocks;\n-    this.boundedThreadPool = BlockingThreadPoolExecutorService.newInstance(\n-        abfsConfiguration.getWriteMaxConcurrentRequestCount(),\n-        abfsConfiguration.getMaxWriteRequestsToQueue(),\n-        10L, TimeUnit.SECONDS,\n-        \"abfs-bounded\");\n+    if (abfsConfiguration.isDynamicWriteThreadPoolEnablement()) {\n+      this.poolSizeManager = WriteThreadPoolSizeManager.getInstance(\n+          getClient().getFileSystem() + \"-\" + UUID.randomUUID(),\n+          abfsConfiguration);\n+      poolSizeManager.startCPUMonitoring();\n\nReview Comment:\n   It creates a new instance every time which is not null, can you please elaborate on your concern\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T13:28:48.349+0000", "updated": "2025-10-27T13:28:48.349+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033246", "id": "18033246", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2465682108\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -459,10 +459,43 @@ public static String containerProperty(String property, String fsName, String ac\n   public static final String FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD = \"fs.azure.blob.dir.rename.max.thread\";\n   /**Maximum number of thread per blob-delete orchestration: {@value}*/\n   public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\";\n+  /**\n+   * Configuration key for the keep-alive time for the write thread pool.\n+   * This value specifies the amount of time that threads in the write thread pool\n+   * will remain idle before being terminated.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME = \"fs.azure.write.threadpool.keep.alive.time\";\n+\n+  public static final String FS_AZURE_WRITE_CPU_MONITORING_INTERVAL = \"fs.azure.write.cpu.monitoring.interval\";\n+\n+  public static final String FS_AZURE_WRITE_DYNAMIC_THREADPOOL_ENABLEMENT = \"fs.azure.write.dynamic.threadpool.enablement\";\n+\n+  public static final String FS_AZURE_WRITE_HIGH_CPU_THRESHOLD = \"fs.azure.write.high.cpu.threshold\";\n+\n+  public static final String FS_AZURE_WRITE_MEDIUM_CPU_THRESHOLD = \"fs.azure.write.medium.cpu.threshold\";\n+\n+  public static final String FS_AZURE_WRITE_LOW_CPU_THRESHOLD = \"fs.azure.write.low.cpu.threshold\";\n+\n+  public static final String FS_AZURE_WRITE_LOW_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.low.tier.memory.multiplier\";\n+\n+  public static final String FS_AZURE_WRITE_MEDIUM_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.medium.tier.memory.multiplier\";\n+\n+  public static final String FS_AZURE_WRITE_HIGH_TIER_MEMORY_MULTIPLIER = \"fs.azure.write.high.tier.memory.multiplier\";\n+\n+\n+\n   /**Flag to enable/disable sending client transactional ID during create/rename operations: {@value}*/\n   public static final String FS_AZURE_ENABLE_CLIENT_TRANSACTION_ID = \"fs.azure.enable.client.transaction.id\";\n   /**Flag to enable/disable create idempotency during create operation: {@value}*/\n   public static final String FS_AZURE_ENABLE_CREATE_BLOB_IDEMPOTENCY = \"fs.azure.enable.create.blob.idempotency\";\n \n+  /**\n\nReview Comment:\n   this config was no longer needed, hence removed it\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T13:30:37.531+0000", "updated": "2025-10-27T13:30:37.531+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033247", "id": "18033247", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2465687329\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -459,10 +459,43 @@ public static String containerProperty(String property, String fsName, String ac\n   public static final String FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD = \"fs.azure.blob.dir.rename.max.thread\";\n   /**Maximum number of thread per blob-delete orchestration: {@value}*/\n   public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\";\n+  /**\n+   * Configuration key for the keep-alive time for the write thread pool.\n+   * This value specifies the amount of time that threads in the write thread pool\n+   * will remain idle before being terminated.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME = \"fs.azure.write.threadpool.keep.alive.time\";\n\nReview Comment:\n   This is time in seconds\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T13:31:47.924+0000", "updated": "2025-10-27T13:31:47.924+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033257", "id": "18033257", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#discussion_r2465902645\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -459,10 +459,43 @@ public static String containerProperty(String property, String fsName, String ac\n   public static final String FS_AZURE_BLOB_DIR_RENAME_MAX_THREAD = \"fs.azure.blob.dir.rename.max.thread\";\n   /**Maximum number of thread per blob-delete orchestration: {@value}*/\n   public static final String FS_AZURE_BLOB_DIR_DELETE_MAX_THREAD = \"fs.azure.blob.dir.delete.max.thread\";\n+  /**\n+   * Configuration key for the keep-alive time for the write thread pool.\n+   * This value specifies the amount of time that threads in the write thread pool\n+   * will remain idle before being terminated.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME = \"fs.azure.write.threadpool.keep.alive.time\";\n\nReview Comment:\n   It is good if we rename this variable to FS_AZURE_WRITE_THREADPOOL_KEEP_ALIVE_TIME_SECONDS = \"fs.azure.write.threadpool.keep.alive.time.seconds\". This will help us to know in future in which unit this time is.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T14:28:01.104+0000", "updated": "2025-10-27T14:28:01.104+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033262", "id": "18033262", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3451664411\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   8m 28s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 46s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  7s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 19s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 50 new + 1472 unchanged - 0 fixed = 1522 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 50 new + 1413 unchanged - 0 fixed = 1463 total (was 1413)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  15m 38s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  69m  8s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 5e54e7b9a336 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 78f54686e9b34146f014af16b4cbf30945a659c9 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/testReport/ |\r\n   | Max. process+thread count | 612 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/20/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-27T14:46:27.117+0000", "updated": "2025-10-27T14:46:27.117+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033484", "id": "18033484", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3455405212\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 21s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 45s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  13m 58s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 11s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 49 new + 1472 unchanged - 0 fixed = 1521 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 49 new + 1413 unchanged - 0 fixed = 1462 total (was 1413)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 44s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 28s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  6s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  58m 46s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 567ceba3cb8b 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a72afe4d40534d5cf403d52e90e6e990504d2967 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/testReport/ |\r\n   | Max. process+thread count | 632 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/21/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T09:21:46.453+0000", "updated": "2025-10-28T09:21:46.453+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033549", "id": "18033549", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3456331644\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 19s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 20s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 31s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 44s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 20s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 49 new + 1472 unchanged - 0 fixed = 1521 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 49 new + 1413 unchanged - 0 fixed = 1462 total (was 1413)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 43s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  59m 50s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux c2d6c89e9f53 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1cbce0190d046d02d388d1634e13c2398c68b243 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/testReport/ |\r\n   | Max. process+thread count | 618 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/22/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-28T12:52:50.179+0000", "updated": "2025-10-28T12:52:50.179+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033755", "id": "18033755", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3460085414\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 872, Failures: 0, Errors: 0, Skipped: 217\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 875, Failures: 0, Errors: 0, Skipped: 169\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 714, Failures: 0, Errors: 0, Skipped: 282\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 872, Failures: 0, Errors: 0, Skipped: 228\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 721, Failures: 0, Errors: 0, Skipped: 140\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 711, Failures: 0, Errors: 0, Skipped: 284\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 718, Failures: 0, Errors: 0, Skipped: 152\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 713, Failures: 0, Errors: 0, Skipped: 198\r\n   [WARNING] Tests run: 135, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 746, Failures: 0, Errors: 0, Skipped: 226\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 8\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 205, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 711, Failures: 0, Errors: 0, Skipped: 281\r\n   [WARNING] Tests run: 158, Failures: 0, Errors: 0, Skipped: 9\r\n   [WARNING] Tests run: 271, Failures: 0, Errors: 0, Skipped: 24\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T06:50:17.207+0000", "updated": "2025-10-29T06:50:17.207+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033788", "id": "18033788", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3460789622\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m  3s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 16s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 11s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 1 new + 3 unchanged - 0 fixed = 4 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 49 new + 1472 unchanged - 0 fixed = 1521 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 49 new + 1413 unchanged - 0 fixed = 1462 total (was 1413)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 45s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 32s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 19s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  59m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux dce0f60a12b6 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 30e66b4abd5a0c836980f9ab6baa8f7f0e6e1aae |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/testReport/ |\r\n   | Max. process+thread count | 638 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/23/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T10:27:51.497+0000", "updated": "2025-10-29T10:27:51.497+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18033805", "id": "18033805", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3461151631\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  21m  6s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 17s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 44s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 30s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 43s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 18s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 18s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 11s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 18s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 49 new + 1472 unchanged - 0 fixed = 1521 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 49 new + 1413 unchanged - 0 fixed = 1462 total (was 1413)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 42s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 23s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m  6s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  59m 39s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 84b747ef07c0 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2f9687561916ad049b94d710d3882545dd9b31c3 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/testReport/ |\r\n   | Max. process+thread count | 613 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/24/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-29T11:59:51.031+0000", "updated": "2025-10-29T11:59:51.031+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18034116", "id": "18034116", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3466698904\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 20s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  22m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 25s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 26s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 19s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 22s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  14m 45s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  14m 58s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 21 new + 1472 unchanged - 0 fixed = 1493 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 17s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 21 new + 1413 unchanged - 0 fixed = 1434 total (was 1413)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 48s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  17m 11s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 21s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  64m 27s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ce684690898c 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / b3b2c8c29493adbdab9fb7886f5e74ba2dc31b69 |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/testReport/ |\r\n   | Max. process+thread count | 640 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/25/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T08:47:19.739+0000", "updated": "2025-10-30T08:47:19.739+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18034141", "id": "18034141", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3467262942\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  25m 47s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 23s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 16s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 26s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 24s |  |  trunk passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  trunk passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | -1 :x: |  spotbugs  |   0m 43s | [/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/artifact/out/branch-spotbugs-hadoop-tools_hadoop-azure-warnings.html) |  hadoop-tools/hadoop-azure in trunk has 178 extant spotbugs warnings.  |\r\n   | +1 :green_heart: |  shadedclient  |  17m 47s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  18m  0s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 19s |  |  the patch passed with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 19s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 21s |  |  the patch passed with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 10s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 21s |  |  the patch passed  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-21.0.7+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 generated 24 new + 1440 unchanged - 32 fixed = 1464 total (was 1472)  |\r\n   | -1 :x: |  javadoc  |   0m 16s | [/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/artifact/out/results-javadoc-javadoc-hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04.txt) |  hadoop-tools_hadoop-azure-jdkUbuntu-17.0.15+6-Ubuntu-0ubuntu120.04 with JDK Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 generated 3 new + 1387 unchanged - 26 fixed = 1390 total (was 1413)  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 46s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  18m  1s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 13s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 20s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  71m 20s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7669 |\r\n   | JIRA Issue | HADOOP-19472 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 997ebef4e669 5.15.0-156-generic #166-Ubuntu SMP Sat Aug 9 00:02:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / a062843e2a6033d501196e66c17e5632d8a068bc |\r\n   | Default Java | Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-21-openjdk-amd64:Ubuntu-21.0.7+6-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-17-openjdk-amd64:Ubuntu-17.0.15+6-Ubuntu-0ubuntu120.04 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/testReport/ |\r\n   | Max. process+thread count | 640 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7669/26/console |\r\n   | versions | git=2.25.1 maven=3.9.11 spotbugs=4.9.7 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T10:36:37.969+0000", "updated": "2025-10-30T10:36:37.969+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18034142", "id": "18034142", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669#issuecomment-3467277715\n\n   Thank you everyone who reviewed.\r\n   The spotbugs and javadocs reported above are due to https://issues.apache.org/jira/browse/HADOOP-19731\r\n   \r\n   Other Yetus checks looks good. Going ahead with the merge.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T10:40:01.482+0000", "updated": "2025-10-30T10:40:01.482+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13609690/comment/18034143", "id": "18034143", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7669:\nURL: https://github.com/apache/hadoop/pull/7669\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-10-30T10:40:30.194+0000", "updated": "2025-10-30T10:40:30.194+0000"}], "maxResults": 57, "total": 57, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/4", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg", "name": "Minor", "id": "4"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/1", "description": "The issue is open and ready for the assignee to start work on it.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png", "name": "Open", "id": "1", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2", "id": 2, "key": "new", "colorName": "blue-gray", "name": "To Do"}}}}