{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13611595", "self": "https://issues.apache.org/jira/rest/api/2/issue/13611595", "key": "HADOOP-19497", "fields": {"summary": "[ABFS] Enable rename and create recovery from client transaction id over DFS endpoint", "description": "We have implemented create and rename recovery using client transaction IDs over the DFS endpoint ([HADOOP-19450] [ABFS] Rename/Create path idempotency client-level resolution - ASF JIRA). Since the backend changes were not fully rolled out, we initially implemented the changes with the flag disabled. With this update, we aim to enable the flag, which will start sending client transaction IDs. In case of a failure, we will attempt to recover from the failed state if possible. Here are the detailed steps and considerations for this process:\r\n\r\n1. **Implementation Overview**: We introduced a mechanism for create and rename recovery via client transaction IDs to enhance reliability and data integrity over the DFS endpoint. This change was initially flagged as disabled due to incomplete backend rollouts.\r\n\r\n2. **Current Update**: With the backend changes now in place, we are ready to enable the flag. This will activate the sending of client transaction IDs, allowing us to track and manage transactions more effectively.\r\n\r\n3. **Failure Recovery**: The primary advantage of enabling this flag is the potential for recovery from failed states. If a transaction fails, we can use the client transaction ID to attempt a recovery, minimizing data loss and ensuring continuity.\r\n\r\n4. **Next Steps**: We will proceed with enabling the flag and closely monitor the system's performance. Any issues or failures will be documented and addressed promptly to ensure a smooth transition.", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=bhattmanish98", "name": "bhattmanish98", "key": "JIRAUSER306911", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Manish Bhatt", "active": true, "timeZone": "Asia/Kolkata"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17935707", "id": "17935707", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 opened a new pull request, #7509:\nURL: https://github.com/apache/hadoop/pull/7509\n\n   We have implemented create and rename recovery using client transaction IDs over the DFS endpoint ([HADOOP-19450](https://issues.apache.org/jira/browse/HADOOP-19450) [ABFS] Rename/Create path idempotency client-level resolution - ASF JIRA). Since the backend changes were not fully rolled out, we initially implemented the changes with the flag disabled. With this update, we aim to enable the flag, which will start sending client transaction IDs. In case of a failure, we will attempt to recover from the failed state if possible. Here are the detailed steps and considerations for this process:\r\n   \r\n   1. *Implementation Overview*: We introduced a mechanism for create and rename recovery via client transaction IDs to enhance reliability and data integrity over the DFS endpoint. This change was initially flagged as disabled due to incomplete backend rollouts.\r\n   \r\n   2. *Current Update*: With the backend changes now in place, we are ready to enable the flag. This will activate the sending of client transaction IDs, allowing us to track and manage transactions more effectively.\r\n   \r\n   3. *Failure Recovery*: The primary advantage of enabling this flag is the potential for recovery from failed states. If a transaction fails, we can use the client transaction ID to attempt a recovery, minimizing data loss and ensuring continuity.\r\n   \r\n   4. *Next Steps*: We will proceed with enabling the flag and closely monitor the system's performance. Any issues or failures will be documented and addressed promptly to ensure a smooth transition.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-15T06:59:02.725+0000", "updated": "2025-03-15T06:59:02.725+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17935708", "id": "17935708", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#issuecomment-2726292282\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 796, Failures: 0, Errors: 0, Skipped: 152\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 799, Failures: 0, Errors: 0, Skipped: 106\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 638, Failures: 0, Errors: 0, Skipped: 214\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 796, Failures: 0, Errors: 0, Skipped: 163\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 2\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 641, Failures: 0, Errors: 0, Skipped: 144\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 635, Failures: 0, Errors: 0, Skipped: 215\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 638, Failures: 0, Errors: 0, Skipped: 145\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 636, Failures: 0, Errors: 0, Skipped: 163\r\n   [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 670, Failures: 0, Errors: 0, Skipped: 159\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 1\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 635, Failures: 0, Errors: 0, Skipped: 213\r\n   [WARNING] Tests run: 147, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-15T07:00:19.696+0000", "updated": "2025-03-15T07:00:19.696+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17935723", "id": "17935723", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#issuecomment-2726370077\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 49s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 37s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 40s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  5s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m  0s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 10s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 33s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 135m 25s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7509 |\r\n   | JIRA Issue | HADOOP-19497 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 22cf15295129 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / d3493c94846ed4ac94ba064f22c0350826dc7193 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/1/testReport/ |\r\n   | Max. process+thread count | 535 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-15T09:15:46.555+0000", "updated": "2025-03-15T09:15:46.555+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17936449", "id": "17936449", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#issuecomment-2732645028\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 48s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 32s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 41s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 31s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  39m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 49s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 58s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 35s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 135m 11s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7509 |\r\n   | JIRA Issue | HADOOP-19497 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 865c3aeef8f2 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / df4e3f6582a1e8982c437aec6d6aace26e86aa1c |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/2/testReport/ |\r\n   | Max. process+thread count | 529 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-18T10:45:29.438+0000", "updated": "2025-03-18T10:45:29.438+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17936995", "id": "17936995", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2004765573\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -1664,4 +1528,301 @@ public String addClientTransactionIdToHeader(List<AbfsHttpHeader> requestHeaders\n     }\n     return clientTransactionId;\n   }\n+\n+  /**\n+   * Attempts to rename a path with client transaction ID (CTId) recovery mechanism.\n+   * If the initial rename attempt fails, it tries to recover using CTId or ETag\n+   * and retries the operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation\n+   */\n+  private AbfsClientRenameResult renameWithCTIdRecovery(String source,\n+      String destination, String continuation, TracingContext tracingContext,\n+      String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+    // Add client transaction ID to the request headers\n+    String clientTransactionId = addClientTransactionIdToHeader(requestHeaders);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // AbfsClientResult contains the AbfsOperation, If recovery happened or\n+      // not, and the incompleteMetaDataState is true or false.\n+      // If we successfully rename a path and isMetadataIncompleteState was\n+      // true, then rename was recovered, else it didn't, this is why\n+      // isMetadataIncompleteState is used for renameRecovery(as the 2nd param).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n+          tracingContext, sourceEtag, op, isMetadataIncompleteState, e);\n+\n+      // Check if the operation is a retried request and if the error code indicates\n+      // that the source path was not found. If so, attempt recovery using CTId.\n+      if (op.isARetriedRequest()\n+          && SOURCE_PATH_NOT_FOUND.getErrorCode()\n+          .equalsIgnoreCase(op.getResult().getStorageErrorCode())) {\n+        if (recoveryUsingCTId(destination, tracingContext, clientTransactionId)) {\n+          return new AbfsClientRenameResult(\n+              getSuccessOp(AbfsRestOperationType.RenamePath,\n+                  HTTP_METHOD_PUT, url, requestHeaders),\n+              true, isMetadataIncompleteState);\n+        }\n+      }\n+\n+      // Attempt recovery using ETag if applicable\n+      if (recoveryUsingEtag(source, destination, sourceEtag,\n+          op, tracingContext, true)) {\n+        return new AbfsClientRenameResult(\n+            getSuccessOp(AbfsRestOperationType.RenamePath,\n+                HTTP_METHOD_PUT, url, requestHeaders),\n+            true,\n+            isMetadataIncompleteState);\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Attempts to recover a rename operation using ETag. If the source ETag is not provided, it attempts\n+   * to fetch it and retry the operation. If recovery fails, it throws the exception.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation or recovery\n+   */\n+  private AbfsClientRenameResult renameWithETagRecovery(String source,\n+      String destination, String continuation,\n+      TracingContext tracingContext, String sourceEtag,\n+      boolean isMetadataIncompleteState) throws IOException {\n+    boolean hasEtag = !isEmpty(sourceEtag);\n+    boolean shouldAttemptRecovery = isRenameResilience() && getIsNamespaceEnabled();\n+    if (!hasEtag && shouldAttemptRecovery) {\n+      // in case eTag is already not supplied to the API\n+      // and rename resilience is expected and it is an HNS enabled account\n+      // fetch the source etag to be used later in recovery\n+      try {\n+        final AbfsRestOperation srcStatusOp = getPathStatus(source,\n+            false, tracingContext, null);\n+        if (srcStatusOp.hasResult()) {\n+          final AbfsHttpOperation result = srcStatusOp.getResult();\n+          sourceEtag = extractEtagHeader(result);\n+          // and update the directory status.\n+          boolean isDir = checkIsDir(result);\n+          shouldAttemptRecovery = !isDir;\n+          LOG.debug(\n+              \"Retrieved etag of source for rename recovery: {}; isDir={}\",\n+              sourceEtag, isDir);\n+        }\n+      } catch (AbfsRestOperationException e) {\n+        throw new AbfsRestOperationException(e.getStatusCode(),\n+            SOURCE_PATH_NOT_FOUND.getErrorCode(), e.getMessage(), e);\n+      }\n+    }\n+\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // AbfsClientResult contains the AbfsOperation, If recovery happened or\n+      // not, and the incompleteMetaDataState is true or false.\n+      // If we successfully rename a path and isMetadataIncompleteState was\n+      // true, then rename was recovered, else it didn't, this is why\n+      // isMetadataIncompleteState is used for renameRecovery(as the 2nd param).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n+          tracingContext, sourceEtag, op, isMetadataIncompleteState, e);\n+\n+      // Attempt recovery using ETag if applicable\n+      if (recoveryUsingEtag(source, destination, sourceEtag,\n+          op, tracingContext, shouldAttemptRecovery)) {\n+        return new AbfsClientRenameResult(\n+            getSuccessOp(AbfsRestOperationType.RenamePath,\n+                HTTP_METHOD_PUT, url, requestHeaders),\n+            true, isMetadataIncompleteState);\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Creates a list of HTTP headers required for a rename operation, including the encoded source path\n+   * and SAS token if applicable.\n+   *\n+   * @param source the source path for the rename operation\n+   * @return a list of {@link AbfsHttpHeader} containing the headers for the rename request\n+   * @throws IOException if an error occurs while creating the headers or encoding the source path\n+   */\n+  private List<AbfsHttpHeader> getHeadersForRename(final String source)\n+      throws IOException {\n+    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n+    String encodedRenameSource = urlEncode(\n+        FORWARD_SLASH + this.getFileSystem() + source);\n+\n+    if (getAuthType() == AuthType.SAS) {\n+      final AbfsUriQueryBuilder srcQueryBuilder = new AbfsUriQueryBuilder();\n+      appendSASTokenToQuery(source,\n+          SASTokenProvider.RENAME_SOURCE_OPERATION, srcQueryBuilder);\n+      encodedRenameSource += srcQueryBuilder.toString();\n+    }\n+\n+    LOG.trace(\"Rename source queryparam added {}\", encodedRenameSource);\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_RENAME_SOURCE, encodedRenameSource));\n+    requestHeaders.add(new AbfsHttpHeader(IF_NONE_MATCH, STAR));\n+    return requestHeaders;\n+  }\n+\n+  /**\n+   * Builds a query builder for the rename operation URL, including the continuation token and SAS token\n+   * for the destination path.\n+   *\n+   * @param destination the destination path for the rename operation\n+   * @param continuation the continuation token for the operation\n+   * @return an {@link AbfsUriQueryBuilder} containing the query parameters for the rename operation\n+   * @throws AzureBlobFileSystemException if an error occurs while appending the SAS token\n+   */\n+  private AbfsUriQueryBuilder getRenameQueryBuilder(final String destination,\n+      final String continuation) throws AzureBlobFileSystemException {\n+    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n+    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CONTINUATION, continuation);\n+    appendSASTokenToQuery(destination,\n+        SASTokenProvider.RENAME_DESTINATION_OPERATION, abfsUriQueryBuilder);\n+    return abfsUriQueryBuilder;\n+  }\n+\n+  /**\n+   * Attempts to recover a rename operation using the client transaction ID (CTId).\n+   * It checks if the provided CTId matches the one in the response header for the destination path.\n+   *\n+   * @param destination the destination path for the rename operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param clientTransactionId the client transaction ID to be used for recovery\n+   * @return true if the client transaction ID matches, indicating recovery can proceed; false otherwise\n+   * @throws AzureBlobFileSystemException if an error occurs while retrieving the path status\n+   */\n+  private boolean recoveryUsingCTId(String destination,\n+      TracingContext tracingContext, String clientTransactionId)\n+      throws AzureBlobFileSystemException {\n+    try {\n+      final AbfsHttpOperation abfsHttpOperation =\n+          getPathStatus(destination, false,\n+              tracingContext, null).getResult();\n+      return clientTransactionId.equals(\n+          abfsHttpOperation.getResponseHeader(X_MS_CLIENT_TRANSACTION_ID));\n+    } catch (AzureBlobFileSystemException exception) {\n+      throw new AbfsDriverException(ERR_RENAME_RECOVERY, exception);\n+    }\n+  }\n+\n+  /**\n+   * Attempts recovery using an ETag for the given source and destination.\n+   * If recovery is enabled and rename resilience is supported, performs an idempotency check\n+   * for the rename operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param op the AbfsRestOperation object for the rename operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param shouldAttemptRecovery flag indicating whether recovery should be attempted\n+   * @return true if the recovery attempt was successful, false otherwise\n+   */\n+  private boolean recoveryUsingEtag(String source, String destination,\n+      String sourceEtag, AbfsRestOperation op, TracingContext tracingContext,\n+      boolean shouldAttemptRecovery) {\n+    if (shouldAttemptRecovery && isRenameResilience()) {\n+      return renameIdempotencyCheckOp(source, sourceEtag,\n+          op, destination, tracingContext);\n+    }\n+    return false;\n+  }\n+\n+  /**\n+   * Checks for rename operation exceptions and handles them accordingly.\n+   * Throws an exception or retries the operation if certain error conditions are met,\n+   * such as unauthorized overwrite or missing destination parent path.\n+   *\n+   * @param source The source path for the rename operation.\n+   * @param destination The destination path for the rename operation.\n+   * @param continuation Continuation token for the operation, if applicable.\n+   * @param tracingContext The tracing context for tracking the operation.\n+   * @param sourceEtag The ETag of the source path for metadata validation.\n+   * @param op The ABFS operation result for the rename attempt.\n+   * @param isMetadataIncompleteState Flag indicating if metadata is incomplete.\n+   * @throws IOException If an I/O error occurs during the rename operation.\n+   * @throws FileAlreadyExistsException If the destination file already exists and overwrite is unauthorized.\n\nReview Comment:\n   Unauthorized Blob Overwrite error we only get when the destination file already exists. For invalid permissions we have separate errors thrown. We can remove the \"unauthorized overwrite\" parts here. \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-20T03:50:43.020+0000", "updated": "2025-03-20T03:50:43.020+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17939647", "id": "17939647", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2020879650\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -1664,4 +1528,301 @@ public String addClientTransactionIdToHeader(List<AbfsHttpHeader> requestHeaders\n     }\n     return clientTransactionId;\n   }\n+\n+  /**\n+   * Attempts to rename a path with client transaction ID (CTId) recovery mechanism.\n+   * If the initial rename attempt fails, it tries to recover using CTId or ETag\n+   * and retries the operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation\n+   */\n+  private AbfsClientRenameResult renameWithCTIdRecovery(String source,\n+      String destination, String continuation, TracingContext tracingContext,\n+      String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+    // Add client transaction ID to the request headers\n+    String clientTransactionId = addClientTransactionIdToHeader(requestHeaders);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // AbfsClientResult contains the AbfsOperation, If recovery happened or\n\nReview Comment:\n   grammar doesn't look correct\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-31T11:35:09.158+0000", "updated": "2025-03-31T11:35:09.158+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17939650", "id": "17939650", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2020886737\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -1664,4 +1528,301 @@ public String addClientTransactionIdToHeader(List<AbfsHttpHeader> requestHeaders\n     }\n     return clientTransactionId;\n   }\n+\n+  /**\n+   * Attempts to rename a path with client transaction ID (CTId) recovery mechanism.\n+   * If the initial rename attempt fails, it tries to recover using CTId or ETag\n+   * and retries the operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation\n+   */\n+  private AbfsClientRenameResult renameWithCTIdRecovery(String source,\n+      String destination, String continuation, TracingContext tracingContext,\n+      String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+    // Add client transaction ID to the request headers\n+    String clientTransactionId = addClientTransactionIdToHeader(requestHeaders);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // AbfsClientResult contains the AbfsOperation, If recovery happened or\n+      // not, and the incompleteMetaDataState is true or false.\n+      // If we successfully rename a path and isMetadataIncompleteState was\n+      // true, then rename was recovered, else it didn't, this is why\n+      // isMetadataIncompleteState is used for renameRecovery(as the 2nd param).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n+          tracingContext, sourceEtag, op, isMetadataIncompleteState, e);\n+\n+      // Check if the operation is a retried request and if the error code indicates\n+      // that the source path was not found. If so, attempt recovery using CTId.\n+      if (op.isARetriedRequest()\n+          && SOURCE_PATH_NOT_FOUND.getErrorCode()\n+          .equalsIgnoreCase(op.getResult().getStorageErrorCode())) {\n+        if (recoveryUsingCTId(destination, tracingContext, clientTransactionId)) {\n+          return new AbfsClientRenameResult(\n+              getSuccessOp(AbfsRestOperationType.RenamePath,\n+                  HTTP_METHOD_PUT, url, requestHeaders),\n+              true, isMetadataIncompleteState);\n+        }\n+      }\n+\n+      // Attempt recovery using ETag if applicable\n+      if (recoveryUsingEtag(source, destination, sourceEtag,\n+          op, tracingContext, true)) {\n+        return new AbfsClientRenameResult(\n+            getSuccessOp(AbfsRestOperationType.RenamePath,\n+                HTTP_METHOD_PUT, url, requestHeaders),\n+            true,\n+            isMetadataIncompleteState);\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Attempts to recover a rename operation using ETag. If the source ETag is not provided, it attempts\n+   * to fetch it and retry the operation. If recovery fails, it throws the exception.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation or recovery\n+   */\n+  private AbfsClientRenameResult renameWithETagRecovery(String source,\n+      String destination, String continuation,\n+      TracingContext tracingContext, String sourceEtag,\n+      boolean isMetadataIncompleteState) throws IOException {\n+    boolean hasEtag = !isEmpty(sourceEtag);\n+    boolean shouldAttemptRecovery = isRenameResilience() && getIsNamespaceEnabled();\n+    if (!hasEtag && shouldAttemptRecovery) {\n+      // in case eTag is already not supplied to the API\n+      // and rename resilience is expected and it is an HNS enabled account\n+      // fetch the source etag to be used later in recovery\n+      try {\n+        final AbfsRestOperation srcStatusOp = getPathStatus(source,\n+            false, tracingContext, null);\n+        if (srcStatusOp.hasResult()) {\n+          final AbfsHttpOperation result = srcStatusOp.getResult();\n+          sourceEtag = extractEtagHeader(result);\n+          // and update the directory status.\n+          boolean isDir = checkIsDir(result);\n+          shouldAttemptRecovery = !isDir;\n+          LOG.debug(\n+              \"Retrieved etag of source for rename recovery: {}; isDir={}\",\n+              sourceEtag, isDir);\n+        }\n+      } catch (AbfsRestOperationException e) {\n+        throw new AbfsRestOperationException(e.getStatusCode(),\n+            SOURCE_PATH_NOT_FOUND.getErrorCode(), e.getMessage(), e);\n+      }\n+    }\n+\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // AbfsClientResult contains the AbfsOperation, If recovery happened or\n+      // not, and the incompleteMetaDataState is true or false.\n+      // If we successfully rename a path and isMetadataIncompleteState was\n+      // true, then rename was recovered, else it didn't, this is why\n+      // isMetadataIncompleteState is used for renameRecovery(as the 2nd param).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n+          tracingContext, sourceEtag, op, isMetadataIncompleteState, e);\n+\n+      // Attempt recovery using ETag if applicable\n+      if (recoveryUsingEtag(source, destination, sourceEtag,\n+          op, tracingContext, shouldAttemptRecovery)) {\n+        return new AbfsClientRenameResult(\n+            getSuccessOp(AbfsRestOperationType.RenamePath,\n+                HTTP_METHOD_PUT, url, requestHeaders),\n+            true, isMetadataIncompleteState);\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Creates a list of HTTP headers required for a rename operation, including the encoded source path\n+   * and SAS token if applicable.\n+   *\n+   * @param source the source path for the rename operation\n+   * @return a list of {@link AbfsHttpHeader} containing the headers for the rename request\n+   * @throws IOException if an error occurs while creating the headers or encoding the source path\n+   */\n+  private List<AbfsHttpHeader> getHeadersForRename(final String source)\n+      throws IOException {\n+    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n+    String encodedRenameSource = urlEncode(\n+        FORWARD_SLASH + this.getFileSystem() + source);\n+\n+    if (getAuthType() == AuthType.SAS) {\n+      final AbfsUriQueryBuilder srcQueryBuilder = new AbfsUriQueryBuilder();\n+      appendSASTokenToQuery(source,\n+          SASTokenProvider.RENAME_SOURCE_OPERATION, srcQueryBuilder);\n+      encodedRenameSource += srcQueryBuilder.toString();\n+    }\n+\n+    LOG.trace(\"Rename source queryparam added {}\", encodedRenameSource);\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_RENAME_SOURCE, encodedRenameSource));\n+    requestHeaders.add(new AbfsHttpHeader(IF_NONE_MATCH, STAR));\n+    return requestHeaders;\n+  }\n+\n+  /**\n+   * Builds a query builder for the rename operation URL, including the continuation token and SAS token\n+   * for the destination path.\n+   *\n+   * @param destination the destination path for the rename operation\n+   * @param continuation the continuation token for the operation\n+   * @return an {@link AbfsUriQueryBuilder} containing the query parameters for the rename operation\n+   * @throws AzureBlobFileSystemException if an error occurs while appending the SAS token\n+   */\n+  private AbfsUriQueryBuilder getRenameQueryBuilder(final String destination,\n+      final String continuation) throws AzureBlobFileSystemException {\n+    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n+    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CONTINUATION, continuation);\n+    appendSASTokenToQuery(destination,\n+        SASTokenProvider.RENAME_DESTINATION_OPERATION, abfsUriQueryBuilder);\n+    return abfsUriQueryBuilder;\n+  }\n+\n+  /**\n+   * Attempts to recover a rename operation using the client transaction ID (CTId).\n+   * It checks if the provided CTId matches the one in the response header for the destination path.\n+   *\n+   * @param destination the destination path for the rename operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param clientTransactionId the client transaction ID to be used for recovery\n+   * @return true if the client transaction ID matches, indicating recovery can proceed; false otherwise\n+   * @throws AzureBlobFileSystemException if an error occurs while retrieving the path status\n+   */\n+  private boolean recoveryUsingCTId(String destination,\n+      TracingContext tracingContext, String clientTransactionId)\n+      throws AzureBlobFileSystemException {\n+    try {\n+      final AbfsHttpOperation abfsHttpOperation =\n+          getPathStatus(destination, false,\n+              tracingContext, null).getResult();\n+      return clientTransactionId.equals(\n+          abfsHttpOperation.getResponseHeader(X_MS_CLIENT_TRANSACTION_ID));\n+    } catch (AzureBlobFileSystemException exception) {\n+      throw new AbfsDriverException(ERR_RENAME_RECOVERY, exception);\n\nReview Comment:\n   add the path in the exception message\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-31T11:41:31.203+0000", "updated": "2025-03-31T11:41:31.203+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17939660", "id": "17939660", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2020916598\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsRenameRetryRecovery.java:\n##########\n@@ -275,9 +284,14 @@ public void testRenameRecoveryEtagMatchFsLevel() throws IOException {\n     // 4 calls should have happened in total for rename\n     // 1 -> original rename rest call, 2 -> first retry,\n     // +2 for getPathStatus calls\n+    int totalConnections = 4;\n+    if (!getConfiguration().getIsClientTransactionIdEnabled()) {\n\nReview Comment:\n   why ! here ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-31T12:06:37.359+0000", "updated": "2025-03-31T12:06:37.359+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17939662", "id": "17939662", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2020917070\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsRenameRetryRecovery.java:\n##########\n@@ -350,21 +364,18 @@ public void testRenameRecoveryFailsForDirFsLevel() throws Exception {\n     if (getConfiguration().getIsClientTransactionIdEnabled()) {\n       // Recovery based on client transaction id should be successful\n       assertTrue(renameResult);\n-      // One extra getPathStatus call should have happened\n-      newConnections = 5;\n     } else {\n       assertFalse(renameResult);\n-      newConnections = 4;\n     }\n \n     // validating stat counters after rename\n-    // 3 calls should have happened in total for rename\n+    // 4 calls should have happened in total for rename\n     // 1 -> original rename rest call, 2 -> first retry,\n     // +1 for getPathStatus calls\n     // last getPathStatus call should be skipped\n     assertThatStatisticCounter(ioStats,\n             CONNECTIONS_MADE.getStatName())\n-            .isEqualTo(newConnections + connMadeBeforeRename);\n+            .isEqualTo(4 + connMadeBeforeRename);\n\nReview Comment:\n   should not be hardcoded here\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-31T12:07:03.292+0000", "updated": "2025-03-31T12:07:03.292+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17939702", "id": "17939702", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2021107942\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsRenameRetryRecovery.java:\n##########\n@@ -275,9 +284,14 @@ public void testRenameRecoveryEtagMatchFsLevel() throws IOException {\n     // 4 calls should have happened in total for rename\n     // 1 -> original rename rest call, 2 -> first retry,\n     // +2 for getPathStatus calls\n+    int totalConnections = 4;\n+    if (!getConfiguration().getIsClientTransactionIdEnabled()) {\n\nReview Comment:\n   In case of recovery using client transaction id: It makes total 4 calls\r\n   1. getFileStatus - AzureBlobFileSystem\r\n   2. rename (without retry)\r\n   3. rename (1st retry)\r\n   4. getPathStatus -> to get client transaction Id\r\n   \r\n   In case of recovery using Etag: It makes total 5 calls\r\n   1. getFileStatus - AzureBlobFileSystem\r\n   2. getPathStatus - to fetch etag of source\r\n   3. rename (without retry)\r\n   4. rename (1st retry)\r\n   5. getPathStatus - to fetch etag of destination\r\n   \r\n   Before this change, `2. getPathStatus - to fetch etag of source` in case 2 was done even in case 1. \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-31T14:02:23.103+0000", "updated": "2025-03-31T14:02:23.103+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17939703", "id": "17939703", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2021132520\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsRenameRetryRecovery.java:\n##########\n@@ -350,21 +364,18 @@ public void testRenameRecoveryFailsForDirFsLevel() throws Exception {\n     if (getConfiguration().getIsClientTransactionIdEnabled()) {\n       // Recovery based on client transaction id should be successful\n       assertTrue(renameResult);\n-      // One extra getPathStatus call should have happened\n-      newConnections = 5;\n     } else {\n       assertFalse(renameResult);\n-      newConnections = 4;\n     }\n \n     // validating stat counters after rename\n-    // 3 calls should have happened in total for rename\n+    // 4 calls should have happened in total for rename\n     // 1 -> original rename rest call, 2 -> first retry,\n     // +1 for getPathStatus calls\n     // last getPathStatus call should be skipped\n     assertThatStatisticCounter(ioStats,\n             CONNECTIONS_MADE.getStatName())\n-            .isEqualTo(newConnections + connMadeBeforeRename);\n+            .isEqualTo(4 + connMadeBeforeRename);\n\nReview Comment:\n   In the rename operation, we are adding 4 extra connections, which is why it\u2019s kept like this. We are following the same format in other places as well.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-03-31T14:13:12.147+0000", "updated": "2025-03-31T14:13:12.147+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17939904", "id": "17939904", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2022158193\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -1664,4 +1528,301 @@ public String addClientTransactionIdToHeader(List<AbfsHttpHeader> requestHeaders\n     }\n     return clientTransactionId;\n   }\n+\n+  /**\n+   * Attempts to rename a path with client transaction ID (CTId) recovery mechanism.\n+   * If the initial rename attempt fails, it tries to recover using CTId or ETag\n+   * and retries the operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation\n+   */\n+  private AbfsClientRenameResult renameWithCTIdRecovery(String source,\n+      String destination, String continuation, TracingContext tracingContext,\n+      String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+    // Add client transaction ID to the request headers\n+    String clientTransactionId = addClientTransactionIdToHeader(requestHeaders);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // AbfsClientResult contains the AbfsOperation, If recovery happened or\n+      // not, and the incompleteMetaDataState is true or false.\n+      // If we successfully rename a path and isMetadataIncompleteState was\n+      // true, then rename was recovered, else it didn't, this is why\n+      // isMetadataIncompleteState is used for renameRecovery(as the 2nd param).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n+          tracingContext, sourceEtag, op, isMetadataIncompleteState, e);\n+\n+      // Check if the operation is a retried request and if the error code indicates\n+      // that the source path was not found. If so, attempt recovery using CTId.\n+      if (op.isARetriedRequest()\n+          && SOURCE_PATH_NOT_FOUND.getErrorCode()\n+          .equalsIgnoreCase(op.getResult().getStorageErrorCode())) {\n+        if (recoveryUsingCTId(destination, tracingContext, clientTransactionId)) {\n+          return new AbfsClientRenameResult(\n+              getSuccessOp(AbfsRestOperationType.RenamePath,\n+                  HTTP_METHOD_PUT, url, requestHeaders),\n+              true, isMetadataIncompleteState);\n+        }\n+      }\n+\n+      // Attempt recovery using ETag if applicable\n+      if (recoveryUsingEtag(source, destination, sourceEtag,\n+          op, tracingContext, true)) {\n+        return new AbfsClientRenameResult(\n+            getSuccessOp(AbfsRestOperationType.RenamePath,\n+                HTTP_METHOD_PUT, url, requestHeaders),\n+            true,\n+            isMetadataIncompleteState);\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Attempts to recover a rename operation using ETag. If the source ETag is not provided, it attempts\n+   * to fetch it and retry the operation. If recovery fails, it throws the exception.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation or recovery\n+   */\n+  private AbfsClientRenameResult renameWithETagRecovery(String source,\n+      String destination, String continuation,\n+      TracingContext tracingContext, String sourceEtag,\n+      boolean isMetadataIncompleteState) throws IOException {\n+    boolean hasEtag = !isEmpty(sourceEtag);\n+    boolean shouldAttemptRecovery = isRenameResilience() && getIsNamespaceEnabled();\n+    if (!hasEtag && shouldAttemptRecovery) {\n+      // in case eTag is already not supplied to the API\n+      // and rename resilience is expected and it is an HNS enabled account\n+      // fetch the source etag to be used later in recovery\n+      try {\n+        final AbfsRestOperation srcStatusOp = getPathStatus(source,\n+            false, tracingContext, null);\n+        if (srcStatusOp.hasResult()) {\n+          final AbfsHttpOperation result = srcStatusOp.getResult();\n+          sourceEtag = extractEtagHeader(result);\n+          // and update the directory status.\n+          boolean isDir = checkIsDir(result);\n+          shouldAttemptRecovery = !isDir;\n+          LOG.debug(\n+              \"Retrieved etag of source for rename recovery: {}; isDir={}\",\n+              sourceEtag, isDir);\n+        }\n+      } catch (AbfsRestOperationException e) {\n+        throw new AbfsRestOperationException(e.getStatusCode(),\n+            SOURCE_PATH_NOT_FOUND.getErrorCode(), e.getMessage(), e);\n+      }\n+    }\n+\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // AbfsClientResult contains the AbfsOperation, If recovery happened or\n+      // not, and the incompleteMetaDataState is true or false.\n+      // If we successfully rename a path and isMetadataIncompleteState was\n+      // true, then rename was recovered, else it didn't, this is why\n+      // isMetadataIncompleteState is used for renameRecovery(as the 2nd param).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n+          tracingContext, sourceEtag, op, isMetadataIncompleteState, e);\n+\n+      // Attempt recovery using ETag if applicable\n+      if (recoveryUsingEtag(source, destination, sourceEtag,\n+          op, tracingContext, shouldAttemptRecovery)) {\n+        return new AbfsClientRenameResult(\n+            getSuccessOp(AbfsRestOperationType.RenamePath,\n+                HTTP_METHOD_PUT, url, requestHeaders),\n+            true, isMetadataIncompleteState);\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Creates a list of HTTP headers required for a rename operation, including the encoded source path\n+   * and SAS token if applicable.\n+   *\n+   * @param source the source path for the rename operation\n+   * @return a list of {@link AbfsHttpHeader} containing the headers for the rename request\n+   * @throws IOException if an error occurs while creating the headers or encoding the source path\n+   */\n+  private List<AbfsHttpHeader> getHeadersForRename(final String source)\n+      throws IOException {\n+    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n+    String encodedRenameSource = urlEncode(\n+        FORWARD_SLASH + this.getFileSystem() + source);\n+\n+    if (getAuthType() == AuthType.SAS) {\n+      final AbfsUriQueryBuilder srcQueryBuilder = new AbfsUriQueryBuilder();\n+      appendSASTokenToQuery(source,\n+          SASTokenProvider.RENAME_SOURCE_OPERATION, srcQueryBuilder);\n+      encodedRenameSource += srcQueryBuilder.toString();\n+    }\n+\n+    LOG.trace(\"Rename source queryparam added {}\", encodedRenameSource);\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_RENAME_SOURCE, encodedRenameSource));\n+    requestHeaders.add(new AbfsHttpHeader(IF_NONE_MATCH, STAR));\n+    return requestHeaders;\n+  }\n+\n+  /**\n+   * Builds a query builder for the rename operation URL, including the continuation token and SAS token\n+   * for the destination path.\n+   *\n+   * @param destination the destination path for the rename operation\n+   * @param continuation the continuation token for the operation\n+   * @return an {@link AbfsUriQueryBuilder} containing the query parameters for the rename operation\n+   * @throws AzureBlobFileSystemException if an error occurs while appending the SAS token\n+   */\n+  private AbfsUriQueryBuilder getRenameQueryBuilder(final String destination,\n+      final String continuation) throws AzureBlobFileSystemException {\n+    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n+    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CONTINUATION, continuation);\n+    appendSASTokenToQuery(destination,\n+        SASTokenProvider.RENAME_DESTINATION_OPERATION, abfsUriQueryBuilder);\n+    return abfsUriQueryBuilder;\n+  }\n+\n+  /**\n+   * Attempts to recover a rename operation using the client transaction ID (CTId).\n+   * It checks if the provided CTId matches the one in the response header for the destination path.\n+   *\n+   * @param destination the destination path for the rename operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param clientTransactionId the client transaction ID to be used for recovery\n+   * @return true if the client transaction ID matches, indicating recovery can proceed; false otherwise\n+   * @throws AzureBlobFileSystemException if an error occurs while retrieving the path status\n+   */\n+  private boolean recoveryUsingCTId(String destination,\n+      TracingContext tracingContext, String clientTransactionId)\n+      throws AzureBlobFileSystemException {\n+    try {\n+      final AbfsHttpOperation abfsHttpOperation =\n+          getPathStatus(destination, false,\n+              tracingContext, null).getResult();\n+      return clientTransactionId.equals(\n+          abfsHttpOperation.getResponseHeader(X_MS_CLIENT_TRANSACTION_ID));\n+    } catch (AzureBlobFileSystemException exception) {\n+      throw new AbfsDriverException(ERR_RENAME_RECOVERY, exception);\n\nReview Comment:\n   Taken!\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-01T05:17:44.354+0000", "updated": "2025-04-01T05:17:44.354+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17939905", "id": "17939905", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2022158427\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -1664,4 +1528,301 @@ public String addClientTransactionIdToHeader(List<AbfsHttpHeader> requestHeaders\n     }\n     return clientTransactionId;\n   }\n+\n+  /**\n+   * Attempts to rename a path with client transaction ID (CTId) recovery mechanism.\n+   * If the initial rename attempt fails, it tries to recover using CTId or ETag\n+   * and retries the operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation\n+   */\n+  private AbfsClientRenameResult renameWithCTIdRecovery(String source,\n+      String destination, String continuation, TracingContext tracingContext,\n+      String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+    // Add client transaction ID to the request headers\n+    String clientTransactionId = addClientTransactionIdToHeader(requestHeaders);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // AbfsClientResult contains the AbfsOperation, If recovery happened or\n\nReview Comment:\n   Correction Done.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-01T05:17:59.389+0000", "updated": "2025-04-01T05:17:59.389+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17939923", "id": "17939923", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2022205902\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -1664,4 +1528,301 @@ public String addClientTransactionIdToHeader(List<AbfsHttpHeader> requestHeaders\n     }\n     return clientTransactionId;\n   }\n+\n+  /**\n+   * Attempts to rename a path with client transaction ID (CTId) recovery mechanism.\n+   * If the initial rename attempt fails, it tries to recover using CTId or ETag\n+   * and retries the operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation\n+   */\n+  private AbfsClientRenameResult renameWithCTIdRecovery(String source,\n+      String destination, String continuation, TracingContext tracingContext,\n+      String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+    // Add client transaction ID to the request headers\n+    String clientTransactionId = addClientTransactionIdToHeader(requestHeaders);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // AbfsClientResult contains the AbfsOperation, If recovery happened or\n+      // not, and the incompleteMetaDataState is true or false.\n+      // If we successfully rename a path and isMetadataIncompleteState was\n+      // true, then rename was recovered, else it didn't, this is why\n+      // isMetadataIncompleteState is used for renameRecovery(as the 2nd param).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n+          tracingContext, sourceEtag, op, isMetadataIncompleteState, e);\n+\n+      // Check if the operation is a retried request and if the error code indicates\n+      // that the source path was not found. If so, attempt recovery using CTId.\n+      if (op.isARetriedRequest()\n+          && SOURCE_PATH_NOT_FOUND.getErrorCode()\n+          .equalsIgnoreCase(op.getResult().getStorageErrorCode())) {\n+        if (recoveryUsingCTId(destination, tracingContext, clientTransactionId)) {\n+          return new AbfsClientRenameResult(\n+              getSuccessOp(AbfsRestOperationType.RenamePath,\n+                  HTTP_METHOD_PUT, url, requestHeaders),\n+              true, isMetadataIncompleteState);\n+        }\n+      }\n+\n+      // Attempt recovery using ETag if applicable\n+      if (recoveryUsingEtag(source, destination, sourceEtag,\n+          op, tracingContext, true)) {\n+        return new AbfsClientRenameResult(\n+            getSuccessOp(AbfsRestOperationType.RenamePath,\n+                HTTP_METHOD_PUT, url, requestHeaders),\n+            true,\n+            isMetadataIncompleteState);\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Attempts to recover a rename operation using ETag. If the source ETag is not provided, it attempts\n+   * to fetch it and retry the operation. If recovery fails, it throws the exception.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation or recovery\n+   */\n+  private AbfsClientRenameResult renameWithETagRecovery(String source,\n+      String destination, String continuation,\n+      TracingContext tracingContext, String sourceEtag,\n+      boolean isMetadataIncompleteState) throws IOException {\n+    boolean hasEtag = !isEmpty(sourceEtag);\n+    boolean shouldAttemptRecovery = isRenameResilience() && getIsNamespaceEnabled();\n+    if (!hasEtag && shouldAttemptRecovery) {\n+      // in case eTag is already not supplied to the API\n+      // and rename resilience is expected and it is an HNS enabled account\n+      // fetch the source etag to be used later in recovery\n+      try {\n+        final AbfsRestOperation srcStatusOp = getPathStatus(source,\n+            false, tracingContext, null);\n+        if (srcStatusOp.hasResult()) {\n+          final AbfsHttpOperation result = srcStatusOp.getResult();\n+          sourceEtag = extractEtagHeader(result);\n+          // and update the directory status.\n+          boolean isDir = checkIsDir(result);\n+          shouldAttemptRecovery = !isDir;\n+          LOG.debug(\n+              \"Retrieved etag of source for rename recovery: {}; isDir={}\",\n+              sourceEtag, isDir);\n+        }\n+      } catch (AbfsRestOperationException e) {\n+        throw new AbfsRestOperationException(e.getStatusCode(),\n+            SOURCE_PATH_NOT_FOUND.getErrorCode(), e.getMessage(), e);\n+      }\n+    }\n+\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // AbfsClientResult contains the AbfsOperation, If recovery happened or\n+      // not, and the incompleteMetaDataState is true or false.\n+      // If we successfully rename a path and isMetadataIncompleteState was\n+      // true, then rename was recovered, else it didn't, this is why\n+      // isMetadataIncompleteState is used for renameRecovery(as the 2nd param).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n+          tracingContext, sourceEtag, op, isMetadataIncompleteState, e);\n+\n+      // Attempt recovery using ETag if applicable\n+      if (recoveryUsingEtag(source, destination, sourceEtag,\n+          op, tracingContext, shouldAttemptRecovery)) {\n+        return new AbfsClientRenameResult(\n+            getSuccessOp(AbfsRestOperationType.RenamePath,\n+                HTTP_METHOD_PUT, url, requestHeaders),\n+            true, isMetadataIncompleteState);\n+      }\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Creates a list of HTTP headers required for a rename operation, including the encoded source path\n+   * and SAS token if applicable.\n+   *\n+   * @param source the source path for the rename operation\n+   * @return a list of {@link AbfsHttpHeader} containing the headers for the rename request\n+   * @throws IOException if an error occurs while creating the headers or encoding the source path\n+   */\n+  private List<AbfsHttpHeader> getHeadersForRename(final String source)\n+      throws IOException {\n+    final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n+    String encodedRenameSource = urlEncode(\n+        FORWARD_SLASH + this.getFileSystem() + source);\n+\n+    if (getAuthType() == AuthType.SAS) {\n+      final AbfsUriQueryBuilder srcQueryBuilder = new AbfsUriQueryBuilder();\n+      appendSASTokenToQuery(source,\n+          SASTokenProvider.RENAME_SOURCE_OPERATION, srcQueryBuilder);\n+      encodedRenameSource += srcQueryBuilder.toString();\n+    }\n+\n+    LOG.trace(\"Rename source queryparam added {}\", encodedRenameSource);\n+    requestHeaders.add(new AbfsHttpHeader(X_MS_RENAME_SOURCE, encodedRenameSource));\n+    requestHeaders.add(new AbfsHttpHeader(IF_NONE_MATCH, STAR));\n+    return requestHeaders;\n+  }\n+\n+  /**\n+   * Builds a query builder for the rename operation URL, including the continuation token and SAS token\n+   * for the destination path.\n+   *\n+   * @param destination the destination path for the rename operation\n+   * @param continuation the continuation token for the operation\n+   * @return an {@link AbfsUriQueryBuilder} containing the query parameters for the rename operation\n+   * @throws AzureBlobFileSystemException if an error occurs while appending the SAS token\n+   */\n+  private AbfsUriQueryBuilder getRenameQueryBuilder(final String destination,\n+      final String continuation) throws AzureBlobFileSystemException {\n+    final AbfsUriQueryBuilder abfsUriQueryBuilder = createDefaultUriQueryBuilder();\n+    abfsUriQueryBuilder.addQuery(QUERY_PARAM_CONTINUATION, continuation);\n+    appendSASTokenToQuery(destination,\n+        SASTokenProvider.RENAME_DESTINATION_OPERATION, abfsUriQueryBuilder);\n+    return abfsUriQueryBuilder;\n+  }\n+\n+  /**\n+   * Attempts to recover a rename operation using the client transaction ID (CTId).\n+   * It checks if the provided CTId matches the one in the response header for the destination path.\n+   *\n+   * @param destination the destination path for the rename operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param clientTransactionId the client transaction ID to be used for recovery\n+   * @return true if the client transaction ID matches, indicating recovery can proceed; false otherwise\n+   * @throws AzureBlobFileSystemException if an error occurs while retrieving the path status\n+   */\n+  private boolean recoveryUsingCTId(String destination,\n+      TracingContext tracingContext, String clientTransactionId)\n+      throws AzureBlobFileSystemException {\n+    try {\n+      final AbfsHttpOperation abfsHttpOperation =\n+          getPathStatus(destination, false,\n+              tracingContext, null).getResult();\n+      return clientTransactionId.equals(\n+          abfsHttpOperation.getResponseHeader(X_MS_CLIENT_TRANSACTION_ID));\n+    } catch (AzureBlobFileSystemException exception) {\n+      throw new AbfsDriverException(ERR_RENAME_RECOVERY, exception);\n+    }\n+  }\n+\n+  /**\n+   * Attempts recovery using an ETag for the given source and destination.\n+   * If recovery is enabled and rename resilience is supported, performs an idempotency check\n+   * for the rename operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param op the AbfsRestOperation object for the rename operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param shouldAttemptRecovery flag indicating whether recovery should be attempted\n+   * @return true if the recovery attempt was successful, false otherwise\n+   */\n+  private boolean recoveryUsingEtag(String source, String destination,\n+      String sourceEtag, AbfsRestOperation op, TracingContext tracingContext,\n+      boolean shouldAttemptRecovery) {\n+    if (shouldAttemptRecovery && isRenameResilience()) {\n+      return renameIdempotencyCheckOp(source, sourceEtag,\n+          op, destination, tracingContext);\n+    }\n+    return false;\n+  }\n+\n+  /**\n+   * Checks for rename operation exceptions and handles them accordingly.\n+   * Throws an exception or retries the operation if certain error conditions are met,\n+   * such as unauthorized overwrite or missing destination parent path.\n+   *\n+   * @param source The source path for the rename operation.\n+   * @param destination The destination path for the rename operation.\n+   * @param continuation Continuation token for the operation, if applicable.\n+   * @param tracingContext The tracing context for tracking the operation.\n+   * @param sourceEtag The ETag of the source path for metadata validation.\n+   * @param op The ABFS operation result for the rename attempt.\n+   * @param isMetadataIncompleteState Flag indicating if metadata is incomplete.\n+   * @throws IOException If an I/O error occurs during the rename operation.\n+   * @throws FileAlreadyExistsException If the destination file already exists and overwrite is unauthorized.\n\nReview Comment:\n   Removed `and overwrite is unauthorized` from the message\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-01T06:10:33.237+0000", "updated": "2025-04-01T06:10:33.237+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17939981", "id": "17939981", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#issuecomment-2768657792\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  20m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  42m 28s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 34s |  |  trunk passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  39m  5s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 56s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m 21s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 47s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 155m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7509 |\r\n   | JIRA Issue | HADOOP-19497 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux d76a0acd1258 5.15.0-130-generic #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 1b339fc3670efe7f929d6262e8dccc25810fcb59 |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/4/testReport/ |\r\n   | Max. process+thread count | 524 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7509/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-01T08:55:00.051+0000", "updated": "2025-04-01T08:55:00.051+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17940261", "id": "17940261", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2024113926\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -1691,4 +1555,297 @@ public String addClientTransactionIdToHeader(List<AbfsHttpHeader> requestHeaders\n     }\n     return clientTransactionId;\n   }\n+\n+  /**\n+   * Attempts to rename a path with client transaction ID (CTId) recovery mechanism.\n+   * If the initial rename attempt fails, it tries to recover using CTId or ETag\n+   * and retries the operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation\n+   */\n+  private AbfsClientRenameResult renameWithCTIdRecovery(String source,\n+      String destination, String continuation, TracingContext tracingContext,\n+      String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+    // Add client transaction ID to the request headers\n+    String clientTransactionId = addClientTransactionIdToHeader(requestHeaders);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // If we successfully rename a path and isMetadataIncompleteState is true,\n+      // then the rename was recovered; otherwise, it wasn\u2019t.\n+      // This is why isMetadataIncompleteState is used for renameRecovery (as the second parameter).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n\nReview Comment:\n   There is a rename call happening inside this as well and another recovery happening below.\r\n   Can you explain whats the difference and add some comments around it?\r\n   \n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java:\n##########\n@@ -2289,6 +2289,6 @@ public void answer(final AbfsRestOperation mockedObj,\n                   null, op);\n             }\n           }\n-        });\n+        }, 0);\n\nReview Comment:\n   What is this change about?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java:\n##########\n@@ -199,13 +199,10 @@ public String toString() {\n     }\n \n     public static ApiVersion getCurrentVersion() {\n-      return DEC_12_2019;\n+      return NOV_04_2024;\n\nReview Comment:\n   Are we upgrading version for all the APIs? Or just Rename-delete?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-02T05:53:04.804+0000", "updated": "2025-04-02T05:53:04.804+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17940340", "id": "17940340", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2024668490\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java:\n##########\n@@ -199,13 +199,10 @@ public String toString() {\n     }\n \n     public static ApiVersion getCurrentVersion() {\n-      return DEC_12_2019;\n+      return NOV_04_2024;\n\nReview Comment:\n   Yes, we are updating it for all the APIs. It was discussed offline to not keep separate versions for different APIs and instead go for an overall API version upgrade.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-02T11:53:49.100+0000", "updated": "2025-04-02T11:53:49.100+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17940342", "id": "17940342", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2024677585\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -1691,4 +1555,297 @@ public String addClientTransactionIdToHeader(List<AbfsHttpHeader> requestHeaders\n     }\n     return clientTransactionId;\n   }\n+\n+  /**\n+   * Attempts to rename a path with client transaction ID (CTId) recovery mechanism.\n+   * If the initial rename attempt fails, it tries to recover using CTId or ETag\n+   * and retries the operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation\n+   */\n+  private AbfsClientRenameResult renameWithCTIdRecovery(String source,\n+      String destination, String continuation, TracingContext tracingContext,\n+      String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+    // Add client transaction ID to the request headers\n+    String clientTransactionId = addClientTransactionIdToHeader(requestHeaders);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // If we successfully rename a path and isMetadataIncompleteState is true,\n+      // then the rename was recovered; otherwise, it wasn\u2019t.\n+      // This is why isMetadataIncompleteState is used for renameRecovery (as the second parameter).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n\nReview Comment:\n   Yes, it is the existing code where in case we get RENAME_DESTINATION_PARENT_PATH_NOT_FOUND error from server, we retrigger the rename operation after fetching src etag value.\r\n   Recovery happens in case server error is SOURCE_PATH_NOT_FOUND.\r\n   ```\r\n   // ref: HADOOP-18242. Rename failure occurring due to a rare case of\r\n   // tracking metadata being in incomplete state.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-02T11:59:19.711+0000", "updated": "2025-04-02T11:59:19.711+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17940343", "id": "17940343", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2024682024\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java:\n##########\n@@ -2289,6 +2289,6 @@ public void answer(final AbfsRestOperation mockedObj,\n                   null, op);\n             }\n           }\n-        });\n+        }, 0);\n\nReview Comment:\n   We are using mockAbfsOperationCreation to simulate HTTP failures. In some cases, we need the first call to fail, and in others, we need the second call to fail, depending on which call is a rename in the flow. This change is necessary to specify which API call should fail.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-02T12:02:24.590+0000", "updated": "2025-04-02T12:02:24.590+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17940536", "id": "17940536", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509#discussion_r2026158411\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsDfsClient.java:\n##########\n@@ -1691,4 +1555,297 @@ public String addClientTransactionIdToHeader(List<AbfsHttpHeader> requestHeaders\n     }\n     return clientTransactionId;\n   }\n+\n+  /**\n+   * Attempts to rename a path with client transaction ID (CTId) recovery mechanism.\n+   * If the initial rename attempt fails, it tries to recover using CTId or ETag\n+   * and retries the operation.\n+   *\n+   * @param source the source path to be renamed\n+   * @param destination the destination path for the rename\n+   * @param continuation the continuation token for the operation\n+   * @param tracingContext the context for tracing the operation\n+   * @param sourceEtag the ETag of the source path for conditional requests\n+   * @param isMetadataIncompleteState flag indicating if the metadata state is incomplete\n+   * @return an {@link AbfsClientRenameResult} containing the result of the rename operation\n+   * @throws IOException if an error occurs during the rename operation\n+   */\n+  private AbfsClientRenameResult renameWithCTIdRecovery(String source,\n+      String destination, String continuation, TracingContext tracingContext,\n+      String sourceEtag, boolean isMetadataIncompleteState) throws IOException {\n+    // Get request headers for rename operation\n+    List<AbfsHttpHeader> requestHeaders = getHeadersForRename(source);\n+    // Add client transaction ID to the request headers\n+    String clientTransactionId = addClientTransactionIdToHeader(requestHeaders);\n+\n+    // Create the URL for the rename operation\n+    final URL url = createRequestUrl(destination,\n+        getRenameQueryBuilder(destination, continuation).toString());\n+\n+    // Create the rename operation\n+    AbfsRestOperation op = createRenameRestOperation(url, requestHeaders);\n+    try {\n+      incrementAbfsRenamePath();\n+      op.execute(tracingContext);\n+      // If we successfully rename a path and isMetadataIncompleteState is true,\n+      // then the rename was recovered; otherwise, it wasn\u2019t.\n+      // This is why isMetadataIncompleteState is used for renameRecovery (as the second parameter).\n+      return new AbfsClientRenameResult(op, isMetadataIncompleteState,\n+          isMetadataIncompleteState);\n+    } catch (AzureBlobFileSystemException e) {\n+      // Handle rename exceptions and retry if applicable\n+      handleRenameException(source, destination, continuation,\n\nReview Comment:\n   Makes sense. Please add some code comments to make all this more readable and easily understandable.\r\n   \r\n   In the main Javadoc for both types(Etag and CT) of rename it would be godd to explain the whole flow of recovery\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-03T04:30:18.449+0000", "updated": "2025-04-03T04:30:18.449+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17944321", "id": "17944321", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7509:\nURL: https://github.com/apache/hadoop/pull/7509\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-14T10:27:19.835+0000", "updated": "2025-04-14T10:27:19.835+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17944322", "id": "17944322", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 opened a new pull request, #7612:\nURL: https://github.com/apache/hadoop/pull/7612\n\n   Jira: https://issues.apache.org/jira/browse/HADOOP-19497\r\n   ------------------------------------------------------------------\r\n   Linked PR: https://github.com/apache/hadoop/pull/7364\r\n   Main PR: https://github.com/apache/hadoop/pull/7509\r\n   We have implemented create and rename recovery using client transaction IDs over the DFS endpoint ([HADOOP-19450](https://issues.apache.org/jira/browse/HADOOP-19450) [ABFS] Rename/Create path idempotency client-level resolution - ASF JIRA). Since the backend changes were not fully rolled out, we initially implemented the changes with the flag disabled. With this update, we aim to enable the flag, which will start sending client transaction IDs. In case of a failure, we will attempt to recover from the failed state if possible. Here are the detailed steps and considerations for this process:\r\n   \r\n   1. *Implementation Overview*: We introduced a mechanism for create and rename recovery via client transaction IDs to enhance reliability and data integrity over the DFS endpoint. This change was initially flagged as disabled due to incomplete backend rollouts.\r\n   \r\n   2. *Current Update*: With the backend changes now in place, we are ready to enable the flag. This will activate the sending of client transaction IDs, allowing us to track and manage transactions more effectively.\r\n   \r\n   3. *Failure Recovery*: The primary advantage of enabling this flag is the potential for recovery from failed states. If a transaction fails, we can use the client transaction ID to attempt a recovery, minimizing data loss and ensuring continuity.\r\n   \r\n   4. *Next Steps*: We will proceed with enabling the flag and closely monitor the system's performance. Any issues or failures will be documented and addressed promptly to ensure a smooth transition.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-14T10:33:50.202+0000", "updated": "2025-04-14T10:33:50.202+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17944359", "id": "17944359", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7612:\nURL: https://github.com/apache/hadoop/pull/7612#issuecomment-2801604658\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  12m 35s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 4 new or modified test files.  |\r\n   |||| _ branch-3.4 Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  35m 57s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  branch-3.4 passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  branch-3.4 passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 31s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 39s |  |  branch-3.4 passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  branch-3.4 passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  branch-3.4 passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 44s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  javac  |   0m 27s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 23s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 25s |  |  the patch passed with JDK Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 36s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 27s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 133m 36s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.48 ServerAPI=1.48 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7612/1/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7612 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 8c86c6c3960e 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | branch-3.4 / eb9d1e002e08f6b7e77025e71400d3b0b66dc1ee |\r\n   | Default Java | Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.26+4-post-Ubuntu-1ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_442-8u442-b06~us1-0ubuntu1~20.04-b06 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7612/1/testReport/ |\r\n   | Max. process+thread count | 552 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7612/1/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-14T12:48:45.306+0000", "updated": "2025-04-14T12:48:45.306+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17944574", "id": "17944574", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on PR #7612:\nURL: https://github.com/apache/hadoop/pull/7612#issuecomment-2803795357\n\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 809, Failures: 0, Errors: 0, Skipped: 168\r\n   [WARNING] Tests run: 180, Failures: 0, Errors: 0, Skipped: 30\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 809, Failures: 0, Errors: 0, Skipped: 134\r\n   [WARNING] Tests run: 180, Failures: 0, Errors: 0, Skipped: 30\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 10\r\n    \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 793, Failures: 0, Errors: 0, Skipped: 369\r\n   [WARNING] Tests run: 180, Failures: 0, Errors: 0, Skipped: 31\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n    \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 809, Failures: 0, Errors: 0, Skipped: 179\r\n   [WARNING] Tests run: 180, Failures: 0, Errors: 0, Skipped: 54\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 793, Failures: 0, Errors: 0, Skipped: 295\r\n   [WARNING] Tests run: 180, Failures: 0, Errors: 0, Skipped: 27\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 11\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 793, Failures: 0, Errors: 0, Skipped: 373\r\n   [WARNING] Tests run: 180, Failures: 0, Errors: 0, Skipped: 31\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 793, Failures: 0, Errors: 0, Skipped: 299\r\n   [WARNING] Tests run: 180, Failures: 0, Errors: 0, Skipped: 27\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 793, Failures: 0, Errors: 0, Skipped: 319\r\n   [WARNING] Tests run: 180, Failures: 0, Errors: 0, Skipped: 51\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n    \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 809, Failures: 0, Errors: 0, Skipped: 301\r\n   [WARNING] Tests run: 180, Failures: 0, Errors: 0, Skipped: 30\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 23\r\n    \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n    \r\n   [WARNING] Tests run: 174, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 793, Failures: 0, Errors: 0, Skipped: 371\r\n   [WARNING] Tests run: 180, Failures: 0, Errors: 0, Skipped: 31\r\n   [WARNING] Tests run: 272, Failures: 0, Errors: 0, Skipped: 24\r\n   Branch: HADOOP-19497_branch-3.4, Commit: eb9d1e002e08f6b7e77025e71400d3b0b66dc1ee\r\n    \r\n   Time taken: 121 mins 2 secs.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-15T04:56:23.247+0000", "updated": "2025-04-15T04:56:23.247+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13611595/comment/17944746", "id": "17944746", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7612:\nURL: https://github.com/apache/hadoop/pull/7612\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-04-15T14:42:04.979+0000", "updated": "2025-04-15T14:42:04.979+0000"}], "maxResults": 25, "total": 25, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/5", "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png", "name": "Resolved", "id": "5", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3", "id": 3, "key": "done", "colorName": "green", "name": "Done"}}}}