{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13622989", "self": "https://issues.apache.org/jira/rest/api/2/issue/13622989", "key": "HADOOP-19613", "fields": {"summary": "ABFS: [ReadAheadV2] Refactor ReadBufferManager to isolate new code with the current working code", "description": "Read Buffer Manager used today was introduced way back and has been stable for quite a while.\r\nRead Buffer Manager to be introduced as part of https://issues.apache.org/jira/browse/HADOOP-19596 will introduce many changes incrementally over time. While the development goes on and we are able to fully stabilise the optimized version we need the current flow to be functional and undisturbed.\u00a0\r\n\r\nThis work item is to isolate that from new code by refactoring ReadBufferManager class to have 2 different implementations with same public interfaces: ReadBufferManagerV1 and ReadBufferManagerV2.\r\n\r\nThis will also introduce new configs that can be used to toggle between new and old code.\u00a0", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=anujmodi", "name": "anujmodi", "key": "JIRAUSER307456", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34055", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"}, "displayName": "Anuj Modi", "active": true, "timeZone": "Asia/Kolkata"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18005034", "id": "18005034", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 opened a new pull request, #7801:\nURL: https://github.com/apache/hadoop/pull/7801\n\n   This is the first PR in series of work done under Parent Jira: [HADOOP-19596](https://issues.apache.org/jira/browse/HADOOP-19596) to improve the performance of sequential reads in ABFS Driver. \r\n   Please refer to Parent JIRA for more details.\r\n   \r\n   ### Description of PR\r\n   Jira: https://issues.apache.org/jira/browse/HADOOP-19613\r\n   \r\n   Read Buffer Manager used today was introduced way back and has been stable for quite a while.\r\n   Read Buffer Manager to be introduced as part of [HADOOP-19596](https://issues.apache.org/jira/browse/HADOOP-19596) will introduce many changes incrementally over time. While the development goes on and we are able to fully stabilise the optimized version we need the current flow to be functional and undisturbed. \r\n   \r\n   This work item is to isolate that from new code by refactoring ReadBufferManager class to have 2 different implementations with same public interfaces: ReadBufferManagerV1 and ReadBufferManagerV2.\r\n   \r\n   This will also introduce new configs that can be used to toggle between new and old code. \r\n   \r\n   ### How was this patch tested?\r\n   Existing tests were modified to work with the Refactored Classes.\r\n   More tests will be added with coming up PRs where new implementation will be introduced.\r\n   Test suite result added.\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-14T03:03:00.672+0000", "updated": "2025-07-14T03:03:00.672+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18005292", "id": "18005292", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3072020510\n\n   \r\n   ============================================================\r\n   HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 818, Failures: 0, Errors: 0, Skipped: 165\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   HNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 4\r\n   [WARNING] Tests run: 821, Failures: 0, Errors: 0, Skipped: 117\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 10\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 660, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   AppendBlob-HNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 818, Failures: 0, Errors: 0, Skipped: 176\r\n   [WARNING] Tests run: 133, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-SharedKey-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 664, Failures: 0, Errors: 0, Skipped: 134\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 11\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 657, Failures: 0, Errors: 0, Skipped: 225\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 661, Failures: 0, Errors: 0, Skipped: 147\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 5\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   AppendBlob-NonHNS-OAuth-Blob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 659, Failures: 0, Errors: 0, Skipped: 165\r\n   [WARNING] Tests run: 133, Failures: 0, Errors: 0, Skipped: 6\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \r\n   ============================================================\r\n   HNS-Oauth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 3\r\n   [WARNING] Tests run: 692, Failures: 0, Errors: 0, Skipped: 172\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 23\r\n   \r\n   ============================================================\r\n   NonHNS-OAuth-DFS-IngressBlob\r\n   ============================================================\r\n   \r\n   [WARNING] Tests run: 177, Failures: 0, Errors: 0, Skipped: 10\r\n   [WARNING] Tests run: 657, Failures: 0, Errors: 0, Skipped: 223\r\n   [WARNING] Tests run: 156, Failures: 0, Errors: 0, Skipped: 11\r\n   [WARNING] Tests run: 269, Failures: 0, Errors: 0, Skipped: 24\r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-15T05:26:45.344+0000", "updated": "2025-07-15T05:26:45.344+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18007770", "id": "18007770", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2212899426\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -773,9 +788,14 @@ byte[] getBuffer() {\n     return buffer;\n   }\n \n+  /**\n+   * Checks if any version of read ahead is enabled.\n+   * If both are disabled, then skip read ahead logic.\n+   * @return true if read ahead is enabled, false otherwise.\n+   */\n   @VisibleForTesting\n   public boolean isReadAheadEnabled() {\n-    return readAheadEnabled;\n+    return (readAheadEnabled || readAheadV2Enabled) && readBufferManager != null;\n\nReview Comment:\n   The method name suggests we're only checking readAhead, but we're also checking readBufferManager here. Should we either move this check outside the method or update the method name to reflect its functionality?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -259,10 +259,15 @@ public final class ConfigurationKeys {\n   public static final String AZURE_KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT = \"fs.azure.shellkeyprovider.script\";\n \n   /**\n-   * Enable or disable readahead buffer in AbfsInputStream.\n+   * Enable or disable readahead V1 in AbfsInputStream.\n    * Value: {@value}.\n    */\n   public static final String FS_AZURE_ENABLE_READAHEAD = \"fs.azure.enable.readahead\";\n+  /**\n+   * Enable or disable readahead V2 in AbfsInputStream. This will work independent of V1.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_ENABLE_READAHEAD_V2 = \"fs.azure.enable.readahead.v2\";\n\nReview Comment:\n   If both V1 and V2 are enabled, does V2 take precedence over V1? If so, would it be inaccurate to refer to them as independent?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-17T10:04:41.950+0000", "updated": "2025-07-17T10:04:41.950+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18007985", "id": "18007985", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2214975298\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -259,10 +259,15 @@ public final class ConfigurationKeys {\n   public static final String AZURE_KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT = \"fs.azure.shellkeyprovider.script\";\n \n   /**\n-   * Enable or disable readahead buffer in AbfsInputStream.\n+   * Enable or disable readahead V1 in AbfsInputStream.\n    * Value: {@value}.\n    */\n   public static final String FS_AZURE_ENABLE_READAHEAD = \"fs.azure.enable.readahead\";\n+  /**\n+   * Enable or disable readahead V2 in AbfsInputStream. This will work independent of V1.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_ENABLE_READAHEAD_V2 = \"fs.azure.enable.readahead.v2\";\n\nReview Comment:\n   By independent I mean, users don't have to explicitly enable `fs.azure.enable.readahead` for `fs.azure.enable.readahead.v2` to work.\r\n   \r\n   And yes if both are set, preference will be given to V2.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-18T05:43:29.284+0000", "updated": "2025-07-18T05:43:29.284+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18007986", "id": "18007986", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2214978490\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -773,9 +788,14 @@ byte[] getBuffer() {\n     return buffer;\n   }\n \n+  /**\n+   * Checks if any version of read ahead is enabled.\n+   * If both are disabled, then skip read ahead logic.\n+   * @return true if read ahead is enabled, false otherwise.\n+   */\n   @VisibleForTesting\n   public boolean isReadAheadEnabled() {\n-    return readAheadEnabled;\n+    return (readAheadEnabled || readAheadV2Enabled) && readBufferManager != null;\n\nReview Comment:\n   method is to check if readahaead is enabled or not and for that we need readBufferManager to be not null.\r\n   This is added only as a precaution to avoid NPE\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-18T05:44:41.049+0000", "updated": "2025-07-18T05:44:41.049+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18007998", "id": "18007998", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2215342272\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java:\n##########\n@@ -15,636 +15,166 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.hadoop.fs.azurebfs.services;\n \n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n+package org.apache.hadoop.fs.azurebfs.services;\n \n import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n import java.util.List;\n-import java.util.Queue;\n-import java.util.Stack;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n \n /**\n- * The Read Buffer Manager for Rest AbfsClient.\n+ * Interface for managing read buffers for Azure Blob File System input streams.\n  */\n-final class ReadBufferManager {\n-  private static final Logger LOGGER = LoggerFactory.getLogger(ReadBufferManager.class);\n-  private static final int ONE_KB = 1024;\n-  private static final int ONE_MB = ONE_KB * ONE_KB;\n-\n-  private static final int NUM_BUFFERS = 16;\n-  private static final int NUM_THREADS = 8;\n-  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n-\n-  private static int blockSize = 4 * ONE_MB;\n-  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n-  private Thread[] threads = new Thread[NUM_THREADS];\n-  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n-  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n-\n-  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n-  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n-  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n-  private static ReadBufferManager bufferManager; // singleton, initialized in static initialization block\n-  private static final ReentrantLock LOCK = new ReentrantLock();\n-\n-  static ReadBufferManager getBufferManager() {\n-    if (bufferManager == null) {\n-      LOCK.lock();\n-      try {\n-        if (bufferManager == null) {\n-          bufferManager = new ReadBufferManager();\n-          bufferManager.init();\n-        }\n-      } finally {\n-        LOCK.unlock();\n-      }\n-    }\n-    return bufferManager;\n-  }\n-\n-  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n-    if (bufferManager == null) {\n-      LOGGER.debug(\n-          \"ReadBufferManager not initialized yet. Overriding readAheadBlockSize as {}\",\n-          readAheadBlockSize);\n-      blockSize = readAheadBlockSize;\n-    }\n-  }\n-\n-  private void init() {\n-    buffers = new byte[NUM_BUFFERS][];\n-    for (int i = 0; i < NUM_BUFFERS; i++) {\n-      buffers[i] = new byte[blockSize];  // same buffers are reused. The byte array never goes back to GC\n-      freeList.add(i);\n-    }\n-    for (int i = 0; i < NUM_THREADS; i++) {\n-      Thread t = new Thread(new ReadBufferWorker(i));\n-      t.setDaemon(true);\n-      threads[i] = t;\n-      t.setName(\"ABFS-prefetch-\" + i);\n-      t.start();\n-    }\n-    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n-  }\n-\n-  // hide instance constructor\n-  private ReadBufferManager() {\n-    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n-  }\n-\n-\n-  /*\n-   *\n-   *  AbfsInputStream-facing methods\n-   *\n-   */\n-\n+public interface ReadBufferManager {\n \n   /**\n-   * {@link AbfsInputStream} calls this method to queue read-aheads.\n-   *\n-   * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n-   * @param requestedOffset The offset in the file which shoukd be read\n-   * @param requestedLength The length to read\n+   * Queues a read-ahead request from {@link AbfsInputStream}\n+   * for a given offset in file and given length.\n+   * @param stream the input stream requesting the read-ahead\n+   * @param requestedOffset the offset in the remote file to start reading\n+   * @param requestedLength the number of bytes to read from file\n+   * @param tracingContext the tracing context for diagnostics\n    */\n-  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength,\n-                      TracingContext tracingContext) {\n-    if (LOGGER.isTraceEnabled()) {\n-      LOGGER.trace(\"Start Queueing readAhead for {} offset {} length {}\",\n-          stream.getPath(), requestedOffset, requestedLength);\n-    }\n-    ReadBuffer buffer;\n-    synchronized (this) {\n-      if (isAlreadyQueued(stream, requestedOffset)) {\n-        return; // already queued, do not queue again\n-      }\n-      if (freeList.isEmpty() && !tryEvict()) {\n-        return; // no buffers available, cannot queue anything\n-      }\n-\n-      buffer = new ReadBuffer();\n-      buffer.setStream(stream);\n-      buffer.setOffset(requestedOffset);\n-      buffer.setLength(0);\n-      buffer.setRequestedLength(requestedLength);\n-      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n-      buffer.setLatch(new CountDownLatch(1));\n-      buffer.setTracingContext(tracingContext);\n-\n-      Integer bufferIndex = freeList.pop();  // will return a value, since we have checked size > 0 already\n-\n-      buffer.setBuffer(buffers[bufferIndex]);\n-      buffer.setBufferindex(bufferIndex);\n-      readAheadQueue.add(buffer);\n-      notifyAll();\n-      if (LOGGER.isTraceEnabled()) {\n-        LOGGER.trace(\"Done q-ing readAhead for file {} offset {} buffer idx {}\",\n-            stream.getPath(), requestedOffset, buffer.getBufferindex());\n-      }\n-    }\n-  }\n+  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext);\n \n+  /**\n+   * Gets a block of data from the prefetched data by ReadBufferManager.\n+   * {@link AbfsInputStream} calls this method to read data\n+   * @param stream the input stream requesting the block\n+   * @param position the position in the file to read from\n+   * @param length the number of bytes to read\n+   * @param buffer the buffer to store the read data\n+   * @return the number of bytes actually read\n+   * @throws IOException if an I/O error occurs\n+   */\n+  int getBlock(final AbfsInputStream stream,\n+      final long position,\n+      final int length,\n+      final byte[] buffer)\n+      throws IOException;\n \n   /**\n-   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n-   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n-   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n-   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n-   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do it's own\n-   * read to get the data faster (copmared to the read waiting in queue for an indeterminate amount of time).\n+   * {@link ReadBufferWorker} calls this to get the next buffer to read from read-ahead queue.\n+   * Requested read will be performed by background thread.\n    *\n-   * @param stream   the file to read bytes for\n-   * @param position the offset in the file to do a read for\n-   * @param length   the length to read\n-   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n-   * @return the number of bytes read\n+   * @return the next {@link ReadBuffer} to read\n+   * @throws InterruptedException if interrupted while waiting\n    */\n-  int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n-      throws IOException {\n-    // not synchronized, so have to be careful with locking\n-    if (LOGGER.isTraceEnabled()) {\n-      LOGGER.trace(\"getBlock for file {}  position {}  thread {}\",\n-          stream.getPath(), position, Thread.currentThread().getName());\n-    }\n-\n-    waitForProcess(stream, position);\n-\n-    int bytesRead = 0;\n-    synchronized (this) {\n-      bytesRead = getBlockFromCompletedQueue(stream, position, length, buffer);\n-    }\n-    if (bytesRead > 0) {\n-      if (LOGGER.isTraceEnabled()) {\n-        LOGGER.trace(\"Done read from Cache for {} position {} length {}\",\n-            stream.getPath(), position, bytesRead);\n-      }\n-      return bytesRead;\n-    }\n+  ReadBuffer getNextBlockToRead() throws InterruptedException;\n \n-    // otherwise, just say we got nothing - calling thread can do its own read\n-    return 0;\n-  }\n-\n-  /*\n-   *\n-   *  Internal methods\n-   *\n+  /**\n+   * Marks the specified buffer as done reading and updates its status.\n+   * Called by {@link ReadBufferWorker} after reading is complete.\n+   * @param buffer the buffer that was read by worker thread\n+   * @param result the status of the read operation\n+   * @param bytesActuallyRead the number of bytes actually read\n    */\n-\n-  private void waitForProcess(final AbfsInputStream stream, final long position) {\n-    ReadBuffer readBuf;\n-    synchronized (this) {\n-      clearFromReadAheadQueue(stream, position);\n-      readBuf = getFromList(inProgressList, stream, position);\n-    }\n-    if (readBuf != null) {         // if in in-progress queue, then block for it\n-      try {\n-        if (LOGGER.isTraceEnabled()) {\n-          LOGGER.trace(\"got a relevant read buffer for file {} offset {} buffer idx {}\",\n-              stream.getPath(), readBuf.getOffset(), readBuf.getBufferindex());\n-        }\n-        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n-        // Note on correctness: readBuf gets out of inProgressList only in 1 place: after worker thread\n-        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n-        // inProgressList. So this latch is safe to be outside the synchronized block.\n-        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n-        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n-        // then the latch cane be removed and replaced with wait/notify whenever inProgressList is touched.\n-      } catch (InterruptedException ex) {\n-        Thread.currentThread().interrupt();\n-      }\n-      if (LOGGER.isTraceEnabled()) {\n-        LOGGER.trace(\"latch done for file {} buffer idx {} length {}\",\n-            stream.getPath(), readBuf.getBufferindex(), readBuf.getLength());\n-      }\n-    }\n-  }\n+  void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n+      final int bytesActuallyRead);\n \n   /**\n-   * If any buffer in the completedlist can be reclaimed then reclaim it and return the buffer to free list.\n-   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * Purges all buffers associated with the calling {@link AbfsInputStream}.\n    *\n-   * @return whether the eviction succeeeded - i.e., were we able to free up one buffer\n+   * @param stream the input stream whose buffers should be purged\n    */\n-  private synchronized boolean tryEvict() {\n-    ReadBuffer nodeToEvict = null;\n-    if (completedReadList.size() <= 0) {\n-      return false;  // there are no evict-able buffers\n-    }\n-\n-    long currentTimeInMs = currentTimeMillis();\n-\n-    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n-    for (ReadBuffer buf : completedReadList) {\n-      if (buf.isFirstByteConsumed() && buf.isLastByteConsumed()) {\n-        nodeToEvict = buf;\n-        break;\n-      }\n-    }\n-    if (nodeToEvict != null) {\n-      return evict(nodeToEvict);\n-    }\n-\n-    // next, try buffers where any bytes have been consumed (may be a bad idea? have to experiment and see)\n-    for (ReadBuffer buf : completedReadList) {\n-      if (buf.isAnyByteConsumed()) {\n-        nodeToEvict = buf;\n-        break;\n-      }\n-    }\n+  void purgeBuffersForStream(AbfsInputStream stream);\n \n-    if (nodeToEvict != null) {\n-      return evict(nodeToEvict);\n-    }\n-\n-    // next, try any old nodes that have not been consumed\n-    // Failed read buffers (with buffer index=-1) that are older than\n-    // thresholdAge should be cleaned up, but at the same time should not\n-    // report successful eviction.\n-    // Queue logic expects that a buffer is freed up for read ahead when\n-    // eviction is successful, whereas a failed ReadBuffer would have released\n-    // its buffer when its status was set to READ_FAILED.\n-    long earliestBirthday = Long.MAX_VALUE;\n-    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n-    for (ReadBuffer buf : completedReadList) {\n-      if ((buf.getBufferindex() != -1)\n-          && (buf.getTimeStamp() < earliestBirthday)) {\n-        nodeToEvict = buf;\n-        earliestBirthday = buf.getTimeStamp();\n-      } else if ((buf.getBufferindex() == -1)\n-          && (currentTimeInMs - buf.getTimeStamp()) > thresholdAgeMilliseconds) {\n-        oldFailedBuffers.add(buf);\n-      }\n-    }\n-\n-    for (ReadBuffer buf : oldFailedBuffers) {\n-      evict(buf);\n-    }\n-\n-    if ((currentTimeInMs - earliestBirthday > thresholdAgeMilliseconds) && (nodeToEvict != null)) {\n-      return evict(nodeToEvict);\n-    }\n-\n-    LOGGER.trace(\"No buffer eligible for eviction\");\n-    // nothing can be evicted\n-    return false;\n-  }\n-\n-  private boolean evict(final ReadBuffer buf) {\n-    // As failed ReadBuffers (bufferIndx = -1) are saved in completedReadList,\n-    // avoid adding it to freeList.\n-    if (buf.getBufferindex() != -1) {\n-      freeList.push(buf.getBufferindex());\n-    }\n-\n-    completedReadList.remove(buf);\n-    buf.setTracingContext(null);\n-    if (LOGGER.isTraceEnabled()) {\n-      LOGGER.trace(\"Evicting buffer idx {}; was used for file {} offset {} length {}\",\n-          buf.getBufferindex(), buf.getStream().getPath(), buf.getOffset(), buf.getLength());\n-    }\n-    return true;\n-  }\n-\n-  private boolean isAlreadyQueued(final AbfsInputStream stream, final long requestedOffset) {\n-    // returns true if any part of the buffer is already queued\n-    return (isInList(readAheadQueue, stream, requestedOffset)\n-        || isInList(inProgressList, stream, requestedOffset)\n-        || isInList(completedReadList, stream, requestedOffset));\n-  }\n-\n-  private boolean isInList(final Collection<ReadBuffer> list, final AbfsInputStream stream, final long requestedOffset) {\n-    return (getFromList(list, stream, requestedOffset) != null);\n-  }\n-\n-  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInputStream stream, final long requestedOffset) {\n-    for (ReadBuffer buffer : list) {\n-      if (buffer.getStream() == stream) {\n-        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n-            && requestedOffset >= buffer.getOffset()\n-            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n-          return buffer;\n-        } else if (requestedOffset >= buffer.getOffset()\n-            && requestedOffset < buffer.getOffset() + buffer.getRequestedLength()) {\n-          return buffer;\n-        }\n-      }\n-    }\n-    return null;\n-  }\n+  // Following Methods are for testing purposes only and should not be used in production code.\n \n   /**\n-   * Returns buffers that failed or passed from completed queue.\n-   * @param stream\n-   * @param requestedOffset\n-   * @return\n+   * Resets the read buffer manager for testing purposes.\n    */\n-  private ReadBuffer getBufferFromCompletedQueue(final AbfsInputStream stream, final long requestedOffset) {\n-    for (ReadBuffer buffer : completedReadList) {\n-      // Buffer is returned if the requestedOffset is at or above buffer's\n-      // offset but less than buffer's length or the actual requestedLength\n-      if ((buffer.getStream() == stream)\n-          && (requestedOffset >= buffer.getOffset())\n-          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n-          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n-          return buffer;\n-        }\n-      }\n-\n-    return null;\n-  }\n-\n-  private void clearFromReadAheadQueue(final AbfsInputStream stream, final long requestedOffset) {\n-    ReadBuffer buffer = getFromList(readAheadQueue, stream, requestedOffset);\n-    if (buffer != null) {\n-      readAheadQueue.remove(buffer);\n-      notifyAll();   // lock is held in calling method\n-      freeList.push(buffer.getBufferindex());\n-    }\n-  }\n-\n-  private int getBlockFromCompletedQueue(final AbfsInputStream stream, final long position, final int length,\n-                                         final byte[] buffer) throws IOException {\n-    ReadBuffer buf = getBufferFromCompletedQueue(stream, position);\n-\n-    if (buf == null) {\n-      return 0;\n-    }\n-\n-    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n-      // To prevent new read requests to fail due to old read-ahead attempts,\n-      // return exception only from buffers that failed within last thresholdAgeMilliseconds\n-      if ((currentTimeMillis() - (buf.getTimeStamp()) < thresholdAgeMilliseconds)) {\n-        throw buf.getErrException();\n-      } else {\n-        return 0;\n-      }\n-    }\n-\n-    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n-        || (position >= buf.getOffset() + buf.getLength())) {\n-      return 0;\n-    }\n-\n-    int cursor = (int) (position - buf.getOffset());\n-    int availableLengthInBuffer = buf.getLength() - cursor;\n-    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n-    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n-    if (cursor == 0) {\n-      buf.setFirstByteConsumed(true);\n-    }\n-    if (cursor + lengthToCopy == buf.getLength()) {\n-      buf.setLastByteConsumed(true);\n-    }\n-    buf.setAnyByteConsumed(true);\n-    return lengthToCopy;\n-  }\n+  @VisibleForTesting\n+  void testResetReadBufferManager();\n\nReview Comment:\n   why does the method name in production have test ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-18T08:11:01.295+0000", "updated": "2025-07-18T08:11:01.295+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008001", "id": "18008001", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2215365910\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 implements ReadBufferManager {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManagerV1.class);\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+\n+  private static int blockSize = 4 * ONE_MB;\n+  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n+  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+\n+  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n+  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n+  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n+  private static ReadBufferManagerV1 bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+\n+  static ReadBufferManagerV1 getBufferManager() {\n\nReview Comment:\n   since we are refactoring the code, we can add javadocs for all the methods as well\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-18T08:19:31.705+0000", "updated": "2025-07-18T08:19:31.705+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008002", "id": "18008002", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2215380532\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 implements ReadBufferManager {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManagerV1.class);\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+\n+  private static int blockSize = 4 * ONE_MB;\n+  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n+  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+\n+  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n+  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n+  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n+  private static ReadBufferManagerV1 bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      blockSize = readAheadBlockSize;\n+    }\n+  }\n+\n+  private void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[blockSize];  // same buffers are reused. The byte array never goes back to GC\n+      freeList.add(i);\n+    }\n+    for (int i = 0; i < NUM_THREADS; i++) {\n+      Thread t = new Thread(new ReadBufferWorker(i));\n+      t.setDaemon(true);\n+      threads[i] = t;\n+      t.setName(\"ABFS-prefetch-\" + i);\n+      t.start();\n+    }\n+    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+  }\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method to queue read-aheads.\n+   *\n+   * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n+   * @param requestedOffset The offset in the file which shoukd be read\n+   * @param requestedLength The length to read\n\nReview Comment:\n   tracing context missing in params\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-18T08:24:52.351+0000", "updated": "2025-07-18T08:24:52.351+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008021", "id": "18008021", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2215686620\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 implements ReadBufferManager {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManagerV1.class);\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+\n+  private static int blockSize = 4 * ONE_MB;\n+  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n+  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+\n+  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n+  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n+  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n+  private static ReadBufferManagerV1 bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      blockSize = readAheadBlockSize;\n+    }\n+  }\n+\n+  private void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[blockSize];  // same buffers are reused. The byte array never goes back to GC\n+      freeList.add(i);\n+    }\n+    for (int i = 0; i < NUM_THREADS; i++) {\n+      Thread t = new Thread(new ReadBufferWorker(i));\n+      t.setDaemon(true);\n+      threads[i] = t;\n+      t.setName(\"ABFS-prefetch-\" + i);\n+      t.start();\n+    }\n+    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+  }\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method to queue read-aheads.\n+   *\n+   * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n+   * @param requestedOffset The offset in the file which shoukd be read\n+   * @param requestedLength The length to read\n+   */\n+  @Override\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength,\n+      TracingContext tracingContext) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"Start Queueing readAhead for {} offset {} length {}\",\n+          stream.getPath(), requestedOffset, requestedLength);\n+    }\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream, requestedOffset)) {\n+        return; // already queued, do not queue again\n+      }\n+      if (freeList.isEmpty() && !tryEvict()) {\n+        return; // no buffers available, cannot queue anything\n+      }\n+\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream);\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      Integer bufferIndex = freeList.pop();  // will return a value, since we have checked size > 0 already\n+\n+      buffer.setBuffer(buffers[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      readAheadQueue.add(buffer);\n+      notifyAll();\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"Done q-ing readAhead for file {} offset {} buffer idx {}\",\n+            stream.getPath(), requestedOffset, buffer.getBufferindex());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do it's own\n+   * read to get the data faster (copmared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream   the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n+   */\n+  @Override\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"getBlock for file {}  position {}  thread {}\",\n+          stream.getPath(), position, Thread.currentThread().getName());\n+    }\n+\n+    waitForProcess(stream, position);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(stream, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"Done read from Cache for {} position {} length {}\",\n+            stream.getPath(), position, bytesRead);\n+      }\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n+    return 0;\n+  }\n+\n+  /**\n+   * ReadBufferWorker thread calls this to get the next buffer that it should work on.\n+   *\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n+   */\n+  @Override\n+  public ReadBuffer getNextBlockToRead() throws InterruptedException {\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      //buffer = readAheadQueue.take();  // blocking method\n\nReview Comment:\n   remove comments\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-18T10:23:40.761+0000", "updated": "2025-07-18T10:23:40.761+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008570", "id": "18008570", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2218097940\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java:\n##########\n@@ -26,7 +26,7 @@\n \n import static org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus.READ_FAILED;\n \n-class ReadBuffer {\n+public class ReadBuffer {\n\nReview Comment:\n   why did we make it public? The new classes seem to be under the same package as this\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T02:53:13.034+0000", "updated": "2025-07-21T02:53:13.034+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008571", "id": "18008571", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2218101802\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 implements ReadBufferManager {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManagerV1.class);\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+\n+  private static int blockSize = 4 * ONE_MB;\n+  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n+  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+\n+  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n+  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n+  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n+  private static ReadBufferManagerV1 bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      blockSize = readAheadBlockSize;\n+    }\n+  }\n+\n+  private void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[blockSize];  // same buffers are reused. The byte array never goes back to GC\n+      freeList.add(i);\n+    }\n+    for (int i = 0; i < NUM_THREADS; i++) {\n+      Thread t = new Thread(new ReadBufferWorker(i));\n+      t.setDaemon(true);\n+      threads[i] = t;\n+      t.setName(\"ABFS-prefetch-\" + i);\n+      t.start();\n+    }\n+    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+  }\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method to queue read-aheads.\n+   *\n+   * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n+   * @param requestedOffset The offset in the file which shoukd be read\n+   * @param requestedLength The length to read\n+   */\n+  @Override\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength,\n+      TracingContext tracingContext) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"Start Queueing readAhead for {} offset {} length {}\",\n+          stream.getPath(), requestedOffset, requestedLength);\n+    }\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream, requestedOffset)) {\n+        return; // already queued, do not queue again\n+      }\n+      if (freeList.isEmpty() && !tryEvict()) {\n+        return; // no buffers available, cannot queue anything\n+      }\n+\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream);\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      Integer bufferIndex = freeList.pop();  // will return a value, since we have checked size > 0 already\n+\n+      buffer.setBuffer(buffers[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      readAheadQueue.add(buffer);\n+      notifyAll();\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"Done q-ing readAhead for file {} offset {} buffer idx {}\",\n+            stream.getPath(), requestedOffset, buffer.getBufferindex());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do it's own\n+   * read to get the data faster (copmared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream   the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n\nReview Comment:\n   nit: we can add the @throws IOException line\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T03:00:27.937+0000", "updated": "2025-07-21T03:00:27.937+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008572", "id": "18008572", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2218102748\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 implements ReadBufferManager {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManagerV1.class);\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+\n+  private static int blockSize = 4 * ONE_MB;\n+  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n+  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+\n+  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n+  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n+  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n+  private static ReadBufferManagerV1 bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      blockSize = readAheadBlockSize;\n+    }\n+  }\n+\n+  private void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[blockSize];  // same buffers are reused. The byte array never goes back to GC\n+      freeList.add(i);\n+    }\n+    for (int i = 0; i < NUM_THREADS; i++) {\n+      Thread t = new Thread(new ReadBufferWorker(i));\n+      t.setDaemon(true);\n+      threads[i] = t;\n+      t.setName(\"ABFS-prefetch-\" + i);\n+      t.start();\n+    }\n+    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+  }\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method to queue read-aheads.\n+   *\n+   * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n+   * @param requestedOffset The offset in the file which shoukd be read\n+   * @param requestedLength The length to read\n+   */\n+  @Override\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength,\n+      TracingContext tracingContext) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"Start Queueing readAhead for {} offset {} length {}\",\n+          stream.getPath(), requestedOffset, requestedLength);\n+    }\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream, requestedOffset)) {\n+        return; // already queued, do not queue again\n+      }\n+      if (freeList.isEmpty() && !tryEvict()) {\n+        return; // no buffers available, cannot queue anything\n+      }\n+\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream);\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      Integer bufferIndex = freeList.pop();  // will return a value, since we have checked size > 0 already\n+\n+      buffer.setBuffer(buffers[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      readAheadQueue.add(buffer);\n+      notifyAll();\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"Done q-ing readAhead for file {} offset {} buffer idx {}\",\n+            stream.getPath(), requestedOffset, buffer.getBufferindex());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do it's own\n+   * read to get the data faster (copmared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream   the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n+   */\n+  @Override\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"getBlock for file {}  position {}  thread {}\",\n+          stream.getPath(), position, Thread.currentThread().getName());\n+    }\n+\n+    waitForProcess(stream, position);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(stream, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"Done read from Cache for {} position {} length {}\",\n+            stream.getPath(), position, bytesRead);\n+      }\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n+    return 0;\n+  }\n+\n+  /**\n+   * ReadBufferWorker thread calls this to get the next buffer that it should work on.\n+   *\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n+   */\n+  @Override\n+  public ReadBuffer getNextBlockToRead() throws InterruptedException {\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      //buffer = readAheadQueue.take();  // blocking method\n+      while (readAheadQueue.size() == 0) {\n+        wait();\n+      }\n+      buffer = readAheadQueue.remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;            // should never happen\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      inProgressList.add(buffer);\n+    }\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"ReadBufferWorker picked file {} for offset {}\",\n+          buffer.getStream().getPath(), buffer.getOffset());\n+    }\n+    return buffer;\n+  }\n+\n+  /**\n+   * ReadBufferWorker thread calls this method to post completion.\n+   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n+   */\n+  @Override\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"ReadBufferWorker completed read file {} for offset {} outcome {} bytes {}\",\n+          buffer.getStream().getPath(),  buffer.getOffset(), result, bytesActuallyRead);\n+    }\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (inProgressList.contains(buffer)) {\n+        inProgressList.remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          freeList.push(buffer.getBufferindex());\n+          // buffer will be deleted as per the eviction policy.\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        completedReadList.add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n+  }\n+\n+  /**\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV1} when stream is closed.\n+   * @param stream input stream.\n+   */\n+  @Override\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    LOGGER.debug(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    readAheadQueue.removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, completedReadList);\n+  }\n+\n+  private void waitForProcess(final AbfsInputStream stream, final long position) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      clearFromReadAheadQueue(stream, position);\n+      readBuf = getFromList(inProgressList, stream, position);\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        if (LOGGER.isTraceEnabled()) {\n+          LOGGER.trace(\"got a relevant read buffer for file {} offset {} buffer idx {}\",\n+              stream.getPath(), readBuf.getOffset(), readBuf.getBufferindex());\n+        }\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of inProgressList only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // inProgressList. So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever inProgressList is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"latch done for file {} buffer idx {} length {}\",\n+            stream.getPath(), readBuf.getBufferindex(), readBuf.getLength());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * If any buffer in the completedlist can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   *\n+   * @return whether the eviction succeeeded - i.e., were we able to free up one buffer\n+   */\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (completedReadList.size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : completedReadList) {\n+      if (buf.isFirstByteConsumed() && buf.isLastByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return evict(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (may be a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : completedReadList) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return evict(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : completedReadList) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > thresholdAgeMilliseconds) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      evict(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > thresholdAgeMilliseconds) && (nodeToEvict != null)) {\n+      return evict(nodeToEvict);\n+    }\n+\n+    LOGGER.trace(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in completedReadList,\n+    // avoid adding it to freeList.\n+    if (buf.getBufferindex() != -1) {\n+      freeList.push(buf.getBufferindex());\n+    }\n+\n+    completedReadList.remove(buf);\n+    buf.setTracingContext(null);\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"Evicting buffer idx {}; was used for file {} offset {} length {}\",\n+          buf.getBufferindex(), buf.getStream().getPath(), buf.getOffset(), buf.getLength());\n+    }\n+    return true;\n+  }\n+\n+  private boolean isAlreadyQueued(final AbfsInputStream stream, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(readAheadQueue, stream, requestedOffset)\n+        || isInList(inProgressList, stream, requestedOffset)\n+        || isInList(completedReadList, stream, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final AbfsInputStream stream, final long requestedOffset) {\n+    return (getFromList(list, stream, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInputStream stream, final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (buffer.getStream() == stream) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Returns buffers that failed or passed from completed queue.\n+   * @param stream\n+   * @param requestedOffset\n+   * @return\n\nReview Comment:\n   nit: missing @return ReadBuffer\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T03:02:36.068+0000", "updated": "2025-07-21T03:02:36.068+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008573", "id": "18008573", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2218115856\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferWorker.java:\n##########\n@@ -51,7 +51,7 @@ public void run() {\n     } catch (InterruptedException ex) {\n       Thread.currentThread().interrupt();\n     }\n-    ReadBufferManager bufferManager = ReadBufferManager.getBufferManager();\n+    ReadBufferManager bufferManager = ReadBufferManagerV1.getBufferManager();\n\nReview Comment:\n   will this always require ReadBufferManagerV1?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T03:20:59.951+0000", "updated": "2025-07-21T03:20:59.951+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008576", "id": "18008576", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2218118739\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -921,10 +921,14 @@ private AzureBlobFileSystem createTestFile(Path testFilePath, long testFileSize,\n   }\n \n   private void resetReadBufferManager(int bufferSize, int threshold) {\n-    ReadBufferManager.getBufferManager()\n+    getBufferManager()\n         .testResetReadBufferManager(bufferSize, threshold);\n     // Trigger GC as aggressive recreation of ReadBufferManager buffers\n     // by successive tests can lead to OOM based on the dev VM/machine capacity.\n     System.gc();\n   }\n+\n+  private ReadBufferManager getBufferManager() {\n+    return ReadBufferManagerV1.getBufferManager();\n+  }\n }\n\nReview Comment:\n   nit: add EOL\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T03:26:10.508+0000", "updated": "2025-07-21T03:26:10.508+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008593", "id": "18008593", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2218255246\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java:\n##########\n@@ -15,636 +15,166 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.hadoop.fs.azurebfs.services;\n \n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n+package org.apache.hadoop.fs.azurebfs.services;\n \n import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n import java.util.List;\n-import java.util.Queue;\n-import java.util.Stack;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n \n /**\n- * The Read Buffer Manager for Rest AbfsClient.\n+ * Interface for managing read buffers for Azure Blob File System input streams.\n  */\n-final class ReadBufferManager {\n-  private static final Logger LOGGER = LoggerFactory.getLogger(ReadBufferManager.class);\n-  private static final int ONE_KB = 1024;\n-  private static final int ONE_MB = ONE_KB * ONE_KB;\n-\n-  private static final int NUM_BUFFERS = 16;\n-  private static final int NUM_THREADS = 8;\n-  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n-\n-  private static int blockSize = 4 * ONE_MB;\n-  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n-  private Thread[] threads = new Thread[NUM_THREADS];\n-  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n-  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n-\n-  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n-  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n-  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n-  private static ReadBufferManager bufferManager; // singleton, initialized in static initialization block\n-  private static final ReentrantLock LOCK = new ReentrantLock();\n-\n-  static ReadBufferManager getBufferManager() {\n-    if (bufferManager == null) {\n-      LOCK.lock();\n-      try {\n-        if (bufferManager == null) {\n-          bufferManager = new ReadBufferManager();\n-          bufferManager.init();\n-        }\n-      } finally {\n-        LOCK.unlock();\n-      }\n-    }\n-    return bufferManager;\n-  }\n-\n-  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n-    if (bufferManager == null) {\n-      LOGGER.debug(\n-          \"ReadBufferManager not initialized yet. Overriding readAheadBlockSize as {}\",\n-          readAheadBlockSize);\n-      blockSize = readAheadBlockSize;\n-    }\n-  }\n-\n-  private void init() {\n-    buffers = new byte[NUM_BUFFERS][];\n-    for (int i = 0; i < NUM_BUFFERS; i++) {\n-      buffers[i] = new byte[blockSize];  // same buffers are reused. The byte array never goes back to GC\n-      freeList.add(i);\n-    }\n-    for (int i = 0; i < NUM_THREADS; i++) {\n-      Thread t = new Thread(new ReadBufferWorker(i));\n-      t.setDaemon(true);\n-      threads[i] = t;\n-      t.setName(\"ABFS-prefetch-\" + i);\n-      t.start();\n-    }\n-    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n-  }\n-\n-  // hide instance constructor\n-  private ReadBufferManager() {\n-    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n-  }\n-\n-\n-  /*\n-   *\n-   *  AbfsInputStream-facing methods\n-   *\n-   */\n-\n+public interface ReadBufferManager {\n \n   /**\n-   * {@link AbfsInputStream} calls this method to queue read-aheads.\n-   *\n-   * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n-   * @param requestedOffset The offset in the file which shoukd be read\n-   * @param requestedLength The length to read\n+   * Queues a read-ahead request from {@link AbfsInputStream}\n+   * for a given offset in file and given length.\n+   * @param stream the input stream requesting the read-ahead\n+   * @param requestedOffset the offset in the remote file to start reading\n+   * @param requestedLength the number of bytes to read from file\n+   * @param tracingContext the tracing context for diagnostics\n    */\n-  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength,\n-                      TracingContext tracingContext) {\n-    if (LOGGER.isTraceEnabled()) {\n-      LOGGER.trace(\"Start Queueing readAhead for {} offset {} length {}\",\n-          stream.getPath(), requestedOffset, requestedLength);\n-    }\n-    ReadBuffer buffer;\n-    synchronized (this) {\n-      if (isAlreadyQueued(stream, requestedOffset)) {\n-        return; // already queued, do not queue again\n-      }\n-      if (freeList.isEmpty() && !tryEvict()) {\n-        return; // no buffers available, cannot queue anything\n-      }\n-\n-      buffer = new ReadBuffer();\n-      buffer.setStream(stream);\n-      buffer.setOffset(requestedOffset);\n-      buffer.setLength(0);\n-      buffer.setRequestedLength(requestedLength);\n-      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n-      buffer.setLatch(new CountDownLatch(1));\n-      buffer.setTracingContext(tracingContext);\n-\n-      Integer bufferIndex = freeList.pop();  // will return a value, since we have checked size > 0 already\n-\n-      buffer.setBuffer(buffers[bufferIndex]);\n-      buffer.setBufferindex(bufferIndex);\n-      readAheadQueue.add(buffer);\n-      notifyAll();\n-      if (LOGGER.isTraceEnabled()) {\n-        LOGGER.trace(\"Done q-ing readAhead for file {} offset {} buffer idx {}\",\n-            stream.getPath(), requestedOffset, buffer.getBufferindex());\n-      }\n-    }\n-  }\n+  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset,\n+      final int requestedLength, TracingContext tracingContext);\n \n+  /**\n+   * Gets a block of data from the prefetched data by ReadBufferManager.\n+   * {@link AbfsInputStream} calls this method to read data\n+   * @param stream the input stream requesting the block\n+   * @param position the position in the file to read from\n+   * @param length the number of bytes to read\n+   * @param buffer the buffer to store the read data\n+   * @return the number of bytes actually read\n+   * @throws IOException if an I/O error occurs\n+   */\n+  int getBlock(final AbfsInputStream stream,\n+      final long position,\n+      final int length,\n+      final byte[] buffer)\n+      throws IOException;\n \n   /**\n-   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n-   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n-   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n-   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n-   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do it's own\n-   * read to get the data faster (copmared to the read waiting in queue for an indeterminate amount of time).\n+   * {@link ReadBufferWorker} calls this to get the next buffer to read from read-ahead queue.\n+   * Requested read will be performed by background thread.\n    *\n-   * @param stream   the file to read bytes for\n-   * @param position the offset in the file to do a read for\n-   * @param length   the length to read\n-   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n-   * @return the number of bytes read\n+   * @return the next {@link ReadBuffer} to read\n+   * @throws InterruptedException if interrupted while waiting\n    */\n-  int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n-      throws IOException {\n-    // not synchronized, so have to be careful with locking\n-    if (LOGGER.isTraceEnabled()) {\n-      LOGGER.trace(\"getBlock for file {}  position {}  thread {}\",\n-          stream.getPath(), position, Thread.currentThread().getName());\n-    }\n-\n-    waitForProcess(stream, position);\n-\n-    int bytesRead = 0;\n-    synchronized (this) {\n-      bytesRead = getBlockFromCompletedQueue(stream, position, length, buffer);\n-    }\n-    if (bytesRead > 0) {\n-      if (LOGGER.isTraceEnabled()) {\n-        LOGGER.trace(\"Done read from Cache for {} position {} length {}\",\n-            stream.getPath(), position, bytesRead);\n-      }\n-      return bytesRead;\n-    }\n+  ReadBuffer getNextBlockToRead() throws InterruptedException;\n \n-    // otherwise, just say we got nothing - calling thread can do its own read\n-    return 0;\n-  }\n-\n-  /*\n-   *\n-   *  Internal methods\n-   *\n+  /**\n+   * Marks the specified buffer as done reading and updates its status.\n+   * Called by {@link ReadBufferWorker} after reading is complete.\n+   * @param buffer the buffer that was read by worker thread\n+   * @param result the status of the read operation\n+   * @param bytesActuallyRead the number of bytes actually read\n    */\n-\n-  private void waitForProcess(final AbfsInputStream stream, final long position) {\n-    ReadBuffer readBuf;\n-    synchronized (this) {\n-      clearFromReadAheadQueue(stream, position);\n-      readBuf = getFromList(inProgressList, stream, position);\n-    }\n-    if (readBuf != null) {         // if in in-progress queue, then block for it\n-      try {\n-        if (LOGGER.isTraceEnabled()) {\n-          LOGGER.trace(\"got a relevant read buffer for file {} offset {} buffer idx {}\",\n-              stream.getPath(), readBuf.getOffset(), readBuf.getBufferindex());\n-        }\n-        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n-        // Note on correctness: readBuf gets out of inProgressList only in 1 place: after worker thread\n-        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n-        // inProgressList. So this latch is safe to be outside the synchronized block.\n-        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n-        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n-        // then the latch cane be removed and replaced with wait/notify whenever inProgressList is touched.\n-      } catch (InterruptedException ex) {\n-        Thread.currentThread().interrupt();\n-      }\n-      if (LOGGER.isTraceEnabled()) {\n-        LOGGER.trace(\"latch done for file {} buffer idx {} length {}\",\n-            stream.getPath(), readBuf.getBufferindex(), readBuf.getLength());\n-      }\n-    }\n-  }\n+  void doneReading(final ReadBuffer buffer, final ReadBufferStatus result,\n+      final int bytesActuallyRead);\n \n   /**\n-   * If any buffer in the completedlist can be reclaimed then reclaim it and return the buffer to free list.\n-   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   * Purges all buffers associated with the calling {@link AbfsInputStream}.\n    *\n-   * @return whether the eviction succeeeded - i.e., were we able to free up one buffer\n+   * @param stream the input stream whose buffers should be purged\n    */\n-  private synchronized boolean tryEvict() {\n-    ReadBuffer nodeToEvict = null;\n-    if (completedReadList.size() <= 0) {\n-      return false;  // there are no evict-able buffers\n-    }\n-\n-    long currentTimeInMs = currentTimeMillis();\n-\n-    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n-    for (ReadBuffer buf : completedReadList) {\n-      if (buf.isFirstByteConsumed() && buf.isLastByteConsumed()) {\n-        nodeToEvict = buf;\n-        break;\n-      }\n-    }\n-    if (nodeToEvict != null) {\n-      return evict(nodeToEvict);\n-    }\n-\n-    // next, try buffers where any bytes have been consumed (may be a bad idea? have to experiment and see)\n-    for (ReadBuffer buf : completedReadList) {\n-      if (buf.isAnyByteConsumed()) {\n-        nodeToEvict = buf;\n-        break;\n-      }\n-    }\n+  void purgeBuffersForStream(AbfsInputStream stream);\n \n-    if (nodeToEvict != null) {\n-      return evict(nodeToEvict);\n-    }\n-\n-    // next, try any old nodes that have not been consumed\n-    // Failed read buffers (with buffer index=-1) that are older than\n-    // thresholdAge should be cleaned up, but at the same time should not\n-    // report successful eviction.\n-    // Queue logic expects that a buffer is freed up for read ahead when\n-    // eviction is successful, whereas a failed ReadBuffer would have released\n-    // its buffer when its status was set to READ_FAILED.\n-    long earliestBirthday = Long.MAX_VALUE;\n-    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n-    for (ReadBuffer buf : completedReadList) {\n-      if ((buf.getBufferindex() != -1)\n-          && (buf.getTimeStamp() < earliestBirthday)) {\n-        nodeToEvict = buf;\n-        earliestBirthday = buf.getTimeStamp();\n-      } else if ((buf.getBufferindex() == -1)\n-          && (currentTimeInMs - buf.getTimeStamp()) > thresholdAgeMilliseconds) {\n-        oldFailedBuffers.add(buf);\n-      }\n-    }\n-\n-    for (ReadBuffer buf : oldFailedBuffers) {\n-      evict(buf);\n-    }\n-\n-    if ((currentTimeInMs - earliestBirthday > thresholdAgeMilliseconds) && (nodeToEvict != null)) {\n-      return evict(nodeToEvict);\n-    }\n-\n-    LOGGER.trace(\"No buffer eligible for eviction\");\n-    // nothing can be evicted\n-    return false;\n-  }\n-\n-  private boolean evict(final ReadBuffer buf) {\n-    // As failed ReadBuffers (bufferIndx = -1) are saved in completedReadList,\n-    // avoid adding it to freeList.\n-    if (buf.getBufferindex() != -1) {\n-      freeList.push(buf.getBufferindex());\n-    }\n-\n-    completedReadList.remove(buf);\n-    buf.setTracingContext(null);\n-    if (LOGGER.isTraceEnabled()) {\n-      LOGGER.trace(\"Evicting buffer idx {}; was used for file {} offset {} length {}\",\n-          buf.getBufferindex(), buf.getStream().getPath(), buf.getOffset(), buf.getLength());\n-    }\n-    return true;\n-  }\n-\n-  private boolean isAlreadyQueued(final AbfsInputStream stream, final long requestedOffset) {\n-    // returns true if any part of the buffer is already queued\n-    return (isInList(readAheadQueue, stream, requestedOffset)\n-        || isInList(inProgressList, stream, requestedOffset)\n-        || isInList(completedReadList, stream, requestedOffset));\n-  }\n-\n-  private boolean isInList(final Collection<ReadBuffer> list, final AbfsInputStream stream, final long requestedOffset) {\n-    return (getFromList(list, stream, requestedOffset) != null);\n-  }\n-\n-  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInputStream stream, final long requestedOffset) {\n-    for (ReadBuffer buffer : list) {\n-      if (buffer.getStream() == stream) {\n-        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n-            && requestedOffset >= buffer.getOffset()\n-            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n-          return buffer;\n-        } else if (requestedOffset >= buffer.getOffset()\n-            && requestedOffset < buffer.getOffset() + buffer.getRequestedLength()) {\n-          return buffer;\n-        }\n-      }\n-    }\n-    return null;\n-  }\n+  // Following Methods are for testing purposes only and should not be used in production code.\n \n   /**\n-   * Returns buffers that failed or passed from completed queue.\n-   * @param stream\n-   * @param requestedOffset\n-   * @return\n+   * Resets the read buffer manager for testing purposes.\n    */\n-  private ReadBuffer getBufferFromCompletedQueue(final AbfsInputStream stream, final long requestedOffset) {\n-    for (ReadBuffer buffer : completedReadList) {\n-      // Buffer is returned if the requestedOffset is at or above buffer's\n-      // offset but less than buffer's length or the actual requestedLength\n-      if ((buffer.getStream() == stream)\n-          && (requestedOffset >= buffer.getOffset())\n-          && ((requestedOffset < buffer.getOffset() + buffer.getLength())\n-          || (requestedOffset < buffer.getOffset() + buffer.getRequestedLength()))) {\n-          return buffer;\n-        }\n-      }\n-\n-    return null;\n-  }\n-\n-  private void clearFromReadAheadQueue(final AbfsInputStream stream, final long requestedOffset) {\n-    ReadBuffer buffer = getFromList(readAheadQueue, stream, requestedOffset);\n-    if (buffer != null) {\n-      readAheadQueue.remove(buffer);\n-      notifyAll();   // lock is held in calling method\n-      freeList.push(buffer.getBufferindex());\n-    }\n-  }\n-\n-  private int getBlockFromCompletedQueue(final AbfsInputStream stream, final long position, final int length,\n-                                         final byte[] buffer) throws IOException {\n-    ReadBuffer buf = getBufferFromCompletedQueue(stream, position);\n-\n-    if (buf == null) {\n-      return 0;\n-    }\n-\n-    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n-      // To prevent new read requests to fail due to old read-ahead attempts,\n-      // return exception only from buffers that failed within last thresholdAgeMilliseconds\n-      if ((currentTimeMillis() - (buf.getTimeStamp()) < thresholdAgeMilliseconds)) {\n-        throw buf.getErrException();\n-      } else {\n-        return 0;\n-      }\n-    }\n-\n-    if ((buf.getStatus() != ReadBufferStatus.AVAILABLE)\n-        || (position >= buf.getOffset() + buf.getLength())) {\n-      return 0;\n-    }\n-\n-    int cursor = (int) (position - buf.getOffset());\n-    int availableLengthInBuffer = buf.getLength() - cursor;\n-    int lengthToCopy = Math.min(length, availableLengthInBuffer);\n-    System.arraycopy(buf.getBuffer(), cursor, buffer, 0, lengthToCopy);\n-    if (cursor == 0) {\n-      buf.setFirstByteConsumed(true);\n-    }\n-    if (cursor + lengthToCopy == buf.getLength()) {\n-      buf.setLastByteConsumed(true);\n-    }\n-    buf.setAnyByteConsumed(true);\n-    return lengthToCopy;\n-  }\n+  @VisibleForTesting\n+  void testResetReadBufferManager();\n\nReview Comment:\n   It was there already in RBMV1. It makes sense to keep it this way because this should never be called from production flow and its critical to test some really important scenarios so we can't void it.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 implements ReadBufferManager {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManagerV1.class);\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+\n+  private static int blockSize = 4 * ONE_MB;\n+  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n+  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+\n+  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n+  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n+  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n+  private static ReadBufferManagerV1 bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+\n+  static ReadBufferManagerV1 getBufferManager() {\n\nReview Comment:\n   Makes sense. Will take it\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T05:53:34.366+0000", "updated": "2025-07-21T05:53:34.366+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008728", "id": "18008728", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2219354195\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferWorker.java:\n##########\n@@ -51,7 +51,7 @@ public void run() {\n     } catch (InterruptedException ex) {\n       Thread.currentThread().interrupt();\n     }\n-    ReadBufferManager bufferManager = ReadBufferManager.getBufferManager();\n+    ReadBufferManager bufferManager = ReadBufferManagerV1.getBufferManager();\n\nReview Comment:\n   No, this will be change in next PR when ReadBufferManagerV2 will be introduced.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T14:14:31.904+0000", "updated": "2025-07-21T14:14:31.904+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008778", "id": "18008778", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2219957932\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java:\n##########\n@@ -26,7 +26,7 @@\n \n import static org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus.READ_FAILED;\n \n-class ReadBuffer {\n+public class ReadBuffer {\n\nReview Comment:\n   reverted\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T18:31:58.867+0000", "updated": "2025-07-21T18:31:58.867+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008780", "id": "18008780", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2219958922\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 implements ReadBufferManager {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManagerV1.class);\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+\n+  private static int blockSize = 4 * ONE_MB;\n+  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n+  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+\n+  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n+  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n+  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n+  private static ReadBufferManagerV1 bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      blockSize = readAheadBlockSize;\n+    }\n+  }\n+\n+  private void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[blockSize];  // same buffers are reused. The byte array never goes back to GC\n+      freeList.add(i);\n+    }\n+    for (int i = 0; i < NUM_THREADS; i++) {\n+      Thread t = new Thread(new ReadBufferWorker(i));\n+      t.setDaemon(true);\n+      threads[i] = t;\n+      t.setName(\"ABFS-prefetch-\" + i);\n+      t.start();\n+    }\n+    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+  }\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method to queue read-aheads.\n+   *\n+   * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n+   * @param requestedOffset The offset in the file which shoukd be read\n+   * @param requestedLength The length to read\n+   */\n+  @Override\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength,\n+      TracingContext tracingContext) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"Start Queueing readAhead for {} offset {} length {}\",\n+          stream.getPath(), requestedOffset, requestedLength);\n+    }\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream, requestedOffset)) {\n+        return; // already queued, do not queue again\n+      }\n+      if (freeList.isEmpty() && !tryEvict()) {\n+        return; // no buffers available, cannot queue anything\n+      }\n+\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream);\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      Integer bufferIndex = freeList.pop();  // will return a value, since we have checked size > 0 already\n+\n+      buffer.setBuffer(buffers[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      readAheadQueue.add(buffer);\n+      notifyAll();\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"Done q-ing readAhead for file {} offset {} buffer idx {}\",\n+            stream.getPath(), requestedOffset, buffer.getBufferindex());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do it's own\n+   * read to get the data faster (copmared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream   the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n\nReview Comment:\n   Added in base class\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T18:32:33.923+0000", "updated": "2025-07-21T18:32:33.923+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008781", "id": "18008781", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2219959703\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 implements ReadBufferManager {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManagerV1.class);\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+\n+  private static int blockSize = 4 * ONE_MB;\n+  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n+  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+\n+  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n+  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n+  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n+  private static ReadBufferManagerV1 bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      blockSize = readAheadBlockSize;\n+    }\n+  }\n+\n+  private void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[blockSize];  // same buffers are reused. The byte array never goes back to GC\n+      freeList.add(i);\n+    }\n+    for (int i = 0; i < NUM_THREADS; i++) {\n+      Thread t = new Thread(new ReadBufferWorker(i));\n+      t.setDaemon(true);\n+      threads[i] = t;\n+      t.setName(\"ABFS-prefetch-\" + i);\n+      t.start();\n+    }\n+    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+  }\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method to queue read-aheads.\n+   *\n+   * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n+   * @param requestedOffset The offset in the file which shoukd be read\n+   * @param requestedLength The length to read\n+   */\n+  @Override\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength,\n+      TracingContext tracingContext) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"Start Queueing readAhead for {} offset {} length {}\",\n+          stream.getPath(), requestedOffset, requestedLength);\n+    }\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream, requestedOffset)) {\n+        return; // already queued, do not queue again\n+      }\n+      if (freeList.isEmpty() && !tryEvict()) {\n+        return; // no buffers available, cannot queue anything\n+      }\n+\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream);\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      Integer bufferIndex = freeList.pop();  // will return a value, since we have checked size > 0 already\n+\n+      buffer.setBuffer(buffers[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      readAheadQueue.add(buffer);\n+      notifyAll();\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"Done q-ing readAhead for file {} offset {} buffer idx {}\",\n+            stream.getPath(), requestedOffset, buffer.getBufferindex());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do it's own\n+   * read to get the data faster (copmared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream   the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n+   */\n+  @Override\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"getBlock for file {}  position {}  thread {}\",\n+          stream.getPath(), position, Thread.currentThread().getName());\n+    }\n+\n+    waitForProcess(stream, position);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(stream, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"Done read from Cache for {} position {} length {}\",\n+            stream.getPath(), position, bytesRead);\n+      }\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n+    return 0;\n+  }\n+\n+  /**\n+   * ReadBufferWorker thread calls this to get the next buffer that it should work on.\n+   *\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n+   */\n+  @Override\n+  public ReadBuffer getNextBlockToRead() throws InterruptedException {\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      //buffer = readAheadQueue.take();  // blocking method\n+      while (readAheadQueue.size() == 0) {\n+        wait();\n+      }\n+      buffer = readAheadQueue.remove();\n+      notifyAll();\n+      if (buffer == null) {\n+        return null;            // should never happen\n+      }\n+      buffer.setStatus(ReadBufferStatus.READING_IN_PROGRESS);\n+      inProgressList.add(buffer);\n+    }\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"ReadBufferWorker picked file {} for offset {}\",\n+          buffer.getStream().getPath(), buffer.getOffset());\n+    }\n+    return buffer;\n+  }\n+\n+  /**\n+   * ReadBufferWorker thread calls this method to post completion.\n+   *\n+   * @param buffer            the buffer whose read was completed\n+   * @param result            the {@link ReadBufferStatus} after the read operation in the worker thread\n+   * @param bytesActuallyRead the number of bytes that the worker thread was actually able to read\n+   */\n+  @Override\n+  public void doneReading(final ReadBuffer buffer, final ReadBufferStatus result, final int bytesActuallyRead) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"ReadBufferWorker completed read file {} for offset {} outcome {} bytes {}\",\n+          buffer.getStream().getPath(),  buffer.getOffset(), result, bytesActuallyRead);\n+    }\n+    synchronized (this) {\n+      // If this buffer has already been purged during\n+      // close of InputStream then we don't update the lists.\n+      if (inProgressList.contains(buffer)) {\n+        inProgressList.remove(buffer);\n+        if (result == ReadBufferStatus.AVAILABLE && bytesActuallyRead > 0) {\n+          buffer.setStatus(ReadBufferStatus.AVAILABLE);\n+          buffer.setLength(bytesActuallyRead);\n+        } else {\n+          freeList.push(buffer.getBufferindex());\n+          // buffer will be deleted as per the eviction policy.\n+        }\n+        // completed list also contains FAILED read buffers\n+        // for sending exception message to clients.\n+        buffer.setStatus(result);\n+        buffer.setTimeStamp(currentTimeMillis());\n+        completedReadList.add(buffer);\n+      }\n+    }\n+\n+    //outside the synchronized, since anyone receiving a wake-up from the latch must see safe-published results\n+    buffer.getLatch().countDown(); // wake up waiting threads (if any)\n+  }\n+\n+  /**\n+   * Purging the buffers associated with an {@link AbfsInputStream}\n+   * from {@link ReadBufferManagerV1} when stream is closed.\n+   * @param stream input stream.\n+   */\n+  @Override\n+  public synchronized void purgeBuffersForStream(AbfsInputStream stream) {\n+    LOGGER.debug(\"Purging stale buffers for AbfsInputStream {} \", stream);\n+    readAheadQueue.removeIf(readBuffer -> readBuffer.getStream() == stream);\n+    purgeList(stream, completedReadList);\n+  }\n+\n+  private void waitForProcess(final AbfsInputStream stream, final long position) {\n+    ReadBuffer readBuf;\n+    synchronized (this) {\n+      clearFromReadAheadQueue(stream, position);\n+      readBuf = getFromList(inProgressList, stream, position);\n+    }\n+    if (readBuf != null) {         // if in in-progress queue, then block for it\n+      try {\n+        if (LOGGER.isTraceEnabled()) {\n+          LOGGER.trace(\"got a relevant read buffer for file {} offset {} buffer idx {}\",\n+              stream.getPath(), readBuf.getOffset(), readBuf.getBufferindex());\n+        }\n+        readBuf.getLatch().await();  // blocking wait on the caller stream's thread\n+        // Note on correctness: readBuf gets out of inProgressList only in 1 place: after worker thread\n+        // is done processing it (in doneReading). There, the latch is set after removing the buffer from\n+        // inProgressList. So this latch is safe to be outside the synchronized block.\n+        // Putting it in synchronized would result in a deadlock, since this thread would be holding the lock\n+        // while waiting, so no one will be able to  change any state. If this becomes more complex in the future,\n+        // then the latch cane be removed and replaced with wait/notify whenever inProgressList is touched.\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+      }\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"latch done for file {} buffer idx {} length {}\",\n+            stream.getPath(), readBuf.getBufferindex(), readBuf.getLength());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * If any buffer in the completedlist can be reclaimed then reclaim it and return the buffer to free list.\n+   * The objective is to find just one buffer - there is no advantage to evicting more than one.\n+   *\n+   * @return whether the eviction succeeeded - i.e., were we able to free up one buffer\n+   */\n+  private synchronized boolean tryEvict() {\n+    ReadBuffer nodeToEvict = null;\n+    if (completedReadList.size() <= 0) {\n+      return false;  // there are no evict-able buffers\n+    }\n+\n+    long currentTimeInMs = currentTimeMillis();\n+\n+    // first, try buffers where all bytes have been consumed (approximated as first and last bytes consumed)\n+    for (ReadBuffer buf : completedReadList) {\n+      if (buf.isFirstByteConsumed() && buf.isLastByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+    if (nodeToEvict != null) {\n+      return evict(nodeToEvict);\n+    }\n+\n+    // next, try buffers where any bytes have been consumed (may be a bad idea? have to experiment and see)\n+    for (ReadBuffer buf : completedReadList) {\n+      if (buf.isAnyByteConsumed()) {\n+        nodeToEvict = buf;\n+        break;\n+      }\n+    }\n+\n+    if (nodeToEvict != null) {\n+      return evict(nodeToEvict);\n+    }\n+\n+    // next, try any old nodes that have not been consumed\n+    // Failed read buffers (with buffer index=-1) that are older than\n+    // thresholdAge should be cleaned up, but at the same time should not\n+    // report successful eviction.\n+    // Queue logic expects that a buffer is freed up for read ahead when\n+    // eviction is successful, whereas a failed ReadBuffer would have released\n+    // its buffer when its status was set to READ_FAILED.\n+    long earliestBirthday = Long.MAX_VALUE;\n+    ArrayList<ReadBuffer> oldFailedBuffers = new ArrayList<>();\n+    for (ReadBuffer buf : completedReadList) {\n+      if ((buf.getBufferindex() != -1)\n+          && (buf.getTimeStamp() < earliestBirthday)) {\n+        nodeToEvict = buf;\n+        earliestBirthday = buf.getTimeStamp();\n+      } else if ((buf.getBufferindex() == -1)\n+          && (currentTimeInMs - buf.getTimeStamp()) > thresholdAgeMilliseconds) {\n+        oldFailedBuffers.add(buf);\n+      }\n+    }\n+\n+    for (ReadBuffer buf : oldFailedBuffers) {\n+      evict(buf);\n+    }\n+\n+    if ((currentTimeInMs - earliestBirthday > thresholdAgeMilliseconds) && (nodeToEvict != null)) {\n+      return evict(nodeToEvict);\n+    }\n+\n+    LOGGER.trace(\"No buffer eligible for eviction\");\n+    // nothing can be evicted\n+    return false;\n+  }\n+\n+  private boolean evict(final ReadBuffer buf) {\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in completedReadList,\n+    // avoid adding it to freeList.\n+    if (buf.getBufferindex() != -1) {\n+      freeList.push(buf.getBufferindex());\n+    }\n+\n+    completedReadList.remove(buf);\n+    buf.setTracingContext(null);\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"Evicting buffer idx {}; was used for file {} offset {} length {}\",\n+          buf.getBufferindex(), buf.getStream().getPath(), buf.getOffset(), buf.getLength());\n+    }\n+    return true;\n+  }\n+\n+  private boolean isAlreadyQueued(final AbfsInputStream stream, final long requestedOffset) {\n+    // returns true if any part of the buffer is already queued\n+    return (isInList(readAheadQueue, stream, requestedOffset)\n+        || isInList(inProgressList, stream, requestedOffset)\n+        || isInList(completedReadList, stream, requestedOffset));\n+  }\n+\n+  private boolean isInList(final Collection<ReadBuffer> list, final AbfsInputStream stream, final long requestedOffset) {\n+    return (getFromList(list, stream, requestedOffset) != null);\n+  }\n+\n+  private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInputStream stream, final long requestedOffset) {\n+    for (ReadBuffer buffer : list) {\n+      if (buffer.getStream() == stream) {\n+        if (buffer.getStatus() == ReadBufferStatus.AVAILABLE\n+            && requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getLength()) {\n+          return buffer;\n+        } else if (requestedOffset >= buffer.getOffset()\n+            && requestedOffset < buffer.getOffset() + buffer.getRequestedLength()) {\n+          return buffer;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  /**\n+   * Returns buffers that failed or passed from completed queue.\n+   * @param stream\n+   * @param requestedOffset\n+   * @return\n\nReview Comment:\n   Added in base class\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java:\n##########\n@@ -921,10 +921,14 @@ private AzureBlobFileSystem createTestFile(Path testFilePath, long testFileSize,\n   }\n \n   private void resetReadBufferManager(int bufferSize, int threshold) {\n-    ReadBufferManager.getBufferManager()\n+    getBufferManager()\n         .testResetReadBufferManager(bufferSize, threshold);\n     // Trigger GC as aggressive recreation of ReadBufferManager buffers\n     // by successive tests can lead to OOM based on the dev VM/machine capacity.\n     System.gc();\n   }\n+\n+  private ReadBufferManager getBufferManager() {\n+    return ReadBufferManagerV1.getBufferManager();\n+  }\n }\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T18:32:48.966+0000", "updated": "2025-07-21T18:32:48.966+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008782", "id": "18008782", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2219960453\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 implements ReadBufferManager {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManagerV1.class);\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+\n+  private static int blockSize = 4 * ONE_MB;\n+  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n+  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+\n+  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n+  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n+  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n+  private static ReadBufferManagerV1 bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      blockSize = readAheadBlockSize;\n+    }\n+  }\n+\n+  private void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[blockSize];  // same buffers are reused. The byte array never goes back to GC\n+      freeList.add(i);\n+    }\n+    for (int i = 0; i < NUM_THREADS; i++) {\n+      Thread t = new Thread(new ReadBufferWorker(i));\n+      t.setDaemon(true);\n+      threads[i] = t;\n+      t.setName(\"ABFS-prefetch-\" + i);\n+      t.start();\n+    }\n+    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+  }\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method to queue read-aheads.\n+   *\n+   * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n+   * @param requestedOffset The offset in the file which shoukd be read\n+   * @param requestedLength The length to read\n+   */\n+  @Override\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength,\n+      TracingContext tracingContext) {\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"Start Queueing readAhead for {} offset {} length {}\",\n+          stream.getPath(), requestedOffset, requestedLength);\n+    }\n+    ReadBuffer buffer;\n+    synchronized (this) {\n+      if (isAlreadyQueued(stream, requestedOffset)) {\n+        return; // already queued, do not queue again\n+      }\n+      if (freeList.isEmpty() && !tryEvict()) {\n+        return; // no buffers available, cannot queue anything\n+      }\n+\n+      buffer = new ReadBuffer();\n+      buffer.setStream(stream);\n+      buffer.setOffset(requestedOffset);\n+      buffer.setLength(0);\n+      buffer.setRequestedLength(requestedLength);\n+      buffer.setStatus(ReadBufferStatus.NOT_AVAILABLE);\n+      buffer.setLatch(new CountDownLatch(1));\n+      buffer.setTracingContext(tracingContext);\n+\n+      Integer bufferIndex = freeList.pop();  // will return a value, since we have checked size > 0 already\n+\n+      buffer.setBuffer(buffers[bufferIndex]);\n+      buffer.setBufferindex(bufferIndex);\n+      readAheadQueue.add(buffer);\n+      notifyAll();\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"Done q-ing readAhead for file {} offset {} buffer idx {}\",\n+            stream.getPath(), requestedOffset, buffer.getBufferindex());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method read any bytes already available in a buffer (thereby saving a\n+   * remote read). This returns the bytes if the data already exists in buffer. If there is a buffer that is reading\n+   * the requested offset, then this method blocks until that read completes. If the data is queued in a read-ahead\n+   * but not picked up by a worker thread yet, then it cancels that read-ahead and reports cache miss. This is because\n+   * depending on worker thread availability, the read-ahead may take a while - the calling thread can do it's own\n+   * read to get the data faster (copmared to the read waiting in queue for an indeterminate amount of time).\n+   *\n+   * @param stream   the file to read bytes for\n+   * @param position the offset in the file to do a read for\n+   * @param length   the length to read\n+   * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n+   * @return the number of bytes read\n+   */\n+  @Override\n+  public int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws IOException {\n+    // not synchronized, so have to be careful with locking\n+    if (LOGGER.isTraceEnabled()) {\n+      LOGGER.trace(\"getBlock for file {}  position {}  thread {}\",\n+          stream.getPath(), position, Thread.currentThread().getName());\n+    }\n+\n+    waitForProcess(stream, position);\n+\n+    int bytesRead = 0;\n+    synchronized (this) {\n+      bytesRead = getBlockFromCompletedQueue(stream, position, length, buffer);\n+    }\n+    if (bytesRead > 0) {\n+      if (LOGGER.isTraceEnabled()) {\n+        LOGGER.trace(\"Done read from Cache for {} position {} length {}\",\n+            stream.getPath(), position, bytesRead);\n+      }\n+      return bytesRead;\n+    }\n+\n+    // otherwise, just say we got nothing - calling thread can do its own read\n+    return 0;\n+  }\n+\n+  /**\n+   * ReadBufferWorker thread calls this to get the next buffer that it should work on.\n+   *\n+   * @return {@link ReadBuffer}\n+   * @throws InterruptedException if thread is interrupted\n+   */\n+  @Override\n+  public ReadBuffer getNextBlockToRead() throws InterruptedException {\n+    ReadBuffer buffer = null;\n+    synchronized (this) {\n+      //buffer = readAheadQueue.take();  // blocking method\n\nReview Comment:\n   Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,637 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 implements ReadBufferManager {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManagerV1.class);\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+\n+  private static int blockSize = 4 * ONE_MB;\n+  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n+  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+\n+  private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n+  private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n+  private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n+  private static ReadBufferManagerV1 bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n+\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      blockSize = readAheadBlockSize;\n+    }\n+  }\n+\n+  private void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[blockSize];  // same buffers are reused. The byte array never goes back to GC\n+      freeList.add(i);\n+    }\n+    for (int i = 0; i < NUM_THREADS; i++) {\n+      Thread t = new Thread(new ReadBufferWorker(i));\n+      t.setDaemon(true);\n+      threads[i] = t;\n+      t.setName(\"ABFS-prefetch-\" + i);\n+      t.start();\n+    }\n+    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+  }\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * {@link AbfsInputStream} calls this method to queue read-aheads.\n+   *\n+   * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n+   * @param requestedOffset The offset in the file which shoukd be read\n+   * @param requestedLength The length to read\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T18:33:08.975+0000", "updated": "2025-07-21T18:33:08.975+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008792", "id": "18008792", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3098584653\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 21s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  33m  1s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 27s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 29s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 23s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   0m 49s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  26m 29s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 21s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 22s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 20s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 20s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 14s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/4/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 3 unchanged - 0 fixed = 13 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 18s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 19s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 47s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/4/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  26m 30s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 20s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 26s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   |  96m  6s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.bufferManager from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.resetBufferManager()  At ReadBufferManagerV1.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.resetBufferManager()  At ReadBufferManagerV1.java:[line 708] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.blockSize from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.setReadAheadBlockSize(int)  At ReadBufferManagerV1.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.setReadAheadBlockSize(int)  At ReadBufferManagerV1.java:[line 595] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.thresholdAgeMilliseconds from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.setThresholdAgeMilliseconds(int)  At ReadBufferManagerV1.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.setThresholdAgeMilliseconds(int)  At ReadBufferManagerV1.java:[line 577] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.bufferManager from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.resetBufferManager()  At ReadBufferManagerV2.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.resetBufferManager()  At ReadBufferManagerV2.java:[line 225] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/4/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux e39ee6a9f073 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 643d30c0f6276f0475ea3d02c43c82fe32e7d149 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/4/testReport/ |\r\n   | Max. process+thread count | 545 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/4/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T20:22:23.675+0000", "updated": "2025-07-21T20:22:23.675+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008806", "id": "18008806", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3098852733\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 51s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 46s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 33s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  40m 54s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 17s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 28s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 33s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 33s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 29s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 29s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 3 unchanged - 0 fixed = 13 total (was 3)  |\r\n   | -1 :x: |  mvnsite  |   0m 29s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 27s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  43m 45s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 33s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 139m 51s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 6addad023f56 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 99fb6a9675947f90328fc0094768bb2242ed41f5 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/testReport/ |\r\n   | Max. process+thread count | 524 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/3/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T20:57:12.764+0000", "updated": "2025-07-21T20:57:12.764+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008808", "id": "18008808", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3098863332\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 24s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m  2s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 37s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 27s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  35m 48s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 22s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/2/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 3 unchanged - 0 fixed = 13 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 30s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   1m 10s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/2/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  38m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  7s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m 35s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.bufferManager from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.resetBufferManager()  At ReadBufferManagerV1.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.resetBufferManager()  At ReadBufferManagerV1.java:[line 708] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.blockSize from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.setReadAheadBlockSize(int)  At ReadBufferManagerV1.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.setReadAheadBlockSize(int)  At ReadBufferManagerV1.java:[line 595] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.thresholdAgeMilliseconds from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.setThresholdAgeMilliseconds(int)  At ReadBufferManagerV1.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.setThresholdAgeMilliseconds(int)  At ReadBufferManagerV1.java:[line 577] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.bufferManager from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.resetBufferManager()  At ReadBufferManagerV2.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.resetBufferManager()  At ReadBufferManagerV2.java:[line 225] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/2/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux b0d289daaa7f 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 643d30c0f6276f0475ea3d02c43c82fe32e7d149 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/2/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/2/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-21T20:58:23.741+0000", "updated": "2025-07-21T20:58:23.741+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008885", "id": "18008885", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2221149264\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -259,10 +259,15 @@ public final class ConfigurationKeys {\n   public static final String AZURE_KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT = \"fs.azure.shellkeyprovider.script\";\n \n   /**\n-   * Enable or disable readahead buffer in AbfsInputStream.\n+   * Enable or disable readahead V1 in AbfsInputStream.\n    * Value: {@value}.\n    */\n   public static final String FS_AZURE_ENABLE_READAHEAD = \"fs.azure.enable.readahead\";\n+  /**\n+   * Enable or disable readahead V2 in AbfsInputStream. This will work independent of V1.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_ENABLE_READAHEAD_V2 = \"fs.azure.enable.readahead.v2\";\n\nReview Comment:\n   Can you add this precedence thing in the comments just for better visibility?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-22T05:01:00.487+0000", "updated": "2025-07-22T05:01:00.487+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008892", "id": "18008892", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2221275241\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -259,10 +259,15 @@ public final class ConfigurationKeys {\n   public static final String AZURE_KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT = \"fs.azure.shellkeyprovider.script\";\n \n   /**\n-   * Enable or disable readahead buffer in AbfsInputStream.\n+   * Enable or disable readahead V1 in AbfsInputStream.\n    * Value: {@value}.\n    */\n   public static final String FS_AZURE_ENABLE_READAHEAD = \"fs.azure.enable.readahead\";\n+  /**\n+   * Enable or disable readahead V2 in AbfsInputStream. This will work independent of V1.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_ENABLE_READAHEAD_V2 = \"fs.azure.enable.readahead.v2\";\n\nReview Comment:\n   Already Added in AbfsInputStream where this precedence is actually taken into account.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-22T05:45:49.338+0000", "updated": "2025-07-22T05:45:49.338+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18008921", "id": "18008921", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3101568175\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 53s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  47m  0s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 37s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  42m  0s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 29s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 33s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 33s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 29s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 29s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 20s |  |  the patch passed  |\r\n   | -1 :x: |  mvnsite  |   0m 30s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 29s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  44m  6s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 34s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 142m 18s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux a64c78f09725 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 0bfbfb90002cf726dfb0b3ca2461099a15aac13f |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/testReport/ |\r\n   | Max. process+thread count | 524 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/5/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-22T08:11:30.226+0000", "updated": "2025-07-22T08:11:30.226+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009000", "id": "18009000", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3102703577\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 54s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 35s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 45s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 13s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  35m 35s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 22s |  |  the patch passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   1m 12s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/6/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 13s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 124m 41s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  Inconsistent synchronization of org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.thresholdAgeMilliseconds; locked 60% of time  Unsynchronized access at ReadBufferManagerV1.java:60% of time  Unsynchronized access at ReadBufferManagerV1.java:[line 576] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.bufferManager from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.resetBufferManager()  At ReadBufferManagerV1.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.resetBufferManager()  At ReadBufferManagerV1.java:[line 705] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.blockSize from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.setReadAheadBlockSize(int)  At ReadBufferManagerV1.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV1.setReadAheadBlockSize(int)  At ReadBufferManagerV1.java:[line 593] |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.bufferManager from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.resetBufferManager()  At ReadBufferManagerV2.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManagerV2.resetBufferManager()  At ReadBufferManagerV2.java:[line 222] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/6/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux f1ceb569da5e 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 3dda35dc8c92300e247d6f299369e832f079e27c |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/6/testReport/ |\r\n   | Max. process+thread count | 554 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/6/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-22T13:24:23.007+0000", "updated": "2025-07-22T13:24:23.007+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009031", "id": "18009031", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3103535760\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 14s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 41s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 11s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 14s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  35m 35s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 31s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/7/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 27 new + 3 unchanged - 0 fixed = 30 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   1m 13s | [/new-spotbugs-hadoop-tools_hadoop-azure.html](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/7/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html) |  hadoop-tools/hadoop-azure generated 4 new + 0 unchanged - 0 fixed = 4 total (was 0)  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 11s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 39s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 125m 58s |  |  |\r\n   \r\n   \r\n   | Reason | Tests |\r\n   |-------:|:------|\r\n   | SpotBugs | module:hadoop-tools/hadoop-azure |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBufferManager.blockSize should be package protected  At ReadBufferManager.java: At ReadBufferManager.java:[line 48] |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBufferManager.bufferManager should be package protected  In ReadBufferManager.java: In ReadBufferManager.java |\r\n   |  |  org.apache.hadoop.fs.azurebfs.services.ReadBufferManager.thresholdAgeMilliseconds should be package protected  In ReadBufferManager.java: In ReadBufferManager.java |\r\n   |  |  Write to static field org.apache.hadoop.fs.azurebfs.services.ReadBufferManager.blockSize from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManager.setReadAheadBlockSize(int)  At ReadBufferManager.java:from instance method org.apache.hadoop.fs.azurebfs.services.ReadBufferManager.setReadAheadBlockSize(int)  At ReadBufferManager.java:[line 197] |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/7/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 0ffedcfe8c51 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 2019935b271ac00c3588026d2597518f769652c7 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/7/testReport/ |\r\n   | Max. process+thread count | 559 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/7/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-22T15:48:57.097+0000", "updated": "2025-07-22T15:48:57.097+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009087", "id": "18009087", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3104700177\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 34s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m  3s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 26s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  35m 48s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 29s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 29s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 23s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/8/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 27 new + 3 unchanged - 0 fixed = 30 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 14s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  8s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 40s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 126m  7s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/8/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 876cdedb2eb7 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 5b93b325a2e5dd0e1a27d5be95990e785864c41c |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/8/testReport/ |\r\n   | Max. process+thread count | 707 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/8/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-22T20:16:12.738+0000", "updated": "2025-07-22T20:16:12.738+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009144", "id": "18009144", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2224523780\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java:\n##########\n@@ -15,636 +15,288 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.hadoop.fs.azurebfs.services;\n \n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n+package org.apache.hadoop.fs.azurebfs.services;\n \n import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Queue;\n import java.util.Stack;\n-import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n \n /**\n- * The Read Buffer Manager for Rest AbfsClient.\n+ * Interface for managing read buffers for Azure Blob File System input streams.\n\nReview Comment:\n   Can change it to abstract class\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T06:23:07.808+0000", "updated": "2025-07-23T06:23:07.808+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009146", "id": "18009146", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "manika137 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2224553438\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,612 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.concurrent.CountDownLatch;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 extends ReadBufferManager {\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000;\n+\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;\n+  private static  ReadBufferManagerV1 bufferManager;\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * Sets the read buffer manager configurations.\n+   * @param readAheadBlockSize the size of the read-ahead block in bytes\n+   */\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      setReadAheadBlockSize(readAheadBlockSize);\n+      setThresholdAgeMilliseconds(DEFAULT_THRESHOLD_AGE_MILLISECONDS);\n+    }\n+  }\n+\n+  /**\n+   * Returns the singleton instance of ReadBufferManagerV1.\n+   * @return the singleton instance of ReadBufferManagerV1\n+   */\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  /**\n+   * {@inheritDoc}\n+   */\n+  @Override\n+  void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[getReadAheadBlockSize()];  // same buffers are reused. The byte array never goes back to GC\n+      getFreeList().add(i);\n+    }\n+    for (int i = 0; i < NUM_THREADS; i++) {\n+      Thread t = new Thread(new ReadBufferWorker(i, this));\n+      t.setDaemon(true);\n+      threads[i] = t;\n+      t.setName(\"ABFS-prefetch-\" + i);\n+      t.start();\n+    }\n+    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+  }\n+\n+  /**\n+   * {@inheritDoc}\n+   */\n+  @Override\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength,\n+      TracingContext tracingContext) {\n+    if (LOGGER.isTraceEnabled()) {\n\nReview Comment:\n   does anything happen if we skip the if condition and use LOG.trace() directly?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T06:36:39.291+0000", "updated": "2025-07-23T06:36:39.291+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009179", "id": "18009179", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3106485966\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 52s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  45m 48s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 37s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 34s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 34s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 28s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 28s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/9/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 9 new + 3 unchanged - 0 fixed = 12 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 27s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 29s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   2m 45s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 143m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/9/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 7c78ece5f186 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 7000ead0e55844a90c845d1e7899f6e267b401bf |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/9/testReport/ |\r\n   | Max. process+thread count | 524 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/9/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T08:27:16.021+0000", "updated": "2025-07-23T08:27:16.021+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009231", "id": "18009231", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "Copilot commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2225196847\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -259,10 +259,46 @@ public final class ConfigurationKeys {\n   public static final String AZURE_KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT = \"fs.azure.shellkeyprovider.script\";\n \n   /**\n-   * Enable or disable readahead buffer in AbfsInputStream.\n+   * Enable or disable readahead V1 in AbfsInputStream.\n    * Value: {@value}.\n    */\n   public static final String FS_AZURE_ENABLE_READAHEAD = \"fs.azure.enable.readahead\";\n+  /**\n+   * Enable or disable readahead V2 in AbfsInputStream. This will work independent of V1.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_ENABLE_READAHEAD_V2 = \"fs.azure.enable.readahead.v2\";\n+\n+  /**\n+   * Minimum number of prefetch threads in the thread pool for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MIN_THREAD_POOL_SIZE = \"fs.azure.readahead.v2.min.thread.pool.size\";\n+  /**\n+   * Maximum number of prefetch threads in the thread pool for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MAX_THREAD_POOL_SIZE = \"fs.azure.readahead.v2.max.thread.pool.size\";\n+  /**\n+   * Minimum size of the buffer pool for caching prefetched data for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MIN_BUFFER_POOL_SIZE = \"fs.azure.readahead.v2.min.buffer.pool.size\";\n+  /**\n+   * Maximum size of the buffer pool for caching prefetched data for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MAX_BUFFER_POOL_SIZE = \"fs.azure.readahead.v2.max.buffer.pool.size\";\n+\n+  /**\n+   * TTL in milliseconds for the idle threads in executor service used by read ahead v2.\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLISECONDS = \"fs.azure.readahead.v2.executor.service.ttl.seconds\";\n+\n+  /**\n+   * TTL in milliseconds for the cached buffers in buffer pool used by read ahead v2.\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_CACHED_BUFFER_TTL_MILLISECONDS = \"fs.azure.readahead.v2.cachec.buffer.ttl.milliseconds\";\n\nReview Comment:\n   There's a typo in the configuration key: 'cachec' should be 'cached'.\n   ```suggestion\n     public static final String FS_AZURE_READAHEAD_V2_CACHED_BUFFER_TTL_MILLISECONDS = \"fs.azure.readahead.v2.cached.buffer.ttl.milliseconds\";\n   ```\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,612 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.concurrent.CountDownLatch;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 extends ReadBufferManager {\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000;\n+\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;\n+  private static  ReadBufferManagerV1 bufferManager;\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * Sets the read buffer manager configurations.\n+   * @param readAheadBlockSize the size of the read-ahead block in bytes\n+   */\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      setReadAheadBlockSize(readAheadBlockSize);\n+      setThresholdAgeMilliseconds(DEFAULT_THRESHOLD_AGE_MILLISECONDS);\n+    }\n+  }\n+\n+  /**\n+   * Returns the singleton instance of ReadBufferManagerV1.\n+   * @return the singleton instance of ReadBufferManagerV1\n+   */\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  /**\n+   * {@inheritDoc}\n+   */\n+  @Override\n+  void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[getReadAheadBlockSize()];  // same buffers are reused. The byte array never goes back to GC\n\nReview Comment:\n   Comment has grammatical error: 'The byte array never goes back to GC' should be 'The byte arrays never go back to GC' or 'These byte arrays are never garbage collected'.\n   ```suggestion\n         buffers[i] = new byte[getReadAheadBlockSize()];  // same buffers are reused. These byte arrays are never garbage collected.\n   ```\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -0,0 +1,228 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.concurrent.SynchronousQueue;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+\n+final class ReadBufferManagerV2 extends ReadBufferManager {\n+\n+  // Thread Pool Configurations\n+  private static int minThreadPoolSize;\n+  private static int maxThreadPoolSize;\n+  private static int executorServiceKeepAliveTimeInMilliSec;\n+  private ThreadPoolExecutor workerPool;\n+\n+  // Buffer Pool Configurations\n+  private static int minBufferPoolSize;\n+  private static int maxBufferPoolSize;\n+  private int numberOfActiveBuffers = 0;\n+  private byte[][] bufferPool;\n+\n+  private static ReadBufferManagerV2 bufferManager;\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV2() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * Sets the read buffer manager configurations.\n+   * @param readAheadBlockSize the size of the read-ahead block in bytes\n+   * @param abfsConfiguration the AbfsConfiguration instance for other configurations\n+   */\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize, AbfsConfiguration abfsConfiguration) {\n+    if (bufferManager == null) {\n+      minThreadPoolSize = abfsConfiguration.getMinReadAheadV2ThreadPoolSize();\n+      maxThreadPoolSize = abfsConfiguration.getMaxReadAheadV2ThreadPoolSize();\n+      executorServiceKeepAliveTimeInMilliSec = abfsConfiguration.getReadAheadExecutorServiceTTLInMilliSeconds();\n+\n+      minBufferPoolSize = abfsConfiguration.getMinReadAheadV2BufferPoolSize();\n+      maxBufferPoolSize = abfsConfiguration.getMaxReadAheadV2BufferPoolSize();\n+      setThresholdAgeMilliseconds(abfsConfiguration.getReadAheadV2CachedBufferTTLMilliseconds());\n+      setReadAheadBlockSize(readAheadBlockSize);\n+    }\n+  }\n+\n+  /**\n+   * Returns the singleton instance of ReadBufferManagerV2.\n+   * @return the singleton instance of ReadBufferManagerV2\n+   */\n+  static ReadBufferManagerV2 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV2();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return (ReadBufferManagerV2) bufferManager;\n+  }\n+\n+  /**\n+   * {@inheritDoc}\n+   */\n+  @Override\n+  void init() {\n+    // Initialize Buffer Pool\n+    bufferPool = new byte[maxBufferPoolSize][];\n+    for (int i = 0; i < minBufferPoolSize; i++) {\n+      bufferPool[i] = new byte[getReadAheadBlockSize()];  // same buffers are reused. The byte array never goes back to GC\n\nReview Comment:\n   Comment has grammatical error: 'The byte array never goes back to GC' should be 'The byte arrays never go back to GC' or 'These byte arrays are never garbage collected'.\n   ```suggestion\n         bufferPool[i] = new byte[getReadAheadBlockSize()];  // same buffers are reused. These byte arrays are never garbage collected\n   ```\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -0,0 +1,228 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.concurrent.SynchronousQueue;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+\n+final class ReadBufferManagerV2 extends ReadBufferManager {\n+\n+  // Thread Pool Configurations\n+  private static int minThreadPoolSize;\n+  private static int maxThreadPoolSize;\n+  private static int executorServiceKeepAliveTimeInMilliSec;\n+  private ThreadPoolExecutor workerPool;\n+\n+  // Buffer Pool Configurations\n+  private static int minBufferPoolSize;\n+  private static int maxBufferPoolSize;\n+  private int numberOfActiveBuffers = 0;\n+  private byte[][] bufferPool;\n+\n+  private static ReadBufferManagerV2 bufferManager;\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV2() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * Sets the read buffer manager configurations.\n+   * @param readAheadBlockSize the size of the read-ahead block in bytes\n+   * @param abfsConfiguration the AbfsConfiguration instance for other configurations\n+   */\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize, AbfsConfiguration abfsConfiguration) {\n+    if (bufferManager == null) {\n+      minThreadPoolSize = abfsConfiguration.getMinReadAheadV2ThreadPoolSize();\n+      maxThreadPoolSize = abfsConfiguration.getMaxReadAheadV2ThreadPoolSize();\n+      executorServiceKeepAliveTimeInMilliSec = abfsConfiguration.getReadAheadExecutorServiceTTLInMilliSeconds();\n+\n+      minBufferPoolSize = abfsConfiguration.getMinReadAheadV2BufferPoolSize();\n+      maxBufferPoolSize = abfsConfiguration.getMaxReadAheadV2BufferPoolSize();\n+      setThresholdAgeMilliseconds(abfsConfiguration.getReadAheadV2CachedBufferTTLMilliseconds());\n+      setReadAheadBlockSize(readAheadBlockSize);\n+    }\n+  }\n+\n+  /**\n+   * Returns the singleton instance of ReadBufferManagerV2.\n+   * @return the singleton instance of ReadBufferManagerV2\n+   */\n+  static ReadBufferManagerV2 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV2();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return (ReadBufferManagerV2) bufferManager;\n\nReview Comment:\n   The cast to ReadBufferManagerV2 is unsafe. The static field 'bufferManager' is of type ReadBufferManagerV2, but the return statement suggests it could be something else. Either change the field type or remove the unnecessary cast.\n   ```suggestion\n       return bufferManager;\n   ```\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T11:09:33.500+0000", "updated": "2025-07-23T11:09:33.500+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009294", "id": "18009294", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2225889793\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java:\n##########\n@@ -15,636 +15,288 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.hadoop.fs.azurebfs.services;\n \n-import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n+package org.apache.hadoop.fs.azurebfs.services;\n \n import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Queue;\n import java.util.Stack;\n-import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.locks.ReentrantLock;\n \n-import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n \n /**\n- * The Read Buffer Manager for Rest AbfsClient.\n+ * Interface for managing read buffers for Azure Blob File System input streams.\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T15:03:55.140+0000", "updated": "2025-07-23T15:03:55.140+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009295", "id": "18009295", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2225893577\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -0,0 +1,612 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.concurrent.CountDownLatch;\n+\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+import org.apache.hadoop.classification.VisibleForTesting;\n+\n+/**\n+ * The Read Buffer Manager for Rest AbfsClient.\n+ * V1 implementation of ReadBufferManager.\n+ */\n+final class ReadBufferManagerV1 extends ReadBufferManager {\n+\n+  private static final int NUM_BUFFERS = 16;\n+  private static final int NUM_THREADS = 8;\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000;\n+\n+  private Thread[] threads = new Thread[NUM_THREADS];\n+  private byte[][] buffers;\n+  private static  ReadBufferManagerV1 bufferManager;\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV1() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * Sets the read buffer manager configurations.\n+   * @param readAheadBlockSize the size of the read-ahead block in bytes\n+   */\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize) {\n+    if (bufferManager == null) {\n+      LOGGER.debug(\n+          \"ReadBufferManagerV1 not initialized yet. Overriding readAheadBlockSize as {}\",\n+          readAheadBlockSize);\n+      setReadAheadBlockSize(readAheadBlockSize);\n+      setThresholdAgeMilliseconds(DEFAULT_THRESHOLD_AGE_MILLISECONDS);\n+    }\n+  }\n+\n+  /**\n+   * Returns the singleton instance of ReadBufferManagerV1.\n+   * @return the singleton instance of ReadBufferManagerV1\n+   */\n+  static ReadBufferManagerV1 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV1();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return bufferManager;\n+  }\n+\n+  /**\n+   * {@inheritDoc}\n+   */\n+  @Override\n+  void init() {\n+    buffers = new byte[NUM_BUFFERS][];\n+    for (int i = 0; i < NUM_BUFFERS; i++) {\n+      buffers[i] = new byte[getReadAheadBlockSize()];  // same buffers are reused. The byte array never goes back to GC\n+      getFreeList().add(i);\n+    }\n+    for (int i = 0; i < NUM_THREADS; i++) {\n+      Thread t = new Thread(new ReadBufferWorker(i, this));\n+      t.setDaemon(true);\n+      threads[i] = t;\n+      t.setName(\"ABFS-prefetch-\" + i);\n+      t.start();\n+    }\n+    ReadBufferWorker.UNLEASH_WORKERS.countDown();\n+  }\n+\n+  /**\n+   * {@inheritDoc}\n+   */\n+  @Override\n+  public void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength,\n+      TracingContext tracingContext) {\n+    if (LOGGER.isTraceEnabled()) {\n\nReview Comment:\n   Not very sure but i think it will be computationally faster to check and not post the log if trace is anyway disabled.\r\n   Also, this was always there in RBM as multiple threads hit this part and logs can grow really large.\r\n   Therefore, decided to retain it.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T15:05:30.088+0000", "updated": "2025-07-23T15:05:30.088+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009296", "id": "18009296", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2225895885\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -0,0 +1,228 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.concurrent.SynchronousQueue;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+\n+final class ReadBufferManagerV2 extends ReadBufferManager {\n+\n+  // Thread Pool Configurations\n+  private static int minThreadPoolSize;\n+  private static int maxThreadPoolSize;\n+  private static int executorServiceKeepAliveTimeInMilliSec;\n+  private ThreadPoolExecutor workerPool;\n+\n+  // Buffer Pool Configurations\n+  private static int minBufferPoolSize;\n+  private static int maxBufferPoolSize;\n+  private int numberOfActiveBuffers = 0;\n+  private byte[][] bufferPool;\n+\n+  private static ReadBufferManagerV2 bufferManager;\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV2() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * Sets the read buffer manager configurations.\n+   * @param readAheadBlockSize the size of the read-ahead block in bytes\n+   * @param abfsConfiguration the AbfsConfiguration instance for other configurations\n+   */\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize, AbfsConfiguration abfsConfiguration) {\n+    if (bufferManager == null) {\n+      minThreadPoolSize = abfsConfiguration.getMinReadAheadV2ThreadPoolSize();\n+      maxThreadPoolSize = abfsConfiguration.getMaxReadAheadV2ThreadPoolSize();\n+      executorServiceKeepAliveTimeInMilliSec = abfsConfiguration.getReadAheadExecutorServiceTTLInMilliSeconds();\n+\n+      minBufferPoolSize = abfsConfiguration.getMinReadAheadV2BufferPoolSize();\n+      maxBufferPoolSize = abfsConfiguration.getMaxReadAheadV2BufferPoolSize();\n+      setThresholdAgeMilliseconds(abfsConfiguration.getReadAheadV2CachedBufferTTLMilliseconds());\n+      setReadAheadBlockSize(readAheadBlockSize);\n+    }\n+  }\n+\n+  /**\n+   * Returns the singleton instance of ReadBufferManagerV2.\n+   * @return the singleton instance of ReadBufferManagerV2\n+   */\n+  static ReadBufferManagerV2 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV2();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return (ReadBufferManagerV2) bufferManager;\n\nReview Comment:\n   This cast was not needed. Removed\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T15:06:24.107+0000", "updated": "2025-07-23T15:06:24.107+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009297", "id": "18009297", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2225897996\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV2.java:\n##########\n@@ -0,0 +1,228 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.concurrent.SynchronousQueue;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.classification.VisibleForTesting;\n+import org.apache.hadoop.fs.azurebfs.AbfsConfiguration;\n+import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n+import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n+\n+final class ReadBufferManagerV2 extends ReadBufferManager {\n+\n+  // Thread Pool Configurations\n+  private static int minThreadPoolSize;\n+  private static int maxThreadPoolSize;\n+  private static int executorServiceKeepAliveTimeInMilliSec;\n+  private ThreadPoolExecutor workerPool;\n+\n+  // Buffer Pool Configurations\n+  private static int minBufferPoolSize;\n+  private static int maxBufferPoolSize;\n+  private int numberOfActiveBuffers = 0;\n+  private byte[][] bufferPool;\n+\n+  private static ReadBufferManagerV2 bufferManager;\n+\n+  // hide instance constructor\n+  private ReadBufferManagerV2() {\n+    LOGGER.trace(\"Creating readbuffer manager with HADOOP-18546 patch\");\n+  }\n+\n+  /**\n+   * Sets the read buffer manager configurations.\n+   * @param readAheadBlockSize the size of the read-ahead block in bytes\n+   * @param abfsConfiguration the AbfsConfiguration instance for other configurations\n+   */\n+  static void setReadBufferManagerConfigs(int readAheadBlockSize, AbfsConfiguration abfsConfiguration) {\n+    if (bufferManager == null) {\n+      minThreadPoolSize = abfsConfiguration.getMinReadAheadV2ThreadPoolSize();\n+      maxThreadPoolSize = abfsConfiguration.getMaxReadAheadV2ThreadPoolSize();\n+      executorServiceKeepAliveTimeInMilliSec = abfsConfiguration.getReadAheadExecutorServiceTTLInMilliSeconds();\n+\n+      minBufferPoolSize = abfsConfiguration.getMinReadAheadV2BufferPoolSize();\n+      maxBufferPoolSize = abfsConfiguration.getMaxReadAheadV2BufferPoolSize();\n+      setThresholdAgeMilliseconds(abfsConfiguration.getReadAheadV2CachedBufferTTLMilliseconds());\n+      setReadAheadBlockSize(readAheadBlockSize);\n+    }\n+  }\n+\n+  /**\n+   * Returns the singleton instance of ReadBufferManagerV2.\n+   * @return the singleton instance of ReadBufferManagerV2\n+   */\n+  static ReadBufferManagerV2 getBufferManager() {\n+    if (bufferManager == null) {\n+      LOCK.lock();\n+      try {\n+        if (bufferManager == null) {\n+          bufferManager = new ReadBufferManagerV2();\n+          bufferManager.init();\n+        }\n+      } finally {\n+        LOCK.unlock();\n+      }\n+    }\n+    return (ReadBufferManagerV2) bufferManager;\n+  }\n+\n+  /**\n+   * {@inheritDoc}\n+   */\n+  @Override\n+  void init() {\n+    // Initialize Buffer Pool\n+    bufferPool = new byte[maxBufferPoolSize][];\n+    for (int i = 0; i < minBufferPoolSize; i++) {\n+      bufferPool[i] = new byte[getReadAheadBlockSize()];  // same buffers are reused. The byte array never goes back to GC\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T15:07:08.270+0000", "updated": "2025-07-23T15:07:08.270+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009334", "id": "18009334", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3109474371\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |   0m 33s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  40m 18s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 34s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 44s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 36s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 16s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  35m 38s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 32s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/10/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 11 new + 3 unchanged - 0 fixed = 14 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  8s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  34m 54s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  7s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 125m 43s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/10/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 2df037f2c83b 5.15.0-139-generic #149-Ubuntu SMP Fri Apr 11 22:06:13 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 108dc0865841171611385b891d45a218dbb3c780 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/10/testReport/ |\r\n   | Max. process+thread count | 556 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/10/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-23T17:15:59.663+0000", "updated": "2025-07-23T17:15:59.663+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009469", "id": "18009469", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2227549674\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java:\n##########\n@@ -19,16 +19,43 @@\n package org.apache.hadoop.fs.azurebfs.services;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import org.apache.hadoop.classification.VisibleForTesting;\n import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n \n /**\n- * Interface for managing read buffers for Azure Blob File System input streams.\n+ * Abstract class for managing read buffers for Azure Blob File System input streams.\n  */\n-public interface ReadBufferManager {\n+public abstract class ReadBufferManager {\n+  protected static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManager.class);\n+  protected static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static int thresholdAgeMilliseconds;\n+  private static int blockSize = 4 * ONE_MB; // default block size for read-ahead in bytes\n\nReview Comment:\n   this should also be configurable right or should come from configuration class as default value \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T06:25:49.641+0000", "updated": "2025-07-24T06:25:49.641+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009470", "id": "18009470", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2227556210\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -39,28 +33,39 @@\n  * The Read Buffer Manager for Rest AbfsClient.\n  * V1 implementation of ReadBufferManager.\n  */\n-final class ReadBufferManagerV1 implements ReadBufferManager {\n-  private static final Logger LOGGER = LoggerFactory.getLogger(\n-      ReadBufferManagerV1.class);\n-  private static final int ONE_KB = 1024;\n-  private static final int ONE_MB = ONE_KB * ONE_KB;\n+final class ReadBufferManagerV1 extends ReadBufferManager {\n \n   private static final int NUM_BUFFERS = 16;\n   private static final int NUM_THREADS = 8;\n-  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000;\n\nReview Comment:\n   All the configs in this class should come from configurationKeys \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T06:29:50.422+0000", "updated": "2025-07-24T06:29:50.422+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009471", "id": "18009471", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2227571699\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -39,28 +33,39 @@\n  * The Read Buffer Manager for Rest AbfsClient.\n  * V1 implementation of ReadBufferManager.\n  */\n-final class ReadBufferManagerV1 implements ReadBufferManager {\n-  private static final Logger LOGGER = LoggerFactory.getLogger(\n-      ReadBufferManagerV1.class);\n-  private static final int ONE_KB = 1024;\n-  private static final int ONE_MB = ONE_KB * ONE_KB;\n+final class ReadBufferManagerV1 extends ReadBufferManager {\n \n   private static final int NUM_BUFFERS = 16;\n   private static final int NUM_THREADS = 8;\n-  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000;\n \n-  private static int blockSize = 4 * ONE_MB;\n-  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n   private Thread[] threads = new Thread[NUM_THREADS];\n-  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n-  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+  private byte[][] buffers;\n+  private static  ReadBufferManagerV1 bufferManager;\n\nReview Comment:\n   since this is a singleton instance should be declared as volatile ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T06:39:02.952+0000", "updated": "2025-07-24T06:39:02.952+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009530", "id": "18009530", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2228214679\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestGetNameSpaceEnabled.java:\n##########\n@@ -471,10 +472,10 @@ private Configuration getConfigurationWithoutHnsConfig() {\n     rawConfig.unset(FS_AZURE_ACCOUNT_IS_HNS_ENABLED);\n     rawConfig.unset(accountProperty(FS_AZURE_ACCOUNT_IS_HNS_ENABLED,\n         this.getAccountName()));\n-    String testAccountName = \"testAccount.dfs.core.windows.net\";\n-    String defaultUri = this.getTestUrl().replace(this.getAccountName(), testAccountName);\n+    String defaultUri = getRawConfiguration().get(FS_DEFAULT_NAME_KEY).\n+        replace(\"blob.core.windows.net\",\"dfs.core.windows.net\");\n\nReview Comment:\n   should we use our available \".blob.\" constants here ?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T11:13:12.282+0000", "updated": "2025-07-24T11:13:12.282+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009539", "id": "18009539", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2228240528\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java:\n##########\n@@ -19,16 +19,43 @@\n package org.apache.hadoop.fs.azurebfs.services;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import org.apache.hadoop.classification.VisibleForTesting;\n import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n \n /**\n- * Interface for managing read buffers for Azure Blob File System input streams.\n+ * Abstract class for managing read buffers for Azure Blob File System input streams.\n  */\n-public interface ReadBufferManager {\n+public abstract class ReadBufferManager {\n+  protected static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManager.class);\n+  protected static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static int thresholdAgeMilliseconds;\n+  private static int blockSize = 4 * ONE_MB; // default block size for read-ahead in bytes\n\nReview Comment:\n   This is already configurable. 4 MB is just default value\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T11:26:02.927+0000", "updated": "2025-07-24T11:26:02.927+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009540", "id": "18009540", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2228241908\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -39,28 +33,39 @@\n  * The Read Buffer Manager for Rest AbfsClient.\n  * V1 implementation of ReadBufferManager.\n  */\n-final class ReadBufferManagerV1 implements ReadBufferManager {\n-  private static final Logger LOGGER = LoggerFactory.getLogger(\n-      ReadBufferManagerV1.class);\n-  private static final int ONE_KB = 1024;\n-  private static final int ONE_MB = ONE_KB * ONE_KB;\n+final class ReadBufferManagerV1 extends ReadBufferManager {\n \n   private static final int NUM_BUFFERS = 16;\n   private static final int NUM_THREADS = 8;\n-  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000;\n\nReview Comment:\n   Already done in RBMV2.\r\n   RBMV1, I am not touching\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T11:26:42.974+0000", "updated": "2025-07-24T11:26:42.974+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009544", "id": "18009544", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2228246216\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -39,28 +33,39 @@\n  * The Read Buffer Manager for Rest AbfsClient.\n  * V1 implementation of ReadBufferManager.\n  */\n-final class ReadBufferManagerV1 implements ReadBufferManager {\n-  private static final Logger LOGGER = LoggerFactory.getLogger(\n-      ReadBufferManagerV1.class);\n-  private static final int ONE_KB = 1024;\n-  private static final int ONE_MB = ONE_KB * ONE_KB;\n+final class ReadBufferManagerV1 extends ReadBufferManager {\n \n   private static final int NUM_BUFFERS = 16;\n   private static final int NUM_THREADS = 8;\n-  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000;\n \n-  private static int blockSize = 4 * ONE_MB;\n-  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n   private Thread[] threads = new Thread[NUM_THREADS];\n-  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n-  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+  private byte[][] buffers;\n+  private static  ReadBufferManagerV1 bufferManager;\n\nReview Comment:\n   Not needed I think. Volatile is needed if someone is updating the value, but here only once it is init and a static single copy of vairable is shared among all streams.\r\n   Also, this was like this always\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T11:28:52.317+0000", "updated": "2025-07-24T11:28:52.317+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009546", "id": "18009546", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2228247351\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestGetNameSpaceEnabled.java:\n##########\n@@ -471,10 +472,10 @@ private Configuration getConfigurationWithoutHnsConfig() {\n     rawConfig.unset(FS_AZURE_ACCOUNT_IS_HNS_ENABLED);\n     rawConfig.unset(accountProperty(FS_AZURE_ACCOUNT_IS_HNS_ENABLED,\n         this.getAccountName()));\n-    String testAccountName = \"testAccount.dfs.core.windows.net\";\n-    String defaultUri = this.getTestUrl().replace(this.getAccountName(), testAccountName);\n+    String defaultUri = getRawConfiguration().get(FS_DEFAULT_NAME_KEY).\n+        replace(\"blob.core.windows.net\",\"dfs.core.windows.net\");\n\nReview Comment:\n   Yes. Will take it up.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T11:29:23.188+0000", "updated": "2025-07-24T11:29:23.188+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009553", "id": "18009553", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2228265680\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManagerV1.java:\n##########\n@@ -39,28 +33,39 @@\n  * The Read Buffer Manager for Rest AbfsClient.\n  * V1 implementation of ReadBufferManager.\n  */\n-final class ReadBufferManagerV1 implements ReadBufferManager {\n-  private static final Logger LOGGER = LoggerFactory.getLogger(\n-      ReadBufferManagerV1.class);\n-  private static final int ONE_KB = 1024;\n-  private static final int ONE_MB = ONE_KB * ONE_KB;\n+final class ReadBufferManagerV1 extends ReadBufferManager {\n \n   private static final int NUM_BUFFERS = 16;\n   private static final int NUM_THREADS = 8;\n-  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n+  private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000;\n \n-  private static int blockSize = 4 * ONE_MB;\n-  private static int thresholdAgeMilliseconds = DEFAULT_THRESHOLD_AGE_MILLISECONDS;\n   private Thread[] threads = new Thread[NUM_THREADS];\n-  private byte[][] buffers;    // array of byte[] buffers, to hold the data that is read\n-  private Stack<Integer> freeList = new Stack<>();   // indices in buffers[] array that are available\n+  private byte[][] buffers;\n+  private static  ReadBufferManagerV1 bufferManager;\n\nReview Comment:\n   volatile is needed in double-checked locking to prevent instruction reordering and ensure visibility across threads. Without it, a thread may see a partially constructed Singleton instance. This is a new learning I had, can be added or kept as is.\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T11:38:13.904+0000", "updated": "2025-07-24T11:38:13.904+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009556", "id": "18009556", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anmolanmol1234 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2228270551\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java:\n##########\n@@ -19,16 +19,43 @@\n package org.apache.hadoop.fs.azurebfs.services;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n import java.util.List;\n+import java.util.Queue;\n+import java.util.Stack;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import org.apache.hadoop.classification.VisibleForTesting;\n import org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus;\n import org.apache.hadoop.fs.azurebfs.utils.TracingContext;\n \n /**\n- * Interface for managing read buffers for Azure Blob File System input streams.\n+ * Abstract class for managing read buffers for Azure Blob File System input streams.\n  */\n-public interface ReadBufferManager {\n+public abstract class ReadBufferManager {\n+  protected static final Logger LOGGER = LoggerFactory.getLogger(\n+      ReadBufferManager.class);\n+  protected static final ReentrantLock LOCK = new ReentrantLock();\n+  private static final int ONE_KB = 1024;\n+  private static final int ONE_MB = ONE_KB * ONE_KB;\n+\n+  private static int thresholdAgeMilliseconds;\n+  private static int blockSize = 4 * ONE_MB; // default block size for read-ahead in bytes\n\nReview Comment:\n   Right if we are refactoring, we can make this also come from the configuration class?\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T11:40:39.048+0000", "updated": "2025-07-24T11:40:39.048+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18009576", "id": "18009576", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "bhattmanish98 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2228398088\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -381,6 +381,41 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_ENABLE_READAHEAD)\n   private boolean enabledReadAhead;\n \n+  @BooleanConfigurationValidatorAnnotation(\n+      ConfigurationKey = FS_AZURE_ENABLE_READAHEAD_V2,\n+      DefaultValue = DEFAULT_ENABLE_READAHEAD_V2)\n+  private boolean isReadAheadV2Enabled;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MIN_THREAD_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MIN_THREAD_POOL_SIZE)\n+  private int minReadAheadV2ThreadPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MAX_THREAD_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MAX_THREAD_POOL_SIZE)\n+  private int maxReadAheadV2ThreadPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MIN_BUFFER_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MIN_BUFFER_POOL_SIZE)\n+  private int minReadAheadV2BufferPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MAX_BUFFER_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MAX_BUFFER_POOL_SIZE)\n+  private int maxReadAheadV2BufferPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLISECONDS,\n+      DefaultValue = DEFAULT_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLISECONDS)\n+  private int readAheadExecutorServiceTTLInMilliSeconds;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_CACHED_BUFFER_TTL_MILLISECONDS,\n+      DefaultValue = DEFAULT_READAHEAD_V2_CACHED_BUFFER_TTL_MILLISECONDS)\n+  private int readAheadV2CachedBufferTTLMilliseconds;\n\nReview Comment:\n   Can we keep attribute naming format constant? Above attribute name ends with TTLInMilliSeconds, this one has TTLMilliseconds.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -173,9 +176,19 @@ public AbfsInputStream(\n     this.fsBackRef = abfsInputStreamContext.getFsBackRef();\n     contextEncryptionAdapter = abfsInputStreamContext.getEncryptionAdapter();\n \n-    // Propagate the config values to ReadBufferManager so that the first instance\n-    // to initialize can set the readAheadBlockSize\n-    ReadBufferManager.setReadBufferManagerConfigs(readAheadBlockSize);\n+    /*\n+     * Initialize the ReadBufferManager based on whether readAheadV2 is enabled or not.\n+     * Precedence is given to ReadBufferManagerV2.\n+     * If none of the V1 and V2 are enabled, then no read ahead will be done.\n+     */\n+    if (readAheadV2Enabled) {\n+      ReadBufferManagerV2.setReadBufferManagerConfigs(\n+          readAheadBlockSize, client.getAbfsConfiguration());\n+      readBufferManager = ReadBufferManagerV2.getBufferManager();\n+    } else {\n\nReview Comment:\n   This should be under else if (readAheadEnabled) instead of else?\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -259,10 +259,46 @@ public final class ConfigurationKeys {\n   public static final String AZURE_KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT = \"fs.azure.shellkeyprovider.script\";\n \n   /**\n-   * Enable or disable readahead buffer in AbfsInputStream.\n+   * Enable or disable readahead V1 in AbfsInputStream.\n    * Value: {@value}.\n    */\n   public static final String FS_AZURE_ENABLE_READAHEAD = \"fs.azure.enable.readahead\";\n+  /**\n+   * Enable or disable readahead V2 in AbfsInputStream. This will work independent of V1.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_ENABLE_READAHEAD_V2 = \"fs.azure.enable.readahead.v2\";\n+\n+  /**\n+   * Minimum number of prefetch threads in the thread pool for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MIN_THREAD_POOL_SIZE = \"fs.azure.readahead.v2.min.thread.pool.size\";\n+  /**\n+   * Maximum number of prefetch threads in the thread pool for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MAX_THREAD_POOL_SIZE = \"fs.azure.readahead.v2.max.thread.pool.size\";\n+  /**\n+   * Minimum size of the buffer pool for caching prefetched data for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MIN_BUFFER_POOL_SIZE = \"fs.azure.readahead.v2.min.buffer.pool.size\";\n+  /**\n+   * Maximum size of the buffer pool for caching prefetched data for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MAX_BUFFER_POOL_SIZE = \"fs.azure.readahead.v2.max.buffer.pool.size\";\n+\n+  /**\n+   * TTL in milliseconds for the idle threads in executor service used by read ahead v2.\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLISECONDS = \"fs.azure.readahead.v2.executor.service.ttl.seconds\";\n\nReview Comment:\n   \"fs.azure.readahead.v2.executor.service.ttl.seconds\" -> \"fs.azure.readahead.v2.executor.service.ttl.milliseconds\"\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1368,6 +1403,54 @@ public boolean isReadAheadEnabled() {\n     return this.enabledReadAhead;\n   }\n \n+  public int getMinReadAheadV2ThreadPoolSize() {\n+    if (minReadAheadV2ThreadPoolSize <= 0) {\n+      // If the minReadAheadV2ThreadPoolSize is not set, use the default value\n+      return 2 * Runtime.getRuntime().availableProcessors();\n+    }\n+    return minReadAheadV2ThreadPoolSize;\n+  }\n+\n+  public int getMaxReadAheadV2ThreadPoolSize() {\n+    if (maxReadAheadV2ThreadPoolSize <= 0) {\n+      // If the maxReadAheadV2ThreadPoolSize is not set, use the default value\n+      return 4 * Runtime.getRuntime().availableProcessors();\n+    }\n+    return maxReadAheadV2ThreadPoolSize;\n+  }\n+\n+  public int getMinReadAheadV2BufferPoolSize() {\n+    if (minReadAheadV2BufferPoolSize <= 0) {\n+      // If the minReadAheadV2BufferPoolSize is not set, use the default value\n+      return 2 * Runtime.getRuntime().availableProcessors();\n+    }\n+    return minReadAheadV2BufferPoolSize;\n+  }\n+\n+  public int getMaxReadAheadV2BufferPoolSize() {\n+    if (maxReadAheadV2BufferPoolSize <= 0) {\n+      // If the maxReadAheadV2BufferPoolSize is not set, use the default value\n+      return 4 * Runtime.getRuntime().availableProcessors();\n+    }\n+    return maxReadAheadV2BufferPoolSize;\n+  }\n+\n+  public int getReadAheadExecutorServiceTTLInMilliSeconds() {\n+    return readAheadExecutorServiceTTLInMilliSeconds;\n+  }\n+\n+  public int getReadAheadV2CachedBufferTTLMilliseconds() {\n\nReview Comment:\n   Same as above, method name can follow same naming format every where.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -381,6 +381,41 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_ENABLE_READAHEAD)\n   private boolean enabledReadAhead;\n \n+  @BooleanConfigurationValidatorAnnotation(\n+      ConfigurationKey = FS_AZURE_ENABLE_READAHEAD_V2,\n+      DefaultValue = DEFAULT_ENABLE_READAHEAD_V2)\n+  private boolean isReadAheadV2Enabled;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MIN_THREAD_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MIN_THREAD_POOL_SIZE)\n+  private int minReadAheadV2ThreadPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MAX_THREAD_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MAX_THREAD_POOL_SIZE)\n+  private int maxReadAheadV2ThreadPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MIN_BUFFER_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MIN_BUFFER_POOL_SIZE)\n+  private int minReadAheadV2BufferPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MAX_BUFFER_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MAX_BUFFER_POOL_SIZE)\n+  private int maxReadAheadV2BufferPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLISECONDS,\n+      DefaultValue = DEFAULT_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLISECONDS)\n+  private int readAheadExecutorServiceTTLInMilliSeconds;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_CACHED_BUFFER_TTL_MILLISECONDS,\n+      DefaultValue = DEFAULT_READAHEAD_V2_CACHED_BUFFER_TTL_MILLISECONDS)\n+  private int readAheadV2CachedBufferTTLMilliseconds;\n\nReview Comment:\n   Also, if possible we can shorten the attribute name like readAheadV2CachedBufferTTLInMillis\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-24T13:17:01.921+0000", "updated": "2025-07-24T13:17:01.921+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18010263", "id": "18010263", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2234848778\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java:\n##########\n@@ -259,10 +259,46 @@ public final class ConfigurationKeys {\n   public static final String AZURE_KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT = \"fs.azure.shellkeyprovider.script\";\n \n   /**\n-   * Enable or disable readahead buffer in AbfsInputStream.\n+   * Enable or disable readahead V1 in AbfsInputStream.\n    * Value: {@value}.\n    */\n   public static final String FS_AZURE_ENABLE_READAHEAD = \"fs.azure.enable.readahead\";\n+  /**\n+   * Enable or disable readahead V2 in AbfsInputStream. This will work independent of V1.\n+   * Value: {@value}.\n+   */\n+  public static final String FS_AZURE_ENABLE_READAHEAD_V2 = \"fs.azure.enable.readahead.v2\";\n+\n+  /**\n+   * Minimum number of prefetch threads in the thread pool for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MIN_THREAD_POOL_SIZE = \"fs.azure.readahead.v2.min.thread.pool.size\";\n+  /**\n+   * Maximum number of prefetch threads in the thread pool for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MAX_THREAD_POOL_SIZE = \"fs.azure.readahead.v2.max.thread.pool.size\";\n+  /**\n+   * Minimum size of the buffer pool for caching prefetched data for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MIN_BUFFER_POOL_SIZE = \"fs.azure.readahead.v2.min.buffer.pool.size\";\n+  /**\n+   * Maximum size of the buffer pool for caching prefetched data for readahead V2.\n+   * {@value }\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_MAX_BUFFER_POOL_SIZE = \"fs.azure.readahead.v2.max.buffer.pool.size\";\n+\n+  /**\n+   * TTL in milliseconds for the idle threads in executor service used by read ahead v2.\n+   */\n+  public static final String FS_AZURE_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLISECONDS = \"fs.azure.readahead.v2.executor.service.ttl.seconds\";\n\nReview Comment:\n   Good catc. Fixed it\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-28T06:29:09.705+0000", "updated": "2025-07-28T06:29:09.705+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18010264", "id": "18010264", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2234849377\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -1368,6 +1403,54 @@ public boolean isReadAheadEnabled() {\n     return this.enabledReadAhead;\n   }\n \n+  public int getMinReadAheadV2ThreadPoolSize() {\n+    if (minReadAheadV2ThreadPoolSize <= 0) {\n+      // If the minReadAheadV2ThreadPoolSize is not set, use the default value\n+      return 2 * Runtime.getRuntime().availableProcessors();\n+    }\n+    return minReadAheadV2ThreadPoolSize;\n+  }\n+\n+  public int getMaxReadAheadV2ThreadPoolSize() {\n+    if (maxReadAheadV2ThreadPoolSize <= 0) {\n+      // If the maxReadAheadV2ThreadPoolSize is not set, use the default value\n+      return 4 * Runtime.getRuntime().availableProcessors();\n+    }\n+    return maxReadAheadV2ThreadPoolSize;\n+  }\n+\n+  public int getMinReadAheadV2BufferPoolSize() {\n+    if (minReadAheadV2BufferPoolSize <= 0) {\n+      // If the minReadAheadV2BufferPoolSize is not set, use the default value\n+      return 2 * Runtime.getRuntime().availableProcessors();\n+    }\n+    return minReadAheadV2BufferPoolSize;\n+  }\n+\n+  public int getMaxReadAheadV2BufferPoolSize() {\n+    if (maxReadAheadV2BufferPoolSize <= 0) {\n+      // If the maxReadAheadV2BufferPoolSize is not set, use the default value\n+      return 4 * Runtime.getRuntime().availableProcessors();\n+    }\n+    return maxReadAheadV2BufferPoolSize;\n+  }\n+\n+  public int getReadAheadExecutorServiceTTLInMilliSeconds() {\n+    return readAheadExecutorServiceTTLInMilliSeconds;\n+  }\n+\n+  public int getReadAheadV2CachedBufferTTLMilliseconds() {\n\nReview Comment:\n   Good suggestion. Taken\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java:\n##########\n@@ -381,6 +381,41 @@ public class AbfsConfiguration{\n       DefaultValue = DEFAULT_ENABLE_READAHEAD)\n   private boolean enabledReadAhead;\n \n+  @BooleanConfigurationValidatorAnnotation(\n+      ConfigurationKey = FS_AZURE_ENABLE_READAHEAD_V2,\n+      DefaultValue = DEFAULT_ENABLE_READAHEAD_V2)\n+  private boolean isReadAheadV2Enabled;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MIN_THREAD_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MIN_THREAD_POOL_SIZE)\n+  private int minReadAheadV2ThreadPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MAX_THREAD_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MAX_THREAD_POOL_SIZE)\n+  private int maxReadAheadV2ThreadPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MIN_BUFFER_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MIN_BUFFER_POOL_SIZE)\n+  private int minReadAheadV2BufferPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_MAX_BUFFER_POOL_SIZE,\n+      DefaultValue = DEFAULT_READAHEAD_V2_MAX_BUFFER_POOL_SIZE)\n+  private int maxReadAheadV2BufferPoolSize;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLISECONDS,\n+      DefaultValue = DEFAULT_READAHEAD_V2_EXECUTOR_SERVICE_TTL_MILLISECONDS)\n+  private int readAheadExecutorServiceTTLInMilliSeconds;\n+\n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey =\n+      FS_AZURE_READAHEAD_V2_CACHED_BUFFER_TTL_MILLISECONDS,\n+      DefaultValue = DEFAULT_READAHEAD_V2_CACHED_BUFFER_TTL_MILLISECONDS)\n+  private int readAheadV2CachedBufferTTLMilliseconds;\n\nReview Comment:\n   Great suggestion. Taken\r\n   \n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-28T06:29:24.720+0000", "updated": "2025-07-28T06:29:24.720+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18010265", "id": "18010265", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 commented on code in PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#discussion_r2234852223\n\n\n##########\nhadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java:\n##########\n@@ -173,9 +176,19 @@ public AbfsInputStream(\n     this.fsBackRef = abfsInputStreamContext.getFsBackRef();\n     contextEncryptionAdapter = abfsInputStreamContext.getEncryptionAdapter();\n \n-    // Propagate the config values to ReadBufferManager so that the first instance\n-    // to initialize can set the readAheadBlockSize\n-    ReadBufferManager.setReadBufferManagerConfigs(readAheadBlockSize);\n+    /*\n+     * Initialize the ReadBufferManager based on whether readAheadV2 is enabled or not.\n+     * Precedence is given to ReadBufferManagerV2.\n+     * If none of the V1 and V2 are enabled, then no read ahead will be done.\n+     */\n+    if (readAheadV2Enabled) {\n+      ReadBufferManagerV2.setReadBufferManagerConfigs(\n+          readAheadBlockSize, client.getAbfsConfiguration());\n+      readBufferManager = ReadBufferManagerV2.getBufferManager();\n+    } else {\n\nReview Comment:\n   We always had RBM initlialised today.\r\n   Wanted to retain it to avoid any unexplored usage through NPE.\n\n\n\n##########\nhadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestGetNameSpaceEnabled.java:\n##########\n@@ -471,10 +472,10 @@ private Configuration getConfigurationWithoutHnsConfig() {\n     rawConfig.unset(FS_AZURE_ACCOUNT_IS_HNS_ENABLED);\n     rawConfig.unset(accountProperty(FS_AZURE_ACCOUNT_IS_HNS_ENABLED,\n         this.getAccountName()));\n-    String testAccountName = \"testAccount.dfs.core.windows.net\";\n-    String defaultUri = this.getTestUrl().replace(this.getAccountName(), testAccountName);\n+    String defaultUri = getRawConfiguration().get(FS_DEFAULT_NAME_KEY).\n+        replace(\"blob.core.windows.net\",\"dfs.core.windows.net\");\n\nReview Comment:\n   Taken\n\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-28T06:30:14.757+0000", "updated": "2025-07-28T06:30:14.757+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18010308", "id": "18010308", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3126288139\n\n   :broken_heart: **-1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  22m  2s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  43m 51s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 44s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 38s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 33s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 42s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 12s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  41m 15s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  41m 38s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | -1 :x: |  mvninstall  |   0m 23s | [/patch-mvninstall-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/artifact/out/patch-mvninstall-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | -1 :x: |  compile  |   0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  javac  |   0m 24s | [/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkUbuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.txt) |  hadoop-azure in the patch failed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04.  |\r\n   | -1 :x: |  compile  |   0m 23s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | -1 :x: |  javac  |   0m 23s | [/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/artifact/out/patch-compile-hadoop-tools_hadoop-azure-jdkPrivateBuild-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.txt) |  hadoop-azure in the patch failed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09.  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 10 new + 3 unchanged - 0 fixed = 13 total (was 3)  |\r\n   | -1 :x: |  mvnsite  |   0m 24s | [/patch-mvnsite-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/artifact/out/patch-mvnsite-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | -1 :x: |  spotbugs  |   0m 23s | [/patch-spotbugs-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/artifact/out/patch-spotbugs-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  shadedclient  |  45m 25s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | -1 :x: |  unit  |   0m 27s | [/patch-unit-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/artifact/out/patch-unit-hadoop-tools_hadoop-azure.txt) |  hadoop-azure in the patch failed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 38s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 160m 48s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux ef87c3fbf779 5.15.0-143-generic #153-Ubuntu SMP Fri Jun 13 19:10:45 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / e9ad12a5b88119446950fd42fb7b2b5a890bcdb6 |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/testReport/ |\r\n   | Max. process+thread count | 570 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/11/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-28T09:10:58.131+0000", "updated": "2025-07-28T09:10:58.131+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18010372", "id": "18010372", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "hadoop-yetus commented on PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801#issuecomment-3126854843\n\n   :confetti_ball: **+1 overall**\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \r\n   | Vote | Subsystem | Runtime |  Logfile | Comment |\r\n   |:----:|----------:|--------:|:--------:|:-------:|\r\n   | +0 :ok: |  reexec  |  14m 42s |  |  Docker mode activated.  |\r\n   |||| _ Prechecks _ |\r\n   | +1 :green_heart: |  dupname  |   0m  0s |  |  No case conflicting files found.  |\r\n   | +0 :ok: |  codespell  |   0m  1s |  |  codespell was not available.  |\r\n   | +0 :ok: |  detsecrets  |   0m  1s |  |  detect-secrets was not available.  |\r\n   | +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n   | +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n   |||| _ trunk Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |  38m 27s |  |  trunk passed  |\r\n   | +1 :green_heart: |  compile  |   0m 42s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  compile  |   0m 39s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  checkstyle  |   0m 36s |  |  trunk passed  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 43s |  |  trunk passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 43s |  |  trunk passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 35s |  |  trunk passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m 10s |  |  trunk passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 42s |  |  branch has no errors when building and testing our client artifacts.  |\r\n   | -0 :warning: |  patch  |  36m  4s |  |  Used diff version of patch file. Binary files and potentially other changes not applied. Please rebase and squash commits if necessary.  |\r\n   |||| _ Patch Compile Tests _ |\r\n   | +1 :green_heart: |  mvninstall  |   0m 32s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 33s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javac  |   0m 33s |  |  the patch passed  |\r\n   | +1 :green_heart: |  compile  |   0m 30s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  javac  |   0m 30s |  |  the patch passed  |\r\n   | +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n   | -0 :warning: |  checkstyle  |   0m 21s | [/results-checkstyle-hadoop-tools_hadoop-azure.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/12/artifact/out/results-checkstyle-hadoop-tools_hadoop-azure.txt) |  hadoop-tools/hadoop-azure: The patch generated 9 new + 3 unchanged - 0 fixed = 12 total (was 3)  |\r\n   | +1 :green_heart: |  mvnsite  |   0m 31s |  |  the patch passed  |\r\n   | +1 :green_heart: |  javadoc  |   0m 29s |  |  the patch passed with JDK Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04  |\r\n   | +1 :green_heart: |  javadoc  |   0m 26s |  |  the patch passed with JDK Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09  |\r\n   | +1 :green_heart: |  spotbugs  |   1m  9s |  |  the patch passed  |\r\n   | +1 :green_heart: |  shadedclient  |  35m 18s |  |  patch has no errors when building and testing our client artifacts.  |\r\n   |||| _ Other Tests _ |\r\n   | +1 :green_heart: |  unit  |   3m  4s |  |  hadoop-azure in the patch passed.  |\r\n   | +1 :green_heart: |  asflicense  |   0m 37s |  |  The patch does not generate ASF License warnings.  |\r\n   |  |   | 138m 50s |  |  |\r\n   \r\n   \r\n   | Subsystem | Report/Notes |\r\n   |----------:|:-------------|\r\n   | Docker | ClientAPI=1.51 ServerAPI=1.51 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/12/artifact/out/Dockerfile |\r\n   | GITHUB PR | https://github.com/apache/hadoop/pull/7801 |\r\n   | Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets |\r\n   | uname | Linux 16dbb289dfab 5.15.0-144-generic #157-Ubuntu SMP Mon Jun 16 07:33:10 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux |\r\n   | Build tool | maven |\r\n   | Personality | dev-support/bin/hadoop.sh |\r\n   | git revision | trunk / 92dfbda9419ed5ef4d8dae0ce20a880f3fc24c1d |\r\n   | Default Java | Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   | Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Ubuntu-11.0.27+6-post-Ubuntu-0ubuntu120.04 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09 |\r\n   |  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/12/testReport/ |\r\n   | Max. process+thread count | 546 (vs. ulimit of 5500) |\r\n   | modules | C: hadoop-tools/hadoop-azure U: hadoop-tools/hadoop-azure |\r\n   | Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-7801/12/console |\r\n   | versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n   | Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n   \r\n   \r\n   This message was automatically generated.\r\n   \r\n   \n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-28T11:47:01.541+0000", "updated": "2025-07-28T11:47:01.541+0000"}, {"self": "https://issues.apache.org/jira/rest/api/2/issue/13622989/comment/18010400", "id": "18010400", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "body": "anujmodi2021 merged PR #7801:\nURL: https://github.com/apache/hadoop/pull/7801\n\n\n", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot", "name": "githubbot", "key": "githubbot", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "ASF GitHub Bot", "active": true, "timeZone": "Etc/UTC"}, "created": "2025-07-28T13:31:31.076+0000", "updated": "2025-07-28T13:31:31.076+0000"}], "maxResults": 55, "total": 55, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/3", "description": "This issue is being actively worked on at the moment by the assignee.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/inprogress.png", "name": "In Progress", "id": "3", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/4", "id": 4, "key": "indeterminate", "colorName": "yellow", "name": "In Progress"}}}}