{"expand": "renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations", "id": "13631611", "self": "https://issues.apache.org/jira/rest/api/2/issue/13631611", "key": "SPARK-53917", "fields": {"summary": "[CONNECT] Supporting large LocalRelations", "description": "h1. Problem description\r\n\r\nLocalRelation is a Catalyst logical operator used to represent a dataset of rows inline as part of the LogicalPlan. LocalRelations represent dataframes created directly from Python and Scala objects, e.g., Python and Scala lists, pandas dataframes, csv files loaded in memory, etc.\r\n\r\nIn Spark Connect, local relations are transferred over gRPC using LocalRelation (for relations under 64MB) and CachedLocalRelation (larger relations over 64MB) messages.\r\n\r\nCachedLocalRelations currently have a hard size limit of 2GB, which means that spark users can\u2019t execute queries with local client data, pandas dataframes, csv files of over 2GB.\r\nh1. Design\r\n\r\nIn Spark Connect, the client needs to serialize the local relation before transferring it to the server. It serializes data via an Arrow IPC stream as a single record batch and schema as a json string. It then embeds data and schema as LocalRelation\\{schema,data} proto message.\r\nSmall local relations (under 64MB) are sent directly as part of the ExecutePlanRequest.\r\n\r\n!image-2025-10-15-13-50-04-179.png!\r\n\r\nLarger local relations are first sent to the server via addArtifact and stored in memory or on disk via BlockManager. Then an ExecutePlanRequest is sent containing CachedLocalRelation\\{hash}, where hash is the artifact hash. The server retrieves the cached LocalRelation from the BlockManager via the hash, deserializes it, adds it to the LogicalPlan and then executes it.\r\n\r\n!image-2025-10-15-13-50-44-333.png!\r\n\r\n\u00a0\r\n\r\nThe server reads the data from the BlockManager as a stream and tries to create proto.LocalRelation via\r\n{quote}proto.Relation\r\n.newBuilder()\r\n.getLocalRelation\r\n.getParserForType\r\n.parseFrom(blockData.toInputStream())\r\n{quote}\r\nThis fails, because java protobuf library has a 2GB limit on deserializing protobuf messages from a string.\r\n{quote}org.sparkproject.connect.com.google.protobuf.InvalidProtocolBufferException) CodedInputStream encountered an embedded string or message which claimed to have negative size.\r\n{quote}\r\n!image-2025-10-15-13-53-40-306.png!\r\n\r\nTo fix this, I propose avoiding the protobuf layer during the serialization on the client and deserialization on the server. Instead of caching the full protobuf LocalRelation message, we cache the data and schema as separate artifacts, send two hashes \\{data_hash, schema_hash} to the server, load them both from BlockManager directly and create a LocalRelation on the server based on the unpacked data and schema.\r\n\r\n!image-2025-10-15-13-56-46-840.png!\r\n\r\nAfter creating a prototype with the new proto message, I discovered that there are additional limits for CachedLocalRelations. Both the Scala Client and the Server store the data in a single Java\u00a0{{{}Array[Byte]{}}}, which has a 2GB size limit in Java. To avoid this limit, I propose transferring data in chunks. The Python and Scala clients will split data into multiple Arrow batches and upload them separately to the server. Each batch will be uploaded and stored a separate artifact. The Server will then load and process each batch separately. We will keep batch sizes around 16MB (TBD), well below the 2GB limit. This way we will avoid 2GB limits on both clients and on the server.\r\n\r\n!image-2025-10-15-13-59-08-081.png!\r\n\r\nThe final proto message looks like this:\r\n{quote}message ChunkedCachedLocalRelation {\r\n\u00a0 // (Required) A list of sha-256 hashes for representing LocalRelation.data.\r\n\u00a0 repeated string dataHashes = 1;\r\n\u00a0 // (Optional) A sha-256 hash of the serialized LocalRelation.schema.\r\n\u00a0 optional string schemaHash = 2;\r\n}\r\n{quote}\r\nImplementation details are discussed in the PR [https://github.com/apache/spark/pull/52613].", "reporter": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=khakhlyuk", "name": "khakhlyuk", "key": "JIRAUSER308064", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34058", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"}, "displayName": "Alex Khakhlyuk", "active": true, "timeZone": "Etc/UTC"}, "comment": {"comments": [{"self": "https://issues.apache.org/jira/rest/api/2/issue/13631611/comment/18032106", "id": "18032106", "author": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=hvanhovell", "name": "hvanhovell", "key": "hvanhovell", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Herman van H\u00f6vell", "active": true, "timeZone": "Europe/Amsterdam"}, "body": "Issue resolved by pull request 52613\n[https://github.com/apache/spark/pull/52613]", "updateAuthor": {"self": "https://issues.apache.org/jira/rest/api/2/user?username=hvanhovell", "name": "hvanhovell", "key": "hvanhovell", "avatarUrls": {"48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452", "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452", "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452", "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"}, "displayName": "Herman van H\u00f6vell", "active": true, "timeZone": "Europe/Amsterdam"}, "created": "2025-10-22T13:49:49.624+0000", "updated": "2025-10-22T13:49:49.624+0000"}], "maxResults": 1, "total": 1, "startAt": 0}, "priority": {"self": "https://issues.apache.org/jira/rest/api/2/priority/3", "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg", "name": "Major", "id": "3"}, "status": {"self": "https://issues.apache.org/jira/rest/api/2/status/5", "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.", "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png", "name": "Resolved", "id": "5", "statusCategory": {"self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3", "id": 3, "key": "done", "colorName": "green", "name": "Done"}}}}